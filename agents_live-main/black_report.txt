error: cannot format /home/runner/work/ymera_y/ymera_y/.github.workflows.load-test.py: Cannot parse for target version Python 3.12: 2:11: name: Load Testing
--- /home/runner/work/ymera_y/ymera_y/001_initial_schema.py	2025-10-19 22:47:02.783878+00:00
+++ /home/runner/work/ymera_y/ymera_y/001_initial_schema.py	2025-10-19 23:08:44.482314+00:00
@@ -8,23 +8,25 @@
 from sqlalchemy import text
 
 
 class Migration(BaseMigration):
     """Initial database schema migration"""
-    
+
     def __init__(self):
         super().__init__()
         self.version = 1
         self.name = "initial_schema"
         self.description = "Create initial database schema with all core tables"
         self.dependencies = []
-    
+
     async def up(self, session: AsyncSession) -> None:
         """Create initial schema"""
-        
+
         # Users table
-        await session.execute(text("""
+        await session.execute(
+            text(
+                """
             CREATE TABLE IF NOT EXISTS users (
                 id VARCHAR(36) PRIMARY KEY,
                 username VARCHAR(255) UNIQUE NOT NULL,
                 email VARCHAR(255) UNIQUE NOT NULL,
                 password_hash VARCHAR(255) NOT NULL,
@@ -50,31 +52,55 @@
                 deleted_at TIMESTAMP,
                 created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                 updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                 CONSTRAINT username_min_length CHECK (length(username) >= 3)
             )
-        """))
-        
+        """
+            )
+        )
+
         # Create indexes for users
-        await session.execute(text("""
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_users_username ON users(username)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_users_email ON users(email)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_user_email_active ON users(email, is_active)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_users_role ON users(role)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_users_is_deleted ON users(is_deleted)
-        """))
-        
+        """
+            )
+        )
+
         # Projects table
-        await session.execute(text("""
+        await session.execute(
+            text(
+                """
             CREATE TABLE IF NOT EXISTS projects (
                 id VARCHAR(36) PRIMARY KEY,
                 name VARCHAR(255) NOT NULL,
                 description TEXT,
                 owner_id VARCHAR(36) NOT NULL,
@@ -102,31 +128,55 @@
                 updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                 FOREIGN KEY (owner_id) REFERENCES users(id) ON DELETE CASCADE,
                 CONSTRAINT progress_range CHECK (progress >= 0 AND progress <= 100),
                 CONSTRAINT success_rate_range CHECK (success_rate >= 0 AND success_rate <= 100)
             )
-        """))
-        
+        """
+            )
+        )
+
         # Create indexes for projects
-        await session.execute(text("""
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_projects_name ON projects(name)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_projects_owner_id ON projects(owner_id)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_project_owner_status ON projects(owner_id, status)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_projects_status ON projects(status)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_projects_priority ON projects(priority)
-        """))
-        
+        """
+            )
+        )
+
         # Agents table
-        await session.execute(text("""
+        await session.execute(
+            text(
+                """
             CREATE TABLE IF NOT EXISTS agents (
                 id VARCHAR(36) PRIMARY KEY,
                 name VARCHAR(255) NOT NULL,
                 agent_type VARCHAR(100) NOT NULL,
                 description TEXT,
@@ -151,31 +201,55 @@
                 created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                 updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                 CONSTRAINT load_factor_range CHECK (load_factor >= 0 AND load_factor <= 100),
                 CONSTRAINT agent_success_rate_range CHECK (success_rate >= 0 AND success_rate <= 100)
             )
-        """))
-        
+        """
+            )
+        )
+
         # Create indexes for agents
-        await session.execute(text("""
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_agents_name ON agents(name)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_agents_agent_type ON agents(agent_type)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_agent_type_status ON agents(agent_type, status)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_agents_status ON agents(status)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_agents_health_status ON agents(health_status)
-        """))
-        
+        """
+            )
+        )
+
         # Tasks table
-        await session.execute(text("""
+        await session.execute(
+            text(
+                """
             CREATE TABLE IF NOT EXISTS tasks (
                 id VARCHAR(36) PRIMARY KEY,
                 title VARCHAR(500) NOT NULL,
                 description TEXT,
                 task_type VARCHAR(100) NOT NULL,
@@ -212,43 +286,83 @@
                 FOREIGN KEY (agent_id) REFERENCES agents(id) ON DELETE SET NULL,
                 FOREIGN KEY (parent_task_id) REFERENCES tasks(id) ON DELETE SET NULL,
                 CONSTRAINT task_progress_range CHECK (progress >= 0 AND progress <= 100),
                 CONSTRAINT urgency_range CHECK (urgency >= 1 AND urgency <= 10)
             )
-        """))
-        
+        """
+            )
+        )
+
         # Create indexes for tasks
-        await session.execute(text("""
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_tasks_title ON tasks(title)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_tasks_task_type ON tasks(task_type)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_tasks_user_id ON tasks(user_id)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_tasks_project_id ON tasks(project_id)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_tasks_agent_id ON tasks(agent_id)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_tasks_status ON tasks(status)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_task_status_priority ON tasks(status, priority)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_task_agent_status ON tasks(agent_id, status)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_tasks_scheduled_at ON tasks(scheduled_at)
-        """))
-        
+        """
+            )
+        )
+
         # Files table
-        await session.execute(text("""
+        await session.execute(
+            text(
+                """
             CREATE TABLE IF NOT EXISTS files (
                 id VARCHAR(36) PRIMARY KEY,
                 filename VARCHAR(500) NOT NULL,
                 original_filename VARCHAR(500) NOT NULL,
                 file_path VARCHAR(1000) NOT NULL,
@@ -274,40 +388,76 @@
                 updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                 FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE,
                 FOREIGN KEY (project_id) REFERENCES projects(id) ON DELETE CASCADE,
                 CONSTRAINT positive_file_size CHECK (file_size > 0)
             )
-        """))
-        
+        """
+            )
+        )
+
         # Create indexes for files
-        await session.execute(text("""
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_files_filename ON files(filename)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_files_user_id ON files(user_id)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_files_project_id ON files(project_id)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_file_user_project ON files(user_id, project_id)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_files_checksum_md5 ON files(checksum_md5)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_files_checksum_sha256 ON files(checksum_sha256)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_files_mime_type ON files(mime_type)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_files_access_level ON files(access_level)
-        """))
-        
+        """
+            )
+        )
+
         # Audit logs table
-        await session.execute(text("""
+        await session.execute(
+            text(
+                """
             CREATE TABLE IF NOT EXISTS audit_logs (
                 id VARCHAR(36) PRIMARY KEY,
                 user_id VARCHAR(36),
                 action VARCHAR(200) NOT NULL,
                 resource_type VARCHAR(100) NOT NULL,
@@ -323,109 +473,141 @@
                 error_message TEXT,
                 execution_time FLOAT,
                 created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                 FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE SET NULL
             )
-        """))
-        
+        """
+            )
+        )
+
         # Create indexes for audit_logs
-        await session.execute(text("""
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_audit_logs_user_id ON audit_logs(user_id)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_audit_logs_action ON audit_logs(action)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_audit_logs_resource_type ON audit_logs(resource_type)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_audit_logs_resource_id ON audit_logs(resource_id)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_audit_action_time ON audit_logs(action, created_at)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_audit_resource ON audit_logs(resource_type, resource_id)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_audit_logs_session_id ON audit_logs(session_id)
-        """))
-        await session.execute(text("""
+        """
+            )
+        )
+        await session.execute(
+            text(
+                """
             CREATE INDEX IF NOT EXISTS idx_audit_logs_success ON audit_logs(success)
-        """))
-        
+        """
+            )
+        )
+
         # Project-Agents association table
-        await session.execute(text("""
+        await session.execute(
+            text(
+                """
             CREATE TABLE IF NOT EXISTS project_agents (
                 project_id VARCHAR(36) NOT NULL,
                 agent_id VARCHAR(36) NOT NULL,
                 role VARCHAR(50) DEFAULT 'member',
                 assigned_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                 PRIMARY KEY (project_id, agent_id),
                 FOREIGN KEY (project_id) REFERENCES projects(id) ON DELETE CASCADE,
                 FOREIGN KEY (agent_id) REFERENCES agents(id) ON DELETE CASCADE
             )
-        """))
-        
+        """
+            )
+        )
+
         await session.commit()
-    
+
     async def down(self, session: AsyncSession) -> None:
         """Rollback initial schema"""
         # Drop tables in reverse order of dependencies
-        tables = [
-            'audit_logs',
-            'project_agents',
-            'files',
-            'tasks',
-            'agents',
-            'projects',
-            'users'
-        ]
-        
+        tables = ["audit_logs", "project_agents", "files", "tasks", "agents", "projects", "users"]
+
         for table in tables:
             await session.execute(text(f"DROP TABLE IF EXISTS {table}"))
-        
+
         await session.commit()
-    
+
     async def validate_preconditions(self, session: AsyncSession) -> bool:
         """Validate before migration - check no tables exist"""
         try:
             # For SQLite
-            result = await session.execute(text(
-                "SELECT name FROM sqlite_master WHERE type='table' AND name='users'"
-            ))
+            result = await session.execute(
+                text("SELECT name FROM sqlite_master WHERE type='table' AND name='users'")
+            )
             return result.fetchone() is None
         except:
             # For PostgreSQL/MySQL
             try:
-                result = await session.execute(text(
-                    "SELECT 1 FROM information_schema.tables WHERE table_name='users'"
-                ))
+                result = await session.execute(
+                    text("SELECT 1 FROM information_schema.tables WHERE table_name='users'")
+                )
                 return result.fetchone() is None
             except:
                 return True
-    
+
     async def validate_postconditions(self, session: AsyncSession) -> bool:
         """Validate after migration - check all tables exist"""
         try:
-            required_tables = ['users', 'projects', 'agents', 'tasks', 'files', 'audit_logs']
-            
+            required_tables = ["users", "projects", "agents", "tasks", "files", "audit_logs"]
+
             # For SQLite
             try:
                 for table in required_tables:
-                    result = await session.execute(text(
-                        f"SELECT name FROM sqlite_master WHERE type='table' AND name='{table}'"
-                    ))
+                    result = await session.execute(
+                        text(
+                            f"SELECT name FROM sqlite_master WHERE type='table' AND name='{table}'"
+                        )
+                    )
                     if result.fetchone() is None:
                         return False
                 return True
             except:
                 # For PostgreSQL/MySQL
                 for table in required_tables:
-                    result = await session.execute(text(
-                        f"SELECT 1 FROM information_schema.tables WHERE table_name='{table}'"
-                    ))
+                    result = await session.execute(
+                        text(f"SELECT 1 FROM information_schema.tables WHERE table_name='{table}'")
+                    )
                     if result.fetchone() is None:
                         return False
                 return True
         except:
             return False
would reformat /home/runner/work/ymera_y/ymera_y/001_initial_schema.py
--- /home/runner/work/ymera_y/ymera_y/HSMCrypto.py	2025-10-19 22:47:02.786432+00:00
+++ /home/runner/work/ymera_y/ymera_y/HSMCrypto.py	2025-10-19 23:08:44.724571+00:00
@@ -1,14 +1,14 @@
 # Enhanced encryption utilities with HSM integration
 class HSMCrypto:
     def __init__(self):
         self.hsm_enabled = os.getenv("HSM_ENABLED", "false").lower() == "true"
         self.hsm_client = None
-        
+
         if self.hsm_enabled:
             self._init_hsm_client()
-    
+
     def _init_hsm_client(self):
         """Initialize HSM client"""
         try:
             # This would be actual HSM client initialization
             # For AWS CloudHSM:
@@ -16,11 +16,11 @@
             # self.hsm_client = boto3.client('cloudhsm')
             logger.info("HSM client initialized")
         except Exception as e:
             logger.error(f"Failed to initialize HSM: {e}")
             self.hsm_enabled = False
-    
+
     async def encrypt_data(self, data: bytes, key_id: str = None) -> Tuple[bytes, str]:
         """Encrypt data with HSM or local encryption"""
         if self.hsm_enabled and key_id:
             # Use HSM for encryption
             try:
@@ -33,11 +33,11 @@
                 # Fallback to local encryption
                 return SecurityUtils.encrypt_data(data), "local"
         else:
             # Use local encryption
             return SecurityUtils.encrypt_data(data), "local"
-    
+
     async def decrypt_data(self, encrypted_data: bytes, key_id: str) -> bytes:
         """Decrypt data with HSM or local decryption"""
         if self.hsm_enabled and key_id != "local":
             try:
                 # This would be actual HSM decryption call
@@ -49,155 +49,157 @@
                 # Fallback to local decryption
                 return SecurityUtils.decrypt_data(encrypted_data)
         else:
             return SecurityUtils.decrypt_data(encrypted_data)
 
+
 # Enhanced field-level encryption
 class EncryptedField:
     def __init__(self, field_name: str, sensitive: bool = True):
         self.field_name = field_name
         self.sensitive = sensitive
-    
+
     def __get__(self, instance, owner):
         if instance is None:
             return self
-        
+
         encrypted_value = getattr(instance, f"_{self.field_name}_encrypted")
         key_id = getattr(instance, f"_{self.field_name}_key_id", "local")
-        
+
         if encrypted_value is None:
             return None
-        
+
         # Decrypt the value
         hsm_crypto = HSMCrypto()
         decrypted_value = asyncio.run(hsm_crypto.decrypt_data(encrypted_value, key_id))
         return decrypted_value.decode()
-    
+
     def __set__(self, instance, value):
         if value is None:
             setattr(instance, f"_{self.field_name}_encrypted", None)
             setattr(instance, f"_{self.field_name}_key_id", None)
             return
-        
+
         # Encrypt the value
         hsm_crypto = HSMCrypto()
         encrypted_value, key_id = asyncio.run(hsm_crypto.encrypt_data(value.encode()))
-        
+
         setattr(instance, f"_{self.field_name}_encrypted", encrypted_value)
         setattr(instance, f"_{self.field_name}_key_id", key_id)
+
 
 # Enhanced UserRecord with encrypted fields
 class UserRecord(Base):
     __tablename__ = "users"
-    
+
     # Encrypted fields
     _ssn_encrypted = Column(LargeBinary, nullable=True)
     _ssn_key_id = Column(String(100), nullable=True)
-    
+
     @property
     def ssn(self):
         encrypted_value = getattr(self, "_ssn_encrypted")
         key_id = getattr(self, "_ssn_key_id", "local")
-        
+
         if encrypted_value is None:
             return None
-        
+
         hsm_crypto = HSMCrypto()
         decrypted_value = asyncio.run(hsm_crypto.decrypt_data(encrypted_value, key_id))
         return decrypted_value.decode()
-    
+
     @ssn.setter
     def ssn(self, value):
         if value is None:
             self._ssn_encrypted = None
             self._ssn_key_id = None
             return
-        
+
         hsm_crypto = HSMCrypto()
         encrypted_value, key_id = asyncio.run(hsm_crypto.encrypt_data(value.encode()))
         self._ssn_encrypted = encrypted_value
         self._ssn_key_id = key_id
+
 
 # Data Loss Prevention (DLP) integration
 class DLPEngine:
     @staticmethod
     async def inspect_content(content: str, content_type: str = "text") -> Dict[str, Any]:
         """Inspect content for sensitive data"""
         findings = []
-        
+
         # PII detection patterns
         pii_patterns = {
             "ssn": r"\b\d{3}-\d{2}-\d{4}\b",
             "credit_card": r"\b\d{4}[- ]?\d{4}[- ]?\d{4}[- ]?\d{4}\b",
             "phone": r"\b\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}\b",
-            "email": r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b"
+            "email": r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b",
         }
-        
+
         for pii_type, pattern in pii_patterns.items():
             matches = re.findall(pattern, content)
             if matches:
-                findings.append({
-                    "type": pii_type,
-                    "count": len(matches),
-                    "matches": matches[:5]  # Limit exposed data
-                })
-        
+                findings.append(
+                    {
+                        "type": pii_type,
+                        "count": len(matches),
+                        "matches": matches[:5],  # Limit exposed data
+                    }
+                )
+
         # Data classification
         classification = "public"
         if findings:
             classification = "confidential"
-        
+
         return {
             "findings": findings,
             "classification": classification,
-            "recommended_action": "encrypt" if findings else "none"
+            "recommended_action": "encrypt" if findings else "none",
         }
-    
+
     @staticmethod
     async def anonymize_content(content: str, content_type: str = "text") -> str:
         """Anonymize sensitive content"""
         anonymized = content
-        
+
         # Anonymize PII
         anonymization_patterns = {
             "ssn": (r"\b\d{3}-\d{2}-\d{4}\b", "XXX-XX-XXXX"),
             "credit_card": (r"\b\d{4}[- ]?\d{4}[- ]?\d{4}[- ]?\d{4}\b", "XXXX-XXXX-XXXX-XXXX"),
-            "phone": (r"\b\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}\b", "(XXX) XXX-XXXX")
+            "phone": (r"\b\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}\b", "(XXX) XXX-XXXX"),
         }
-        
+
         for pii_type, (pattern, replacement) in anonymization_patterns.items():
             anonymized = re.sub(pattern, replacement, anonymized)
-        
+
         return anonymized
+
 
 # DLP middleware
 @app.middleware("http")
 async def dlp_middleware(request: Request, call_next):
     """Middleware to inspect and protect sensitive data"""
     response = await call_next(request)
-    
+
     # Check if response contains sensitive data
     if response.headers.get("content-type", "").startswith("application/json"):
         content = await response.body()
         try:
             json_content = json.loads(content.decode())
-            
+
             # Inspect JSON content for sensitive data
             dlp_result = await DLPEngine.inspect_content(json.dumps(json_content))
-            
+
             if dlp_result["findings"]:
                 # Log the finding
-                await AuditLogger.log_dlp_event(
-                    request, 
-                    dlp_result, 
-                    "sensitive_data_exposure"
-                )
-                
+                await AuditLogger.log_dlp_event(request, dlp_result, "sensitive_data_exposure")
+
                 # For highly sensitive data, we might want to redact or block
                 if dlp_result["classification"] == "confidential":
                     # In production, implement proper redaction logic
                     pass
-                    
+
         except (json.JSONDecodeError, UnicodeDecodeError):
             pass
-    
-    return response
\ No newline at end of file
+
+    return response
would reformat /home/runner/work/ymera_y/ymera_y/HSMCrypto.py
--- /home/runner/work/ymera_y/ymera_y/BaseEvent.py	2025-10-19 22:47:02.784432+00:00
+++ /home/runner/work/ymera_y/ymera_y/BaseEvent.py	2025-10-19 23:08:44.895291+00:00
@@ -6,40 +6,53 @@
     event_data: Dict[str, Any]
     timestamp: datetime = Field(default_factory=datetime.utcnow)
     version: int = 1
     metadata: Dict[str, Any] = Field(default_factory=dict)
 
+
 class ProjectEvent(BaseEvent):
-    event_type: Literal["project_created", "project_updated", "project_deleted",
-                       "task_added", "task_completed", "member_added"]
+    event_type: Literal[
+        "project_created",
+        "project_updated",
+        "project_deleted",
+        "task_added",
+        "task_completed",
+        "member_added",
+    ]
+
 
 class TaskEvent(BaseEvent):
-    event_type: Literal["task_created", "task_assigned", "task_started", 
-                       "task_completed", "task_failed"]
+    event_type: Literal[
+        "task_created", "task_assigned", "task_started", "task_completed", "task_failed"
+    ]
+
 
 class UserEvent(BaseEvent):
-    event_type: Literal["user_created", "user_updated", "user_deleted",
-                       "user_logged_in", "user_logged_out"]
+    event_type: Literal[
+        "user_created", "user_updated", "user_deleted", "user_logged_in", "user_logged_out"
+    ]
+
 
 class EventStore:
     def __init__(self):
         self.db = DatabaseUtils()
         self.kafka_producer = self._init_kafka_producer()
-    
+
     def _init_kafka_producer(self):
         """Initialize Kafka producer"""
         try:
             from kafka import KafkaProducer
+
             return KafkaProducer(
-                bootstrap_servers=os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'localhost:9092').split(','),
-                value_serializer=lambda v: json.dumps(v).encode('utf-8'),
-                key_serializer=lambda v: v.encode('utf-8') if v else None
+                bootstrap_servers=os.getenv("KAFKA_BOOTSTRAP_SERVERS", "localhost:9092").split(","),
+                value_serializer=lambda v: json.dumps(v).encode("utf-8"),
+                key_serializer=lambda v: v.encode("utf-8") if v else None,
             )
         except ImportError:
             logger.warning("Kafka not available")
             return None
-    
+
     async def append_event(self, event: BaseEvent) -> None:
         """Store event in database and publish to message queue"""
         # Store in database
         async with self.db.get_session() as session:
             event_record = EventRecord(
@@ -47,314 +60,313 @@
                 aggregate_id=str(event.aggregate_id),
                 event_type=event.event_type,
                 event_data=event.event_data,
                 timestamp=event.timestamp,
                 version=event.version,
-                metadata=event.metadata
+                metadata=event.metadata,
             )
             session.add(event_record)
             await session.commit()
-        
+
         # Publish to Kafka
         if self.kafka_producer:
             try:
                 future = self.kafka_producer.send(
                     topic=f"events-{event.event_type}",
                     value=event.dict(),
-                    key=str(event.aggregate_id)
+                    key=str(event.aggregate_id),
                 )
                 future.get(timeout=10)  # Wait for confirmation
             except Exception as e:
                 logger.error(f"Failed to publish event to Kafka: {e}")
                 # Implement dead letter queue or retry logic
-    
-    async def get_events(self, aggregate_id: UUID, 
-                       event_type: str = None, 
-                       start_version: int = 0) -> List[BaseEvent]:
+
+    async def get_events(
+        self, aggregate_id: UUID, event_type: str = None, start_version: int = 0
+    ) -> List[BaseEvent]:
         """Retrieve events for an aggregate"""
         async with self.db.get_session() as session:
             query = select(EventRecord).where(
-                EventRecord.aggregate_id == str(aggregate_id),
-                EventRecord.version >= start_version
-            )
-            
+                EventRecord.aggregate_id == str(aggregate_id), EventRecord.version >= start_version
+            )
+
             if event_type:
                 query = query.where(EventRecord.event_type == event_type)
-            
-            result = await session.execute(
-                query.order_by(EventRecord.version.asc())
-            )
+
+            result = await session.execute(query.order_by(EventRecord.version.asc()))
             events = result.scalars().all()
-            
+
             return [BaseEvent(**event.to_dict()) for event in events]
-    
+
     async def create_snapshot(self, aggregate_id: UUID, snapshot_data: Dict[str, Any]) -> None:
         """Create snapshot for aggregate"""
         snapshot_id = str(uuid.uuid4())
-        
+
         async with self.db.get_session() as session:
             snapshot = SnapshotRecord(
                 id=snapshot_id,
                 aggregate_id=str(aggregate_id),
                 snapshot_data=snapshot_data,
-                created_at=datetime.utcnow()
+                created_at=datetime.utcnow(),
             )
             session.add(snapshot)
             await session.commit()
+
 
 # Message Queue System with Kafka
 class MessageQueueSystem:
     def __init__(self):
         self.consumers = {}
         self.init_kafka_consumers()
-    
+
     def init_kafka_consumers(self):
         """Initialize Kafka consumers for different topics"""
         topics = [
-            "events-project", "events-task", "events-user",
-            "notifications", "audit-logs", "analytics"
+            "events-project",
+            "events-task",
+            "events-user",
+            "notifications",
+            "audit-logs",
+            "analytics",
         ]
-        
+
         for topic in topics:
-            self.consumers[topic] = asyncio.create_task(
-                self._kafka_consumer(topic)
-            )
-    
+            self.consumers[topic] = asyncio.create_task(self._kafka_consumer(topic))
+
     async def _kafka_consumer(self, topic: str):
         """Kafka consumer for a specific topic"""
         try:
             from kafka import KafkaConsumer
+
             consumer = KafkaConsumer(
                 topic,
-                bootstrap_servers=os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'localhost:9092').split(','),
+                bootstrap_servers=os.getenv("KAFKA_BOOTSTRAP_SERVERS", "localhost:9092").split(","),
                 group_id=f"ymera-{topic}-consumer",
-                value_deserializer=lambda v: json.loads(v.decode('utf-8')),
+                value_deserializer=lambda v: json.loads(v.decode("utf-8")),
                 enable_auto_commit=False,
-                auto_offset_reset='earliest'
-            )
-            
+                auto_offset_reset="earliest",
+            )
+
             for message in consumer:
                 try:
                     await self.process_message(topic, message.value)
                     consumer.commit()
                 except Exception as e:
                     logger.error(f"Failed to process message from {topic}: {e}")
                     # Send to dead letter queue
                     await self.send_to_dlq(topic, message.value, str(e))
-                    
+
         except ImportError:
             logger.warning("Kafka not available, using in-memory queue")
             # Fallback to in-memory processing
             while True:
                 await asyncio.sleep(1)  # Simulate message processing
-    
+
     async def process_message(self, topic: str, message: Dict[str, Any]):
         """Process message based on topic"""
         if topic.startswith("events-"):
             await self.process_event_message(topic, message)
         elif topic == "notifications":
             await self.process_notification_message(message)
         elif topic == "audit-logs":
             await self.process_audit_message(message)
         elif topic == "analytics":
             await self.process_analytics_message(message)
-    
+
     async def process_event_message(self, topic: str, event: Dict[str, Any]):
         """Process event message"""
         event_type = topic.replace("events-", "")
-        
+
         # Update read models, send notifications, trigger workflows
         if event_type == "project_created":
             await self._handle_project_created(event)
         elif event_type == "task_completed":
             await self._handle_task_completed(event)
         # Add other event handlers...
-    
+
     async def _handle_project_created(self, event: Dict[str, Any]):
         """Handle project created event"""
         # Update analytics, send notifications, etc.
         project_id = event["aggregate_id"]
         project_data = event["event_data"]
-        
+
         # Update read model
-        await AnalyticsService().index_document("projects", {
-            "id": project_id,
-            "name": project_data["name"],
-            "owner": project_data["owner_id"],
-            "created_at": event["timestamp"]
-        })
-        
+        await AnalyticsService().index_document(
+            "projects",
+            {
+                "id": project_id,
+                "name": project_data["name"],
+                "owner": project_data["owner_id"],
+                "created_at": event["timestamp"],
+            },
+        )
+
         # Send notification to team
         await NotificationService().send_notification(
             project_data["owner_id"],
             f"Project '{project_data['name']}' created successfully",
-            channels=["email", "slack"]
+            channels=["email", "slack"],
         )
-    
+
     async def _handle_task_completed(self, event: Dict[str, Any]):
         """Handle task completed event"""
         task_id = event["aggregate_id"]
         task_data = event["event_data"]
-        
+
         # Update analytics
-        await AnalyticsService().index_document("tasks", {
-            "id": task_id,
-            "project_id": task_data["project_id"],
-            "status": "completed",
-            "completed_at": event["timestamp"]
-        })
-        
+        await AnalyticsService().index_document(
+            "tasks",
+            {
+                "id": task_id,
+                "project_id": task_data["project_id"],
+                "status": "completed",
+                "completed_at": event["timestamp"],
+            },
+        )
+
         # Check if all tasks in project are completed
         # This would trigger project completion workflow
-    
+
     async def send_to_dlq(self, topic: str, message: Dict[str, Any], error: str):
         """Send failed message to dead letter queue"""
         dlq_message = {
             "original_topic": topic,
             "original_message": message,
             "error": error,
             "timestamp": datetime.utcnow().isoformat(),
-            "retry_count": 0
+            "retry_count": 0,
         }
-        
+
         # Store in database or publish to DLQ topic
         async with DatabaseUtils.get_session() as session:
-            dlq_record = DLQRecord(
-                topic=topic,
-                message=message,
-                error=error,
-                retry_count=0
-            )
+            dlq_record = DLQRecord(topic=topic, message=message, error=error, retry_count=0)
             session.add(dlq_record)
             await session.commit()
-    
+
     async def retry_dlq_messages(self):
         """Retry messages from dead letter queue"""
         while True:
             try:
                 async with DatabaseUtils.get_session() as session:
                     # Get messages that haven't been retried too many times
                     result = await session.execute(
-                        select(DLQRecord).where(
-                            DLQRecord.retry_count < 3,
-                            DLQRecord.next_retry_at <= datetime.utcnow()
-                        ).order_by(DLQRecord.created_at.asc()).limit(10)
+                        select(DLQRecord)
+                        .where(
+                            DLQRecord.retry_count < 3, DLQRecord.next_retry_at <= datetime.utcnow()
+                        )
+                        .order_by(DLQRecord.created_at.asc())
+                        .limit(10)
                     )
                     messages = result.scalars().all()
-                    
+
                     for message in messages:
                         try:
                             await self.process_message(message.topic, message.message)
                             await session.delete(message)  # Remove from DLQ
                         except Exception as e:
                             message.retry_count += 1
                             message.next_retry_at = datetime.utcnow() + timedelta(
-                                minutes=2 ** message.retry_count  # Exponential backoff
+                                minutes=2**message.retry_count  # Exponential backoff
                             )
                             message.last_error = str(e)
-                    
+
                     await session.commit()
-                    
+
             except Exception as e:
                 logger.error(f"Failed to retry DLQ messages: {e}")
-            
+
             await asyncio.sleep(60)  # Check every minute
+
 
 # Saga Pattern Implementation
 class SagaManager:
     def __init__(self):
         self.sagas = {}
         self.compensation_actions = {}
-    
+
     async def execute_saga(self, saga_name: str, saga_data: Dict[str, Any]) -> Dict[str, Any]:
         """Execute a distributed transaction using saga pattern"""
         saga_id = str(uuid.uuid4())
         self.sagas[saga_id] = {
             "name": saga_name,
             "status": "started",
             "current_step": 0,
             "data": saga_data,
-            "created_at": datetime.utcnow()
+            "created_at": datetime.utcnow(),
         }
-        
+
         steps = self.get_saga_steps(saga_name)
         compensation_steps = []
-        
+
         try:
             for i, step in enumerate(steps):
                 self.sagas[saga_id]["current_step"] = i
-                
+
                 # Execute step
                 result = await self.execute_saga_step(step, saga_data)
-                
+
                 # Store compensation action
                 if "compensation" in step:
-                    compensation_steps.append({
-                        "action": step["compensation"],
-                        "data": result
-                    })
-                
+                    compensation_steps.append({"action": step["compensation"], "data": result})
+
                 # Update saga data with step result
                 saga_data.update(result)
-            
+
             # Saga completed successfully
             self.sagas[saga_id]["status"] = "completed"
             self.sagas[saga_id]["completed_at"] = datetime.utcnow()
-            
+
             return {"status": "success", "saga_id": saga_id, "data": saga_data}
-            
+
         except Exception as e:
             # Saga failed, execute compensation actions
             logger.error(f"Saga {saga_id} failed at step {i}: {e}")
             self.sagas[saga_id]["status"] = "failed"
             self.sagas[saga_id]["error"] = str(e)
-            
+
             # Execute compensation in reverse order
             for compensation in reversed(compensation_steps):
                 try:
                     await self.execute_compensation(compensation["action"], compensation["data"])
                 except Exception as comp_error:
                     logger.error(f"Compensation failed: {comp_error}")
                     # Log compensation failure but continue with others
-            
+
             return {"status": "failed", "saga_id": saga_id, "error": str(e)}
-    
+
     def get_saga_steps(self, saga_name: str) -> List[Dict[str, Any]]:
         """Get steps for a saga"""
         sagas = {
             "create_project": [
                 {
                     "service": "project-management",
                     "action": "create_project",
-                    "compensation": "delete_project"
+                    "compensation": "delete_project",
                 },
                 {
                     "service": "task-orchestration",
                     "action": "setup_default_tasks",
-                    "compensation": "delete_tasks"
+                    "compensation": "delete_tasks",
                 },
-                {
-                    "service": "notification",
-                    "action": "send_project_created_notification"
-                }
+                {"service": "notification", "action": "send_project_created_notification"},
             ],
             # Add other sagas...
         }
         return sagas.get(saga_name, [])
-    
+
     async def execute_saga_step(self, step: Dict[str, Any], data: Dict[str, Any]) -> Dict[str, Any]:
         """Execute a single saga step"""
         service = MicroService(step["service"])
         action = getattr(service, step["action"])
-        
+
         result = await action(data)
         return result
-    
+
     async def execute_compensation(self, action: str, data: Dict[str, Any]) -> None:
         """Execute compensation action"""
         # Parse service and action from compensation string
         # Format: "service:action"
         service_name, action_name = action.split(":")
         service = MicroService(service_name)
         compensation_action = getattr(service, action_name)
-        
-        await compensation_action(data)
\ No newline at end of file
+
+        await compensation_action(data)
would reformat /home/runner/work/ymera_y/ymera_y/BaseEvent.py
--- /home/runner/work/ymera_y/ymera_y/PerformanceMonitor.py	2025-10-19 22:47:02.787432+00:00
+++ /home/runner/work/ymera_y/ymera_y/PerformanceMonitor.py	2025-10-19 23:08:45.004105+00:00
@@ -4,115 +4,117 @@
     async def track_response_time(request: Request, call_next):
         """Track API response time"""
         start_time = time.time()
         response = await call_next(request)
         process_time = time.time() - start_time
-        
+
         # Record metrics
         await MetricsManager.record_metric(
             "api_response_time",
             process_time,
             tags={
                 "path": request.url.path,
                 "method": request.method,
-                "status_code": response.status_code
-            }
+                "status_code": response.status_code,
+            },
         )
-        
+
         # Add server timing header
         response.headers["Server-Timing"] = f"total;dur={process_time*1000:.2f}"
-        
+
         return response
+
 
 # Security metrics collection
 class SecurityMetrics:
     @staticmethod
     async def record_security_event(event_type: str, severity: str, details: Dict[str, Any]):
         """Record security metrics"""
         await MetricsManager.record_metric(
-            "security_events",
-            1,
-            tags={
-                "type": event_type,
-                "severity": severity
-            }
+            "security_events", 1, tags={"type": event_type, "severity": severity}
         )
-        
+
         # Send to SIEM
         siem = SIEMIntegration()
-        await siem.send_event({
-            "event_type": event_type,
-            "severity": severity,
-            "details": details
-        })
+        await siem.send_event({"event_type": event_type, "severity": severity, "details": details})
+
 
 # Business metrics
 class BusinessMetrics:
     @staticmethod
     async def record_user_activity(user_id: str, action: str, resource_type: str):
         """Record user activity for business metrics"""
         await MetricsManager.record_metric(
-            "user_activity",
-            1,
-            tags={
-                "action": action,
-                "resource_type": resource_type
-            }
+            "user_activity", 1, tags={"action": action, "resource_type": resource_type}
         )
-    
+
     @staticmethod
     async def calculate_user_satisfaction():
         """Calculate user satisfaction score"""
         # This would integrate with actual user feedback systems
         pass
+
 
 # Health check with detailed metrics
 @app.get("/api/health/detailed")
 async def detailed_health_check():
     """Detailed health check with metrics"""
     health_data = {
         "status": "healthy",
         "timestamp": datetime.utcnow().isoformat(),
         "metrics": {
-            "api_response_time_95p": await MetricsManager.get_metric_percentile("api_response_time", 0.95),
+            "api_response_time_95p": await MetricsManager.get_metric_percentile(
+                "api_response_time", 0.95
+            ),
             "error_rate": await MetricsManager.get_metric_rate("api_errors", "api_requests"),
             "database_latency": await DatabaseUtils.get_performance_metrics(),
             "cache_hit_rate": await CacheManager.get_hit_rate(),
             "active_users": await UserManager.get_active_user_count(),
-            "security_events": await SecurityMetrics.get_event_counts()
+            "security_events": await SecurityMetrics.get_event_counts(),
         },
         "system": {
             "memory_usage": psutil.virtual_memory().percent,
             "cpu_usage": psutil.cpu_percent(),
-            "disk_usage": psutil.disk_usage('/').percent
-        }
+            "disk_usage": psutil.disk_usage("/").percent,
+        },
     }
-    
+
     return health_data
+
 
 # Add metrics middleware
 @app.middleware("http")
 async def metrics_middleware(request: Request, call_next):
     """Middleware for collecting metrics"""
     # Record request
-    await MetricsManager.record_metric("api_requests", 1, tags={"path": request.url.path, "method": request.method})
-    
+    await MetricsManager.record_metric(
+        "api_requests", 1, tags={"path": request.url.path, "method": request.method}
+    )
+
     try:
         response = await call_next(request)
-        
+
         # Record response status
         await MetricsManager.record_metric(
-            "api_responses", 
-            1, 
-            tags={"path": request.url.path, "method": request.method, "status_code": response.status_code}
+            "api_responses",
+            1,
+            tags={
+                "path": request.url.path,
+                "method": request.method,
+                "status_code": response.status_code,
+            },
         )
-        
+
         return response
-        
+
     except Exception as e:
         # Record error
         await MetricsManager.record_metric(
-            "api_errors", 
-            1, 
-            tags={"path": request.url.path, "method": request.method, "error_type": type(e).__name__}
+            "api_errors",
+            1,
+            tags={
+                "path": request.url.path,
+                "method": request.method,
+                "error_type": type(e).__name__,
+            },
         )
-        raise e
\ No newline at end of file
+        raise e
would reformat /home/runner/work/ymera_y/ymera_y/PerformanceMonitor.py
--- /home/runner/work/ymera_y/ymera_y/ProductionConfig (2).py	2025-10-19 22:47:02.787432+00:00
+++ /home/runner/work/ymera_y/ymera_y/ProductionConfig (2).py	2025-10-19 23:08:45.280040+00:00
@@ -5,157 +5,154 @@
     JWT_ALGORITHM = "RS256"
     JWT_PUBLIC_KEY = os.getenv("JWT_PUBLIC_KEY")
     JWT_PRIVATE_KEY = os.getenv("JWT_PRIVATE_KEY")
     SESSION_TIMEOUT = 28800  # 8 hours
     MFA_REQUIRED = True
-    
+
     # Performance
     DATABASE_POOL_SIZE = 50
     DATABASE_MAX_OVERFLOW = 100
     REDIS_CONNECTION_POOL = 50
     CACHE_TTL = 300
-    
+
     # Microservices
     SERVICE_DISCOVERY_ENABLED = True
     SERVICE_HEALTH_CHECK_INTERVAL = 30
     SERVICE_TIMEOUT = 30
-    
+
     # Event Sourcing
     EVENT_STORE_ENABLED = True
     KAFKA_ENABLED = os.getenv("KAFKA_ENABLED", "false").lower() == "true"
     SNAPSHOT_INTERVAL = 100  # Create snapshot every 100 events
-    
+
     # Caching
     CACHE_STRATEGY = "multi_level"  # multi_level, redis_only, memory_only
     CACHE_L1_TTL = 60  # 1 minute
     CACHE_L2_TTL = 300  # 5 minutes
-    
+
     # Database
     READ_REPLICA_ENABLED = os.getenv("READ_REPLICA_ENABLED", "false").lower() == "true"
     SHARDING_ENABLED = os.getenv("SHARDING_ENABLED", "false").lower() == "true"
     MATERIALIZED_VIEWS_ENABLED = True
-    
+
     # Monitoring
     METRICS_RETENTION_DAYS = 90
     LOG_RETENTION_DAYS = 365
     TRACE_SAMPLING_RATE = 0.1
-    
+
     # Rate Limiting
     RATE_LIMIT_ENABLED = True
     RATE_LIMIT_REQUESTS = 1000  # per hour
     RATE_LIMIT_BURST = 100  # burst capacity
-    
+
     @classmethod
     def validate(cls):
         """Validate production configuration"""
-        required_vars = [
-            "JWT_PUBLIC_KEY", "JWT_PRIVATE_KEY",
-            "DATABASE_URL", "REDIS_URL"
-        ]
-        
+        required_vars = ["JWT_PUBLIC_KEY", "JWT_PRIVATE_KEY", "DATABASE_URL", "REDIS_URL"]
+
         missing = [var for var in required_vars if not os.getenv(var)]
         if missing:
             raise ValueError(f"Missing required environment variables: {missing}")
-        
+
         # Validate JWT keys
         try:
             from cryptography.hazmat.primitives import serialization
-            serialization.load_pem_private_key(
-                cls.JWT_PRIVATE_KEY.encode(),
-                password=None
-            )
-            serialization.load_pem_public_key(
-                cls.JWT_PUBLIC_KEY.encode()
-            )
+
+            serialization.load_pem_private_key(cls.JWT_PRIVATE_KEY.encode(), password=None)
+            serialization.load_pem_public_key(cls.JWT_PUBLIC_KEY.encode())
         except Exception as e:
             raise ValueError(f"Invalid JWT keys: {e}")
+
 
 # Configuration initialization
 def init_config():
     """Initialize configuration based on environment"""
     env = os.getenv("ENVIRONMENT", "development")
-    
+
     if env == "production":
         ProductionConfig.validate()
         return ProductionConfig()
     elif env == "staging":
         return StagingConfig()
     else:
         return DevelopmentConfig()
+
 
 # App initialization with microservices
 def create_app():
     """Create FastAPI app with microservices architecture"""
     app = FastAPI(
         title="YMERA Enterprise Microservices API",
         description="Production-ready microservices architecture",
         version="2.0.0",
         docs_url="/api/docs",
         redoc_url=None,
-        openapi_url="/api/openapi.json"
+        openapi_url="/api/openapi.json",
     )
-    
+
     # Initialize services
     services = [
         AuthenticationService(),
         ProjectManagementService(),
         TaskOrchestrationService(),
         FileManagementService(),
         NotificationService(),
         AuditService(),
         AnalyticsService(),
-        IntegrationService()
+        IntegrationService(),
     ]
-    
+
     # Register services
     for service in services:
         app.state.services[service.service_name] = service
-    
+
     # Initialize event store and message queue
     app.state.event_store = EventStore()
     app.state.message_queue = MessageQueueSystem()
     app.state.saga_manager = SagaManager()
-    
+
     # Initialize caching and database
     app.state.cache = MultiLevelCache()
     app.state.db = OptimizedDatabaseUtils()
     app.state.connection_pool = ConnectionPoolManager()
-    
+
     # Add routes
     app.include_router(microservice_router, prefix="/api", tags=["Microservices"])
     app.include_router(event_router, prefix="/api/events", tags=["Events"])
     app.include_router(monitoring_router, prefix="/api/monitoring", tags=["Monitoring"])
-    
+
     # Add startup/shutdown events
     @app.on_event("startup")
     async def startup():
         await init_services(app.state.services)
         await app.state.message_queue.retry_dlq_messages()
-    
+
     @app.on_event("shutdown")
     async def shutdown():
         await shutdown_services(app.state.services)
         await app.state.connection_pool.close()
-    
+
     return app
+
 
 async def init_services(services: Dict[str, MicroService]):
     """Initialize all microservices"""
     for service_name, service in services.items():
         try:
             await service.service_registry.register_service(
                 service_name,
                 f"http://{service_name}:8000",  # In Kubernetes, this would be service DNS
-                tags=["ymera", "microservice"]
+                tags=["ymera", "microservice"],
             )
             logger.info(f"Service {service_name} registered successfully")
         except Exception as e:
             logger.error(f"Failed to register service {service_name}: {e}")
+
 
 async def shutdown_services(services: Dict[str, MicroService]):
     """Shutdown all microservices"""
     for service_name, service in services.items():
         try:
             await service.http_client.aclose()
             logger.info(f"Service {service_name} shutdown successfully")
         except Exception as e:
-            logger.error(f"Failed to shutdown service {service_name}: {e}")
\ No newline at end of file
+            logger.error(f"Failed to shutdown service {service_name}: {e}")
would reformat /home/runner/work/ymera_y/ymera_y/ProductionConfig (2).py
--- /home/runner/work/ymera_y/ymera_y/MultiLevelCache.py	2025-10-19 22:47:02.786432+00:00
+++ /home/runner/work/ymera_y/ymera_y/MultiLevelCache.py	2025-10-19 23:08:45.340873+00:00
@@ -3,228 +3,231 @@
     def __init__(self):
         self.l1_cache = {}  # In-memory cache (thread-safe)
         self.l1_lock = asyncio.Lock()
         self.redis_client = None
         self.init_redis()
-    
+
     def init_redis(self):
         """Initialize Redis client"""
         try:
             import redis
+
             self.redis_client = redis.Redis.from_url(
-                os.getenv('REDIS_URL', 'redis://localhost:6379'),
-                decode_responses=True
+                os.getenv("REDIS_URL", "redis://localhost:6379"), decode_responses=True
             )
             self.redis_client.ping()  # Test connection
         except (ImportError, redis.ConnectionError):
             logger.warning("Redis not available, using in-memory cache only")
             self.redis_client = None
-    
+
     async def get(self, key: str, default: Any = None) -> Any:
         """Get value from multi-level cache"""
         # L1: In-memory cache
         async with self.l1_lock:
             if key in self.l1_cache:
                 item = self.l1_cache[key]
                 if item["expires_at"] > datetime.utcnow():
-                    MetricsCollector.record_cache_hit('l1')
+                    MetricsCollector.record_cache_hit("l1")
                     return item["value"]
                 else:
                     # Remove expired item
                     del self.l1_cache[key]
-        
+
         # L2: Redis cache
         if self.redis_client:
             try:
                 value = self.redis_client.get(key)
                 if value is not None:
                     # Deserialize if needed
                     try:
                         value = json.loads(value)
                     except json.JSONDecodeError:
                         pass  # Keep as string
-                    
+
                     # Store in L1 cache
                     async with self.l1_lock:
                         self.l1_cache[key] = {
                             "value": value,
-                            "expires_at": datetime.utcnow() + timedelta(seconds=60)  # Short TTL for L1
+                            "expires_at": datetime.utcnow()
+                            + timedelta(seconds=60),  # Short TTL for L1
                         }
-                    
-                    MetricsCollector.record_cache_hit('l2')
+
+                    MetricsCollector.record_cache_hit("l2")
                     return value
             except Exception as e:
                 logger.error(f"Redis cache error: {e}")
-        
+
         # L3: Database (caller should handle this)
-        MetricsCollector.record_cache_miss('all')
+        MetricsCollector.record_cache_miss("all")
         return default
-    
+
     async def set(self, key: str, value: Any, ttl: int = 300) -> None:
         """Set value in multi-level cache"""
         # Serialize if needed
         if not isinstance(value, (str, bytes)):
             try:
                 value = json.dumps(value)
             except (TypeError, ValueError):
                 pass  # Keep as is if not serializable
-        
+
         # L1: In-memory cache
         async with self.l1_lock:
             self.l1_cache[key] = {
                 "value": value,
-                "expires_at": datetime.utcnow() + timedelta(seconds=min(60, ttl))  # Cap L1 TTL
+                "expires_at": datetime.utcnow() + timedelta(seconds=min(60, ttl)),  # Cap L1 TTL
             }
-        
+
         # L2: Redis cache
         if self.redis_client:
             try:
                 self.redis_client.setex(key, ttl, value)
             except Exception as e:
                 logger.error(f"Failed to set Redis cache: {e}")
-    
+
     async def delete(self, key: str) -> None:
         """Delete key from all cache levels"""
         # L1: In-memory cache
         async with self.l1_lock:
             if key in self.l1_cache:
                 del self.l1_cache[key]
-        
+
         # L2: Redis cache
         if self.redis_client:
             try:
                 self.redis_client.delete(key)
             except Exception as e:
                 logger.error(f"Failed to delete from Redis: {e}")
-    
+
     async def clear(self) -> None:
         """Clear all cache levels"""
         # L1: In-memory cache
         async with self.l1_lock:
             self.l1_cache.clear()
-        
+
         # L2: Redis cache
         if self.redis_client:
             try:
                 self.redis_client.flushdb()
             except Exception as e:
                 logger.error(f"Failed to clear Redis: {e}")
+
 
 # Database Optimization with Read Replicas and Sharding
 class OptimizedDatabaseUtils:
     def __init__(self):
         self.primary_engine = None
         self.replica_engines = []
         self.session_factories = {}
         self.init_engines()
-    
+
     def init_engines(self):
         """Initialize database engines with read replicas"""
         # Primary database (writes)
         self.primary_engine = create_async_engine(
-            os.getenv('DATABASE_PRIMARY_URL', os.getenv('DATABASE_URL')),
+            os.getenv("DATABASE_PRIMARY_URL", os.getenv("DATABASE_URL")),
             pool_size=20,
             max_overflow=10,
             pool_timeout=30,
             pool_recycle=1800,
-            pool_pre_ping=True
+            pool_pre_ping=True,
         )
-        
+
         # Read replicas
-        replica_urls = os.getenv('DATABASE_REPLICA_URLS', '').split(',')
+        replica_urls = os.getenv("DATABASE_REPLICA_URLS", "").split(",")
         for i, replica_url in enumerate(replica_urls):
             if replica_url.strip():
                 engine = create_async_engine(
                     replica_url.strip(),
                     pool_size=15,
                     max_overflow=5,
                     pool_timeout=30,
                     pool_recycle=1800,
-                    pool_pre_ping=True
+                    pool_pre_ping=True,
                 )
                 self.replica_engines.append(engine)
-        
+
         # Session factories
-        self.session_factories['primary'] = async_sessionmaker(
+        self.session_factories["primary"] = async_sessionmaker(
             self.primary_engine, expire_on_commit=False
         )
-        
+
         for i, engine in enumerate(self.replica_engines):
-            self.session_factories[f'replica_{i}'] = async_sessionmaker(
+            self.session_factories[f"replica_{i}"] = async_sessionmaker(
                 engine, expire_on_commit=False
             )
-    
+
     def get_session_factory(self, for_write: bool = False) -> async_sessionmaker:
         """Get appropriate session factory"""
         if for_write or not self.replica_engines:
-            return self.session_factories['primary']
+            return self.session_factories["primary"]
         else:
             # Round-robin load balancing for reads
             replica_id = hash(str(asyncio.current_task())) % len(self.replica_engines)
-            return self.session_factories[f'replica_{replica_id}']
-    
+            return self.session_factories[f"replica_{replica_id}"]
+
     @asynccontextmanager
     async def get_session(self, for_write: bool = False):
         """Get database session with read/write routing"""
         session_factory = self.get_session_factory(for_write)
         session = session_factory()
-        
+
         try:
             yield session
             await session.commit()
         except Exception:
             await session.rollback()
             raise
         finally:
             await session.close()
-    
+
     # Sharding implementation
     def get_shard_for_entity(self, entity_id: UUID, total_shards: int = 4) -> int:
         """Determine which shard an entity belongs to"""
         return hash(str(entity_id)) % total_shards
-    
+
     async def get_sharded_session(self, entity_id: UUID, for_write: bool = False):
         """Get session for specific shard"""
         shard_id = self.get_shard_for_entity(entity_id)
-        shard_url = os.getenv(f'DATABASE_SHARD_{shard_id}_URL')
-        
+        shard_url = os.getenv(f"DATABASE_SHARD_{shard_id}_URL")
+
         if not shard_url:
             # Fallback to primary if shard not configured
             return self.get_session(for_write)
-        
+
         # Create engine for shard if not exists
-        if f'shard_{shard_id}' not in self.session_factories:
+        if f"shard_{shard_id}" not in self.session_factories:
             engine = create_async_engine(
                 shard_url,
                 pool_size=10,
                 max_overflow=5,
                 pool_timeout=30,
                 pool_recycle=1800,
-                pool_pre_ping=True
-            )
-            self.session_factories[f'shard_{shard_id}'] = async_sessionmaker(
+                pool_pre_ping=True,
+            )
+            self.session_factories[f"shard_{shard_id}"] = async_sessionmaker(
                 engine, expire_on_commit=False
             )
-        
-        session = self.session_factories[f'shard_{shard_id}']()
+
+        session = self.session_factories[f"shard_{shard_id}"]()
         return session
+
 
 # Materialized Views for Analytics
 class MaterializedViewManager:
     def __init__(self):
         self.db = DatabaseUtils()
         self.refresh_intervals = {
-            'project_stats_hourly': 3600,
-            'user_activity_daily': 86400,
-            'system_metrics_5min': 300
+            "project_stats_hourly": 3600,
+            "user_activity_daily": 86400,
+            "system_metrics_5min": 300,
         }
         self.refresh_tasks = {}
-    
+
     async def create_materialized_views(self):
         """Create materialized views for analytics"""
         views = {
-            'project_stats_hourly': """
+            "project_stats_hourly": """
                 CREATE MATERIALIZED VIEW IF NOT EXISTS project_stats_hourly AS
                 SELECT 
                     project_id,
                     COUNT(*) as total_tasks,
                     COUNT(*) FILTER (WHERE status = 'completed') as completed_tasks,
@@ -232,151 +235,152 @@
                     date_trunc('hour', created_at) as time_bucket
                 FROM tasks
                 WHERE created_at >= NOW() - INTERVAL '24 hours'
                 GROUP BY project_id, time_bucket
             """,
-            'user_activity_daily': """
+            "user_activity_daily": """
                 CREATE MATERIALIZED VIEW IF NOT EXISTS user_activity_daily AS
                 SELECT 
                     user_id,
                     COUNT(*) as total_actions,
                     COUNT(DISTINCT action) as unique_actions,
                     date_trunc('day', timestamp) as date
                 FROM audit_logs
                 WHERE timestamp >= NOW() - INTERVAL '7 days'
                 GROUP BY user_id, date
-            """
+            """,
         }
-        
+
         async with self.db.get_session() as session:
             for view_name, view_sql in views.items():
                 await session.execute(text(view_sql))
             await session.commit()
-    
+
     async def refresh_materialized_views(self):
         """Refresh materialized views on schedule"""
         for view_name, interval in self.refresh_intervals.items():
             self.refresh_tasks[view_name] = asyncio.create_task(
                 self._refresh_view_periodically(view_name, interval)
             )
-    
+
     async def _refresh_view_periodically(self, view_name: str, interval: int):
         """Refresh a materialized view periodically"""
         while True:
             try:
                 async with self.db.get_session() as session:
                     await session.execute(text(f"REFRESH MATERIALIZED VIEW {view_name}"))
                     await session.commit()
                     logger.info(f"Refreshed materialized view: {view_name}")
             except Exception as e:
                 logger.error(f"Failed to refresh view {view_name}: {e}")
-            
+
             await asyncio.sleep(interval)
-    
-    async def get_view_data(self, view_name: str, filters: Dict[str, Any] = None) -> List[Dict[str, Any]]:
+
+    async def get_view_data(
+        self, view_name: str, filters: Dict[str, Any] = None
+    ) -> List[Dict[str, Any]]:
         """Get data from materialized view"""
         async with self.db.get_session() as session:
             query = f"SELECT * FROM {view_name}"
             params = {}
-            
+
             if filters:
                 where_clauses = []
                 for key, value in filters.items():
                     where_clauses.append(f"{key} = :{key}")
                     params[key] = value
-                
+
                 if where_clauses:
                     query += " WHERE " + " AND ".join(where_clauses)
-            
+
             result = await session.execute(text(query), params)
             rows = result.fetchall()
-            
+
             return [dict(row) for row in rows]
+
 
 # Connection Pooling with Failover
 class ConnectionPoolManager:
     def __init__(self):
         self.primary_pool = None
         self.replica_pools = []
         self.health_check_task = None
         self.init_pools()
-    
+
     def init_pools(self):
         """Initialize connection pools"""
         # Primary pool
         self.primary_pool = self._create_pool(
-            os.getenv('DATABASE_PRIMARY_URL', os.getenv('DATABASE_URL'))
+            os.getenv("DATABASE_PRIMARY_URL", os.getenv("DATABASE_URL"))
         )
-        
+
         # Replica pools
-        replica_urls = os.getenv('DATABASE_REPLICA_URLS', '').split(',')
+        replica_urls = os.getenv("DATABASE_REPLICA_URLS", "").split(",")
         for replica_url in replica_urls:
             if replica_url.strip():
                 pool = self._create_pool(replica_url.strip())
-                self.replica_pools.append({
-                    'pool': pool,
-                    'healthy': True,
-                    'last_check': datetime.utcnow()
-                })
-        
+                self.replica_pools.append(
+                    {"pool": pool, "healthy": True, "last_check": datetime.utcnow()}
+                )
+
         # Start health checks
         self.health_check_task = asyncio.create_task(self._health_check_loop())
-    
+
     def _create_pool(self, database_url: str):
         """Create a database connection pool"""
         return asyncpg.create_pool(
             database_url,
             min_size=5,
             max_size=20,
             max_queries=10000,
             max_inactive_connection_lifetime=300,
-            timeout=30
+            timeout=30,
         )
-    
+
     async def get_connection(self, for_write: bool = False):
         """Get database connection with failover"""
         if for_write or not self.replica_pools:
             return await self.primary_pool.acquire()
-        
+
         # Try healthy replicas in round-robin
-        healthy_replicas = [r for r in self.replica_pools if r['healthy']]
+        healthy_replicas = [r for r in self.replica_pools if r["healthy"]]
         if not healthy_replicas:
             # Fallback to primary if no replicas are healthy
             return await self.primary_pool.acquire()
-        
+
         replica = healthy_replicas[hash(str(asyncio.current_task())) % len(healthy_replicas)]
-        return await replica['pool'].acquire()
-    
+        return await replica["pool"].acquire()
+
     async def _health_check_loop(self):
         """Periodically check database health"""
         while True:
             try:
                 for replica in self.replica_pools:
                     try:
-                        async with replica['pool'].acquire() as conn:
+                        async with replica["pool"].acquire() as conn:
                             await conn.execute("SELECT 1")
-                        replica['healthy'] = True
+                        replica["healthy"] = True
                     except Exception as e:
-                        replica['healthy'] = False
+                        replica["healthy"] = False
                         logger.warning(f"Replica database unhealthy: {e}")
-                
+
                 # Check primary
                 try:
                     async with self.primary_pool.acquire() as conn:
                         await conn.execute("SELECT 1")
                 except Exception as e:
                     logger.error(f"Primary database unhealthy: {e}")
                     # Implement failover logic here
-                
+
             except Exception as e:
                 logger.error(f"Health check failed: {e}")
-            
+
             await asyncio.sleep(30)  # Check every 30 seconds
-    
+
     async def close(self):
         """Close all connection pools"""
         if self.health_check_task:
             self.health_check_task.cancel()
-        
+
         await self.primary_pool.close()
         for replica in self.replica_pools:
-            await replica['pool'].close()
\ No newline at end of file
+            await replica["pool"].close()
would reformat /home/runner/work/ymera_y/ymera_y/MultiLevelCache.py
--- /home/runner/work/ymera_y/ymera_y/ProductionConfig.py	2025-10-19 22:47:02.787432+00:00
+++ /home/runner/work/ymera_y/ymera_y/ProductionConfig.py	2025-10-19 23:08:45.455973+00:00
@@ -10,108 +10,111 @@
     PASSWORD_COMPLEXITY = {
         "min_length": 12,
         "require_uppercase": True,
         "require_lowercase": True,
         "require_numbers": True,
-        "require_special": True
+        "require_special": True,
     }
-    
+
     # Performance
     DATABASE_POOL_SIZE = 50
     DATABASE_MAX_OVERFLOW = 100
     REDIS_CONNECTION_POOL = 50
     CACHE_TTL = 300  # 5 minutes
-    
+
     # Monitoring
     METRICS_RETENTION_DAYS = 90
     LOG_RETENTION_DAYS = 365
     TRACE_SAMPLING_RATE = 0.1  # 10% sampling in production
     AUDIT_LOG_RETENTION_DAYS = 365
-    
+
     # Compliance
     GDPR_COMPLIANCE_ENABLED = True
     HIPAA_COMPLIANCE_ENABLED = os.getenv("HIPAA_COMPLIANCE_ENABLED", "false").lower() == "true"
     PCI_COMPLIANCE_ENABLED = os.getenv("PCI_COMPLIANCE_ENABLED", "false").lower() == "true"
-    
+
     # External integrations
     SIEM_ENABLED = True
     HSM_ENABLED = os.getenv("HSM_ENABLED", "false").lower() == "true"
     DLP_ENABLED = True
-    
+
     # Rate limiting
     RATE_LIMIT_REQUESTS = 1000  # requests per hour
     RATE_LIMIT_BURST = 100  # burst capacity
-    
+
     # Backup
     BACKUP_ENCRYPTION = True
     BACKUP_RETENTION_DAYS = 30
     BACKUP_SCHEDULE = "0 2 * * *"  # Daily at 2 AM
 
+
 # Configuration validation
 def validate_production_config():
     """Validate production configuration"""
     required_env_vars = [
-        "JWT_PUBLIC_KEY", "JWT_PRIVATE_KEY",
-        "DATABASE_URL", "REDIS_URL",
-        "ENCRYPTION_KEY", "SENTRY_DSN"
+        "JWT_PUBLIC_KEY",
+        "JWT_PRIVATE_KEY",
+        "DATABASE_URL",
+        "REDIS_URL",
+        "ENCRYPTION_KEY",
+        "SENTRY_DSN",
     ]
-    
+
     missing_vars = [var for var in required_env_vars if not os.getenv(var)]
     if missing_vars:
         raise ConfigurationError(f"Missing required environment variables: {missing_vars}")
-    
+
     # Validate JWT keys
     try:
         serialization.load_pem_private_key(
-            ProductionConfig.JWT_PRIVATE_KEY.encode(),
-            password=None,
-            backend=default_backend()
+            ProductionConfig.JWT_PRIVATE_KEY.encode(), password=None, backend=default_backend()
         )
         serialization.load_pem_public_key(
-            ProductionConfig.JWT_PUBLIC_KEY.encode(),
-            backend=default_backend()
+            ProductionConfig.JWT_PUBLIC_KEY.encode(), backend=default_backend()
         )
     except Exception as e:
         raise ConfigurationError(f"Invalid JWT keys: {e}")
+
 
 # Initialize configuration
 def init_config():
     """Initialize configuration based on environment"""
     env = os.getenv("ENVIRONMENT", "development")
-    
+
     if env == "production":
         validate_production_config()
         return ProductionConfig()
     elif env == "staging":
         return StagingConfig()
     else:
         return DevelopmentConfig()
+
 
 # App initialization with enhanced security
 def create_app():
     """Create application with enhanced security"""
     app = FastAPI(
         title="YMERA Enterprise Project Agent",
         description="Production-ready enterprise project management system",
         version="1.0.0",
         docs_url="/api/docs" if os.getenv("ENVIRONMENT") != "production" else None,
         redoc_url=None,
-        openapi_url="/api/openapi.json" if os.getenv("ENVIRONMENT") != "production" else None
+        openapi_url="/api/openapi.json" if os.getenv("ENVIRONMENT") != "production" else None,
     )
-    
+
     # Add security middleware
     app.add_middleware(SessionMiddleware, secret_key=os.getenv("SESSION_SECRET"))
     app.add_middleware(HTTPSRedirectMiddleware)  # Redirect HTTP to HTTPS
-    
+
     # Initialize database
     init_db()
-    
+
     # Initialize monitoring
     init_monitoring()
-    
+
     # Add routes
     app.include_router(auth_router, prefix="/api/auth", tags=["Authentication"])
     app.include_router(projects_router, prefix="/api/projects", tags=["Projects"])
     app.include_router(compliance_router, prefix="/api/compliance", tags=["Compliance"])
     app.include_router(admin_router, prefix="/api/admin", tags=["Administration"])
-    
-    return app
\ No newline at end of file
+
+    return app
would reformat /home/runner/work/ymera_y/ymera_y/ProductionConfig.py
--- /home/runner/work/ymera_y/ymera_y/SIEMIntegration.py	2025-10-19 22:47:02.788432+00:00
+++ /home/runner/work/ymera_y/ymera_y/SIEMIntegration.py	2025-10-19 23:08:45.652690+00:00
@@ -1,235 +1,230 @@
 # Enhanced SIEM integration
 class SIEMIntegration:
     def __init__(self):
         self.siem_enabled = os.getenv("SIEM_ENABLED", "false").lower() == "true"
         self.siem_client = None
-        
+
         if self.siem_enabled:
             self._init_siem_client()
-    
+
     def _init_siem_client(self):
         """Initialize SIEM client"""
         try:
             # This would be actual SIEM client initialization
             # For Splunk: import splunklib.client as client
             # For Elastic: from elasticsearch import Elasticsearch
             logger.info("SIEM client initialized")
         except Exception as e:
             logger.error(f"Failed to initialize SIEM: {e}")
             self.siem_enabled = False
-    
+
     async def send_event(self, event: Dict[str, Any]):
         """Send security event to SIEM"""
         if not self.siem_enabled:
             return
-        
+
         try:
             # Standardize event format
             standardized_event = {
                 "timestamp": datetime.utcnow().isoformat(),
                 "event_type": "security",
                 "source": "ymera_enterprise",
                 "severity": event.get("severity", "medium"),
-                "details": event
+                "details": event,
             }
-            
+
             # This would be actual SIEM integration code
             # For example, with Splunk:
             # self.siem_client.indexes["security"].submit(
             #     json.dumps(standardized_event),
             #     sourcetype="ymera:security"
             # )
-            
+
             logger.info(f"SIEM event sent: {standardized_event}")
-            
+
         except Exception as e:
             logger.error(f"Failed to send SIEM event: {e}")
+
 
 # Enhanced audit logging with compliance features
 class ComplianceAuditLogger:
     @staticmethod
-    async def log_gdpr_event(request: Request, user_id: str, action: str, data: Dict[str, Any] = None):
+    async def log_gdpr_event(
+        request: Request, user_id: str, action: str, data: Dict[str, Any] = None
+    ):
         """Log GDPR-specific audit events"""
         event = {
             "event_type": "gdpr_compliance",
             "user_id": user_id,
             "action": action,
             "timestamp": datetime.utcnow().isoformat(),
             "ip_address": request.client.host,
             "user_agent": request.headers.get("user-agent"),
-            "data": data or {}
+            "data": data or {},
         }
-        
+
         # Send to SIEM
         siem = SIEMIntegration()
         await siem.send_event(event)
-        
+
         # Store in database
         async with DatabaseUtils.get_session() as session:
             audit_log = AuditLogRecord(
                 user_id=user_id,
                 action=action,
                 resource_type="gdpr",
                 resource_id=user_id,
                 ip_address=request.client.host,
                 user_agent=request.headers.get("user-agent"),
-                details=event
+                details=event,
             )
             session.add(audit_log)
             await session.commit()
-    
+
     @staticmethod
-    async def log_hipaa_event(request: Request, user_id: str, action: str, phi_data: Dict[str, Any] = None):
+    async def log_hipaa_event(
+        request: Request, user_id: str, action: str, phi_data: Dict[str, Any] = None
+    ):
         """Log HIPAA-specific audit events"""
         event = {
             "event_type": "hipaa_compliance",
             "user_id": user_id,
             "action": action,
             "timestamp": datetime.utcnow().isoformat(),
             "ip_address": request.client.host,
             "user_agent": request.headers.get("user-agent"),
-            "phi_access": phi_data or {}
+            "phi_access": phi_data or {},
         }
-        
+
         # Send to SIEM
         siem = SIEMIntegration()
         await siem.send_event({**event, "severity": "high"})
-        
+
         # Store in database
         async with DatabaseUtils.get_session() as session:
             audit_log = AuditLogRecord(
                 user_id=user_id,
                 action=action,
                 resource_type="phi",
                 resource_id=user_id,
                 ip_address=request.client.host,
                 user_agent=request.headers.get("user-agent"),
-                details=event
+                details=event,
             )
             session.add(audit_log)
             await session.commit()
 
+
 # Compliance endpoints
 @app.get("/compliance/gdpr/export/{user_id}")
 async def gdpr_export_data(
-    user_id: str,
-    current_user: UserRecord = Depends(require_permission(Permission.SYSTEM_ADMIN))
+    user_id: str, current_user: UserRecord = Depends(require_permission(Permission.SYSTEM_ADMIN))
 ):
     """GDPR data export endpoint"""
     async with DatabaseUtils.get_session() as session:
         # Get all user data
-        user = await session.execute(
-            select(UserRecord).where(UserRecord.id == user_id)
-        )
+        user = await session.execute(select(UserRecord).where(UserRecord.id == user_id))
         user = user.scalar_one_or_none()
-        
+
         if not user:
             raise NotFoundError("User not found")
-        
+
         # Get user's activity logs
         logs = await session.execute(
             select(AuditLogRecord).where(AuditLogRecord.user_id == user_id)
         )
         logs = logs.scalars().all()
-        
+
         # Format for GDPR export
         export_data = {
             "user_data": {
                 "id": str(user.id),
                 "email": user.email,
                 "username": user.username,
                 "full_name": user.full_name,
                 "created_at": user.created_at.isoformat(),
-                "last_login": user.last_login.isoformat() if user.last_login else None
+                "last_login": user.last_login.isoformat() if user.last_login else None,
             },
             "activity_logs": [
                 {
                     "timestamp": log.timestamp.isoformat(),
                     "action": log.action,
                     "resource_type": log.resource_type,
                     "resource_id": log.resource_id,
                     "ip_address": log.ip_address,
-                    "user_agent": log.user_agent
+                    "user_agent": log.user_agent,
                 }
                 for log in logs
-            ]
+            ],
         }
-        
+
         # Log the export event for compliance
         await ComplianceAuditLogger.log_gdpr_event(
-            request, 
-            current_user.id, 
-            "data_export", 
-            {"exported_user_id": user_id}
-        )
-        
+            request, current_user.id, "data_export", {"exported_user_id": user_id}
+        )
+
         return export_data
+
 
 @app.delete("/compliance/gdpr/delete/{user_id}")
 async def gdpr_delete_data(
-    user_id: str,
-    current_user: UserRecord = Depends(require_permission(Permission.SYSTEM_ADMIN))
+    user_id: str, current_user: UserRecord = Depends(require_permission(Permission.SYSTEM_ADMIN))
 ):
     """GDPR data deletion endpoint"""
     async with DatabaseUtils.get_session() as session:
         # Get user
-        user = await session.execute(
-            select(UserRecord).where(UserRecord.id == user_id)
-        )
+        user = await session.execute(select(UserRecord).where(UserRecord.id == user_id))
         user = user.scalar_one_or_none()
-        
+
         if not user:
             raise NotFoundError("User not found")
-        
+
         # Anonymize user data instead of actual deletion (GDPR right to be forgotten)
         user.email = f"deleted_{user.id}@example.com"
         user.username = f"deleted_user_{user.id}"
         user.full_name = "Deleted User"
         user.is_active = False
         user.deleted_at = datetime.utcnow()
-        
+
         # Anonymize audit logs
         await session.execute(
             update(AuditLogRecord)
             .where(AuditLogRecord.user_id == user_id)
             .values(ip_address="0.0.0.0", user_agent="deleted")
         )
-        
+
         await session.commit()
-        
+
         # Log the deletion event for compliance
         await ComplianceAuditLogger.log_gdpr_event(
-            request, 
-            current_user.id, 
-            "data_deletion", 
-            {"deleted_user_id": user_id}
-        )
-        
+            request, current_user.id, "data_deletion", {"deleted_user_id": user_id}
+        )
+
         return {"message": "User data anonymized successfully"}
+
 
 # Health check endpoint with security headers
 @app.get("/health")
 async def health_check():
     """Health check endpoint with security headers"""
-    return {
-        "status": "healthy",
-        "timestamp": datetime.utcnow().isoformat(),
-        "version": "1.0.0"
-    }
+    return {"status": "healthy", "timestamp": datetime.utcnow().isoformat(), "version": "1.0.0"}
+
 
 # Add security headers middleware
 @app.middleware("http")
 async def add_security_headers(request: Request, call_next):
     """Add security headers to all responses"""
     response = await call_next(request)
-    
+
     # Security headers
     response.headers["Strict-Transport-Security"] = "max-age=31536000; includeSubDomains"
     response.headers["X-Content-Type-Options"] = "nosniff"
     response.headers["X-Frame-Options"] = "DENY"
     response.headers["X-XSS-Protection"] = "1; mode=block"
     response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
-    response.headers["Content-Security-Policy"] = "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'"
+    response.headers["Content-Security-Policy"] = (
+        "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'"
+    )
     response.headers["Permissions-Policy"] = "geolocation=(), microphone=(), camera=()"
-    
-    return response
\ No newline at end of file
+
+    return response
would reformat /home/runner/work/ymera_y/ymera_y/SIEMIntegration.py
--- /home/runner/work/ymera_y/ymera_y/ZeroTrustConfig.py	2025-10-19 22:47:02.789432+00:00
+++ /home/runner/work/ymera_y/ymera_y/ZeroTrustConfig.py	2025-10-19 23:08:46.091297+00:00
@@ -6,109 +6,133 @@
             "client_id": os.getenv("GOOGLE_CLIENT_ID"),
             "client_secret": os.getenv("GOOGLE_CLIENT_SECRET"),
             "authorize_url": "https://accounts.google.com/o/oauth2/auth",
             "token_url": "https://oauth2.googleapis.com/token",
             "userinfo_url": "https://www.googleapis.com/oauth2/v3/userinfo",
-            "scopes": ["openid", "email", "profile"]
+            "scopes": ["openid", "email", "profile"],
         },
         "microsoft": {
             "client_id": os.getenv("MICROSOFT_CLIENT_ID"),
             "client_secret": os.getenv("MICROSOFT_CLIENT_SECRET"),
             "authorize_url": "https://login.microsoftonline.com/common/oauth2/v2.0/authorize",
             "token_url": "https://login.microsoftonline.com/common/oauth2/v2.0/token",
             "userinfo_url": "https://graph.microsoft.com/oidc/userinfo",
-            "scopes": ["openid", "email", "profile"]
-        }
+            "scopes": ["openid", "email", "profile"],
+        },
     }
-    
+
     # JWT with asymmetric keys
     JWT_ALGORITHM = "RS256"
     JWT_PUBLIC_KEY = os.getenv("JWT_PUBLIC_KEY")
     JWT_PRIVATE_KEY = os.getenv("JWT_PRIVATE_KEY")
-    
+
     # Biometric authentication
     BIOMETRIC_ENABLED = os.getenv("BIOMETRIC_ENABLED", "false").lower() == "true"
     BIOMETRIC_PROVIDERS = ["apple", "android", "windows"]
+
 
 # Enhanced permission system
 class Permission(str, Enum):
     # Project permissions
     PROJECT_CREATE = "project:create"
     PROJECT_READ = "project:read"
     PROJECT_UPDATE = "project:update"
     PROJECT_DELETE = "project:delete"
     PROJECT_EXPORT = "project:export"
-    
+
     # Task permissions
     TASK_CREATE = "task:create"
     TASK_READ = "task:read"
     TASK_UPDATE = "task:update"
     TASK_DELETE = "task:delete"
     TASK_ASSIGN = "task:assign"
     TASK_EXECUTE = "task:execute"
-    
+
     # User permissions
     USER_CREATE = "user:create"
     USER_READ = "user:read"
     USER_UPDATE = "user:update"
     USER_DELETE = "user:delete"
-    
+
     # System permissions
     SYSTEM_ADMIN = "system:admin"
     SYSTEM_CONFIG = "system:config"
     SYSTEM_BACKUP = "system:backup"
     SYSTEM_RESTORE = "system:restore"
-    
+
     # Audit permissions
     AUDIT_READ = "audit:read"
     AUDIT_EXPORT = "audit:export"
 
+
 # Role-Permission matrix
 ROLE_PERMISSION_MATRIX = {
     UserRole.ADMIN: [
-        Permission.PROJECT_CREATE, Permission.PROJECT_READ, Permission.PROJECT_UPDATE, Permission.PROJECT_DELETE, Permission.PROJECT_EXPORT,
-        Permission.TASK_CREATE, Permission.TASK_READ, Permission.TASK_UPDATE, Permission.TASK_DELETE, Permission.TASK_ASSIGN, Permission.TASK_EXECUTE,
-        Permission.USER_CREATE, Permission.USER_READ, Permission.USER_UPDATE, Permission.USER_DELETE,
-        Permission.SYSTEM_ADMIN, Permission.SYSTEM_CONFIG, Permission.SYSTEM_BACKUP, Permission.SYSTEM_RESTORE,
-        Permission.AUDIT_READ, Permission.AUDIT_EXPORT
+        Permission.PROJECT_CREATE,
+        Permission.PROJECT_READ,
+        Permission.PROJECT_UPDATE,
+        Permission.PROJECT_DELETE,
+        Permission.PROJECT_EXPORT,
+        Permission.TASK_CREATE,
+        Permission.TASK_READ,
+        Permission.TASK_UPDATE,
+        Permission.TASK_DELETE,
+        Permission.TASK_ASSIGN,
+        Permission.TASK_EXECUTE,
+        Permission.USER_CREATE,
+        Permission.USER_READ,
+        Permission.USER_UPDATE,
+        Permission.USER_DELETE,
+        Permission.SYSTEM_ADMIN,
+        Permission.SYSTEM_CONFIG,
+        Permission.SYSTEM_BACKUP,
+        Permission.SYSTEM_RESTORE,
+        Permission.AUDIT_READ,
+        Permission.AUDIT_EXPORT,
     ],
     UserRole.MANAGER: [
-        Permission.PROJECT_CREATE, Permission.PROJECT_READ, Permission.PROJECT_UPDATE, Permission.PROJECT_EXPORT,
-        Permission.TASK_CREATE, Permission.TASK_READ, Permission.TASK_UPDATE, Permission.TASK_ASSIGN, Permission.TASK_EXECUTE,
+        Permission.PROJECT_CREATE,
+        Permission.PROJECT_READ,
+        Permission.PROJECT_UPDATE,
+        Permission.PROJECT_EXPORT,
+        Permission.TASK_CREATE,
+        Permission.TASK_READ,
+        Permission.TASK_UPDATE,
+        Permission.TASK_ASSIGN,
+        Permission.TASK_EXECUTE,
         Permission.USER_READ,
-        Permission.AUDIT_READ
+        Permission.AUDIT_READ,
     ],
     UserRole.MEMBER: [
         Permission.PROJECT_READ,
-        Permission.TASK_READ, Permission.TASK_UPDATE, Permission.TASK_EXECUTE
+        Permission.TASK_READ,
+        Permission.TASK_UPDATE,
+        Permission.TASK_EXECUTE,
     ],
-    UserRole.VIEWER: [
-        Permission.PROJECT_READ,
-        Permission.TASK_READ
-    ],
-    UserRole.GUEST: [
-        Permission.PROJECT_READ
-    ]
+    UserRole.VIEWER: [Permission.PROJECT_READ, Permission.TASK_READ],
+    UserRole.GUEST: [Permission.PROJECT_READ],
 }
+
 
 # Enhanced UserRecord with permissions
 class UserRecord(Base):
     __tablename__ = "users"
-    
+
     # Existing fields...
     permissions = Column(JSON, default=[])
     mfa_method = Column(String(20), default="totp")  # totp, webauthn, sms, email
     webauthn_credentials = Column(JSON, default=[])
     risk_score = Column(Float, default=0.0)
     last_risk_assessment = Column(DateTime)
     adaptive_auth_factors = Column(JSON, default={})
-    
+
     # Additional indexes
     __table_args__ = (
-        Index('ix_users_risk_score', 'risk_score'),
-        Index('ix_users_mfa_method', 'mfa_method'),
+        Index("ix_users_risk_score", "risk_score"),
+        Index("ix_users_mfa_method", "mfa_method"),
     )
+
 
 # Enhanced authentication utilities
 class AdvancedAuthUtils:
     @staticmethod
     def generate_rsa_jwt(payload: Dict[str, Any], expires_delta: timedelta = None) -> str:
@@ -117,174 +141,164 @@
         if expires_delta:
             expire = datetime.utcnow() + expires_delta
         else:
             expire = datetime.utcnow() + timedelta(minutes=15)
         to_encode.update({"exp": expire})
-        
+
         private_key = serialization.load_pem_private_key(
-            ZeroTrustConfig.JWT_PRIVATE_KEY.encode(),
-            password=None,
-            backend=default_backend()
+            ZeroTrustConfig.JWT_PRIVATE_KEY.encode(), password=None, backend=default_backend()
         )
-        
-        return jwt.encode(
-            to_encode, 
-            private_key, 
-            algorithm=ZeroTrustConfig.JWT_ALGORITHM
-        )
-    
+
+        return jwt.encode(to_encode, private_key, algorithm=ZeroTrustConfig.JWT_ALGORITHM)
+
     @staticmethod
     def verify_rsa_jwt(token: str) -> Dict[str, Any]:
         """Verify JWT with RSA public key"""
         try:
             public_key = serialization.load_pem_public_key(
-                ZeroTrustConfig.JWT_PUBLIC_KEY.encode(),
-                backend=default_backend()
+                ZeroTrustConfig.JWT_PUBLIC_KEY.encode(), backend=default_backend()
             )
-            
-            payload = jwt.decode(
-                token, 
-                public_key, 
-                algorithms=[ZeroTrustConfig.JWT_ALGORITHM]
-            )
+
+            payload = jwt.decode(token, public_key, algorithms=[ZeroTrustConfig.JWT_ALGORITHM])
             return payload
         except jwt.PyJWTError:
             raise AuthenticationError("Invalid token")
-    
+
     @staticmethod
     async def calculate_risk_score(user: UserRecord, request: Request) -> float:
         """Calculate adaptive authentication risk score"""
         risk_score = 0.0
         factors = {}
-        
+
         # Device fingerprinting
         device_hash = hashlib.sha256(
             f"{request.headers.get('User-Agent')}{request.client.host}".encode()
         ).hexdigest()
-        
+
         # Check if device is known
         async with DatabaseUtils.get_session() as session:
             known_device = await session.execute(
                 select(UserSessionRecord).where(
                     and_(
                         UserSessionRecord.user_id == str(user.id),
                         UserSessionRecord.device_hash == device_hash,
-                        UserSessionRecord.is_revoked == False
+                        UserSessionRecord.is_revoked == False,
                     )
                 )
             )
             known_device = known_device.scalar_one_or_none()
-            
+
             if not known_device:
                 risk_score += 0.3
                 factors["unknown_device"] = True
-            
-        # Geographic location analysis (simplified)
-        # In production, integrate with geoIP service
-            if request.headers.get('X-Forwarded-For'):
-                client_ip = request.headers['X-Forwarded-For'].split(',')[0]
+
+            # Geographic location analysis (simplified)
+            # In production, integrate with geoIP service
+            if request.headers.get("X-Forwarded-For"):
+                client_ip = request.headers["X-Forwarded-For"].split(",")[0]
                 # Simulate geographic risk (e.g., login from different country)
                 risk_score += 0.2
                 factors["geo_risk"] = True
-        
+
         # Time-based analysis
         current_hour = datetime.utcnow().hour
         if current_hour < 6 or current_hour > 22:  # Unusual hours
             risk_score += 0.2
             factors["unusual_time"] = True
-        
+
         # Login velocity
-        login_count = await CacheManager.increment(f"login_attempts:{user.id}:{datetime.utcnow().strftime('%Y%m%d')}")
+        login_count = await CacheManager.increment(
+            f"login_attempts:{user.id}:{datetime.utcnow().strftime('%Y%m%d')}"
+        )
         if login_count > 5:
             risk_score += 0.3
             factors["high_velocity"] = True
-        
+
         return min(risk_score, 1.0), factors
+
 
 # Enhanced permission checking dependency
 async def require_permission(required_permission: Permission):
     """Require specific permission"""
+
     def permission_checker(current_user: UserRecord = Depends(get_current_active_user)):
         if current_user.role == UserRole.ADMIN:
             return current_user
-        
+
         user_permissions = current_user.permissions or []
         if required_permission not in user_permissions:
             raise AuthorizationError(f"Requires permission: {required_permission}")
-        
+
         return current_user
+
     return permission_checker
+
 
 # Enhanced OAuth2 integration
 @app.post("/auth/oauth/{provider}")
-async def oauth_login(
-    provider: str,
-    code: str = Body(...),
-    redirect_uri: str = Body(...)
-):
+async def oauth_login(provider: str, code: str = Body(...), redirect_uri: str = Body(...)):
     """OAuth2 login endpoint"""
     if provider not in ZeroTrustConfig.OAUTH_PROVIDERS:
         raise NotFoundError("OAuth provider not supported")
-    
+
     provider_config = ZeroTrustConfig.OAUTH_PROVIDERS[provider]
-    
+
     async with httpx.AsyncClient() as client:
         # Exchange code for tokens
         token_response = await client.post(
             provider_config["token_url"],
             data={
                 "client_id": provider_config["client_id"],
                 "client_secret": provider_config["client_secret"],
                 "code": code,
                 "redirect_uri": redirect_uri,
-                "grant_type": "authorization_code"
-            }
+                "grant_type": "authorization_code",
+            },
         )
-        
+
         if token_response.status_code != 200:
             raise AuthenticationError("Failed to exchange OAuth code")
-        
+
         tokens = token_response.json()
         access_token = tokens["access_token"]
-        
+
         # Get user info
         userinfo_response = await client.get(
-            provider_config["userinfo_url"],
-            headers={"Authorization": f"Bearer {access_token}"}
+            provider_config["userinfo_url"], headers={"Authorization": f"Bearer {access_token}"}
         )
-        
+
         if userinfo_response.status_code != 200:
             raise AuthenticationError("Failed to get user info")
-        
+
         user_info = userinfo_response.json()
-        
+
         # Find or create user
         async with DatabaseUtils.get_session() as session:
             user = await session.execute(
-                select(UserRecord).where(
-                    UserRecord.email == user_info["email"]
-                )
+                select(UserRecord).where(UserRecord.email == user_info["email"])
             )
             user = user.scalar_one_or_none()
-            
+
             if not user:
                 # Create new user with OAuth info
                 user = UserRecord(
                     email=user_info["email"],
                     username=user_info.get("preferred_username", user_info["email"].split("@")[0]),
                     full_name=user_info.get("name", ""),
                     is_verified=True,
                     oauth_provider=provider,
-                    oauth_id=user_info["sub"]
+                    oauth_id=user_info["sub"],
                 )
                 session.add(user)
                 await session.commit()
-            
+
             # Generate JWT
-            jwt_token = AdvancedAuthUtils.generate_rsa_jwt({
-                "sub": str(user.id),
-                "username": user.username,
-                "role": user.role,
-                "type": "access"
-            })
-            
-            return {"access_token": jwt_token, "token_type": "bearer"}
\ No newline at end of file
+            jwt_token = AdvancedAuthUtils.generate_rsa_jwt(
+                {
+                    "sub": str(user.id),
+                    "username": user.username,
+                    "role": user.role,
+                    "type": "access",
+                }
+            )
+
+            return {"access_token": jwt_token, "token_type": "bearer"}
would reformat /home/runner/work/ymera_y/ymera_y/ZeroTrustConfig.py
--- /home/runner/work/ymera_y/ymera_y/__init__.py	2025-10-19 22:47:02.789432+00:00
+++ /home/runner/work/ymera_y/ymera_y/__init__.py	2025-10-19 23:08:46.116565+00:00
@@ -5,10 +5,11 @@
 
 __all__ = ["__version__"]
 # Conditional imports to avoid breaking tests
 try:
     from .agent import AgentManager
-    __all__ = ['AgentManager']
+
+    __all__ = ["AgentManager"]
 except ImportError:
     __all__ = []
 
-__version__ = '2.0.0'
+__version__ = "2.0.0"
would reformat /home/runner/work/ymera_y/ymera_y/__init__.py
--- /home/runner/work/ymera_y/ymera_y/access_control.py	2025-10-19 22:47:02.789432+00:00
+++ /home/runner/work/ymera_y/ymera_y/access_control.py	2025-10-19 23:08:46.364326+00:00
@@ -17,178 +17,162 @@
 logger = structlog.get_logger(__name__)
 
 
 class AgentAccessController:
     """Controls agent access and permissions"""
-    
+
     def __init__(self, db_session: AsyncSession, encryption_manager: EncryptionManager):
         self.db = db_session
         self.encryption = encryption_manager
-    
+
     async def create_agent_credentials(
-        self,
-        agent_id: str,
-        agent_type: AgentType,
-        capabilities: List[str]
+        self, agent_id: str, agent_type: AgentType, capabilities: List[str]
     ) -> Dict[str, Any]:
         """Create secure credentials for an agent"""
         try:
             # Generate API key
             api_key = f"ymera_{secrets.token_urlsafe(32)}"
             api_secret = secrets.token_urlsafe(64)
-            
+
             # Encrypt credentials
             encrypted_secret = await self.encryption.encrypt(api_secret)
-            
+
             credentials = {
                 "api_key": api_key,
                 "api_secret_hash": encrypted_secret,
                 "agent_type": agent_type,
                 "capabilities": capabilities,
                 "created_at": datetime.utcnow().isoformat(),
-                "expires_at": (datetime.utcnow() + timedelta(days=365)).isoformat()
+                "expires_at": (datetime.utcnow() + timedelta(days=365)).isoformat(),
             }
-            
+
             return {
                 "api_key": api_key,
                 "api_secret": api_secret,  # Return plaintext only once
-                "expires_at": credentials["expires_at"]
+                "expires_at": credentials["expires_at"],
             }
-            
+
         except Exception as e:
             logger.error("Failed to create credentials", agent_id=agent_id, error=str(e))
             raise
-    
-    async def check_permission(
-        self,
-        agent_id: str,
-        resource: str,
-        action: str
-    ) -> bool:
+
+    async def check_permission(self, agent_id: str, resource: str, action: str) -> bool:
         """Check if agent has permission for action on resource"""
         try:
             stmt = select(AgentPermissionModel).where(
                 and_(
                     AgentPermissionModel.agent_id == agent_id,
                     AgentPermissionModel.resource == resource,
-                    AgentPermissionModel.revoked_at.is_(None)
+                    AgentPermissionModel.revoked_at.is_(None),
                 )
             )
             result = await self.db.execute(stmt)
             permission = result.scalar_one_or_none()
-            
+
             if not permission:
                 return False
-            
+
             # Check if permission has expired
             if permission.expires_at and permission.expires_at < datetime.utcnow():
                 return False
-            
+
             # Check if action is allowed
             return action in permission.actions
-            
+
         except Exception as e:
             logger.error("Permission check failed", agent_id=agent_id, error=str(e))
             return False
-    
+
     async def grant_permission(
         self,
         agent_id: str,
         resource: str,
         actions: List[str],
         granted_by: str,
-        expires_in_days: int = 365
+        expires_in_days: int = 365,
     ) -> Dict[str, Any]:
         """Grant permissions to an agent"""
         try:
             permission = AgentPermissionModel(
                 permission_id=str(uuid.uuid4()),
                 agent_id=agent_id,
                 resource=resource,
                 actions=actions,
                 granted_at=datetime.utcnow(),
                 granted_by=granted_by,
-                expires_at=datetime.utcnow() + timedelta(days=expires_in_days)
+                expires_at=datetime.utcnow() + timedelta(days=expires_in_days),
             )
-            
+
             self.db.add(permission)
             await self.db.commit()
-            
+
             logger.info("Permission granted", agent_id=agent_id, resource=resource)
-            
+
             return {
                 "status": "granted",
                 "permission_id": permission.permission_id,
                 "agent_id": agent_id,
                 "resource": resource,
-                "actions": actions
+                "actions": actions,
             }
-            
+
         except Exception as e:
             logger.error("Failed to grant permission", agent_id=agent_id, error=str(e))
             raise
-    
+
     async def revoke_permission(
-        self,
-        agent_id: str,
-        resource: str,
-        actions: List[str],
-        revoked_by: str
+        self, agent_id: str, resource: str, actions: List[str], revoked_by: str
     ) -> Dict[str, Any]:
         """Revoke permissions from an agent"""
         try:
             stmt = select(AgentPermissionModel).where(
                 and_(
                     AgentPermissionModel.agent_id == agent_id,
                     AgentPermissionModel.resource == resource,
-                    AgentPermissionModel.revoked_at.is_(None)
+                    AgentPermissionModel.revoked_at.is_(None),
                 )
             )
             result = await self.db.execute(stmt)
             permission = result.scalar_one_or_none()
-            
+
             if permission:
                 permission.revoked_at = datetime.utcnow()
                 permission.revoked_by = revoked_by
                 permission.revoke_reason = "Manual revocation"
                 await self.db.commit()
-            
+
             logger.info("Permission revoked", agent_id=agent_id, resource=resource)
-            
-            return {
-                "status": "revoked",
-                "agent_id": agent_id,
-                "resource": resource
-            }
-            
+
+            return {"status": "revoked", "agent_id": agent_id, "resource": resource}
+
         except Exception as e:
             logger.error("Failed to revoke permission", agent_id=agent_id, error=str(e))
             raise
-    
+
     async def revoke_agent_access(self, agent_id: str):
         """Revoke all access for an agent"""
         try:
             stmt = select(AgentPermissionModel).where(
                 and_(
                     AgentPermissionModel.agent_id == agent_id,
-                    AgentPermissionModel.revoked_at.is_(None)
+                    AgentPermissionModel.revoked_at.is_(None),
                 )
             )
             result = await self.db.execute(stmt)
             permissions = result.scalars().all()
-            
+
             for permission in permissions:
                 permission.revoked_at = datetime.utcnow()
                 permission.revoked_by = "system"
                 permission.revoke_reason = "Agent access revoked"
-            
+
             await self.db.commit()
-            
+
             logger.info("All access revoked for agent", agent_id=agent_id)
-            
+
         except Exception as e:
             logger.error("Failed to revoke agent access", agent_id=agent_id, error=str(e))
             raise
-    
+
     async def revoke_all_access(self, agent_id: str):
         """Complete access revocation (alias for revoke_agent_access)"""
         await self.revoke_agent_access(agent_id)
would reformat /home/runner/work/ymera_y/ymera_y/access_control.py
--- /home/runner/work/ymera_y/ymera_y/ServiceRegistry.py	2025-10-19 22:47:02.788432+00:00
+++ /home/runner/work/ymera_y/ymera_y/ServiceRegistry.py	2025-10-19 23:08:46.384836+00:00
@@ -2,60 +2,65 @@
 class ServiceRegistry:
     def __init__(self):
         self.services = {}
         self.consul_client = None
         self.init_consul()
-    
+
     def init_consul(self):
         """Initialize Consul for service discovery"""
         try:
             import consul
+
             self.consul_client = consul.Consul(
-                host=os.getenv('CONSUL_HOST', 'localhost'),
-                port=os.getenv('CONSUL_PORT', 8500)
+                host=os.getenv("CONSUL_HOST", "localhost"), port=os.getenv("CONSUL_PORT", 8500)
             )
         except ImportError:
             logger.warning("Consul not available, using local service registry")
-    
+
     async def register_service(self, service_name: str, service_url: str, tags: List[str] = None):
         """Register a service with the registry"""
         if self.consul_client:
             self.consul_client.agent.service.register(
-                service_name,
-                address=service_url,
-                tags=tags or []
+                service_name, address=service_url, tags=tags or []
             )
         self.services[service_name] = service_url
-    
+
     async def discover_service(self, service_name: str) -> str:
         """Discover service URL"""
         if self.consul_client:
             try:
                 _, services = self.consul_client.health.service(service_name)
                 if services:
-                    return random.choice([s['Service']['Address'] for s in services])
+                    return random.choice([s["Service"]["Address"] for s in services])
             except Exception:
                 logger.warning(f"Consul discovery failed for {service_name}")
-        
+
         return self.services.get(service_name)
+
 
 # Service base class
 class MicroService:
     def __init__(self, service_name: str):
         self.service_name = service_name
         self.service_registry = ServiceRegistry()
         self.http_client = httpx.AsyncClient(timeout=30.0)
-    
-    async def call_service(self, target_service: str, endpoint: str, method: str = "GET", 
-                         data: Any = None, headers: Dict[str, str] = None) -> Any:
+
+    async def call_service(
+        self,
+        target_service: str,
+        endpoint: str,
+        method: str = "GET",
+        data: Any = None,
+        headers: Dict[str, str] = None,
+    ) -> Any:
         """Call another microservice"""
         service_url = await self.service_registry.discover_service(target_service)
         if not service_url:
             raise ServiceUnavailableError(f"Service {target_service} not available")
-        
+
         url = f"{service_url}{endpoint}"
-        
+
         try:
             if method.upper() == "GET":
                 response = await self.http_client.get(url, headers=headers)
             elif method.upper() == "POST":
                 response = await self.http_client.post(url, json=data, headers=headers)
@@ -63,417 +68,431 @@
                 response = await self.http_client.put(url, json=data, headers=headers)
             elif method.upper() == "DELETE":
                 response = await self.http_client.delete(url, headers=headers)
             else:
                 raise ValueError(f"Unsupported HTTP method: {method}")
-            
+
             response.raise_for_status()
             return response.json()
-            
+
         except httpx.RequestError as e:
             logger.error(f"Service call failed to {target_service}: {e}")
             raise ServiceUnavailableError(f"Service {target_service} unavailable")
-    
+
     async def health_check(self) -> Dict[str, Any]:
         """Service health check"""
         return {
             "status": "healthy",
             "service": self.service_name,
-            "timestamp": datetime.utcnow().isoformat()
+            "timestamp": datetime.utcnow().isoformat(),
         }
+
 
 # Authentication Service
 class AuthenticationService(MicroService):
     def __init__(self):
         super().__init__("authentication-service")
         self.oauth_providers = self._init_oauth_providers()
-    
+
     def _init_oauth_providers(self) -> Dict[str, Any]:
         """Initialize OAuth providers"""
         return {
             "google": {
                 "client_id": os.getenv("GOOGLE_CLIENT_ID"),
                 "client_secret": os.getenv("GOOGLE_CLIENT_SECRET"),
                 "authorize_url": "https://accounts.google.com/o/oauth2/auth",
                 "token_url": "https://oauth2.googleapis.com/token",
-                "userinfo_url": "https://www.googleapis.com/oauth2/v3/userinfo"
+                "userinfo_url": "https://www.googleapis.com/oauth2/v3/userinfo",
             },
             # Add other providers...
         }
-    
+
     async def authenticate_user(self, username: str, password: str) -> Dict[str, Any]:
         """Authenticate user with credentials"""
         # Implementation would validate credentials and return JWT
         pass
-    
+
     async def validate_token(self, token: str) -> Dict[str, Any]:
         """Validate JWT token"""
         # Implementation would validate token and return user info
         pass
 
+
 # Project Management Service
 class ProjectManagementService(MicroService):
     def __init__(self):
         super().__init__("project-management-service")
         self.db = DatabaseUtils()
-    
+
     async def create_project(self, project_data: Dict[str, Any], user_id: str) -> Dict[str, Any]:
         """Create a new project"""
         async with self.db.get_session() as session:
-            project = ProjectRecord(
-                **project_data,
-                owner_id=user_id,
-                created_at=datetime.utcnow()
-            )
+            project = ProjectRecord(**project_data, owner_id=user_id, created_at=datetime.utcnow())
             session.add(project)
             await session.commit()
             await session.refresh(project)
             return project.to_dict()
-    
+
     async def get_project(self, project_id: str, user_id: str) -> Dict[str, Any]:
         """Get project details"""
         async with self.db.get_session() as session:
             project = await session.get(ProjectRecord, project_id)
             if not project or project.owner_id != user_id:
                 raise NotFoundError("Project not found")
             return project.to_dict()
 
+
 # Task Orchestration Service
 class TaskOrchestrationService(MicroService):
     def __init__(self):
         super().__init__("task-orchestration-service")
         self.task_queue = asyncio.Queue()
         self.worker_pool = []
         self.init_workers()
-    
+
     def init_workers(self):
         """Initialize task workers"""
-        for i in range(int(os.getenv('TASK_WORKERS', '5'))):
+        for i in range(int(os.getenv("TASK_WORKERS", "5"))):
             worker = asyncio.create_task(self._task_worker(f"worker-{i+1}"))
             self.worker_pool.append(worker)
-    
+
     async def _task_worker(self, worker_id: str):
         """Background task worker"""
         while True:
             try:
                 task = await self.task_queue.get()
                 logger.info(f"Worker {worker_id} processing task {task['id']}")
-                
+
                 # Process task
                 await self.process_task(task)
-                
+
                 self.task_queue.task_done()
             except Exception as e:
                 logger.error(f"Worker {worker_id} failed: {e}")
                 await asyncio.sleep(1)  # Prevent tight error loop
-    
+
     async def create_task(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
         """Create and queue a new task"""
         task_id = str(uuid.uuid4())
         task = {
             "id": task_id,
             "data": task_data,
             "status": "queued",
-            "created_at": datetime.utcnow().isoformat()
+            "created_at": datetime.utcnow().isoformat(),
         }
-        
+
         await self.task_queue.put(task)
         return task
-    
+
     async def process_task(self, task: Dict[str, Any]):
         """Process a task"""
         # Task processing logic would go here
         await asyncio.sleep(random.uniform(0.1, 1.0))  # Simulate work
-        
+
         # Update task status
         task["status"] = "completed"
         task["completed_at"] = datetime.utcnow().isoformat()
 
+
 # File Management Service
 class FileManagementService(MicroService):
     def __init__(self):
         super().__init__("file-management-service")
         self.storage_backend = self._init_storage_backend()
-    
+
     def _init_storage_backend(self):
         """Initialize storage backend (S3, Azure Blob, etc.)"""
-        storage_type = os.getenv('FILE_STORAGE_TYPE', 'local')
-        
-        if storage_type == 's3':
+        storage_type = os.getenv("FILE_STORAGE_TYPE", "local")
+
+        if storage_type == "s3":
             import boto3
-            return boto3.client('s3')
-        elif storage_type == 'azure':
+
+            return boto3.client("s3")
+        elif storage_type == "azure":
             from azure.storage.blob import BlobServiceClient
+
             return BlobServiceClient.from_connection_string(
-                os.getenv('AZURE_STORAGE_CONNECTION_STRING')
+                os.getenv("AZURE_STORAGE_CONNECTION_STRING")
             )
         else:
             # Local filesystem
             return None
-    
-    async def upload_file(self, file_data: bytes, filename: str, metadata: Dict[str, Any] = None) -> str:
+
+    async def upload_file(
+        self, file_data: bytes, filename: str, metadata: Dict[str, Any] = None
+    ) -> str:
         """Upload file to storage"""
         file_id = str(uuid.uuid4())
-        
+
         if self.storage_backend:
             # Cloud storage upload
-            if hasattr(self.storage_backend, 'upload_fileobj'):  # S3
+            if hasattr(self.storage_backend, "upload_fileobj"):  # S3
                 import io
+
                 file_obj = io.BytesIO(file_data)
                 self.storage_backend.upload_fileobj(
-                    file_obj,
-                    os.getenv('S3_BUCKET_NAME'),
-                    f"{file_id}/{filename}"
+                    file_obj, os.getenv("S3_BUCKET_NAME"), f"{file_id}/{filename}"
                 )
             # Other cloud providers...
         else:
             # Local filesystem
-            os.makedirs('uploads', exist_ok=True)
-            with open(f"uploads/{file_id}_{filename}", 'wb') as f:
+            os.makedirs("uploads", exist_ok=True)
+            with open(f"uploads/{file_id}_{filename}", "wb") as f:
                 f.write(file_data)
-        
+
         return file_id
-    
+
     async def download_file(self, file_id: str) -> bytes:
         """Download file from storage"""
         # Implementation would retrieve file from storage
         pass
 
+
 # Notification Service
 class NotificationService(MicroService):
     def __init__(self):
         super().__init__("notification-service")
         self.notification_channels = self._init_channels()
-    
+
     def _init_channels(self) -> Dict[str, Any]:
         """Initialize notification channels"""
         return {
             "email": {
-                "enabled": os.getenv('EMAIL_NOTIFICATIONS_ENABLED', 'false').lower() == 'true',
-                "provider": os.getenv('EMAIL_PROVIDER', 'smtp')
+                "enabled": os.getenv("EMAIL_NOTIFICATIONS_ENABLED", "false").lower() == "true",
+                "provider": os.getenv("EMAIL_PROVIDER", "smtp"),
             },
             "sms": {
-                "enabled": os.getenv('SMS_NOTIFICATIONS_ENABLED', 'false').lower() == 'true',
-                "provider": os.getenv('SMS_PROVIDER', 'twilio')
+                "enabled": os.getenv("SMS_NOTIFICATIONS_ENABLED", "false").lower() == "true",
+                "provider": os.getenv("SMS_PROVIDER", "twilio"),
             },
             "push": {
-                "enabled": os.getenv('PUSH_NOTIFICATIONS_ENABLED', 'false').lower() == 'true',
-                "provider": os.getenv('PUSH_PROVIDER', 'fcm')
-            }
+                "enabled": os.getenv("PUSH_NOTIFICATIONS_ENABLED", "false").lower() == "true",
+                "provider": os.getenv("PUSH_PROVIDER", "fcm"),
+            },
         }
-    
-    async def send_notification(self, user_id: str, message: str, 
-                              channels: List[str] = None, priority: str = "medium") -> bool:
+
+    async def send_notification(
+        self, user_id: str, message: str, channels: List[str] = None, priority: str = "medium"
+    ) -> bool:
         """Send notification to user"""
         channels = channels or ["email"]
-        
+
         for channel in channels:
-            if channel in self.notification_channels and self.notification_channels[channel]["enabled"]:
+            if (
+                channel in self.notification_channels
+                and self.notification_channels[channel]["enabled"]
+            ):
                 try:
                     if channel == "email":
                         await self._send_email(user_id, message, priority)
                     elif channel == "sms":
                         await self._send_sms(user_id, message, priority)
                     elif channel == "push":
                         await self._send_push(user_id, message, priority)
                 except Exception as e:
                     logger.error(f"Failed to send {channel} notification: {e}")
-        
+
         return True
-    
+
     async def _send_email(self, user_id: str, message: str, priority: str):
         """Send email notification"""
         # Email sending implementation
         pass
-    
+
     async def _send_sms(self, user_id: str, message: str, priority: str):
         """Send SMS notification"""
         # SMS sending implementation
         pass
-    
+
     async def _send_push(self, user_id: str, message: str, priority: str):
         """Send push notification"""
         # Push notification implementation
         pass
 
+
 # Audit Service
 class AuditService(MicroService):
     def __init__(self):
         super().__init__("audit-service")
         self.db = DatabaseUtils()
-    
-    async def log_event(self, event_type: str, user_id: str, resource_type: str, 
-                      resource_id: str, details: Dict[str, Any] = None) -> str:
+
+    async def log_event(
+        self,
+        event_type: str,
+        user_id: str,
+        resource_type: str,
+        resource_id: str,
+        details: Dict[str, Any] = None,
+    ) -> str:
         """Log audit event"""
         event_id = str(uuid.uuid4())
-        
+
         async with self.db.get_session() as session:
             audit_log = AuditLogRecord(
                 id=event_id,
                 user_id=user_id,
                 action=event_type,
                 resource_type=resource_type,
                 resource_id=resource_id,
                 details=details or {},
-                timestamp=datetime.utcnow()
+                timestamp=datetime.utcnow(),
             )
             session.add(audit_log)
             await session.commit()
-        
+
         return event_id
-    
-    async def query_events(self, filters: Dict[str, Any] = None, 
-                         limit: int = 100, offset: int = 0) -> List[Dict[str, Any]]:
+
+    async def query_events(
+        self, filters: Dict[str, Any] = None, limit: int = 100, offset: int = 0
+    ) -> List[Dict[str, Any]]:
         """Query audit events"""
         async with self.db.get_session() as session:
             query = select(AuditLogRecord)
-            
+
             if filters:
-                if 'user_id' in filters:
-                    query = query.where(AuditLogRecord.user_id == filters['user_id'])
-                if 'action' in filters:
-                    query = query.where(AuditLogRecord.action == filters['action'])
-                if 'start_time' in filters:
-                    query = query.where(AuditLogRecord.timestamp >= filters['start_time'])
-                if 'end_time' in filters:
-                    query = query.where(AuditLogRecord.timestamp <= filters['end_time'])
-            
+                if "user_id" in filters:
+                    query = query.where(AuditLogRecord.user_id == filters["user_id"])
+                if "action" in filters:
+                    query = query.where(AuditLogRecord.action == filters["action"])
+                if "start_time" in filters:
+                    query = query.where(AuditLogRecord.timestamp >= filters["start_time"])
+                if "end_time" in filters:
+                    query = query.where(AuditLogRecord.timestamp <= filters["end_time"])
+
             result = await session.execute(
-                query.order_by(AuditLogRecord.timestamp.desc())
-                .limit(limit)
-                .offset(offset)
+                query.order_by(AuditLogRecord.timestamp.desc()).limit(limit).offset(offset)
             )
             events = result.scalars().all()
             return [event.to_dict() for event in events]
 
+
 # Analytics Service
 class AnalyticsService(MicroService):
     def __init__(self):
         super().__init__("analytics-service")
         self.elasticsearch = self._init_elasticsearch()
-    
+
     def _init_elasticsearch(self):
         """Initialize Elasticsearch client"""
         try:
             from elasticsearch import AsyncElasticsearch
+
             return AsyncElasticsearch(
-                [os.getenv('ELASTICSEARCH_HOST', 'localhost:9200')],
+                [os.getenv("ELASTICSEARCH_HOST", "localhost:9200")],
                 http_auth=(
-                    os.getenv('ELASTICSEARCH_USERNAME'),
-                    os.getenv('ELASTICSEARCH_PASSWORD')
-                ) if os.getenv('ELASTICSEARCH_USERNAME') else None
+                    (os.getenv("ELASTICSEARCH_USERNAME"), os.getenv("ELASTICSEARCH_PASSWORD"))
+                    if os.getenv("ELASTICSEARCH_USERNAME")
+                    else None
+                ),
             )
         except ImportError:
             logger.warning("Elasticsearch not available")
             return None
-    
+
     async def index_document(self, index_name: str, document: Dict[str, Any], doc_id: str = None):
         """Index document in Elasticsearch"""
         if not self.elasticsearch:
             return
-        
+
         try:
             await self.elasticsearch.index(
-                index=index_name,
-                id=doc_id or str(uuid.uuid4()),
-                document=document
+                index=index_name, id=doc_id or str(uuid.uuid4()), document=document
             )
         except Exception as e:
             logger.error(f"Failed to index document: {e}")
-    
-    async def search_documents(self, index_name: str, query: Dict[str, Any], 
-                            size: int = 10, from_: int = 0) -> Dict[str, Any]:
+
+    async def search_documents(
+        self, index_name: str, query: Dict[str, Any], size: int = 10, from_: int = 0
+    ) -> Dict[str, Any]:
         """Search documents in Elasticsearch"""
         if not self.elasticsearch:
             return {"hits": {"hits": [], "total": {"value": 0}}}
-        
+
         try:
             response = await self.elasticsearch.search(
-                index=index_name,
-                body={"query": query},
-                size=size,
-                from_=from_
+                index=index_name, body={"query": query}, size=size, from_=from_
             )
             return response
         except Exception as e:
             logger.error(f"Failed to search documents: {e}")
             return {"hits": {"hits": [], "total": {"value": 0}}}
 
+
 # Integration Service
 class IntegrationService(MicroService):
     def __init__(self):
         super().__init__("integration-service")
         self.integrations = self._init_integrations()
-    
+
     def _init_integrations(self) -> Dict[str, Any]:
         """Initialize third-party integrations"""
         return {
             "slack": {
-                "enabled": os.getenv('SLACK_INTEGRATION_ENABLED', 'false').lower() == 'true',
-                "webhook_url": os.getenv('SLACK_WEBHOOK_URL')
+                "enabled": os.getenv("SLACK_INTEGRATION_ENABLED", "false").lower() == "true",
+                "webhook_url": os.getenv("SLACK_WEBHOOK_URL"),
             },
             "jira": {
-                "enabled": os.getenv('JIRA_INTEGRATION_ENABLED', 'false').lower() == 'true',
-                "base_url": os.getenv('JIRA_BASE_URL'),
-                "username": os.getenv('JIRA_USERNAME'),
-                "api_token": os.getenv('JIRA_API_TOKEN')
+                "enabled": os.getenv("JIRA_INTEGRATION_ENABLED", "false").lower() == "true",
+                "base_url": os.getenv("JIRA_BASE_URL"),
+                "username": os.getenv("JIRA_USERNAME"),
+                "api_token": os.getenv("JIRA_API_TOKEN"),
             },
             "salesforce": {
-                "enabled": os.getenv('SALESFORCE_INTEGRATION_ENABLED', 'false').lower() == 'true',
-                "instance_url": os.getenv('SALESFORCE_INSTANCE_URL'),
-                "client_id": os.getenv('SALESFORCE_CLIENT_ID'),
-                "client_secret": os.getenv('SALESFORCE_CLIENT_SECRET')
-            }
+                "enabled": os.getenv("SALESFORCE_INTEGRATION_ENABLED", "false").lower() == "true",
+                "instance_url": os.getenv("SALESFORCE_INSTANCE_URL"),
+                "client_id": os.getenv("SALESFORCE_CLIENT_ID"),
+                "client_secret": os.getenv("SALESFORCE_CLIENT_SECRET"),
+            },
         }
-    
-    async def send_to_slack(self, channel: str, message: str, attachments: List[Dict] = None) -> bool:
+
+    async def send_to_slack(
+        self, channel: str, message: str, attachments: List[Dict] = None
+    ) -> bool:
         """Send message to Slack"""
         if not self.integrations["slack"]["enabled"]:
             return False
-        
+
         try:
             async with httpx.AsyncClient() as client:
-                payload = {
-                    "channel": channel,
-                    "text": message,
-                    "attachments": attachments or []
-                }
+                payload = {"channel": channel, "text": message, "attachments": attachments or []}
                 response = await client.post(
-                    self.integrations["slack"]["webhook_url"],
-                    json=payload
+                    self.integrations["slack"]["webhook_url"], json=payload
                 )
                 response.raise_for_status()
                 return True
         except Exception as e:
             logger.error(f"Failed to send to Slack: {e}")
             return False
-    
-    async def create_jira_issue(self, project_key: str, issue_type: str, 
-                              summary: str, description: str) -> Dict[str, Any]:
+
+    async def create_jira_issue(
+        self, project_key: str, issue_type: str, summary: str, description: str
+    ) -> Dict[str, Any]:
         """Create JIRA issue"""
         if not self.integrations["jira"]["enabled"]:
             return {"error": "JIRA integration not enabled"}
-        
+
         try:
             async with httpx.AsyncClient() as client:
-                auth = (self.integrations["jira"]["username"], 
-                       self.integrations["jira"]["api_token"])
-                
+                auth = (
+                    self.integrations["jira"]["username"],
+                    self.integrations["jira"]["api_token"],
+                )
+
                 payload = {
                     "fields": {
                         "project": {"key": project_key},
                         "issuetype": {"name": issue_type},
                         "summary": summary,
-                        "description": description
+                        "description": description,
                     }
                 }
-                
+
                 response = await client.post(
                     f"{self.integrations['jira']['base_url']}/rest/api/2/issue",
                     json=payload,
-                    auth=auth
+                    auth=auth,
                 )
                 response.raise_for_status()
                 return response.json()
         except Exception as e:
             logger.error(f"Failed to create JIRA issue: {e}")
-            return {"error": str(e)}
\ No newline at end of file
+            return {"error": str(e)}
would reformat /home/runner/work/ymera_y/ymera_y/ServiceRegistry.py
--- /home/runner/work/ymera_y/ymera_y/admin_routes.py	2025-10-19 22:47:02.789432+00:00
+++ /home/runner/work/ymera_y/ymera_y/admin_routes.py	2025-10-19 23:08:46.607225+00:00
@@ -7,88 +7,93 @@
 from fastapi import Query, Path, Body
 from typing import List, Dict, Optional, Any
 from datetime import datetime, timedelta
 
 from models import (
-    AdminApproval, AdminApprovalCreate, AdminApprovalUpdate, AdminApprovalResponse,
-    AdminApprovalStatus, AuditLogResponse, AuditLogQuery
+    AdminApproval,
+    AdminApprovalCreate,
+    AdminApprovalUpdate,
+    AdminApprovalResponse,
+    AdminApprovalStatus,
+    AuditLogResponse,
+    AuditLogQuery,
 )
 
 from services import (
-    auth_service, agent_manager, lifecycle_manager, audit_system,
-    notification_manager
+    auth_service,
+    agent_manager,
+    lifecycle_manager,
+    audit_system,
+    notification_manager,
 )
 
 router = APIRouter(prefix="/admin", tags=["admin"])
+
 
 @router.get("/approvals", response_model=List[AdminApprovalResponse])
 async def list_approvals(
     status: Optional[List[str]] = Query(None),
     request_type: Optional[str] = None,
     limit: int = Query(100, gt=0, le=1000),
     offset: int = Query(0, ge=0),
-    current_user = Depends(auth_service.get_admin_user)
+    current_user=Depends(auth_service.get_admin_user),
 ):
     """List approval requests with filtering"""
     return await lifecycle_manager.list_approvals(status, request_type, limit, offset)
 
+
 @router.get("/approvals/{approval_id}", response_model=AdminApprovalResponse)
 async def get_approval(
-    approval_id: str = Path(...),
-    current_user = Depends(auth_service.get_admin_user)
+    approval_id: str = Path(...), current_user=Depends(auth_service.get_admin_user)
 ):
     """Get approval request details"""
     approval = await lifecycle_manager.get_approval(approval_id)
     if not approval:
         raise HTTPException(status_code=404, detail="Approval request not found")
     return approval
 
+
 @router.post("/approvals/{approval_id}/process", response_model=Dict[str, Any])
 async def process_approval(
     approval_id: str = Path(...),
     approval_data: AdminApprovalUpdate = Body(...),
     background_tasks: BackgroundTasks = None,
-    current_user = Depends(auth_service.get_admin_user)
+    current_user=Depends(auth_service.get_admin_user),
 ):
     """Process approval request (approve/reject)"""
     result = await lifecycle_manager.process_approval(
-        approval_id,
-        approval_data.status,
-        current_user.id,
-        approval_data.notes
+        approval_id, approval_data.status, current_user.id, approval_data.notes
     )
-    
+
     # Log audit event
     if background_tasks:
         background_tasks.add_task(
             audit_system.log_event,
             "approval_processed",
             "approval",
             approval_id,
             approval_data.status,
             current_user.id,
-            {"notes": approval_data.notes}
+            {"notes": approval_data.notes},
         )
-    
+
     # Notify requester
     if result.get("requested_by"):
         background_tasks.add_task(
             notification_manager.send_notification,
             result["requested_by"],
             {
                 "type": "approval_processed",
                 "title": f"Your request has been {approval_data.status}",
                 "message": approval_data.notes or f"Your request has been {approval_data.status}",
                 "priority": "medium",
-                "metadata": {
-                    "approval_id": approval_id,
-                    "status": approval_data.status
-                }
-            }
+                "metadata": {"approval_id": approval_id, "status": approval_data.status},
+            },
         )
-    
+
     return result
+
 
 @router.get("/audit-logs", response_model=List[AuditLogResponse])
 async def list_audit_logs(
     event_type: Optional[str] = None,
     resource_type: Optional[str] = None,
@@ -96,49 +101,48 @@
     performed_by: Optional[str] = None,
     start_date: Optional[datetime] = None,
     end_date: Optional[datetime] = None,
     limit: int = Query(100, gt=0, le=1000),
     offset: int = Query(0, ge=0),
-    current_user = Depends(auth_service.get_admin_user)
+    current_user=Depends(auth_service.get_admin_user),
 ):
     """Search audit logs with filtering"""
     query = AuditLogQuery(
         event_type=event_type,
         resource_type=resource_type,
         resource_id=resource_id,
         performed_by=performed_by,
         start_date=start_date,
         end_date=end_date,
         limit=limit,
-        offset=offset
+        offset=offset,
     )
-    
+
     return await audit_system.search_audit_logs(query)
+
 
 @router.post("/agents/{agent_id}/exempt-reporting", response_model=Dict[str, Any])
 async def exempt_agent_from_reporting(
     agent_id: str = Path(...),
     exemption_data: Dict[str, Any] = Body(...),
     background_tasks: BackgroundTasks = None,
-    current_user = Depends(auth_service.get_admin_user)
+    current_user=Depends(auth_service.get_admin_user),
 ):
     """Exempt agent from mandatory reporting requirements"""
     exempt = exemption_data.get("exempt", True)
     reason = exemption_data.get("reason", "Administrative exemption")
-    
-    result = await agent_manager.set_reporting_exemption(
-        agent_id, exempt, reason, current_user.id
-    )
-    
+
+    result = await agent_manager.set_reporting_exemption(agent_id, exempt, reason, current_user.id)
+
     # Log audit event
     if background_tasks:
         background_tasks.add_task(
             audit_system.log_event,
             "reporting_exemption",
             "agent",
             agent_id,
             "exempt" if exempt else "unexempt",
             current_user.id,
-            {"reason": reason}
+            {"reason": reason},
         )
-    
-    return result
\ No newline at end of file
+
+    return result
would reformat /home/runner/work/ymera_y/ymera_y/admin_routes.py
--- /home/runner/work/ymera_y/ymera_y/advanced_features.py	2025-10-19 22:47:02.789432+00:00
+++ /home/runner/work/ymera_y/ymera_y/advanced_features.py	2025-10-19 23:08:46.909232+00:00
@@ -17,87 +17,90 @@
 from functools import wraps
 
 logger = logging.getLogger(__name__)
 
 # Enhanced Prometheus Metrics
-WEBSOCKET_CONNECTIONS = Gauge('websocket_connections_active', 'Active WebSocket connections')
-WEBSOCKET_MESSAGES = Counter('websocket_messages_total', 'Total WebSocket messages', ['type'])
-CACHE_OPERATIONS = Counter('cache_operations_total', 'Cache operations', ['operation', 'result'])
-SECURITY_EVENTS = Counter('security_events_total', 'Security events', ['event_type'])
+WEBSOCKET_CONNECTIONS = Gauge("websocket_connections_active", "Active WebSocket connections")
+WEBSOCKET_MESSAGES = Counter("websocket_messages_total", "Total WebSocket messages", ["type"])
+CACHE_OPERATIONS = Counter("cache_operations_total", "Cache operations", ["operation", "result"])
+SECURITY_EVENTS = Counter("security_events_total", "Security events", ["event_type"])
 
 # =============================================================================
 # WEBSOCKET CONNECTION MANAGER
 # =============================================================================
+
 
 class ConnectionManager:
     """Manage WebSocket connections for real-time communication"""
-    
+
     def __init__(self):
         self.active_connections: Dict[str, Dict[str, WebSocket]] = {}
         self.user_connections: Dict[str, Set[str]] = {}
-    
+
     async def connect(self, websocket: WebSocket, user_id: str, connection_id: str):
         await websocket.accept()
-        
+
         if user_id not in self.active_connections:
             self.active_connections[user_id] = {}
             self.user_connections[user_id] = set()
-        
+
         self.active_connections[user_id][connection_id] = websocket
         self.user_connections[user_id].add(connection_id)
-        
+
         WEBSOCKET_CONNECTIONS.inc()
         logger.info(f"WebSocket connected: user={user_id}, connection={connection_id}")
-    
+
     def disconnect(self, user_id: str, connection_id: str):
         if user_id in self.active_connections:
             if connection_id in self.active_connections[user_id]:
                 del self.active_connections[user_id][connection_id]
                 self.user_connections[user_id].discard(connection_id)
-                
+
                 if not self.active_connections[user_id]:
                     del self.active_connections[user_id]
                     del self.user_connections[user_id]
-                
+
                 WEBSOCKET_CONNECTIONS.dec()
                 logger.info(f"WebSocket disconnected: user={user_id}, connection={connection_id}")
-    
+
     async def send_to_user(self, user_id: str, message: dict):
         """Send message to all connections of a user"""
         if user_id in self.active_connections:
             disconnected = []
             for connection_id, websocket in self.active_connections[user_id].items():
                 try:
                     await websocket.send_text(json.dumps(message))
                     WEBSOCKET_MESSAGES.labels(type="sent").inc()
                 except:
                     disconnected.append(connection_id)
-            
+
             # Clean up disconnected connections
             for connection_id in disconnected:
                 self.disconnect(user_id, connection_id)
-    
+
     async def broadcast_to_all(self, message: dict):
         """Broadcast message to all connected users"""
         for user_id in list(self.active_connections.keys()):
             await self.send_to_user(user_id, message)
 
+
 # Global connection manager
 connection_manager = ConnectionManager()
 
 # =============================================================================
 # INTELLIGENT CACHING SYSTEM
 # =============================================================================
 
+
 class CacheManager:
     """Intelligent multi-level caching with TTL and invalidation"""
-    
+
     def __init__(self, redis_client: aioredis.Redis):
         self.redis = redis_client
         self.local_cache = {}
         self.cache_stats = {"hits": 0, "misses": 0, "sets": 0}
-    
+
     async def get(self, key: str, default=None):
         """Get from cache with fallback"""
         try:
             # Try local cache first (L1)
             if key in self.local_cache:
@@ -106,348 +109,350 @@
                     self.cache_stats["hits"] += 1
                     CACHE_OPERATIONS.labels(operation="get", result="hit").inc()
                     return entry["value"]
                 else:
                     del self.local_cache[key]
-            
+
             # Try Redis cache (L2)
             value = await self.redis.get(key)
             if value:
                 self.cache_stats["hits"] += 1
                 CACHE_OPERATIONS.labels(operation="get", result="hit").inc()
                 return json.loads(value)
-            
+
             self.cache_stats["misses"] += 1
             CACHE_OPERATIONS.labels(operation="get", result="miss").inc()
             return default
-            
+
         except Exception as e:
             logger.error(f"Cache get error: {e}")
             return default
-    
+
     async def set(self, key: str, value, ttl: int = 300):
         """Set cache with TTL"""
         try:
             # Set in local cache (L1)
             self.local_cache[key] = {
                 "value": value,
-                "expires": datetime.utcnow() + timedelta(seconds=min(ttl, 60))  # Max 1min local
+                "expires": datetime.utcnow() + timedelta(seconds=min(ttl, 60)),  # Max 1min local
             }
-            
+
             # Set in Redis cache (L2)
             await self.redis.setex(key, ttl, json.dumps(value, default=str))
-            
+
             self.cache_stats["sets"] += 1
             CACHE_OPERATIONS.labels(operation="set", result="success").inc()
-            
+
         except Exception as e:
             logger.error(f"Cache set error: {e}")
             CACHE_OPERATIONS.labels(operation="set", result="error").inc()
-    
+
     async def delete(self, key: str):
         """Delete from all cache levels"""
         try:
             if key in self.local_cache:
                 del self.local_cache[key]
             await self.redis.delete(key)
             CACHE_OPERATIONS.labels(operation="delete", result="success").inc()
         except Exception as e:
             logger.error(f"Cache delete error: {e}")
-    
+
     async def invalidate_pattern(self, pattern: str):
         """Invalidate cache keys matching pattern"""
         try:
             # Clear local cache matching pattern
             keys_to_delete = [k for k in self.local_cache.keys() if pattern in k]
             for key in keys_to_delete:
                 del self.local_cache[key]
-            
+
             # Clear Redis cache matching pattern
-            cursor = '0'
+            cursor = "0"
             while cursor != 0:
                 cursor, keys = await self.redis.scan(cursor=cursor, match=pattern, count=100)
                 if keys:
                     await self.redis.delete(*keys)
-            
+
         except Exception as e:
             logger.error(f"Cache invalidation error: {e}")
 
+
 # =============================================================================
 # ENHANCED SECURITY MIDDLEWARE
 # =============================================================================
+
 
 class SecurityManager:
     """Enhanced security features"""
-    
+
     def __init__(self, redis_client: aioredis.Redis):
         self.redis = redis_client
         self.rate_limits = {}
         self.failed_attempts = {}
-    
+
     async def rate_limit(self, identifier: str, limit: int = 100, window: int = 3600) -> bool:
         """Rate limiting with sliding window"""
         try:
             current_time = datetime.utcnow().timestamp()
             key = f"rate_limit:{identifier}"
-            
+
             # Remove expired entries
             await self.redis.zremrangebyscore(key, 0, current_time - window)
-            
+
             # Count current requests
             count = await self.redis.zcard(key)
-            
+
             if count >= limit:
                 SECURITY_EVENTS.labels(event_type="rate_limit_exceeded").inc()
                 return False
-            
+
             # Add current request
             await self.redis.zadd(key, {str(current_time): current_time})
             await self.redis.expire(key, window)
-            
+
             return True
-            
+
         except Exception as e:
             logger.error(f"Rate limiting error: {e}")
             return True  # Fail open
-    
+
     async def check_api_key(self, api_key: str, expected_hash: str) -> bool:
         """Secure API key validation"""
         try:
             # Use constant-time comparison to prevent timing attacks
-            return hmac.compare_digest(
-                hashlib.sha256(api_key.encode()).hexdigest(),
-                expected_hash
-            )
+            return hmac.compare_digest(hashlib.sha256(api_key.encode()).hexdigest(), expected_hash)
         except Exception:
             return False
-    
+
     async def log_security_event(self, event_type: str, details: dict, user_id: str = None):
         """Log security events for monitoring"""
         event = {
             "timestamp": datetime.utcnow().isoformat(),
             "event_type": event_type,
             "details": details,
-            "user_id": user_id
+            "user_id": user_id,
         }
-        
+
         # Log to Redis stream for real-time monitoring
         await self.redis.xadd("security_events", event)
-        
+
         # Update metrics
         SECURITY_EVENTS.labels(event_type=event_type).inc()
-        
+
         logger.warning(f"Security event: {event_type} - {details}")
 
+
 # =============================================================================
 # INTELLIGENT TASK SCHEDULER
 # =============================================================================
+
 
 class TaskScheduler:
     """Intelligent task scheduling with priority and resource management"""
-    
+
     def __init__(self, redis_client: aioredis.Redis):
         self.redis = redis_client
         self.running_tasks = {}
         self.max_concurrent_tasks = 10
-    
+
     async def schedule_task(self, task_data: dict, priority: int = 1, delay: int = 0):
         """Schedule task with priority and optional delay"""
-        score = datetime.utcnow().timestamp() + delay + (100 - priority)  # Higher priority = lower score
-        
+        score = (
+            datetime.utcnow().timestamp() + delay + (100 - priority)
+        )  # Higher priority = lower score
+
         queue_name = f"task_queue:{task_data.get('task_type', 'default')}"
         await self.redis.zadd(queue_name, {json.dumps(task_data): score})
-    
+
     async def get_next_task(self, task_types: List[str] = None) -> Optional[dict]:
         """Get next task to process based on priority and timing"""
         current_time = datetime.utcnow().timestamp()
-        
+
         # Default to all task types if none specified
         if not task_types:
             task_types = await self._get_all_task_types()
-        
+
         for task_type in task_types:
             queue_name = f"task_queue:{task_type}"
-            
+
             # Get tasks that are due to run
             tasks = await self.redis.zrangebyscore(queue_name, 0, current_time, start=0, num=1)
-            
+
             if tasks:
                 # Remove from queue and return
                 task_data = json.loads(tasks[0])
                 await self.redis.zrem(queue_name, tasks[0])
                 return task_data
-        
+
         return None
-    
+
     async def _get_all_task_types(self) -> List[str]:
         """Get all available task types from queue keys"""
         keys = await self.redis.keys("task_queue:*")
         return [key.decode().split(":")[-1] for key in keys]
 
+
 # =============================================================================
 # HEALTH MONITORING SYSTEM
 # =============================================================================
+
 
 class HealthMonitor:
     """Comprehensive health monitoring"""
-    
+
     def __init__(self, db_session_factory, redis_client: aioredis.Redis):
         self.db_session_factory = db_session_factory
         self.redis = redis_client
         self.health_status = {"status": "healthy", "last_check": datetime.utcnow()}
-    
+
     async def check_system_health(self) -> dict:
         """Perform comprehensive health check"""
         health_data = {
             "timestamp": datetime.utcnow().isoformat(),
             "status": "healthy",
             "components": {},
-            "metrics": {}
+            "metrics": {},
         }
-        
+
         # Database health
         try:
             async with self.db_session_factory() as session:
                 await session.execute("SELECT 1")
             health_data["components"]["database"] = "healthy"
         except Exception as e:
             health_data["components"]["database"] = f"unhealthy: {str(e)}"
             health_data["status"] = "unhealthy"
-        
+
         # Redis health
         try:
             await self.redis.ping()
             health_data["components"]["redis"] = "healthy"
         except Exception as e:
             health_data["components"]["redis"] = f"unhealthy: {str(e)}"
             health_data["status"] = "unhealthy"
-        
+
         # Memory usage
         import psutil
+
         memory = psutil.virtual_memory()
         health_data["metrics"]["memory_usage_percent"] = memory.percent
         health_data["metrics"]["memory_available_gb"] = memory.available / (1024**3)
-        
+
         # CPU usage
         health_data["metrics"]["cpu_usage_percent"] = psutil.cpu_percent(interval=1)
-        
+
         # Active connections
         health_data["metrics"]["websocket_connections"] = len(connection_manager.active_connections)
-        
+
         self.health_status = health_data
         return health_data
 
+
 # =============================================================================
 # NOTIFICATION SYSTEM
 # =============================================================================
+
 
 class NotificationManager:
     """Multi-channel notification system"""
-    
+
     def __init__(self, redis_client: aioredis.Redis):
         self.redis = redis_client
         self.channels = {}
-    
+
     async def send_notification(self, user_id: str, notification: dict):
         """Send notification through multiple channels"""
-        notification.update({
-            "timestamp": datetime.utcnow().isoformat(),
-            "id": str(uuid.uuid4())
-        })
-        
+        notification.update({"timestamp": datetime.utcnow().isoformat(), "id": str(uuid.uuid4())})
+
         # Store notification in database/cache for persistence
-        await self.redis.lpush(
-            f"notifications:{user_id}",
-            json.dumps(notification)
+        await self.redis.lpush(f"notifications:{user_id}", json.dumps(notification))
+        await self.redis.expire(f"notifications:{user_id}", 86400 * 30)  # 30 days
+
+        # Send real-time via WebSocket
+        await connection_manager.send_to_user(
+            user_id, {"type": "notification", "data": notification}
         )
-        await self.redis.expire(f"notifications:{user_id}", 86400 * 30)  # 30 days
-        
-        # Send real-time via WebSocket
-        await connection_manager.send_to_user(user_id, {
-            "type": "notification",
-            "data": notification
-        })
-        
+
         # Send via email/SMS if configured (placeholder)
         if notification.get("priority") == "high":
             await self._send_email_notification(user_id, notification)
-    
+
     async def get_user_notifications(self, user_id: str, limit: int = 50) -> List[dict]:
         """Get user notifications"""
         try:
             notifications = await self.redis.lrange(f"notifications:{user_id}", 0, limit - 1)
             return [json.loads(n) for n in notifications]
         except Exception as e:
             logger.error(f"Error getting notifications: {e}")
             return []
-    
+
     async def _send_email_notification(self, user_id: str, notification: dict):
         """Send email notification (implement with your email service)"""
         # Placeholder for email integration
         logger.info(f"Email notification sent to user {user_id}: {notification['title']}")
 
+
 # =============================================================================
 # ANALYTICS ENGINE
 # =============================================================================
+
 
 class AnalyticsEngine:
     """Real-time analytics and reporting"""
-    
+
     def __init__(self, redis_client: aioredis.Redis):
         self.redis = redis_client
-    
+
     async def record_event(self, event_type: str, user_id: str, data: dict):
         """Record analytics event"""
         event = {
             "timestamp": datetime.utcnow().isoformat(),
             "event_type": event_type,
             "user_id": user_id,
-            "data": data
+            "data": data,
         }
-        
+
         # Store in Redis streams for real-time analytics
         await self.redis.xadd("analytics_events", event)
-        
+
         # Update counters
         await self.redis.hincrby("analytics_counters", event_type, 1)
         await self.redis.hincrby(f"user_analytics:{user_id}", event_type, 1)
-    
+
     async def get_analytics_summary(self, time_range: int = 3600) -> dict:
         """Get analytics summary for specified time range"""
         try:
             # Get events from the last hour
             end_time = datetime.utcnow().timestamp() * 1000
             start_time = (datetime.utcnow() - timedelta(seconds=time_range)).timestamp() * 1000
-            
+
             events = await self.redis.xrange(
-                "analytics_events",
-                min=int(start_time),
-                max=int(end_time)
+                "analytics_events", min=int(start_time), max=int(end_time)
             )
-            
+
             # Process events
             summary = {
                 "total_events": len(events),
                 "event_types": {},
                 "unique_users": set(),
-                "time_range": time_range
+                "time_range": time_range,
             }
-            
+
             for event_id, fields in events:
                 event_type = fields.get(b"event_type", b"unknown").decode()
                 user_id = fields.get(b"user_id", b"anonymous").decode()
-                
+
                 summary["event_types"][event_type] = summary["event_types"].get(event_type, 0) + 1
                 summary["unique_users"].add(user_id)
-            
+
             summary["unique_users"] = len(summary["unique_users"])
-            
+
             return summary
-            
+
         except Exception as e:
             logger.error(f"Analytics summary error: {e}")
             return {"error": str(e)}
+
 
 # Initialize global instances (these will be initialized in main.py)
 cache_manager = None
 security_manager = None
 task_scheduler = None
would reformat /home/runner/work/ymera_y/ymera_y/advanced_features.py
--- /home/runner/work/ymera_y/ymera_y/agent.py	2025-10-19 22:47:02.789432+00:00
+++ /home/runner/work/ymera_y/ymera_y/agent.py	2025-10-19 23:08:47.057597+00:00
@@ -1,6 +1,5 @@
-
 import asyncio
 from typing import Dict, List, Any, Optional
 from datetime import datetime, timedelta
 import json
 import logging
@@ -10,155 +9,159 @@
 from .models import Experience, KnowledgeItem
 from .ml_pipeline import MLPipeline
 from .knowledge_manager import KnowledgeManager
 
 logger = logging.getLogger(__name__)
+
 
 class YmeraAgent:
     def __init__(self, database: Database, settings):
         self.database = database
         self.settings = settings
         self.ml_pipeline = MLPipeline(settings)
         self.knowledge_manager = KnowledgeManager(database)
         self.is_initialized = False
         self.background_tasks = []
-    
+
     async def initialize(self):
         """Initialize agent components"""
         try:
             await self.ml_pipeline.initialize()
             await self.knowledge_manager.initialize()
-            
+
             # Start background tasks
-            self.background_tasks.extend([
-                asyncio.create_task(self._experience_processing_loop()),
-                asyncio.create_task(self._knowledge_update_loop()),
-                asyncio.create_task(self._metrics_collection_loop())
-            ])
-            
+            self.background_tasks.extend(
+                [
+                    asyncio.create_task(self._experience_processing_loop()),
+                    asyncio.create_task(self._knowledge_update_loop()),
+                    asyncio.create_task(self._metrics_collection_loop()),
+                ]
+            )
+
             self.is_initialized = True
             logger.info("Agent initialized successfully")
-            
+
         except Exception as e:
             logger.error(f"Agent initialization failed: {e}")
             raise
-    
-    async def process_message(self, user_id: UUID, message: str, 
-                            conversation_id: Optional[UUID] = None) -> Dict[str, Any]:
+
+    async def process_message(
+        self, user_id: UUID, message: str, conversation_id: Optional[UUID] = None
+    ) -> Dict[str, Any]:
         """Process incoming message and generate response"""
         if not self.is_initialized:
             raise RuntimeError("Agent not initialized")
-        
+
         conversation_id = conversation_id or uuid4()
-        
+
         try:
             # Create experience record
             experience = Experience(
                 user_id=user_id,
                 conversation_id=conversation_id,
                 input_data={"message": message, "timestamp": datetime.utcnow().isoformat()},
-                metadata={"source": "chat"}
-            )
-            
+                metadata={"source": "chat"},
+            )
+
             # Get relevant knowledge
-            relevant_knowledge = await self.knowledge_manager.search_knowledge(
-                message, limit=5
-            )
-            
+            relevant_knowledge = await self.knowledge_manager.search_knowledge(message, limit=5)
+
             # Generate response using ML pipeline
             response_data = await self.ml_pipeline.generate_response(
                 message=message,
                 context=relevant_knowledge,
-                conversation_history=await self._get_conversation_history(conversation_id)
-            )
-            
+                conversation_history=await self._get_conversation_history(conversation_id),
+            )
+
             # Update experience with response
             experience.output_data = response_data
-            
+
             # Store experience
             await self._store_experience(experience)
-            
+
             return {
                 "response": response_data.get("response", "I understand your message."),
                 "conversation_id": str(conversation_id),
                 "confidence": response_data.get("confidence", 0.8),
                 "suggestions": response_data.get("suggestions", []),
-                "context_used": response_data.get("context_used", False)
+                "context_used": response_data.get("context_used", False),
             }
-            
+
         except Exception as e:
             logger.error(f"Message processing failed: {e}")
             return {
-                "response": "I\"m having trouble processing your message right now. Please try again.",
+                "response": 'I"m having trouble processing your message right now. Please try again.',
                 "conversation_id": str(conversation_id),
                 "confidence": 0.0,
-                "error": True
+                "error": True,
             }
-    
-    async def provide_feedback(self, experience_id: UUID, score: int, 
-                              comment: Optional[str] = None) -> bool:
+
+    async def provide_feedback(
+        self, experience_id: UUID, score: int, comment: Optional[str] = None
+    ) -> bool:
         """Provide feedback on agent response"""
         try:
             query = """
                 UPDATE agent_experiences 
                 SET feedback_score = $1, 
                     metadata = metadata || $2,
                     updated_at = NOW()
                 WHERE id = $3
             """
-            
+
             metadata_update = {"feedback_comment": comment} if comment else {}
-            
+
             await self.database.execute_command(
                 query, score, json.dumps(metadata_update), experience_id
             )
-            
+
             # Trigger learning update
             asyncio.create_task(self._trigger_learning_update())
-            
+
             return True
-            
+
         except Exception as e:
             logger.error(f"Feedback storage failed: {e}")
             return False
-    
+
     async def _store_experience(self, experience: Experience):
         """Store experience in database"""
         query = """
             INSERT INTO agent_experiences (user_id, conversation_id, input_data, 
                                          output_data, metadata)
             VALUES ($1, $2, $3, $4, $5)
             RETURNING id
         """
-        
+
         result = await self.database.execute_single(
             query,
             experience.user_id,
             experience.conversation_id,
             json.dumps(experience.input_data),
             json.dumps(experience.output_data) if experience.output_data else None,
-            json.dumps(experience.metadata)
+            json.dumps(experience.metadata),
         )
-        
+
         return result["id"] if result else None
-    
-    async def _get_conversation_history(self, conversation_id: UUID, 
-                                       limit: int = None) -> List[Dict]:
+
+    async def _get_conversation_history(
+        self, conversation_id: UUID, limit: int = None
+    ) -> List[Dict]:
         """Get conversation history"""
         limit = limit or self.settings.max_conversation_history
-        
+
         query = """
             SELECT input_data, output_data, created_at
             FROM agent_experiences
             WHERE conversation_id = $1
             ORDER BY created_at DESC
             LIMIT $2
         """
-        
+
         results = await self.database.execute_query(query, conversation_id, limit)
         return list(reversed(results))  # Return in chronological order
-    
+
     async def get_user_conversations(self, user_id: UUID, limit: int = 20) -> List[Dict]:
         """Get distinct conversation IDs for a user"""
         query = """
             SELECT DISTINCT conversation_id, MAX(created_at) as last_message_at
             FROM agent_experiences
@@ -174,21 +177,21 @@
         """Background loop to process unprocessed experiences"""
         while True:
             try:
                 # Get unprocessed experiences
                 experiences = await self._get_unprocessed_experiences()
-                
+
                 if experiences:
                     await self.ml_pipeline.batch_process_experiences(experiences)
                     await self._mark_experiences_processed([exp["id"] for exp in experiences])
-                
+
                 await asyncio.sleep(60)  # Check every minute
-                
+
             except Exception as e:
                 logger.error(f"Experience processing loop failed: {e}")
                 await asyncio.sleep(300)  # Wait 5 minutes on error
-    
+
     async def _get_unprocessed_experiences(self) -> List[Dict]:
         """Retrieve unprocessed experiences from the database"""
         query = """
             SELECT id, user_id, conversation_id, input_data, output_data, feedback_score, metadata
             FROM agent_experiences
@@ -218,26 +221,28 @@
         """Background loop to update knowledge base"""
         while True:
             try:
                 await self.knowledge_manager.update_knowledge_scores()
                 await asyncio.sleep(self.settings.model_update_interval)  # Update every hour
-                
+
             except Exception as e:
                 logger.error(f"Knowledge update loop failed: {e}")
-                await asyncio.sleep(self.settings.model_update_interval / 2)  # Wait half interval on error
-    
+                await asyncio.sleep(
+                    self.settings.model_update_interval / 2
+                )  # Wait half interval on error
+
     async def _metrics_collection_loop(self):
         """Background loop to collect and store metrics"""
         while True:
             try:
                 await self._collect_system_metrics()
                 await asyncio.sleep(300)  # Collect every 5 minutes
-                
+
             except Exception as e:
                 logger.error(f"Metrics collection loop failed: {e}")
                 await asyncio.sleep(600)  # Wait 10 minutes on error
-    
+
     async def _collect_system_metrics(self):
         """Collect and store system metrics (placeholder)"""
         # In a real system, this would collect CPU, memory, network, etc.
         # and store them in the system_metrics table.
         logger.debug("Collecting system metrics...")
@@ -250,34 +255,33 @@
         """Check agent health"""
         try:
             # Basic checks
             if not self.is_initialized:
                 return False
-            
+
             # Check components
             ml_healthy = await self.ml_pipeline.health_check()
             knowledge_healthy = await self.knowledge_manager.health_check()
-            
+
             return ml_healthy and knowledge_healthy
-            
+
         except Exception as e:
             logger.error(f"Agent health check failed: {e}")
             return False
-    
+
     async def shutdown(self):
         """Shutdown agent gracefully"""
         logger.info("Shutting down agent...")
-        
+
         # Cancel background tasks
         for task in self.background_tasks:
             task.cancel()
             try:
                 await task
             except asyncio.CancelledError:
                 pass
-        
+
         # Shutdown components
         await self.ml_pipeline.shutdown()
         await self.knowledge_manager.shutdown()
-        
+
         logger.info("Agent shutdown complete")
-
would reformat /home/runner/work/ymera_y/ymera_y/agent.py
--- /home/runner/work/ymera_y/ymera_y/agent_communicator.py	2025-10-19 22:47:02.789432+00:00
+++ /home/runner/work/ymera_y/ymera_y/agent_communicator.py	2025-10-19 23:08:47.146219+00:00
@@ -15,59 +15,59 @@
 logger = structlog.get_logger(__name__)
 
 
 class AgentCommunicator:
     """Handles communication between agents"""
-    
+
     def __init__(self, db_manager: DatabaseManager, settings: Settings):
         self.db_manager = db_manager
         self.settings = settings
         self.logger = structlog.get_logger(__name__)
         self.message_queue = asyncio.Queue()
         self.subscriptions = {}
-    
+
     async def send_to_agent(self, agent_name: str, message: Dict[str, Any]) -> bool:
         """Send message to specific agent"""
         try:
             message_envelope = {
-                'to': agent_name,
-                'from': 'system',
-                'message': message,
-                'timestamp': datetime.utcnow().isoformat(),
-                'id': f"msg_{datetime.utcnow().timestamp()}"
+                "to": agent_name,
+                "from": "system",
+                "message": message,
+                "timestamp": datetime.utcnow().isoformat(),
+                "id": f"msg_{datetime.utcnow().timestamp()}",
             }
-            
+
             await self.message_queue.put(message_envelope)
-            
-            self.logger.info(f"Message sent to {agent_name}", message_id=message_envelope['id'])
+
+            self.logger.info(f"Message sent to {agent_name}", message_id=message_envelope["id"])
             return True
-            
+
         except Exception as e:
             self.logger.error(f"Failed to send message to {agent_name}: {e}", exc_info=True)
             return False
-    
+
     async def send_to_agent_manager(self, message: Dict[str, Any]) -> bool:
         """Send message to agent manager"""
-        return await self.send_to_agent('agent_manager', message)
-    
+        return await self.send_to_agent("agent_manager", message)
+
     async def broadcast(self, message: Dict[str, Any], exclude: list = None) -> int:
         """Broadcast message to all subscribed agents"""
         exclude = exclude or []
         sent_count = 0
-        
+
         for agent_name in self.subscriptions.keys():
             if agent_name not in exclude:
                 if await self.send_to_agent(agent_name, message):
                     sent_count += 1
-        
+
         return sent_count
-    
+
     async def subscribe(self, agent_name: str, callback):
         """Subscribe agent to receive messages"""
         self.subscriptions[agent_name] = callback
         self.logger.info(f"Agent {agent_name} subscribed to messages")
-    
+
     async def unsubscribe(self, agent_name: str):
         """Unsubscribe agent from messages"""
         if agent_name in self.subscriptions:
             del self.subscriptions[agent_name]
             self.logger.info(f"Agent {agent_name} unsubscribed from messages")
would reformat /home/runner/work/ymera_y/ymera_y/agent_communicator.py
--- /home/runner/work/ymera_y/ymera_y/agent_client.py	2025-10-19 22:47:02.789432+00:00
+++ /home/runner/work/ymera_y/ymera_y/agent_client.py	2025-10-19 23:08:47.564516+00:00
@@ -14,88 +14,92 @@
 import aiohttp
 from enum import Enum
 
 # Configure logging
 logging.basicConfig(
-    level=logging.INFO,
-    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
+    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
 )
 logger = logging.getLogger(__name__)
+
 
 class AgentStatus(str, Enum):
     INITIALIZING = "initializing"
     ACTIVE = "active"
     BUSY = "busy"
     ERROR = "error"
     SHUTTING_DOWN = "shutting_down"
 
+
 class AgentClient:
     """Agent client for connecting to YMERA Supreme Manager"""
-    
-    def __init__(self, manager_url: str, agent_id: str, api_key: str,
-                 cert_path: Optional[str] = None, key_path: Optional[str] = None,
-                 ca_path: Optional[str] = None):
-        self.manager_url = manager_url.rstrip('/')
+
+    def __init__(
+        self,
+        manager_url: str,
+        agent_id: str,
+        api_key: str,
+        cert_path: Optional[str] = None,
+        key_path: Optional[str] = None,
+        ca_path: Optional[str] = None,
+    ):
+        self.manager_url = manager_url.rstrip("/")
         self.agent_id = agent_id
         self.api_key = api_key
         self.ws_url = f"{manager_url.replace('http', 'ws')}/ws/agents/{agent_id}"
-        
+
         # SSL context for mTLS
         self.ssl_context = None
         if cert_path and key_path and ca_path:
             self.ssl_context = self._create_ssl_context(cert_path, key_path, ca_path)
-        
+
         # Internal state
         self.status = AgentStatus.INITIALIZING
         self.capabilities = []
         self.task_handlers = {}
         self.websocket = None
         self.heartbeat_interval = 30  # seconds
         self.report_interval = 60  # seconds
         self.reporting_schedule = None
         self.running = False
         self.reconnect_delay = 1  # seconds (with exponential backoff)
-        
+
         # Task queue
         self.task_queue = asyncio.Queue()
         self.active_tasks = {}
-    
+
     def _create_ssl_context(self, cert_path: str, key_path: str, ca_path: str) -> ssl.SSLContext:
         """Create SSL context for mTLS"""
-        context = ssl.create_default_context(
-            purpose=ssl.Purpose.SERVER_AUTH,
-            cafile=ca_path
-        )
+        context = ssl.create_default_context(purpose=ssl.Purpose.SERVER_AUTH, cafile=ca_path)
         context.load_cert_chain(certfile=cert_path, keyfile=key_path)
         context.check_hostname = True
         context.verify_mode = ssl.CERT_REQUIRED
         return context
-    
+
     def register_capability(self, capability: str, handler: Callable):
         """Register agent capability with handler"""
         self.capabilities.append(capability)
         self.task_handlers[capability] = handler
         logger.info(f"Registered capability: {capability}")
-    
+
     async def connect(self):
         """Connect to manager and start agent operation"""
         # First, verify HTTP API connection
         if not await self._verify_api_connection():
             logger.error("Failed to connect to manager API")
             return False
-        
+
         # Start agent operation
         self.running = True
-        
+
         # Start background tasks
         asyncio.create_task(self._websocket_handler())
         asyncio.create_task(self._heartbeat_sender())
         asyncio.create_task(self._status_reporter())
         asyncio.create_task(self._task_processor())
-        
+
         return True
-    
+
     async def _verify_api_connection(self) -> bool:
         """Verify connection to manager API"""
         try:
             # Check health endpoint
             async with aiohttp.ClientSession() as session:
@@ -106,321 +110,313 @@
                         return False
                     return True
         except Exception as e:
             logger.error(f"API connection failed: {e}")
             return False
-    
+
     async def _websocket_handler(self):
         """Handle WebSocket connection and messages"""
         while self.running:
             try:
                 # Connect to WebSocket
                 async with aiohttp.ClientSession() as session:
                     logger.info(f"Connecting to WebSocket: {self.ws_url}")
-                    
+
                     # Prepare headers with authentication
                     headers = {"Authorization": f"Bearer {self.api_key}"}
-                    
+
                     # Connect with proper SSL context if available
                     async with session.ws_connect(
                         self.ws_url,
                         headers=headers,
                         ssl_context=self.ssl_context,
                         heartbeat=30,  # 30 second heartbeat
-                        timeout=60
+                        timeout=60,
                     ) as websocket:
                         self.websocket = websocket
                         logger.info("WebSocket connection established")
                         self.reconnect_delay = 1  # Reset backoff on successful connection
-                        
+
                         # Process incoming messages
                         async for message in websocket:
                             if message.type == aiohttp.WSMsgType.TEXT:
                                 await self._handle_ws_message(message.data)
-                            elif message.type in (aiohttp.WSMsgType.CLOSED, aiohttp.WSMsgType.ERROR):
+                            elif message.type in (
+                                aiohttp.WSMsgType.CLOSED,
+                                aiohttp.WSMsgType.ERROR,
+                            ):
                                 logger.warning(f"WebSocket closed: {message.data}")
                                 break
-                        
+
                         self.websocket = None
-            
+
             except Exception as e:
                 logger.error(f"WebSocket error: {e}")
                 self.websocket = None
-                
+
                 # Exponential backoff for reconnection
                 await asyncio.sleep(self.reconnect_delay)
                 self.reconnect_delay = min(self.reconnect_delay * 2, 60)  # Max 60 seconds
-    
+
     async def _handle_ws_message(self, message_data: str):
         """Handle incoming WebSocket message"""
         try:
             message = json.loads(message_data)
             message_type = message.get("type", "")
-            
+
             logger.debug(f"Received message type: {message_type}")
-            
+
             if message_type == "connection_established":
                 # Handle connection established
                 if "reporting_schedule" in message:
                     self.reporting_schedule = message["reporting_schedule"]
                     self.report_interval = self.reporting_schedule.get("reporting_interval", 60)
                     self.heartbeat_interval = min(30, self.report_interval / 2)
                     logger.info(f"Updated reporting schedule: every {self.report_interval}s")
-                
+
                 self.status = AgentStatus.ACTIVE
-            
+
             elif message_type == "task":
                 # Queue task for processing
                 task_id = message.get("task_id")
                 task_type = message.get("task_type")
                 parameters = message.get("parameters", {})
-                
+
                 logger.info(f"Received task: {task_id} ({task_type})")
-                
+
                 if task_type in self.task_handlers:
-                    await self.task_queue.put({
-                        "task_id": task_id,
-                        "task_type": task_type,
-                        "parameters": parameters
-                    })
+                    await self.task_queue.put(
+                        {"task_id": task_id, "task_type": task_type, "parameters": parameters}
+                    )
                 else:
                     # Report inability to handle task
                     await self._send_task_error(
                         task_id,
                         "unsupported_task_type",
-                        f"Agent does not support task type: {task_type}"
+                        f"Agent does not support task type: {task_type}",
                     )
-            
+
             elif message_type == "terminate":
                 # Handle termination request
                 logger.warning(f"Received termination request: {message.get('reason')}")
                 self.status = AgentStatus.SHUTTING_DOWN
                 self.running = False
-            
+
             elif message_type == "receipt":
                 # Acknowledge message receipt
                 logger.debug(f"Receipt acknowledged for message: {message.get('message_id')}")
-            
+
         except Exception as e:
             logger.error(f"Error handling WebSocket message: {e}")
-    
+
     async def _heartbeat_sender(self):
         """Send periodic heartbeats"""
         while self.running:
             if self.websocket and self.status != AgentStatus.SHUTTING_DOWN:
                 try:
                     heartbeat = {
                         "type": "heartbeat",
                         "message_id": str(uuid.uuid4()),
                         "timestamp": datetime.utcnow().isoformat(),
-                        "status": self.status
+                        "status": self.status,
                     }
-                    
+
                     await self.websocket.send_json(heartbeat)
                     logger.debug("Heartbeat sent")
-                    
+
                 except Exception as e:
                     logger.error(f"Failed to send heartbeat: {e}")
-            
+
             await asyncio.sleep(self.heartbeat_interval)
-    
+
     async def _status_reporter(self):
         """Send periodic status reports"""
         while self.running:
             if self.websocket and self.status != AgentStatus.SHUTTING_DOWN:
                 try:
                     # Gather system metrics
                     metrics = await self._gather_metrics()
-                    
+
                     # Build report
                     report = {
                         "type": "status_report",
                         "message_id": str(uuid.uuid4()),
                         "timestamp": datetime.utcnow().isoformat(),
                         "report": {
                             "status": self.status,
                             "metrics": metrics,
                             "tasks_active": len(self.active_tasks),
-                            "queue_size": self.task_queue.qsize()
-                        }
+                            "queue_size": self.task_queue.qsize(),
+                        },
                     }
-                    
+
                     await self.websocket.send_json(report)
                     logger.debug("Status report sent")
-                    
+
                 except Exception as e:
                     logger.error(f"Failed to send status report: {e}")
-            
+
             # Use reporting interval from schedule if available
             await asyncio.sleep(self.report_interval)
-    
+
     async def _task_processor(self):
         """Process tasks from queue"""
         while self.running:
             try:
                 # Get task from queue
                 task = await self.task_queue.get()
                 task_id = task["task_id"]
                 task_type = task["task_type"]
                 parameters = task["parameters"]
-                
+
                 logger.info(f"Processing task: {task_id}")
                 self.status = AgentStatus.BUSY
                 self.active_tasks[task_id] = task
-                
+
                 # Handle task
                 handler = self.task_handlers.get(task_type)
                 if handler:
                     try:
                         # Execute task handler
                         result = await handler(parameters)
-                        
+
                         # Send success result
                         await self._send_task_result(task_id, result)
-                        
+
                     except Exception as e:
                         logger.error(f"Task {task_id} execution error: {e}")
-                        
+
                         # Send error result
-                        await self._send_task_error(
-                            task_id,
-                            "execution_error",
-                            str(e)
-                        )
+                        await self._send_task_error(task_id, "execution_error", str(e))
                 else:
                     await self._send_task_error(
-                        task_id,
-                        "no_handler",
-                        f"No handler for task type: {task_type}"
+                        task_id, "no_handler", f"No handler for task type: {task_type}"
                     )
-                
+
                 # Cleanup
                 if task_id in self.active_tasks:
                     del self.active_tasks[task_id]
-                
+
                 # Update status if no more active tasks
                 if not self.active_tasks:
                     self.status = AgentStatus.ACTIVE
-                
+
                 self.task_queue.task_done()
-                
+
             except asyncio.CancelledError:
                 break
-                
+
             except Exception as e:
                 logger.error(f"Task processing error: {e}")
                 await asyncio.sleep(1)
-    
+
     async def _send_task_result(self, task_id: str, result: Any):
         """Send task result to manager"""
         if self.websocket:
             try:
                 message = {
                     "type": "task_result",
                     "message_id": str(uuid.uuid4()),
                     "task_id": task_id,
                     "status": "completed",
                     "result": result,
-                    "timestamp": datetime.utcnow().isoformat()
+                    "timestamp": datetime.utcnow().isoformat(),
                 }
-                
+
                 await self.websocket.send_json(message)
                 logger.info(f"Task {task_id} result sent")
-                
+
             except Exception as e:
                 logger.error(f"Failed to send task result: {e}")
-    
+
     async def _send_task_error(self, task_id: str, error_type: str, error_message: str):
         """Send task error to manager"""
         if self.websocket:
             try:
                 message = {
                     "type": "task_result",
                     "message_id": str(uuid.uuid4()),
                     "task_id": task_id,
                     "status": "failed",
-                    "error": {
-                        "type": error_type,
-                        "message": error_message
-                    },
-                    "timestamp": datetime.utcnow().isoformat()
+                    "error": {"type": error_type, "message": error_message},
+                    "timestamp": datetime.utcnow().isoformat(),
                 }
-                
+
                 await self.websocket.send_json(message)
                 logger.info(f"Task {task_id} error sent: {error_type}")
-                
+
             except Exception as e:
                 logger.error(f"Failed to send task error: {e}")
-    
+
     async def report_error(self, error_type: str, error_message: str, severity: str = "medium"):
         """Report agent error to manager"""
         if self.websocket:
             try:
                 message = {
                     "type": "error",
                     "message_id": str(uuid.uuid4()),
                     "error_type": error_type,
                     "error_message": error_message,
                     "severity": severity,
-                    "timestamp": datetime.utcnow().isoformat()
+                    "timestamp": datetime.utcnow().isoformat(),
                 }
-                
+
                 await self.websocket.send_json(message)
                 logger.info(f"Error reported: {error_type}")
-                
+
                 if severity == "critical":
                     self.status = AgentStatus.ERROR
-                
+
             except Exception as e:
                 logger.error(f"Failed to send error report: {e}")
-    
+
     async def shutdown(self):
         """Gracefully shut down agent"""
         logger.info("Shutting down agent")
         self.status = AgentStatus.SHUTTING_DOWN
         self.running = False
-        
+
         # Wait for tasks to complete
         if self.active_tasks:
             try:
                 await asyncio.wait_for(self.task_queue.join(), timeout=30)
             except asyncio.TimeoutError:
                 logger.warning("Shutdown timed out waiting for tasks to complete")
-        
+
         # Close websocket
         if self.websocket:
             await self.websocket.close()
             self.websocket = None
-        
+
         logger.info("Agent shutdown complete")
-    
+
     async def _gather_metrics(self) -> Dict:
         """Gather system metrics for reporting"""
         metrics = {
             "cpu": psutil.cpu_percent(interval=None),
             "memory": psutil.virtual_memory().percent,
-            "disk": psutil.disk_usage('/').percent,
+            "disk": psutil.disk_usage("/").percent,
             "load": len(self.active_tasks) + self.task_queue.qsize(),
             "hostname": socket.gethostname(),
             "platform": platform.platform(),
-            "uptime": self._get_process_uptime()
+            "uptime": self._get_process_uptime(),
         }
-        
+
         # Additional metrics
         try:
             metrics["network"] = {
                 "connections": len(psutil.net_connections()),
                 "bytes_sent": psutil.net_io_counters().bytes_sent,
-                "bytes_recv": psutil.net_io_counters().bytes_recv
+                "bytes_recv": psutil.net_io_counters().bytes_recv,
             }
         except Exception:
             pass
-        
+
         return metrics
-    
+
     def _get_process_uptime(self) -> float:
         """Get process uptime in seconds"""
         try:
             process = psutil.Process(os.getpid())
             return datetime.now().timestamp() - process.create_time()
         except Exception:
-            return 0.0
\ No newline at end of file
+            return 0.0
would reformat /home/runner/work/ymera_y/ymera_y/agent_client.py
--- /home/runner/work/ymera_y/ymera_y/agent_discovery.py	2025-10-19 22:47:02.789432+00:00
+++ /home/runner/work/ymera_y/ymera_y/agent_discovery.py	2025-10-19 23:08:47.911212+00:00
@@ -14,20 +14,22 @@
 logger = structlog.get_logger(__name__)
 
 
 class DiscoveryStrategy(Enum):
     """Agent discovery strategies"""
+
     ROUND_ROBIN = "round_robin"
     LEAST_LOADED = "least_loaded"
     RANDOM = "random"
     HEALTH_WEIGHTED = "health_weighted"
     FASTEST_RESPONSE = "fastest_response"
 
 
 @dataclass
 class DiscoveryRequest:
     """Agent discovery request"""
+
     capability: str
     strategy: DiscoveryStrategy = DiscoveryStrategy.LEAST_LOADED
     exclude_agents: Optional[Set[str]] = None
     min_health_score: float = 0.5
     prefer_version: Optional[str] = None
@@ -35,248 +37,223 @@
 
 
 class AgentDiscovery:
     """
     Agent Discovery Service
-    
+
     Features:
     - Multiple discovery strategies
     - Health-aware selection
     - Version-based routing
     - Tag-based filtering
     - Load balancing
     - Circuit breaker integration
     """
-    
+
     def __init__(self, agent_registry: AgentRegistry):
         self.registry = agent_registry
         self._round_robin_index: Dict[str, int] = {}
-        
+
         logger.info("Agent Discovery initialized")
-    
-    async def discover_agent(
-        self,
-        request: DiscoveryRequest
-    ) -> Optional[AgentRecord]:
+
+    async def discover_agent(self, request: DiscoveryRequest) -> Optional[AgentRecord]:
         """
         Discover best agent for request
-        
+
         Args:
             request: Discovery request with criteria
-            
+
         Returns:
             Selected agent record or None
         """
         # Get all agents with the capability
         agents = await self.registry.get_agents_by_capability(
-            request.capability,
-            only_available=True
+            request.capability, only_available=True
         )
-        
+
         if not agents:
             logger.warning(f"No agents found with capability: {request.capability}")
             return None
-        
+
         # Apply filters
         agents = self._apply_filters(agents, request)
-        
+
         if not agents:
             logger.warning(f"No agents match criteria for capability: {request.capability}")
             return None
-        
+
         # Apply selection strategy
         selected = await self._apply_strategy(agents, request)
-        
+
         if selected:
             logger.info(
                 f"Agent discovered",
                 agent_id=selected.agent_id,
                 capability=request.capability,
-                strategy=request.strategy.value
+                strategy=request.strategy.value,
             )
-        
+
         return selected
-    
-    async def discover_agents(
-        self,
-        request: DiscoveryRequest,
-        count: int = 1
-    ) -> List[AgentRecord]:
+
+    async def discover_agents(self, request: DiscoveryRequest, count: int = 1) -> List[AgentRecord]:
         """
         Discover multiple agents
-        
+
         Args:
             request: Discovery request
             count: Number of agents to discover
-            
+
         Returns:
             List of selected agents
         """
         agents = await self.registry.get_agents_by_capability(
-            request.capability,
-            only_available=True
+            request.capability, only_available=True
         )
-        
+
         agents = self._apply_filters(agents, request)
-        
+
         if request.strategy == DiscoveryStrategy.ROUND_ROBIN:
             return self._round_robin_selection(agents, request.capability, count)
         elif request.strategy == DiscoveryStrategy.LEAST_LOADED:
             return agents[:count]  # Already sorted by load
         elif request.strategy == DiscoveryStrategy.HEALTH_WEIGHTED:
             return self._health_weighted_selection(agents, count)
         elif request.strategy == DiscoveryStrategy.FASTEST_RESPONSE:
             return self._fastest_response_selection(agents, count)
         else:
             import random
+
             return random.sample(agents, min(count, len(agents)))
-    
+
     def _apply_filters(
-        self,
-        agents: List[AgentRecord],
-        request: DiscoveryRequest
+        self, agents: List[AgentRecord], request: DiscoveryRequest
     ) -> List[AgentRecord]:
         """Apply filters to agent list"""
         filtered = agents
-        
+
         # Exclude specific agents
         if request.exclude_agents:
             filtered = [a for a in filtered if a.agent_id not in request.exclude_agents]
-        
+
         # Min health score
         filtered = [a for a in filtered if a.health_score >= request.min_health_score]
-        
+
         # Prefer version
         if request.prefer_version:
             versioned = [a for a in filtered if a.version == request.prefer_version]
             if versioned:
                 filtered = versioned
-        
+
         # Tag matching
         if request.tags:
             filtered = [
-                a for a in filtered
-                if all(a.tags.get(k) == v for k, v in request.tags.items())
+                a for a in filtered if all(a.tags.get(k) == v for k, v in request.tags.items())
             ]
-        
+
         return filtered
-    
+
     async def _apply_strategy(
-        self,
-        agents: List[AgentRecord],
-        request: DiscoveryRequest
+        self, agents: List[AgentRecord], request: DiscoveryRequest
     ) -> Optional[AgentRecord]:
         """Apply selection strategy"""
         if not agents:
             return None
-        
+
         if request.strategy == DiscoveryStrategy.ROUND_ROBIN:
             return self._round_robin_selection(agents, request.capability, 1)[0]
         elif request.strategy == DiscoveryStrategy.LEAST_LOADED:
             return agents[0]  # Already sorted by load
         elif request.strategy == DiscoveryStrategy.HEALTH_WEIGHTED:
             return self._health_weighted_selection(agents, 1)[0]
         elif request.strategy == DiscoveryStrategy.FASTEST_RESPONSE:
             return self._fastest_response_selection(agents, 1)[0]
         else:
             import random
+
             return random.choice(agents)
-    
+
     def _round_robin_selection(
-        self,
-        agents: List[AgentRecord],
-        capability: str,
-        count: int
+        self, agents: List[AgentRecord], capability: str, count: int
     ) -> List[AgentRecord]:
         """Round-robin selection"""
         if not agents:
             return []
-        
+
         if capability not in self._round_robin_index:
             self._round_robin_index[capability] = 0
-        
+
         selected = []
         for _ in range(min(count, len(agents))):
             idx = self._round_robin_index[capability] % len(agents)
             selected.append(agents[idx])
             self._round_robin_index[capability] += 1
-        
+
         return selected
-    
+
     def _health_weighted_selection(
-        self,
-        agents: List[AgentRecord],
-        count: int
+        self, agents: List[AgentRecord], count: int
     ) -> List[AgentRecord]:
         """Health-weighted selection"""
         if not agents:
             return []
-        
+
         # Sort by health score descending, then by load ascending
-        sorted_agents = sorted(
-            agents,
-            key=lambda a: (-a.health_score, a.current_load / a.max_load)
-        )
-        
+        sorted_agents = sorted(agents, key=lambda a: (-a.health_score, a.current_load / a.max_load))
+
         return sorted_agents[:count]
-    
+
     def _fastest_response_selection(
-        self,
-        agents: List[AgentRecord],
-        count: int
+        self, agents: List[AgentRecord], count: int
     ) -> List[AgentRecord]:
         """Fastest response time selection"""
         if not agents:
             return []
-        
+
         # Sort by average response time
         sorted_agents = sorted(
-            agents,
-            key=lambda a: (a.average_response_time_ms, a.current_load / a.max_load)
+            agents, key=lambda a: (a.average_response_time_ms, a.current_load / a.max_load)
         )
-        
+
         return sorted_agents[:count]
-    
+
     async def health_check_agent(self, agent_id: str) -> bool:
         """Check if specific agent is healthy"""
         agent = await self.registry.get_agent(agent_id)
         return agent is not None and agent.is_healthy
-    
+
     async def get_agent_address(self, agent_id: str) -> Optional[str]:
         """Get agent network address"""
         agent = await self.registry.get_agent(agent_id)
         if agent:
             return f"http://{agent.host}:{agent.port}"
         return None
-    
+
     async def get_available_capabilities(self) -> List[str]:
         """Get list of all available capabilities"""
         agents = await self.registry.get_all_agents(only_healthy=True)
         capabilities = set()
         for agent in agents:
             capabilities.update(agent.capabilities)
         return sorted(list(capabilities))
-    
+
     async def get_agents_for_workflow(
-        self,
-        required_capabilities: List[str]
+        self, required_capabilities: List[str]
     ) -> Dict[str, Optional[AgentRecord]]:
         """
         Get agents for all required capabilities
-        
+
         Args:
             required_capabilities: List of required capabilities
-            
+
         Returns:
             Dict mapping capability to selected agent
         """
         result = {}
-        
+
         for capability in required_capabilities:
             request = DiscoveryRequest(
-                capability=capability,
-                strategy=DiscoveryStrategy.LEAST_LOADED
+                capability=capability, strategy=DiscoveryStrategy.LEAST_LOADED
             )
             agent = await self.discover_agent(request)
             result[capability] = agent
-        
+
         return result
would reformat /home/runner/work/ymera_y/ymera_y/agent_discovery.py
--- /home/runner/work/ymera_y/ymera_y/agent_coordinator.py	2025-10-19 22:47:02.789432+00:00
+++ /home/runner/work/ymera_y/ymera_y/agent_coordinator.py	2025-10-19 23:08:48.195234+00:00
@@ -21,12 +21,14 @@
 
 # ============================================================================
 # MODELS & ENUMS
 # ============================================================================
 
+
 class ActionType(str, Enum):
     """Types of actions that can be performed"""
+
     ENHANCE = "enhance"
     DEBUG = "debug"
     REVIEW = "review"
     TEST = "test"
     DOCUMENT = "document"
@@ -37,18 +39,20 @@
     ANALYZE = "analyze"
 
 
 class Priority(str, Enum):
     """Task priority levels"""
+
     LOW = "low"
     MEDIUM = "medium"
     HIGH = "high"
     CRITICAL = "critical"
 
 
 class WorkflowStatus(str, Enum):
     """Workflow execution status"""
+
     PENDING = "pending"
     IN_PROGRESS = "in_progress"
     VALIDATION = "validation"
     INTEGRATION = "integration"
     COMPLETED = "completed"
@@ -56,27 +60,29 @@
     CANCELLED = "cancelled"
 
 
 class UserRequest(BaseModel):
     """User request model"""
+
     request_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
     user_id: str
     message: str = Field(..., min_length=1)
     files: List[Dict[str, Any]] = Field(default_factory=list)
     context: Dict[str, Any] = Field(default_factory=dict)
     priority: Priority = Priority.MEDIUM
-    
-    @validator('files')
+
+    @validator("files")
     def validate_files(cls, v):
         for file in v:
-            if 'name' not in file or 'content' not in file:
+            if "name" not in file or "content" not in file:
                 raise ValueError("Each file must have 'name' and 'content'")
         return v
 
 
 class AgentTask(BaseModel):
     """Task assigned to an agent"""
+
     task_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
     agent_id: str
     action_type: ActionType
     input_data: Dict[str, Any]
     dependencies: List[str] = Field(default_factory=list)
@@ -84,10 +90,11 @@
     timeout_seconds: int = 3600
 
 
 class WorkflowPlan(BaseModel):
     """Planned workflow execution"""
+
     workflow_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
     request_id: str
     actions: List[ActionType]
     task_sequence: List[AgentTask]
     estimated_duration: int
@@ -96,87 +103,75 @@
 
 # ============================================================================
 # AGENT COORDINATOR
 # ============================================================================
 
+
 class AgentCoordinator:
     """
     Coordinates between users, agents, and the agent manager.
-    
+
     Responsibilities:
     - Parse and understand user requests
     - Recommend appropriate actions
     - Create workflow plans
     - Dispatch tasks to appropriate agents
     - Collect and integrate results
     - Communicate status to users
     """
-    
-    def __init__(
-        self,
-        agent_manager,
-        db_session: AsyncSession,
-        config: Dict[str, Any]
-    ):
+
+    def __init__(self, agent_manager, db_session: AsyncSession, config: Dict[str, Any]):
         self.agent_manager = agent_manager
         self.db = db_session
         self.config = config
-        
+
         # Agent type mapping
         self.agent_capabilities = {
             "CODING": ["implement", "develop", "code", "build"],
             "ENHANCEMENT": ["improve", "enhance", "optimize", "refactor"],
             "EXAMINATION": ["review", "analyze", "inspect", "validate"],
             "TESTING": ["test", "verify", "qa", "check"],
             "DOCUMENTATION": ["document", "explain", "describe"],
             "SECURITY": ["secure", "scan", "audit", "protect"],
             "DEPLOYMENT": ["deploy", "release", "publish"],
             "LEARNING": ["learn", "understand", "pattern"],
-            "PROJECT": ["integrate", "assemble", "build", "construct"]
+            "PROJECT": ["integrate", "assemble", "build", "construct"],
         }
-        
+
         # Active workflows
         self.active_workflows: Dict[str, Dict[str, Any]] = {}
-        
+
         # Task results cache
         self.task_results: Dict[str, Any] = {}
-        
+
         logger.info("Agent Coordinator initialized")
-    
+
     # ========================================================================
     # REQUEST PROCESSING
     # ========================================================================
-    
-    async def process_user_request(
-        self,
-        request: UserRequest
-    ) -> Dict[str, Any]:
+
+    async def process_user_request(self, request: UserRequest) -> Dict[str, Any]:
         """
         Process user request and create workflow plan
-        
+
         Args:
             request: User request with files and context
-            
+
         Returns:
             Workflow plan and execution result
         """
         try:
             logger.info(
-                "Processing user request",
-                request_id=request.request_id,
-                user_id=request.user_id
-            )
-            
+                "Processing user request", request_id=request.request_id, user_id=request.user_id
+            )
+
             # 1. Analyze request and identify intent
             intent = await self._analyze_intent(request)
-            
+
             # 2. Recommend actions based on intent
-            recommended_actions = await self._recommend_actions(
-                request,
-                intent
-            )
-            
+            recommended_actions = await self._recommend_actions(request, intent)
+
             # 3. Get user confirmation (or auto-approve for simple requests)
             if request.priority in [Priority.LOW, Priority.MEDIUM]:
                 approved_actions = recommended_actions
             else:
                 # For high priority, return recommendations for approval
@@ -184,53 +179,41 @@
                     "status": "pending_approval",
                     "request_id": request.request_id,
                     "intent": intent,
                     "recommended_actions": [a.value for a in recommended_actions],
                     "estimated_duration": self._estimate_duration(recommended_actions),
-                    "message": "Please confirm the recommended actions"
+                    "message": "Please confirm the recommended actions",
                 }
-            
+
             # 4. Create workflow plan
-            workflow_plan = await self._create_workflow_plan(
-                request,
-                approved_actions
-            )
-            
+            workflow_plan = await self._create_workflow_plan(request, approved_actions)
+
             # 5. Execute workflow
             result = await self._execute_workflow(workflow_plan)
-            
+
             return {
                 "status": "completed",
                 "request_id": request.request_id,
                 "workflow_id": workflow_plan.workflow_id,
-                "result": result
+                "result": result,
             }
-            
+
         except Exception as e:
-            logger.error(
-                "Failed to process request",
-                request_id=request.request_id,
-                error=str(e)
-            )
+            logger.error("Failed to process request", request_id=request.request_id, error=str(e))
             raise
-    
+
     async def _analyze_intent(self, request: UserRequest) -> Dict[str, Any]:
         """
         Analyze user message to understand intent
-        
+
         Uses keyword matching and context analysis
         Could be enhanced with NLP/LLM
         """
         message_lower = request.message.lower()
-        
-        intent = {
-            "primary_action": None,
-            "confidence": 0.0,
-            "entities": [],
-            "context": {}
-        }
-        
+
+        intent = {"primary_action": None, "confidence": 0.0, "entities": [], "context": {}}
+
         # Keyword-based intent detection
         action_keywords = {
             ActionType.ENHANCE: ["improve", "enhance", "better", "upgrade", "optimize"],
             ActionType.DEBUG: ["fix", "bug", "error", "issue", "debug", "problem"],
             ActionType.REVIEW: ["review", "check", "examine", "look at", "analyze"],
@@ -238,155 +221,139 @@
             ActionType.DOCUMENT: ["document", "explain", "describe", "comment"],
             ActionType.REFACTOR: ["refactor", "clean", "restructure", "reorganize"],
             ActionType.SECURITY_SCAN: ["security", "secure", "vulnerabilities", "scan"],
             ActionType.DEPLOY: ["deploy", "release", "publish", "production"],
         }
-        
+
         # Find matching actions
         matches = []
         for action, keywords in action_keywords.items():
             for keyword in keywords:
                 if keyword in message_lower:
                     matches.append(action)
                     break
-        
+
         if matches:
             intent["primary_action"] = matches[0]
             intent["confidence"] = 0.8 if len(matches) == 1 else 0.6
             intent["entities"] = matches
-        
+
         # Analyze files if provided
         if request.files:
             file_types = set()
             for file in request.files:
-                ext = file.get('name', '').split('.')[-1].lower()
+                ext = file.get("name", "").split(".")[-1].lower()
                 file_types.add(ext)
-            
+
             intent["context"]["file_types"] = list(file_types)
             intent["context"]["file_count"] = len(request.files)
-        
+
         return intent
-    
+
     async def _recommend_actions(
-        self,
-        request: UserRequest,
-        intent: Dict[str, Any]
+        self, request: UserRequest, intent: Dict[str, Any]
     ) -> List[ActionType]:
         """
         Recommend appropriate actions based on intent
         """
         recommended = []
-        
+
         primary_action = intent.get("primary_action")
-        
+
         if primary_action:
             recommended.append(primary_action)
-        
+
         # Add supporting actions based on primary action
         if primary_action == ActionType.ENHANCE:
             # Enhancement workflow: review -> enhance -> test
-            recommended.extend([
-                ActionType.REVIEW,
-                ActionType.ENHANCE,
-                ActionType.TEST
-            ])
-        
+            recommended.extend([ActionType.REVIEW, ActionType.ENHANCE, ActionType.TEST])
+
         elif primary_action == ActionType.DEBUG:
             # Debug workflow: analyze -> fix -> test
-            recommended.extend([
-                ActionType.ANALYZE,
-                ActionType.DEBUG,
-                ActionType.TEST
-            ])
-        
+            recommended.extend([ActionType.ANALYZE, ActionType.DEBUG, ActionType.TEST])
+
         elif primary_action == ActionType.DEPLOY:
             # Deployment workflow: test -> security scan -> deploy
-            recommended.extend([
-                ActionType.TEST,
-                ActionType.SECURITY_SCAN,
-                ActionType.DEPLOY
-            ])
-        
+            recommended.extend([ActionType.TEST, ActionType.SECURITY_SCAN, ActionType.DEPLOY])
+
         # Always include review for code changes
         if request.files and ActionType.REVIEW not in recommended:
             recommended.insert(0, ActionType.REVIEW)
-        
+
         # Remove duplicates while preserving order
         seen = set()
         unique_actions = []
         for action in recommended:
             if action not in seen:
                 seen.add(action)
                 unique_actions.append(action)
-        
+
         return unique_actions
-    
+
     async def _create_workflow_plan(
-        self,
-        request: UserRequest,
-        actions: List[ActionType]
+        self, request: UserRequest, actions: List[ActionType]
     ) -> WorkflowPlan:
         """
         Create detailed workflow execution plan
         """
         tasks = []
         previous_task_ids = []
-        
+
         for idx, action in enumerate(actions):
             # Determine which agent type should handle this action
             agent_type = self._get_agent_for_action(action)
-            
+
             # Find available agent of this type
             agent_id = await self._find_available_agent(agent_type)
-            
+
             if not agent_id:
                 raise Exception(f"No available agent for action: {action}")
-            
+
             # Create task
             task = AgentTask(
                 agent_id=agent_id,
                 action_type=action,
                 input_data={
                     "request_id": request.request_id,
                     "files": request.files,
                     "action": action.value,
                     "context": request.context,
-                    "user_message": request.message
+                    "user_message": request.message,
                 },
                 dependencies=previous_task_ids.copy(),
-                priority=request.priority
-            )
-            
+                priority=request.priority,
+            )
+
             tasks.append(task)
             previous_task_ids = [task.task_id]
-        
+
         # Add final integration task to project agent
         project_agent = await self._find_available_agent("PROJECT")
         if project_agent:
             integration_task = AgentTask(
                 agent_id=project_agent,
                 action_type=ActionType.ANALYZE,  # Project agent integrates
                 input_data={
                     "request_id": request.request_id,
                     "action": "integrate",
-                    "previous_tasks": [t.task_id for t in tasks]
+                    "previous_tasks": [t.task_id for t in tasks],
                 },
                 dependencies=[tasks[-1].task_id] if tasks else [],
-                priority=request.priority
+                priority=request.priority,
             )
             tasks.append(integration_task)
-        
+
         workflow = WorkflowPlan(
             request_id=request.request_id,
             actions=actions,
             task_sequence=tasks,
             estimated_duration=self._estimate_duration(actions),
-            requires_validation=True
+            requires_validation=True,
         )
-        
+
         return workflow
-    
+
     def _get_agent_for_action(self, action: ActionType) -> str:
         """Map action to agent type"""
         action_to_agent = {
             ActionType.ENHANCE: "ENHANCEMENT",
             ActionType.DEBUG: "CODING",
@@ -395,28 +362,28 @@
             ActionType.DOCUMENT: "DOCUMENTATION",
             ActionType.REFACTOR: "ENHANCEMENT",
             ActionType.OPTIMIZE: "ENHANCEMENT",
             ActionType.SECURITY_SCAN: "SECURITY",
             ActionType.DEPLOY: "DEPLOYMENT",
-            ActionType.ANALYZE: "EXAMINATION"
+            ActionType.ANALYZE: "EXAMINATION",
         }
-        
+
         return action_to_agent.get(action, "CODING")
-    
+
     async def _find_available_agent(self, agent_type: str) -> Optional[str]:
         """Find available agent of specified type"""
         try:
             # Query agent manager for available agents
             # This is a simplified version - in production, implement proper agent selection
-            
+
             # For now, return a mock agent ID
             return f"{agent_type.lower()}_agent_001"
-            
+
         except Exception as e:
             logger.error(f"Failed to find agent", agent_type=agent_type, error=str(e))
             return None
-    
+
     def _estimate_duration(self, actions: List[ActionType]) -> int:
         """Estimate workflow duration in seconds"""
         # Base durations per action type
         durations = {
             ActionType.REVIEW: 300,
@@ -426,500 +393,430 @@
             ActionType.TEST: 600,
             ActionType.DOCUMENT: 300,
             ActionType.REFACTOR: 1200,
             ActionType.SECURITY_SCAN: 300,
             ActionType.DEPLOY: 600,
-            ActionType.OPTIMIZE: 900
+            ActionType.OPTIMIZE: 900,
         }
-        
+
         total = sum(durations.get(action, 600) for action in actions)
         return total
-    
+
     # ========================================================================
     # WORKFLOW EXECUTION
     # ========================================================================
-    
-    async def _execute_workflow(
-        self,
-        workflow: WorkflowPlan
-    ) -> Dict[str, Any]:
+
+    async def _execute_workflow(self, workflow: WorkflowPlan) -> Dict[str, Any]:
         """
         Execute workflow by dispatching tasks to agents
         """
         try:
             logger.info(
                 "Executing workflow",
                 workflow_id=workflow.workflow_id,
-                task_count=len(workflow.task_sequence)
-            )
-            
+                task_count=len(workflow.task_sequence),
+            )
+
             # Store workflow
             self.active_workflows[workflow.workflow_id] = {
                 "workflow": workflow,
                 "status": WorkflowStatus.IN_PROGRESS,
                 "started_at": datetime.utcnow(),
                 "completed_tasks": [],
-                "results": {}
+                "results": {},
             }
-            
+
             # Execute tasks in sequence (respecting dependencies)
             for task in workflow.task_sequence:
                 # Wait for dependencies to complete
                 if task.dependencies:
-                    await self._wait_for_dependencies(
-                        workflow.workflow_id,
-                        task.dependencies
-                    )
-                
+                    await self._wait_for_dependencies(workflow.workflow_id, task.dependencies)
+
                 # Execute task
                 task_result = await self._execute_task(task)
-                
+
                 # Store result
                 workflow_state = self.active_workflows[workflow.workflow_id]
                 workflow_state["completed_tasks"].append(task.task_id)
                 workflow_state["results"][task.task_id] = task_result
-                
+
                 # Check if validation is needed
                 if task.action_type in [ActionType.ENHANCE, ActionType.DEBUG, ActionType.REFACTOR]:
                     validation_result = await self._validate_output(task_result)
                     if not validation_result["valid"]:
                         logger.warning(
                             "Task output validation failed",
                             task_id=task.task_id,
-                            issues=validation_result.get("issues")
+                            issues=validation_result.get("issues"),
                         )
                         # Optionally retry or handle validation failure
-            
+
             # Update workflow status
             workflow_state = self.active_workflows[workflow.workflow_id]
             workflow_state["status"] = WorkflowStatus.VALIDATION
-            
+
             # Final validation
             final_result = await self._finalize_workflow(workflow.workflow_id)
-            
+
             return final_result
-            
+
         except Exception as e:
             logger.error(
-                "Workflow execution failed",
-                workflow_id=workflow.workflow_id,
-                error=str(e)
-            )
-            
+                "Workflow execution failed", workflow_id=workflow.workflow_id, error=str(e)
+            )
+
             # Update workflow status
             if workflow.workflow_id in self.active_workflows:
                 self.active_workflows[workflow.workflow_id]["status"] = WorkflowStatus.FAILED
                 self.active_workflows[workflow.workflow_id]["error"] = str(e)
-            
+
             raise
-    
+
     async def _execute_task(self, task: AgentTask) -> Dict[str, Any]:
         """
         Execute a single task by dispatching to agent
         """
         try:
             logger.info(
                 "Executing task",
                 task_id=task.task_id,
                 agent_id=task.agent_id,
-                action=task.action_type
-            )
-            
+                action=task.action_type,
+            )
+
             # Dispatch task to agent manager
             result = await self.agent_manager.assign_task(
                 agent_id=task.agent_id,
                 task_type=task.action_type.value,
                 task_data=task.input_data,
                 priority=self._priority_to_int(task.priority),
-                deadline=datetime.utcnow() + timedelta(seconds=task.timeout_seconds)
-            )
-            
+                deadline=datetime.utcnow() + timedelta(seconds=task.timeout_seconds),
+            )
+
             # Wait for task completion (with timeout)
             task_result = await self._wait_for_task_completion(
-                task.task_id,
-                timeout=task.timeout_seconds
-            )
-            
+                task.task_id, timeout=task.timeout_seconds
+            )
+
             return task_result
-            
+
         except Exception as e:
-            logger.error(
-                "Task execution failed",
-                task_id=task.task_id,
-                error=str(e)
-            )
+            logger.error("Task execution failed", task_id=task.task_id, error=str(e))
             raise
-    
-    async def _wait_for_dependencies(
-        self,
-        workflow_id: str,
-        dependencies: List[str]
-    ):
+
+    async def _wait_for_dependencies(self, workflow_id: str, dependencies: List[str]):
         """Wait for dependent tasks to complete"""
         workflow_state = self.active_workflows.get(workflow_id)
         if not workflow_state:
             raise ValueError(f"Workflow {workflow_id} not found")
-        
+
         max_wait = 3600  # 1 hour max
         start_time = time.time()
-        
+
         while True:
             completed = workflow_state["completed_tasks"]
             if all(dep in completed for dep in dependencies):
                 return
-            
+
             if time.time() - start_time > max_wait:
-                raise TimeoutError(
-                    f"Dependencies not completed within {max_wait}s"
-                )
-            
+                raise TimeoutError(f"Dependencies not completed within {max_wait}s")
+
             await asyncio.sleep(5)
-    
-    async def _wait_for_task_completion(
-        self,
-        task_id: str,
-        timeout: int
-    ) -> Dict[str, Any]:
+
+    async def _wait_for_task_completion(self, task_id: str, timeout: int) -> Dict[str, Any]:
         """
         Wait for task completion and retrieve result
-        
+
         In production, this would listen to message broker for task completion events
         """
         # Simulate task execution (replace with actual message broker subscription)
         await asyncio.sleep(2)  # Simulate work
-        
+
         # Mock result for demonstration
         return {
             "task_id": task_id,
             "status": "completed",
             "output": {
                 "files_modified": [],
                 "summary": "Task completed successfully",
-                "metrics": {}
+                "metrics": {},
             },
-            "completed_at": datetime.utcnow().isoformat()
+            "completed_at": datetime.utcnow().isoformat(),
         }
-    
+
     async def _validate_output(self, task_result: Dict[str, Any]) -> Dict[str, Any]:
         """
         Validate task output
-        
+
         This would dispatch to an examination agent for validation
         """
         # Check if output meets basic criteria
-        validation = {
-            "valid": True,
-            "issues": []
-        }
-        
+        validation = {"valid": True, "issues": []}
+
         output = task_result.get("output", {})
-        
+
         # Basic validations
         if not output:
             validation["valid"] = False
             validation["issues"].append("Empty output")
-        
+
         if task_result.get("status") != "completed":
             validation["valid"] = False
             validation["issues"].append("Task not completed successfully")
-        
+
         return validation
-    
-    async def _finalize_workflow(
-        self,
-        workflow_id: str
-    ) -> Dict[str, Any]:
+
+    async def _finalize_workflow(self, workflow_id: str) -> Dict[str, Any]:
         """
         Finalize workflow and prepare final result
         """
         workflow_state = self.active_workflows.get(workflow_id)
         if not workflow_state:
             raise ValueError(f"Workflow {workflow_id} not found")
-        
+
         # Collect all results
         all_results = workflow_state["results"]
-        
+
         # Integration step - send to project agent
         workflow = workflow_state["workflow"]
         final_task_id = workflow.task_sequence[-1].task_id if workflow.task_sequence else None
-        
+
         if final_task_id and final_task_id in all_results:
             integrated_result = all_results[final_task_id]
         else:
             integrated_result = {
                 "status": "completed",
-                "output": {
-                    "files": [],
-                    "summary": "Workflow completed"
-                }
+                "output": {"files": [], "summary": "Workflow completed"},
             }
-        
+
         # Update workflow status
         workflow_state["status"] = WorkflowStatus.COMPLETED
         workflow_state["completed_at"] = datetime.utcnow()
-        
+
         return {
             "workflow_id": workflow_id,
             "status": "completed",
             "result": integrated_result,
             "task_results": all_results,
             "duration": (
                 workflow_state["completed_at"] - workflow_state["started_at"]
             ).total_seconds(),
-            "timestamp": datetime.utcnow().isoformat()
+            "timestamp": datetime.utcnow().isoformat(),
         }
-    
+
     def _priority_to_int(self, priority: Priority) -> int:
         """Convert priority enum to integer"""
-        mapping = {
-            Priority.LOW: 3,
-            Priority.MEDIUM: 5,
-            Priority.HIGH: 7,
-            Priority.CRITICAL: 10
-        }
+        mapping = {Priority.LOW: 3, Priority.MEDIUM: 5, Priority.HIGH: 7, Priority.CRITICAL: 10}
         return mapping.get(priority, 5)
-    
+
     # ========================================================================
     # USER COMMUNICATION
     # ========================================================================
-    
-    async def get_workflow_status(
-        self,
-        workflow_id: str
-    ) -> Dict[str, Any]:
+
+    async def get_workflow_status(self, workflow_id: str) -> Dict[str, Any]:
         """Get current workflow status for user"""
         workflow_state = self.active_workflows.get(workflow_id)
-        
+
         if not workflow_state:
             return {
                 "workflow_id": workflow_id,
                 "status": "not_found",
-                "message": "Workflow not found"
+                "message": "Workflow not found",
             }
-        
+
         workflow = workflow_state["workflow"]
         total_tasks = len(workflow.task_sequence)
         completed = len(workflow_state["completed_tasks"])
-        
+
         return {
             "workflow_id": workflow_id,
             "status": workflow_state["status"].value,
             "progress": {
                 "completed_tasks": completed,
                 "total_tasks": total_tasks,
-                "percentage": round((completed / total_tasks * 100), 2) if total_tasks > 0 else 0
+                "percentage": round((completed / total_tasks * 100), 2) if total_tasks > 0 else 0,
             },
             "started_at": workflow_state["started_at"].isoformat(),
             "estimated_completion": (
-                workflow_state["started_at"] + 
-                timedelta(seconds=workflow.estimated_duration)
+                workflow_state["started_at"] + timedelta(seconds=workflow.estimated_duration)
             ).isoformat(),
-            "current_action": workflow.task_sequence[completed].action_type.value if completed < total_tasks else "finalizing"
+            "current_action": (
+                workflow.task_sequence[completed].action_type.value
+                if completed < total_tasks
+                else "finalizing"
+            ),
         }
-    
+
     async def send_user_update(
-        self,
-        user_id: str,
-        workflow_id: str,
-        message: str,
-        data: Optional[Dict[str, Any]] = None
+        self, user_id: str, workflow_id: str, message: str, data: Optional[Dict[str, Any]] = None
     ):
         """Send status update to user"""
         update = {
             "type": "workflow_update",
             "workflow_id": workflow_id,
             "message": message,
             "data": data or {},
-            "timestamp": datetime.utcnow().isoformat()
+            "timestamp": datetime.utcnow().isoformat(),
         }
-        
+
         # In production, send via WebSocket, SSE, or message broker
         logger.info(
-            "Sending user update",
-            user_id=user_id,
-            workflow_id=workflow_id,
-            message=message
+            "Sending user update", user_id=user_id, workflow_id=workflow_id, message=message
         )
-        
+
         # Mock implementation - replace with actual notification system
         # await self.notification_service.send(user_id, update)
-    
+
     # ========================================================================
     # APPROVAL WORKFLOW
     # ========================================================================
-    
+
     async def approve_workflow(
-        self,
-        request_id: str,
-        approved_actions: List[ActionType],
-        user_id: str
+        self, request_id: str, approved_actions: List[ActionType], user_id: str
     ) -> Dict[str, Any]:
         """
         User approves recommended actions and executes workflow
         """
         try:
             logger.info(
                 "Workflow approved",
                 request_id=request_id,
                 user_id=user_id,
-                actions=approved_actions
-            )
-            
+                actions=approved_actions,
+            )
+
             # Retrieve original request (from cache/database)
             # For now, create a mock request
             request = UserRequest(
                 request_id=request_id,
                 user_id=user_id,
                 message="Approved workflow execution",
-                priority=Priority.HIGH
-            )
-            
+                priority=Priority.HIGH,
+            )
+
             # Create workflow plan
-            workflow_plan = await self._create_workflow_plan(
-                request,
-                approved_actions
-            )
-            
+            workflow_plan = await self._create_workflow_plan(request, approved_actions)
+
             # Execute workflow
             result = await self._execute_workflow(workflow_plan)
-            
+
             return result
-            
+
         except Exception as e:
-            logger.error(
-                "Failed to execute approved workflow",
-                request_id=request_id,
-                error=str(e)
-            )
+            logger.error("Failed to execute approved workflow", request_id=request_id, error=str(e))
             raise
-    
-    async def reject_workflow(
-        self,
-        request_id: str,
-        reason: str,
-        user_id: str
-    ) -> Dict[str, Any]:
+
+    async def reject_workflow(self, request_id: str, reason: str, user_id: str) -> Dict[str, Any]:
         """User rejects recommended workflow"""
-        logger.info(
-            "Workflow rejected",
-            request_id=request_id,
-            user_id=user_id,
-            reason=reason
-        )
-        
+        logger.info("Workflow rejected", request_id=request_id, user_id=user_id, reason=reason)
+
         return {
             "status": "rejected",
             "request_id": request_id,
             "reason": reason,
-            "message": "Workflow cancelled by user"
+            "message": "Workflow cancelled by user",
         }
 
 
 # ============================================================================
 # SIMPLIFIED API INTERFACE
 # ============================================================================
+
 
 class CoordinatorAPI:
     """
     Simplified API for interacting with Agent Coordinator
     """
-    
+
     def __init__(self, coordinator: AgentCoordinator):
         self.coordinator = coordinator
-    
+
     async def submit_request(
         self,
         user_id: str,
         message: str,
         files: Optional[List[Dict[str, Any]]] = None,
-        priority: str = "medium"
+        priority: str = "medium",
     ) -> Dict[str, Any]:
         """
         Submit a user request
-        
+
         Args:
             user_id: User identifier
             message: User's natural language request
             files: Optional list of files (name, content, type)
             priority: Priority level (low, medium, high, critical)
-            
+
         Returns:
             Response with workflow ID and status
         """
         request = UserRequest(
-            user_id=user_id,
-            message=message,
-            files=files or [],
-            priority=Priority(priority)
+            user_id=user_id, message=message, files=files or [], priority=Priority(priority)
         )
-        
+
         result = await self.coordinator.process_user_request(request)
         return result
-    
+
     async def check_status(self, workflow_id: str) -> Dict[str, Any]:
         """Check workflow status"""
         return await self.coordinator.get_workflow_status(workflow_id)
-    
+
     async def approve_actions(
-        self,
-        request_id: str,
-        actions: List[str],
-        user_id: str
+        self, request_id: str, actions: List[str], user_id: str
     ) -> Dict[str, Any]:
         """Approve recommended actions"""
         action_enums = [ActionType(a) for a in actions]
-        return await self.coordinator.approve_workflow(
-            request_id,
-            action_enums,
-            user_id
-        )
+        return await self.coordinator.approve_workflow(request_id, action_enums, user_id)
 
 
 # ============================================================================
 # USAGE EXAMPLE
 # ============================================================================
 
+
 async def example_usage():
     """Example of using the Agent Coordinator"""
-    
+
     # Initialize (assuming agent_manager and db_session are available)
     # coordinator = AgentCoordinator(agent_manager, db_session, config)
     # api = CoordinatorAPI(coordinator)
-    
+
     # Example 1: Simple enhancement request
     result = await api.submit_request(
         user_id="user123",
         message="Please improve the performance of this code",
-        files=[{
-            "name": "algorithm.py",
-            "content": "def slow_function():\n    # implementation",
-            "type": "python"
-        }],
-        priority="medium"
+        files=[
+            {
+                "name": "algorithm.py",
+                "content": "def slow_function():\n    # implementation",
+                "type": "python",
+            }
+        ],
+        priority="medium",
     )
-    
+
     print(f"Request submitted: {result}")
-    
+
     # Example 2: Check status
     if "workflow_id" in result:
         status = await api.check_status(result["workflow_id"])
         print(f"Workflow status: {status}")
-    
+
     # Example 3: Approve pending workflow
     if result.get("status") == "pending_approval":
         approval = await api.approve_actions(
             request_id=result["request_id"],
             actions=result["recommended_actions"],
-            user_id="user123"
+            user_id="user123",
         )
         print(f"Workflow approved: {approval}")
 
 
 if __name__ == "__main__":
     import time
     from datetime import timedelta
-    
+
     # This is just for demonstration
     print("Agent Coordinator - Production Ready")
     print("\nKey Features:")
     print(" Natural language request processing")
     print(" Intelligent action recommendation")
would reformat /home/runner/work/ymera_y/ymera_y/agent_coordinator.py
--- /home/runner/work/ymera_y/ymera_y/agent_lifecycle_manager.py	2025-10-19 22:47:02.789432+00:00
+++ /home/runner/work/ymera_y/ymera_y/agent_lifecycle_manager.py	2025-10-19 23:08:48.448212+00:00
@@ -1,8 +1,8 @@
 # lifecycle/agent_lifecycle_manager.py
 """
-Agent lifecycle manager with admin approval workflow, comprehensive 
+Agent lifecycle manager with admin approval workflow, comprehensive
 audit trail, and controlled state transitions.
 """
 
 import logging
 import asyncio
@@ -15,252 +15,266 @@
 from models import Agent, AdminApproval, AuditLog, AgentStatus, AdminApprovalStatus, AgentAction
 from monitoring.telemetry_manager import tracer
 
 logger = logging.getLogger(__name__)
 
+
 class AgentLifecycleManager:
     """Agent lifecycle manager with admin approval workflow"""
-    
+
     def __init__(self, manager):
         """Initialize with manager reference"""
         self.manager = manager
         self.pending_actions = {}
-        
+
     @tracer.start_as_current_span("request_lifecycle_action")
-    async def request_agent_action(self, agent_id: str, action: AgentAction, 
-                                 reason: str, requested_by: str) -> Dict[str, Any]:
+    async def request_agent_action(
+        self, agent_id: str, action: AgentAction, reason: str, requested_by: str
+    ) -> Dict[str, Any]:
         """
         Request a lifecycle action on an agent, requires admin approval
         """
         try:
             # Validate agent exists
             async with self.manager.get_db_session() as session:
                 agent = await session.get(Agent, agent_id)
                 if not agent:
                     return {"error": "Agent not found", "status": "failed"}
-                
+
                 # Validate action is valid for current status
                 if not self._validate_action_for_status(action, agent.status):
                     return {
                         "error": f"Action {action} not valid for agent status {agent.status}",
-                        "status": "failed"
+                        "status": "failed",
                     }
-                
+
                 # Create admin approval request
                 approval = AdminApproval(
                     request_type="agent_action",
                     resource_id=agent_id,
                     resource_type="agent",
                     action=action.value,
                     reason=reason,
                     requested_by=requested_by,
-                    status=AdminApprovalStatus.PENDING.value
+                    status=AdminApprovalStatus.PENDING.value,
                 )
                 session.add(approval)
-                
+
                 # Create audit log
                 audit_log = AuditLog(
                     event_type="lifecycle_action_request",
                     resource_type="agent",
                     resource_id=agent_id,
                     action=f"request_{action.value}",
                     performed_by=requested_by,
-                    details={
-                        "reason": reason,
-                        "approval_id": approval.id
-                    }
+                    details={"reason": reason, "approval_id": approval.id},
                 )
                 session.add(audit_log)
-                
+
                 await session.commit()
-                
+
                 # Store pending action
                 self.pending_actions[approval.id] = {
                     "agent_id": agent_id,
                     "action": action.value,
                     "requested_by": requested_by,
                     "requested_at": datetime.utcnow().isoformat(),
-                    "reason": reason
+                    "reason": reason,
                 }
-                
+
                 # Notify admin of pending approval
-                await self._notify_admins_of_pending_approval(approval.id, agent_id, action.value, reason)
-                
+                await self._notify_admins_of_pending_approval(
+                    approval.id, agent_id, action.value, reason
+                )
+
                 return {
                     "status": "pending_approval",
                     "message": f"Action {action} requested and pending admin approval",
-                    "approval_id": approval.id
+                    "approval_id": approval.id,
                 }
-                
+
         except Exception as e:
             logger.error(f"Error requesting agent action: {e}")
             return {"error": str(e), "status": "failed"}
-    
+
     @tracer.start_as_current_span("execute_agent_action")
-    async def _execute_agent_action(self, agent_id: str, action: str, 
-                                  approver_id: str, notes: Optional[str] = None) -> Dict[str, Any]:
+    async def _execute_agent_action(
+        self, agent_id: str, action: str, approver_id: str, notes: Optional[str] = None
+    ) -> Dict[str, Any]:
         """
         Execute approved agent action
         """
         try:
             async with self.manager.get_db_session() as session:
                 agent = await session.get(Agent, agent_id)
                 if not agent:
                     return {"error": "Agent not found", "status": "failed"}
-                
+
                 result = {"status": "success", "action": action, "agent_id": agent_id}
-                
+
                 # Perform action based on type
                 if action == AgentAction.FREEZE.value:
                     agent.status = AgentStatus.FROZEN.value
                     agent.frozen_at = datetime.utcnow()
                     agent.frozen_by = approver_id
                     agent.frozen_reason = notes or "Admin action"
                     result["message"] = "Agent frozen successfully"
-                    
+
                 elif action == AgentAction.UNFREEZE.value:
                     previous_status = agent.status
                     agent.status = AgentStatus.INACTIVE.value
                     result["message"] = f"Agent unfrozen from {previous_status} to inactive"
-                    
+
                 elif action == AgentAction.DELETE.value:
                     agent.status = AgentStatus.DELETED.value
                     agent.deleted_at = datetime.utcnow()
                     agent.deleted_by = approver_id
-                    
+
                     # Revoke API keys
                     agent.api_key_hash = None
                     result["message"] = "Agent deleted successfully"
-                    
+
                 elif action == AgentAction.AUDIT.value:
                     # Just log audit event, don't change status
                     result["message"] = f"Agent audit triggered by {approver_id}"
-                    
+
                 elif action == AgentAction.ISOLATE.value:
                     agent.status = AgentStatus.ISOLATED.value
                     agent.isolated_at = datetime.utcnow()
                     agent.isolated_by = approver_id
                     result["message"] = "Agent isolated successfully"
-                
+
                 # Create audit log
                 audit_log = AuditLog(
                     event_type="lifecycle_action_executed",
                     resource_type="agent",
                     resource_id=agent_id,
                     action=action,
                     performed_by=approver_id,
                     details={
                         "notes": notes,
                         "previous_status": agent.status,
-                        "new_status": agent.status
-                    }
+                        "new_status": agent.status,
+                    },
                 )
                 session.add(audit_log)
-                
+
                 await session.commit()
-                
+
                 # Notify agent of status change if connected
                 await self._notify_agent_of_status_change(agent_id, action)
-                
+
                 return result
-                
+
         except Exception as e:
             logger.error(f"Error executing agent action: {e}")
             return {"error": str(e), "status": "failed"}
-    
+
     def _validate_action_for_status(self, action: AgentAction, current_status: str) -> bool:
         """
         Validate if an action is valid for the current agent status
         """
         # Valid status transitions
         valid_transitions = {
             AgentStatus.ACTIVE.value: [
-                AgentAction.FREEZE, AgentAction.WARN, AgentAction.AUDIT, AgentAction.ISOLATE
-            ],
-            AgentStatus.INACTIVE.value: [
-                AgentAction.WARN, AgentAction.DELETE, AgentAction.AUDIT
-            ],
+                AgentAction.FREEZE,
+                AgentAction.WARN,
+                AgentAction.AUDIT,
+                AgentAction.ISOLATE,
+            ],
+            AgentStatus.INACTIVE.value: [AgentAction.WARN, AgentAction.DELETE, AgentAction.AUDIT],
             AgentStatus.BUSY.value: [
-                AgentAction.WARN, AgentAction.FREEZE, AgentAction.AUDIT, AgentAction.ISOLATE
-            ],
-            AgentStatus.FROZEN.value: [
-                AgentAction.UNFREEZE, AgentAction.DELETE, AgentAction.AUDIT
-            ],
+                AgentAction.WARN,
+                AgentAction.FREEZE,
+                AgentAction.AUDIT,
+                AgentAction.ISOLATE,
+            ],
+            AgentStatus.FROZEN.value: [AgentAction.UNFREEZE, AgentAction.DELETE, AgentAction.AUDIT],
             AgentStatus.SUSPENDED.value: [
-                AgentAction.UNFREEZE, AgentAction.DELETE, AgentAction.AUDIT
+                AgentAction.UNFREEZE,
+                AgentAction.DELETE,
+                AgentAction.AUDIT,
             ],
             AgentStatus.ISOLATED.value: [
-                AgentAction.UNFREEZE, AgentAction.DELETE, AgentAction.AUDIT
+                AgentAction.UNFREEZE,
+                AgentAction.DELETE,
+                AgentAction.AUDIT,
             ],
             AgentStatus.ERROR.value: [
-                AgentAction.UNFREEZE, AgentAction.FREEZE, AgentAction.DELETE, AgentAction.AUDIT
-            ]
+                AgentAction.UNFREEZE,
+                AgentAction.FREEZE,
+                AgentAction.DELETE,
+                AgentAction.AUDIT,
+            ],
         }
-        
+
         # Check if action is valid for current status
         if current_status in valid_transitions:
             return action in valid_transitions[current_status]
-        
+
         return False
-    
-    async def _notify_admins_of_pending_approval(self, approval_id: str, agent_id: str, 
-                                              action: str, reason: str) -> None:
+
+    async def _notify_admins_of_pending_approval(
+        self, approval_id: str, agent_id: str, action: str, reason: str
+    ) -> None:
         """
         Notify administrators of pending approval request
         """
         try:
             # Get admin users
             async with self.manager.get_db_session() as session:
                 from sqlalchemy import select
                 from models import User
-                
+
                 # Find users with admin role
-                result = await session.execute(
-                    select(User).where(User.roles.contains(['admin']))
-                )
+                result = await session.execute(select(User).where(User.roles.contains(["admin"])))
                 admin_users = result.scalars().all()
-                
+
                 # Send notification to each admin
                 notification = {
                     "type": "approval_required",
                     "title": f"Agent {action} approval required",
                     "message": f"Action: {action}\nAgent: {agent_id}\nReason: {reason}",
                     "timestamp": datetime.utcnow().isoformat(),
                     "metadata": {
                         "approval_id": approval_id,
                         "agent_id": agent_id,
-                        "action": action
+                        "action": action,
                     },
-                    "priority": "high"
+                    "priority": "high",
                 }
-                
+
                 for admin in admin_users:
                     await self.manager.notification_manager.send_notification(
                         admin.id, notification
                     )
-                    
-                logger.info(f"Admin approval notifications sent for action {action} on agent {agent_id}")
-                
+
+                logger.info(
+                    f"Admin approval notifications sent for action {action} on agent {agent_id}"
+                )
+
         except Exception as e:
             logger.error(f"Failed to notify admins of approval request: {e}")
-    
+
     async def _notify_agent_of_status_change(self, agent_id: str, action: str) -> None:
         """
         Notify agent of status change via WebSocket if connected
         """
         if agent_id in self.manager.active_connections:
             for ws in self.manager.active_connections[agent_id]:
                 try:
-                    await ws.send_json({
-                        "type": "status_change",
-                        "action": action,
-                        "timestamp": datetime.utcnow().isoformat(),
-                        "message": f"Your status has been changed due to admin action: {action}"
-                    })
+                    await ws.send_json(
+                        {
+                            "type": "status_change",
+                            "action": action,
+                            "timestamp": datetime.utcnow().isoformat(),
+                            "message": f"Your status has been changed due to admin action: {action}",
+                        }
+                    )
                 except Exception as e:
                     logger.error(f"Failed to notify agent {agent_id} of status change: {e}")
-    
+
     @tracer.start_as_current_span("get_agent_lifecycle_history")
     async def get_agent_lifecycle_history(self, agent_id: str) -> List[Dict[str, Any]]:
         """
         Get complete lifecycle history for an agent
         """
@@ -268,31 +282,33 @@
             async with self.manager.get_db_session() as session:
                 # Get agent info
                 agent = await session.get(Agent, agent_id)
                 if not agent:
                     return []
-                
+
                 # Get audit logs for this agent
                 from sqlalchemy import select
+
                 result = await session.execute(
-                    select(AuditLog).where(
-                        AuditLog.resource_type == "agent",
-                        AuditLog.resource_id == agent_id
-                    ).order_by(AuditLog.timestamp.desc())
+                    select(AuditLog)
+                    .where(AuditLog.resource_type == "agent", AuditLog.resource_id == agent_id)
+                    .order_by(AuditLog.timestamp.desc())
                 )
                 audit_logs = result.scalars().all()
-                
+
                 # Format history
                 history = []
                 for log in audit_logs:
-                    history.append({
-                        "timestamp": log.timestamp.isoformat(),
-                        "action": log.action,
-                        "performed_by": log.performed_by,
-                        "details": log.details
-                    })
-                
+                    history.append(
+                        {
+                            "timestamp": log.timestamp.isoformat(),
+                            "action": log.action,
+                            "performed_by": log.performed_by,
+                            "details": log.details,
+                        }
+                    )
+
                 return history
-                
+
         except Exception as e:
             logger.error(f"Error fetching agent lifecycle history: {e}")
-            return []
\ No newline at end of file
+            return []
would reformat /home/runner/work/ymera_y/ymera_y/agent_lifecycle_manager.py
--- /home/runner/work/ymera_y/ymera_y/agent_lifecycle_mgr.py	2025-10-19 22:47:02.789432+00:00
+++ /home/runner/work/ymera_y/ymera_y/agent_lifecycle_mgr.py	2025-10-19 23:08:48.823862+00:00
@@ -16,10 +16,11 @@
 from security.rbac_manager import RBACManager, Permission
 from monitoring.telemetry_manager import TelemetryManager
 from monitoring.alert_manager import AlertManager, AlertCategory, AlertSeverity
 
 logger = logging.getLogger(__name__)
+
 
 class AgentStatus(str, Enum):
     REGISTERED = "registered"
     ACTIVE = "active"
     INACTIVE = "inactive"
@@ -28,456 +29,449 @@
     COMPROMISED = "compromised"
     DECOMMISSIONED = "decommissioned"
     SUSPENDED = "suspended"  # NEW: For admin-suspended agents
     FROZEN = "frozen"  # NEW: For investigation
 
+
 class AgentAction(str, Enum):
     """Actions that can be taken on agents"""
+
     SUSPEND = "suspend"
     FREEZE = "freeze"
     RESUME = "resume"
     DECOMMISSION = "decommission"
     RESTART = "restart"
 
+
 class AgentActionRequest(BaseModel):
     """Request model for agent actions"""
+
     action: AgentAction
     reason: str
     admin_id: str
     approval_id: Optional[str] = None  # For tracking admin approvals
     metadata: Optional[Dict[str, Any]] = None
 
+
 class AgentLifecycleManager:
     """Manages the full lifecycle of agents, from registration to decommissioning.
-    
+
     This manager has enhanced authority to:
     - Monitor all agent activities
     - Enforce security policies
     - Suspend/freeze agents on security violations
     - Control API access and information flow
     - Report to admin for critical decisions
     - Require admin approval for destructive actions
     """
 
     def __init__(
-        self, 
-        db_manager: SecureDatabaseManager, 
-        rbac_manager: RBACManager, 
-        telemetry_manager: TelemetryManager, 
-        alert_manager: AlertManager
+        self,
+        db_manager: SecureDatabaseManager,
+        rbac_manager: RBACManager,
+        telemetry_manager: TelemetryManager,
+        alert_manager: AlertManager,
     ):
         self.db_manager = db_manager
         self.rbac_manager = rbac_manager
         self.telemetry_manager = telemetry_manager
         self.alert_manager = alert_manager
-        
+
         # Authority settings
         self.auto_suspend_on_security_violation = getattr(
-            settings.performance, 
-            'auto_suspend_on_security_violation', 
-            True
+            settings.performance, "auto_suspend_on_security_violation", True
         )
         self.require_admin_approval_for_delete = getattr(
-            settings.performance,
-            'require_admin_approval_for_delete',
-            True
+            settings.performance, "require_admin_approval_for_delete", True
         )
-        
+
         logger.info("AgentLifecycleManager initialized with enhanced authority.")
 
     async def register_agent(
-        self, 
-        tenant_id: str, 
-        name: str, 
-        agent_type: str, 
-        version: str, 
+        self,
+        tenant_id: str,
+        name: str,
+        agent_type: str,
+        version: str,
         capabilities: Dict[str, Any],
-        admin_id: Optional[str] = None
+        admin_id: Optional[str] = None,
     ) -> Agent:
         """Registers a new agent in the system with security validation."""
         async with self.db_manager.get_session() as session:
             # Check tenant agent limit
             agent_count = await session.scalar(
                 select(func.count(Agent.id)).where(Agent.tenant_id == tenant_id)
             )
-            
-            max_agents = getattr(settings.performance, 'max_agents_per_tenant', 100)
-            
+
+            max_agents = getattr(settings.performance, "max_agents_per_tenant", 100)
+
             if agent_count >= max_agents:
                 await self.alert_manager.create_alert(
                     category=AlertCategory.SYSTEM,
                     severity=AlertSeverity.WARNING,
                     title="Agent Registration Limit Reached",
                     description=f"Tenant {tenant_id} attempted to register agent {name} but reached the limit of {max_agents}.",
                     source="AgentLifecycleManager",
-                    metadata={"tenant_id": tenant_id, "agent_name": name}
+                    metadata={"tenant_id": tenant_id, "agent_name": name},
                 )
                 raise HTTPException(
                     status_code=status.HTTP_403_FORBIDDEN,
-                    detail=f"Tenant {tenant_id} has reached maximum agent limit of {max_agents}"
+                    detail=f"Tenant {tenant_id} has reached maximum agent limit of {max_agents}",
                 )
 
             new_agent = Agent(
                 tenant_id=tenant_id,
                 name=name,
                 type=agent_type,
                 version=version,
                 capabilities=capabilities,
                 status=AgentStatus.REGISTERED.value,
-                security_score=100  # Start with perfect score
+                security_score=100,  # Start with perfect score
             )
             session.add(new_agent)
             await session.commit()
             await session.refresh(new_agent)
-            
+
             logger.info(f"Agent {new_agent.id} registered for tenant {tenant_id}.")
-            
+
             await self.telemetry_manager.record_event(
                 "agent_registered",
                 {
                     "agent_id": str(new_agent.id),
                     "tenant_id": tenant_id,
                     "agent_type": agent_type,
-                    "registered_by": admin_id or "system"
-                }
-            )
-            
+                    "registered_by": admin_id or "system",
+                },
+            )
+
             return new_agent
 
     async def update_agent_status(
-        self, 
-        agent_id: str, 
-        new_status: AgentStatus, 
+        self,
+        agent_id: str,
+        new_status: AgentStatus,
         performance_metrics: Optional[Dict[str, Any]] = None,
         reason: Optional[str] = None,
-        admin_id: Optional[str] = None
+        admin_id: Optional[str] = None,
     ) -> Optional[Agent]:
         """Updates the status of an existing agent with audit trail."""
         async with self.db_manager.get_session() as session:
             agent = await session.get(Agent, agent_id)
             if agent:
                 old_status = agent.status
                 agent.status = new_status.value
                 agent.last_heartbeat = datetime.utcnow()
-                
+
                 if performance_metrics:
                     agent.performance_metrics = performance_metrics
-                
+
                 await session.commit()
                 await session.refresh(agent)
-                
+
                 logger.info(
                     f"Agent {agent_id} status updated from {old_status} to {new_status.value}. "
                     f"Reason: {reason or 'N/A'}"
                 )
-                
+
                 await self.telemetry_manager.record_event(
                     "agent_status_updated",
                     {
                         "agent_id": agent_id,
                         "old_status": old_status,
                         "new_status": new_status.value,
                         "reason": reason,
-                        "changed_by": admin_id or "system"
-                    }
-                )
-                
+                        "changed_by": admin_id or "system",
+                    },
+                )
+
                 # Critical status changes trigger alerts
-                if new_status in [AgentStatus.COMPROMISED, AgentStatus.FROZEN, AgentStatus.SUSPENDED]:
+                if new_status in [
+                    AgentStatus.COMPROMISED,
+                    AgentStatus.FROZEN,
+                    AgentStatus.SUSPENDED,
+                ]:
                     await self.alert_manager.create_alert(
                         category=AlertCategory.SECURITY,
-                        severity=AlertSeverity.EMERGENCY if new_status == AgentStatus.COMPROMISED else AlertSeverity.CRITICAL,
+                        severity=(
+                            AlertSeverity.EMERGENCY
+                            if new_status == AgentStatus.COMPROMISED
+                            else AlertSeverity.CRITICAL
+                        ),
                         title=f"Agent {new_status.value.title()}",
                         description=f"Agent {agent.id} ({agent.name}) has been marked as {new_status.value}. Reason: {reason or 'Not specified'}",
                         source="AgentLifecycleManager",
                         metadata={
                             "agent_id": agent_id,
                             "tenant_id": agent.tenant_id,
                             "old_status": old_status,
-                            "reason": reason
-                        }
+                            "reason": reason,
+                        },
                     )
-                
+
                 return agent
-            
+
             logger.warning(f"Agent {agent_id} not found for status update.")
             return None
 
     async def execute_agent_action(
-        self,
-        agent_id: str,
-        action_request: AgentActionRequest
+        self, agent_id: str, action_request: AgentActionRequest
     ) -> Dict[str, Any]:
         """Execute administrative action on an agent with proper authorization.
-        
+
         Returns:
             Dict with action result and any required follow-up
         """
         async with self.db_manager.get_session() as session:
             agent = await session.get(Agent, agent_id)
-            
+
             if not agent:
                 raise HTTPException(
-                    status_code=status.HTTP_404_NOT_FOUND,
-                    detail=f"Agent {agent_id} not found"
-                )
-            
+                    status_code=status.HTTP_404_NOT_FOUND, detail=f"Agent {agent_id} not found"
+                )
+
             # Check if action requires admin approval
-            if action_request.action == AgentAction.DECOMMISSION and self.require_admin_approval_for_delete:
+            if (
+                action_request.action == AgentAction.DECOMMISSION
+                and self.require_admin_approval_for_delete
+            ):
                 if not action_request.approval_id:
                     return {
                         "status": "pending_approval",
                         "message": f"Decommissioning agent {agent.name} requires admin approval",
                         "agent_id": agent_id,
-                        "action": action_request.action.value
+                        "action": action_request.action.value,
                     }
-            
+
             # Execute the action
             result = await self._execute_action(agent, action_request, session)
-            
+
             # Log the action
             await self.telemetry_manager.record_event(
                 "agent_action_executed",
                 {
                     "agent_id": agent_id,
                     "action": action_request.action.value,
                     "reason": action_request.reason,
                     "admin_id": action_request.admin_id,
                     "approval_id": action_request.approval_id,
-                    "result": result["status"]
-                }
-            )
-            
+                    "result": result["status"],
+                },
+            )
+
             return result
 
     async def _execute_action(
-        self,
-        agent: Agent,
-        action_request: AgentActionRequest,
-        session: AsyncSession
+        self, agent: Agent, action_request: AgentActionRequest, session: AsyncSession
     ) -> Dict[str, Any]:
         """Internal method to execute specific actions."""
-        
+
         if action_request.action == AgentAction.SUSPEND:
             agent.status = AgentStatus.SUSPENDED.value
             message = f"Agent {agent.name} suspended"
-            
+
         elif action_request.action == AgentAction.FREEZE:
             agent.status = AgentStatus.FROZEN.value
             message = f"Agent {agent.name} frozen for investigation"
-            
+
         elif action_request.action == AgentAction.RESUME:
             if agent.status in [AgentStatus.SUSPENDED.value, AgentStatus.FROZEN.value]:
                 agent.status = AgentStatus.ACTIVE.value
                 message = f"Agent {agent.name} resumed"
             else:
                 return {
                     "status": "error",
-                    "message": f"Cannot resume agent in status {agent.status}"
+                    "message": f"Cannot resume agent in status {agent.status}",
                 }
-        
+
         elif action_request.action == AgentAction.DECOMMISSION:
             await session.delete(agent)
             await session.commit()
-            
+
             await self.telemetry_manager.record_event(
                 "agent_decommissioned",
                 {
                     "agent_id": str(agent.id),
                     "tenant_id": agent.tenant_id,
                     "reason": action_request.reason,
-                    "admin_id": action_request.admin_id
-                }
-            )
-            
+                    "admin_id": action_request.admin_id,
+                },
+            )
+
             return {
                 "status": "success",
                 "message": f"Agent {agent.name} decommissioned",
-                "agent_id": str(agent.id)
+                "agent_id": str(agent.id),
             }
-        
+
         elif action_request.action == AgentAction.RESTART:
             agent.status = AgentStatus.ACTIVE.value
             agent.last_heartbeat = datetime.utcnow()
             message = f"Agent {agent.name} restarted"
-        
+
         else:
-            return {
-                "status": "error",
-                "message": f"Unknown action: {action_request.action}"
-            }
-        
+            return {"status": "error", "message": f"Unknown action: {action_request.action}"}
+
         await session.commit()
         await session.refresh(agent)
-        
+
         return {
             "status": "success",
             "message": message,
             "agent_id": str(agent.id),
-            "new_status": agent.status
+            "new_status": agent.status,
         }
 
     async def handle_security_violation(
-        self,
-        agent_id: str,
-        violation_type: str,
-        severity: AlertSeverity,
-        details: Dict[str, Any]
+        self, agent_id: str, violation_type: str, severity: AlertSeverity, details: Dict[str, Any]
     ) -> Dict[str, Any]:
         """Handle security violations detected by surveillance system.
-        
+
         Can automatically suspend agents based on severity if configured.
         """
         async with self.db_manager.get_session() as session:
             agent = await session.get(Agent, agent_id)
-            
+
             if not agent:
                 return {"status": "error", "message": "Agent not found"}
-            
+
             # Reduce security score
             score_reduction = 30 if severity == AlertSeverity.CRITICAL else 15
             agent.security_score = max(0, agent.security_score - score_reduction)
-            
+
             # Auto-suspend on critical violations if enabled
             auto_suspended = False
             if self.auto_suspend_on_security_violation and severity == AlertSeverity.CRITICAL:
                 agent.status = AgentStatus.SUSPENDED.value
                 auto_suspended = True
-            
+
             await session.commit()
-            
+
             # Create alert for admin
             await self.alert_manager.create_alert(
                 category=AlertCategory.SECURITY,
                 severity=severity,
                 title=f"Security Violation: {violation_type}",
                 description=f"Agent {agent.name} ({agent_id}) violated security policy. "
-                           f"Auto-suspended: {auto_suspended}. Details: {details}",
+                f"Auto-suspended: {auto_suspended}. Details: {details}",
                 source="AgentLifecycleManager",
                 metadata={
                     "agent_id": agent_id,
                     "tenant_id": agent.tenant_id,
                     "violation_type": violation_type,
                     "auto_suspended": auto_suspended,
                     "new_security_score": agent.security_score,
-                    **details
-                }
-            )
-            
+                    **details,
+                },
+            )
+
             return {
                 "status": "success",
                 "agent_id": agent_id,
                 "auto_suspended": auto_suspended,
                 "new_security_score": agent.security_score,
-                "requires_admin_review": True
+                "requires_admin_review": True,
             }
 
     async def get_agent(self, agent_id: str) -> Optional[Agent]:
         """Retrieves an agent by its ID."""
         async with self.db_manager.get_session() as session:
             return await session.get(Agent, agent_id)
 
     async def list_agents(
-        self, 
-        tenant_id: str, 
-        status: Optional[str] = None, 
-        agent_type: Optional[str] = None, 
-        skip: int = 0, 
-        limit: int = 100
+        self,
+        tenant_id: str,
+        status: Optional[str] = None,
+        agent_type: Optional[str] = None,
+        skip: int = 0,
+        limit: int = 100,
     ) -> List[Agent]:
         """Lists agents for a given tenant with filtering and pagination."""
         async with self.db_manager.get_session() as session:
             query = select(Agent).where(Agent.tenant_id == tenant_id)
-            
+
             if status:
                 query = query.where(Agent.status == status)
             if agent_type:
                 query = query.where(Agent.type == agent_type)
-            
+
             result = await session.execute(query.offset(skip).limit(limit))
             return list(result.scalars().all())
 
     async def count_agents(
-        self, 
-        tenant_id: str, 
-        status: Optional[str] = None, 
-        agent_type: Optional[str] = None
+        self, tenant_id: str, status: Optional[str] = None, agent_type: Optional[str] = None
     ) -> int:
         """Counts agents for a given tenant with optional filtering."""
         async with self.db_manager.get_session() as session:
             query = select(func.count(Agent.id)).where(Agent.tenant_id == tenant_id)
-            
+
             if status:
                 query = query.where(Agent.status == status)
             if agent_type:
                 query = query.where(Agent.type == agent_type)
-            
+
             return await session.scalar(query) or 0
 
     async def decommission_agent(
-        self, 
-        agent_id: str,
-        admin_id: str,
-        reason: str,
-        approval_id: Optional[str] = None
+        self, agent_id: str, admin_id: str, reason: str, approval_id: Optional[str] = None
     ) -> bool:
         """Decommissions an agent with proper authorization."""
         action_request = AgentActionRequest(
             action=AgentAction.DECOMMISSION,
             reason=reason,
             admin_id=admin_id,
-            approval_id=approval_id
+            approval_id=approval_id,
         )
-        
+
         result = await self.execute_agent_action(agent_id, action_request)
         return result.get("status") == "success"
 
     async def monitor_agent_health(self) -> None:
         """Periodically monitors agent health and updates statuses."""
         async with self.db_manager.get_session() as session:
-            timeout_seconds = getattr(settings.performance, 'agent_heartbeat_timeout_seconds', 300)
+            timeout_seconds = getattr(settings.performance, "agent_heartbeat_timeout_seconds", 300)
             stale_threshold = datetime.utcnow() - timedelta(seconds=timeout_seconds)
-            
+
             result = await session.execute(
                 select(Agent).where(
-                    Agent.status == AgentStatus.ACTIVE.value,
-                    Agent.last_heartbeat < stale_threshold
+                    Agent.status == AgentStatus.ACTIVE.value, Agent.last_heartbeat < stale_threshold
                 )
             )
             stale_agents = result.scalars().all()
 
             for agent in stale_agents:
                 logger.warning(f"Agent {agent.id} detected as stale. Marking as OFFLINE.")
                 agent.status = AgentStatus.OFFLINE.value
                 await session.commit()
-                
+
                 await self.telemetry_manager.record_event(
                     "agent_offline",
                     {
                         "agent_id": str(agent.id),
                         "tenant_id": agent.tenant_id,
-                        "reason": "heartbeat_timeout"
-                    }
-                )
-                
+                        "reason": "heartbeat_timeout",
+                    },
+                )
+
                 await self.alert_manager.create_alert(
                     category=AlertCategory.SYSTEM,
                     severity=AlertSeverity.CRITICAL,
                     title="Agent Offline",
                     description=f"Agent {agent.id} ({agent.name}) has not sent a heartbeat for over {timeout_seconds} seconds and is now offline.",
                     source="AgentLifecycleManager",
                     metadata={
                         "agent_id": str(agent.id),
                         "tenant_id": agent.tenant_id,
-                        "last_heartbeat": agent.last_heartbeat.isoformat() if agent.last_heartbeat else None
-                    }
+                        "last_heartbeat": (
+                            agent.last_heartbeat.isoformat() if agent.last_heartbeat else None
+                        ),
+                    },
                 )
 
     async def start_health_monitoring_loop(self) -> None:
         """Starts a continuous loop for agent health monitoring."""
-        check_interval = getattr(settings.performance, 'agent_health_check_interval_seconds', 60)
-        
+        check_interval = getattr(settings.performance, "agent_health_check_interval_seconds", 60)
+
         while True:
             try:
                 await self.monitor_agent_health()
                 await asyncio.sleep(check_interval)
             except asyncio.CancelledError:
would reformat /home/runner/work/ymera_y/ymera_y/agent_lifecycle_mgr.py
--- /home/runner/work/ymera_y/ymera_y/agent_management_api.py	2025-10-19 22:47:02.790432+00:00
+++ /home/runner/work/ymera_y/ymera_y/agent_management_api.py	2025-10-19 23:08:49.412046+00:00
@@ -17,54 +17,67 @@
 from pydantic import BaseModel, Field, validator
 import structlog
 
 # Import your existing models and managers
 from main import (
-    Agent, Task, LearningRecord, 
-    DatabaseManager, RedisManager, AIServiceManager,
-    config, logger
+    Agent,
+    Task,
+    LearningRecord,
+    DatabaseManager,
+    RedisManager,
+    AIServiceManager,
+    config,
+    logger,
 )
+
 
 # Pydantic Models for API
 class AgentStatus(str, Enum):
     IDLE = "idle"
     ACTIVE = "active"
     BUSY = "busy"
     ERROR = "error"
     MAINTENANCE = "maintenance"
     TERMINATED = "terminated"
 
+
 class AgentType(str, Enum):
     DEVELOPER = "developer"
     ANALYST = "analyst"
     TESTER = "tester"
     REVIEWER = "reviewer"
     COORDINATOR = "coordinator"
     SPECIALIST = "specialist"
 
+
 class AgentCapability(BaseModel):
     name: str
     level: int = Field(ge=1, le=10)
     description: Optional[str] = None
+
 
 class AgentCreateRequest(BaseModel):
     name: str = Field(..., min_length=1, max_length=100)
     type: AgentType
     capabilities: List[AgentCapability] = []
     metadata: Dict[str, Any] = {}
-    
-    @validator('name')
+
+    @validator("name")
     def name_must_be_alphanumeric(cls, v):
-        if not v.replace('-', '').replace('_', '').replace(' ', '').isalnum():
-            raise ValueError('Name must be alphanumeric with optional hyphens, underscores, or spaces')
+        if not v.replace("-", "").replace("_", "").replace(" ", "").isalnum():
+            raise ValueError(
+                "Name must be alphanumeric with optional hyphens, underscores, or spaces"
+            )
         return v
+
 
 class AgentUpdateRequest(BaseModel):
     name: Optional[str] = Field(None, min_length=1, max_length=100)
     status: Optional[AgentStatus] = None
     capabilities: Optional[List[AgentCapability]] = None
     metadata: Optional[Dict[str, Any]] = None
+
 
 class AgentResponse(BaseModel):
     id: str
     name: str
     type: str
@@ -72,23 +85,25 @@
     capabilities: List[Dict[str, Any]]
     performance_metrics: Dict[str, Any]
     learning_data: Dict[str, Any]
     created_at: datetime
     updated_at: datetime
-    
+
     # Runtime statistics
     active_tasks: Optional[int] = None
     completed_tasks: Optional[int] = None
     success_rate: Optional[float] = None
     average_execution_time: Optional[float] = None
+
 
 class AgentListResponse(BaseModel):
     agents: List[AgentResponse]
     total: int
     page: int
     page_size: int
     has_next: bool
+
 
 class AgentPerformanceMetrics(BaseModel):
     agent_id: str
     total_tasks: int
     completed_tasks: int
@@ -96,36 +111,38 @@
     success_rate: float
     average_execution_time: float
     last_activity: Optional[datetime]
     learning_progress: Dict[str, Any]
 
+
 class TaskAssignmentRequest(BaseModel):
     task_id: str
     priority_override: Optional[int] = Field(None, ge=1, le=10)
     expected_completion: Optional[datetime] = None
 
+
 # Agent Management Service
 class AgentManagementService:
-    def __init__(self, db_manager: DatabaseManager, redis_manager: RedisManager, ai_manager: AIServiceManager):
+    def __init__(
+        self, db_manager: DatabaseManager, redis_manager: RedisManager, ai_manager: AIServiceManager
+    ):
         self.db_manager = db_manager
         self.redis = redis_manager
         self.ai_manager = ai_manager
         self.logger = structlog.get_logger()
-    
+
     async def create_agent(self, agent_data: AgentCreateRequest) -> AgentResponse:
         """Create a new agent with validation and initialization"""
         async with self.db_manager.get_session() as session:
             # Check for duplicate names
-            existing = await session.execute(
-                select(Agent).where(Agent.name == agent_data.name)
-            )
+            existing = await session.execute(select(Agent).where(Agent.name == agent_data.name))
             if existing.scalar_one_or_none():
                 raise HTTPException(
                     status_code=status.HTTP_409_CONFLICT,
-                    detail=f"Agent with name '{agent_data.name}' already exists"
-                )
-            
+                    detail=f"Agent with name '{agent_data.name}' already exists",
+                )
+
             # Create agent
             agent = Agent(
                 id=str(uuid.uuid4()),
                 name=agent_data.name,
                 type=agent_data.type.value,
@@ -134,482 +151,486 @@
                 performance_metrics={
                     "tasks_completed": 0,
                     "tasks_failed": 0,
                     "total_execution_time": 0.0,
                     "success_rate": 0.0,
-                    "created_at": datetime.utcnow().isoformat()
+                    "created_at": datetime.utcnow().isoformat(),
                 },
                 learning_data={
                     "interactions": 0,
                     "feedback_score": 0.0,
                     "learning_rate": config.system.learning_rate,
-                    "knowledge_base": []
-                }
-            )
-            
+                    "knowledge_base": [],
+                },
+            )
+
             session.add(agent)
             await session.commit()
             await session.refresh(agent)
-            
+
             # Cache agent data
             await self._cache_agent_data(agent)
-            
+
             # Initialize agent in AI system
             await self._initialize_agent_ai(agent)
-            
+
             self.logger.info(f"Agent created: {agent.id} ({agent.name})")
-            
+
             return await self._build_agent_response(agent)
-    
+
     async def get_agent(self, agent_id: str) -> AgentResponse:
         """Get agent by ID with performance metrics"""
         async with self.db_manager.get_session() as session:
-            agent = await session.execute(
-                select(Agent).where(Agent.id == agent_id)
-            )
+            agent = await session.execute(select(Agent).where(Agent.id == agent_id))
             agent = agent.scalar_one_or_none()
-            
+
             if not agent:
                 raise HTTPException(
-                    status_code=status.HTTP_404_NOT_FOUND,
-                    detail=f"Agent {agent_id} not found"
-                )
-            
+                    status_code=status.HTTP_404_NOT_FOUND, detail=f"Agent {agent_id} not found"
+                )
+
             return await self._build_agent_response(agent, session)
-    
+
     async def list_agents(
-        self, 
-        page: int = 1, 
+        self,
+        page: int = 1,
         page_size: int = 20,
         status_filter: Optional[AgentStatus] = None,
         type_filter: Optional[AgentType] = None,
-        search: Optional[str] = None
+        search: Optional[str] = None,
     ) -> AgentListResponse:
         """List agents with filtering and pagination"""
         async with self.db_manager.get_session() as session:
             query = select(Agent)
-            
+
             # Apply filters
             if status_filter:
                 query = query.where(Agent.status == status_filter.value)
             if type_filter:
                 query = query.where(Agent.type == type_filter.value)
             if search:
                 query = query.where(Agent.name.ilike(f"%{search}%"))
-            
+
             # Get total count
             count_query = select(func.count(Agent.id)).select_from(query.subquery())
             total_result = await session.execute(count_query)
             total = total_result.scalar()
-            
+
             # Apply pagination
             offset = (page - 1) * page_size
             query = query.offset(offset).limit(page_size).order_by(Agent.created_at.desc())
-            
+
             result = await session.execute(query)
             agents = result.scalars().all()
-            
+
             # Build responses with metrics
             agent_responses = []
             for agent in agents:
                 agent_responses.append(await self._build_agent_response(agent, session))
-            
+
             return AgentListResponse(
                 agents=agent_responses,
                 total=total,
                 page=page,
                 page_size=page_size,
-                has_next=(page * page_size) < total
-            )
-    
+                has_next=(page * page_size) < total,
+            )
+
     async def update_agent(self, agent_id: str, update_data: AgentUpdateRequest) -> AgentResponse:
         """Update agent configuration"""
         async with self.db_manager.get_session() as session:
-            agent = await session.execute(
-                select(Agent).where(Agent.id == agent_id)
-            )
+            agent = await session.execute(select(Agent).where(Agent.id == agent_id))
             agent = agent.scalar_one_or_none()
-            
+
             if not agent:
                 raise HTTPException(
-                    status_code=status.HTTP_404_NOT_FOUND,
-                    detail=f"Agent {agent_id} not found"
-                )
-            
+                    status_code=status.HTTP_404_NOT_FOUND, detail=f"Agent {agent_id} not found"
+                )
+
             # Update fields
             if update_data.name is not None:
                 # Check for duplicate names
                 existing = await session.execute(
                     select(Agent).where(and_(Agent.name == update_data.name, Agent.id != agent_id))
                 )
                 if existing.scalar_one_or_none():
                     raise HTTPException(
                         status_code=status.HTTP_409_CONFLICT,
-                        detail=f"Agent with name '{update_data.name}' already exists"
+                        detail=f"Agent with name '{update_data.name}' already exists",
                     )
                 agent.name = update_data.name
-            
+
             if update_data.status is not None:
                 old_status = agent.status
                 agent.status = update_data.status.value
-                
+
                 # Handle status transitions
                 await self._handle_status_change(agent, old_status, update_data.status.value)
-            
+
             if update_data.capabilities is not None:
                 agent.capabilities = [cap.dict() for cap in update_data.capabilities]
-            
+
             if update_data.metadata is not None:
                 current_metadata = agent.metadata or {}
                 current_metadata.update(update_data.metadata)
                 agent.metadata = current_metadata
-            
+
             agent.updated_at = datetime.utcnow()
             await session.commit()
-            
+
             # Update cache
             await self._cache_agent_data(agent)
-            
+
             self.logger.info(f"Agent updated: {agent.id} ({agent.name})")
-            
+
             return await self._build_agent_response(agent, session)
-    
+
     async def delete_agent(self, agent_id: str, force: bool = False) -> Dict[str, str]:
         """Delete agent with safety checks"""
         async with self.db_manager.get_session() as session:
-            agent = await session.execute(
-                select(Agent).where(Agent.id == agent_id)
-            )
+            agent = await session.execute(select(Agent).where(Agent.id == agent_id))
             agent = agent.scalar_one_or_none()
-            
+
             if not agent:
                 raise HTTPException(
-                    status_code=status.HTTP_404_NOT_FOUND,
-                    detail=f"Agent {agent_id} not found"
-                )
-            
+                    status_code=status.HTTP_404_NOT_FOUND, detail=f"Agent {agent_id} not found"
+                )
+
             # Check for active tasks unless forced
             if not force:
                 active_tasks = await session.execute(
                     select(func.count(Task.id)).where(
                         and_(Task.agent_id == agent_id, Task.status.in_(["pending", "running"]))
                     )
                 )
                 if active_tasks.scalar() > 0:
                     raise HTTPException(
                         status_code=status.HTTP_409_CONFLICT,
-                        detail="Agent has active tasks. Use force=true to delete anyway."
+                        detail="Agent has active tasks. Use force=true to delete anyway.",
                     )
-            
+
             # Terminate agent gracefully
             if agent.status in [AgentStatus.ACTIVE.value, AgentStatus.BUSY.value]:
                 agent.status = AgentStatus.TERMINATED.value
                 await session.commit()
                 await self._terminate_agent_tasks(agent_id, session)
-            
+
             # Delete agent
             await session.delete(agent)
             await session.commit()
-            
+
             # Clean up cache
             await self.redis.delete(f"agent:{agent_id}")
-            
+
             self.logger.info(f"Agent deleted: {agent_id}")
-            
+
             return {"message": f"Agent {agent_id} deleted successfully"}
-    
+
     async def get_agent_performance(self, agent_id: str) -> AgentPerformanceMetrics:
         """Get detailed agent performance metrics"""
         async with self.db_manager.get_session() as session:
-            agent = await session.execute(
-                select(Agent).where(Agent.id == agent_id)
-            )
+            agent = await session.execute(select(Agent).where(Agent.id == agent_id))
             agent = agent.scalar_one_or_none()
-            
+
             if not agent:
                 raise HTTPException(
-                    status_code=status.HTTP_404_NOT_FOUND,
-                    detail=f"Agent {agent_id} not found"
-                )
-            
+                    status_code=status.HTTP_404_NOT_FOUND, detail=f"Agent {agent_id} not found"
+                )
+
             # Get task statistics
             task_stats = await session.execute(
                 select(
                     func.count(Task.id).label("total_tasks"),
-                    func.sum(func.case([(Task.status == "completed", 1)], else_=0)).label("completed_tasks"),
-                    func.sum(func.case([(Task.status == "failed", 1)], else_=0)).label("failed_tasks"),
+                    func.sum(func.case([(Task.status == "completed", 1)], else_=0)).label(
+                        "completed_tasks"
+                    ),
+                    func.sum(func.case([(Task.status == "failed", 1)], else_=0)).label(
+                        "failed_tasks"
+                    ),
                     func.avg(Task.execution_time).label("avg_execution_time"),
-                    func.max(Task.updated_at).label("last_activity")
+                    func.max(Task.updated_at).label("last_activity"),
                 ).where(Task.agent_id == agent_id)
             )
             stats = task_stats.first()
-            
+
             total_tasks = stats.total_tasks or 0
             completed_tasks = stats.completed_tasks or 0
             failed_tasks = stats.failed_tasks or 0
             success_rate = (completed_tasks / total_tasks) if total_tasks > 0 else 0.0
-            
+
             return AgentPerformanceMetrics(
                 agent_id=agent_id,
                 total_tasks=total_tasks,
                 completed_tasks=completed_tasks,
                 failed_tasks=failed_tasks,
                 success_rate=success_rate,
                 average_execution_time=stats.avg_execution_time or 0.0,
                 last_activity=stats.last_activity,
-                learning_progress=agent.learning_data or {}
-            )
-    
+                learning_progress=agent.learning_data or {},
+            )
+
     async def assign_task(self, agent_id: str, assignment: TaskAssignmentRequest) -> Dict[str, str]:
         """Assign task to agent with validation"""
         async with self.db_manager.get_session() as session:
             # Verify agent exists and is available
-            agent = await session.execute(
-                select(Agent).where(Agent.id == agent_id)
-            )
+            agent = await session.execute(select(Agent).where(Agent.id == agent_id))
             agent = agent.scalar_one_or_none()
-            
+
             if not agent:
                 raise HTTPException(
+                    status_code=status.HTTP_404_NOT_FOUND, detail=f"Agent {agent_id} not found"
+                )
+
+            if agent.status not in [AgentStatus.IDLE.value, AgentStatus.ACTIVE.value]:
+                raise HTTPException(
+                    status_code=status.HTTP_409_CONFLICT,
+                    detail=f"Agent is not available (status: {agent.status})",
+                )
+
+            # Verify task exists and is assignable
+            task = await session.execute(select(Task).where(Task.id == assignment.task_id))
+            task = task.scalar_one_or_none()
+
+            if not task:
+                raise HTTPException(
                     status_code=status.HTTP_404_NOT_FOUND,
-                    detail=f"Agent {agent_id} not found"
-                )
-            
-            if agent.status not in [AgentStatus.IDLE.value, AgentStatus.ACTIVE.value]:
+                    detail=f"Task {assignment.task_id} not found",
+                )
+
+            if task.status != "pending":
                 raise HTTPException(
                     status_code=status.HTTP_409_CONFLICT,
-                    detail=f"Agent is not available (status: {agent.status})"
-                )
-            
-            # Verify task exists and is assignable
-            task = await session.execute(
-                select(Task).where(Task.id == assignment.task_id)
-            )
-            task = task.scalar_one_or_none()
-            
-            if not task:
-                raise HTTPException(
-                    status_code=status.HTTP_404_NOT_FOUND,
-                    detail=f"Task {assignment.task_id} not found"
-                )
-            
-            if task.status != "pending":
-                raise HTTPException(
-                    status_code=status.HTTP_409_CONFLICT,
-                    detail=f"Task is not assignable (status: {task.status})"
-                )
-            
+                    detail=f"Task is not assignable (status: {task.status})",
+                )
+
             # Assign task
             task.agent_id = agent_id
             task.status = "assigned"
             if assignment.priority_override:
                 task.priority = assignment.priority_override
-            
+
             # Update agent status
             if agent.status == AgentStatus.IDLE.value:
                 agent.status = AgentStatus.ACTIVE.value
-            
+
             await session.commit()
-            
+
             self.logger.info(f"Task {assignment.task_id} assigned to agent {agent_id}")
-            
+
             return {"message": f"Task {assignment.task_id} assigned to agent {agent_id}"}
-    
+
     # Private helper methods
-    async def _build_agent_response(self, agent: Agent, session: AsyncSession = None) -> AgentResponse:
+    async def _build_agent_response(
+        self, agent: Agent, session: AsyncSession = None
+    ) -> AgentResponse:
         """Build agent response with runtime statistics"""
         response_data = {
             "id": agent.id,
             "name": agent.name,
             "type": agent.type,
             "status": agent.status,
             "capabilities": agent.capabilities or [],
             "performance_metrics": agent.performance_metrics or {},
             "learning_data": agent.learning_data or {},
             "created_at": agent.created_at,
-            "updated_at": agent.updated_at
+            "updated_at": agent.updated_at,
         }
-        
+
         # Add runtime statistics if session available
         if session:
             task_stats = await session.execute(
                 select(
-                    func.sum(func.case([(Task.status.in_(["assigned", "running"]), 1)], else_=0)).label("active_tasks"),
-                    func.sum(func.case([(Task.status == "completed", 1)], else_=0)).label("completed_tasks"),
-                    func.avg(Task.execution_time).label("avg_execution_time")
+                    func.sum(
+                        func.case([(Task.status.in_(["assigned", "running"]), 1)], else_=0)
+                    ).label("active_tasks"),
+                    func.sum(func.case([(Task.status == "completed", 1)], else_=0)).label(
+                        "completed_tasks"
+                    ),
+                    func.avg(Task.execution_time).label("avg_execution_time"),
                 ).where(Task.agent_id == agent.id)
             )
             stats = task_stats.first()
-            
-            response_data.update({
-                "active_tasks": stats.active_tasks or 0,
-                "completed_tasks": stats.completed_tasks or 0,
-                "average_execution_time": stats.avg_execution_time
-            })
-            
+
+            response_data.update(
+                {
+                    "active_tasks": stats.active_tasks or 0,
+                    "completed_tasks": stats.completed_tasks or 0,
+                    "average_execution_time": stats.avg_execution_time,
+                }
+            )
+
             # Calculate success rate
             if stats.completed_tasks:
                 total_tasks = await session.execute(
                     select(func.count(Task.id)).where(Task.agent_id == agent.id)
                 )
                 total = total_tasks.scalar() or 0
-                response_data["success_rate"] = (stats.completed_tasks / total) if total > 0 else 0.0
-        
+                response_data["success_rate"] = (
+                    (stats.completed_tasks / total) if total > 0 else 0.0
+                )
+
         return AgentResponse(**response_data)
-    
+
     async def _cache_agent_data(self, agent: Agent):
         """Cache agent data in Redis"""
         cache_data = {
             "id": agent.id,
             "name": agent.name,
             "type": agent.type,
             "status": agent.status,
             "capabilities": agent.capabilities,
-            "updated_at": agent.updated_at.isoformat()
+            "updated_at": agent.updated_at.isoformat(),
         }
         await self.redis.set(
             f"agent:{agent.id}",
             json.dumps(cache_data, default=str),
-            ttl=config.system.cache_ttl_seconds
+            ttl=config.system.cache_ttl_seconds,
         )
-    
+
     async def _initialize_agent_ai(self, agent: Agent):
         """Initialize agent in AI system"""
         # Create agent-specific AI context
         context = f"Agent {agent.name} ({agent.type}) with capabilities: {', '.join([cap['name'] for cap in agent.capabilities])}"
-        
+
         # Store in vector database if available
         # Implementation depends on your vector database setup
         pass
-    
+
     async def _handle_status_change(self, agent: Agent, old_status: str, new_status: str):
         """Handle agent status transitions"""
         if new_status == AgentStatus.TERMINATED.value:
             await self._terminate_agent_tasks(agent.id)
-        elif new_status == AgentStatus.MAINTENANCE.value and old_status in [AgentStatus.ACTIVE.value, AgentStatus.BUSY.value]:
+        elif new_status == AgentStatus.MAINTENANCE.value and old_status in [
+            AgentStatus.ACTIVE.value,
+            AgentStatus.BUSY.value,
+        ]:
             # Pause active tasks
             async with self.db_manager.get_session() as session:
                 await session.execute(
-                    update(Task).where(
-                        and_(Task.agent_id == agent.id, Task.status == "running")
-                    ).values(status="paused")
+                    update(Task)
+                    .where(and_(Task.agent_id == agent.id, Task.status == "running"))
+                    .values(status="paused")
                 )
                 await session.commit()
-    
+
     async def _terminate_agent_tasks(self, agent_id: str, session: AsyncSession = None):
         """Terminate all active tasks for agent"""
         if not session:
             session = self.db_manager.get_session()
-        
+
         await session.execute(
-            update(Task).where(
+            update(Task)
+            .where(
                 and_(Task.agent_id == agent_id, Task.status.in_(["assigned", "running", "paused"]))
-            ).values(status="cancelled", updated_at=datetime.utcnow())
+            )
+            .values(status="cancelled", updated_at=datetime.utcnow())
         )
         await session.commit()
 
+
 # API Router
 router = APIRouter(prefix="/api/v1/agents", tags=["Agent Management"])
+
 
 # Dependency injection
 async def get_agent_service() -> AgentManagementService:
     # These should be injected from your main application
     from main import db_manager, redis_manager, ai_manager
+
     return AgentManagementService(db_manager, redis_manager, ai_manager)
+
 
 # API Endpoints
 @router.post("/", response_model=AgentResponse, status_code=status.HTTP_201_CREATED)
 async def create_agent(
-    agent_data: AgentCreateRequest,
-    service: AgentManagementService = Depends(get_agent_service)
+    agent_data: AgentCreateRequest, service: AgentManagementService = Depends(get_agent_service)
 ):
     """Create a new agent"""
     return await service.create_agent(agent_data)
 
+
 @router.get("/{agent_id}", response_model=AgentResponse)
-async def get_agent(
-    agent_id: str,
-    service: AgentManagementService = Depends(get_agent_service)
-):
+async def get_agent(agent_id: str, service: AgentManagementService = Depends(get_agent_service)):
     """Get agent by ID"""
     return await service.get_agent(agent_id)
+
 
 @router.get("/", response_model=AgentListResponse)
 async def list_agents(
     page: int = Query(1, ge=1),
     page_size: int = Query(20, ge=1, le=100),
     status: Optional[AgentStatus] = None,
     type: Optional[AgentType] = None,
     search: Optional[str] = None,
-    service: AgentManagementService = Depends(get_agent_service)
+    service: AgentManagementService = Depends(get_agent_service),
 ):
     """List agents with filtering and pagination"""
     return await service.list_agents(page, page_size, status, type, search)
+
 
 @router.put("/{agent_id}", response_model=AgentResponse)
 async def update_agent(
     agent_id: str,
     update_data: AgentUpdateRequest,
-    service: AgentManagementService = Depends(get_agent_service)
+    service: AgentManagementService = Depends(get_agent_service),
 ):
     """Update agent configuration"""
     return await service.update_agent(agent_id, update_data)
+
 
 @router.delete("/{agent_id}")
 async def delete_agent(
     agent_id: str,
     force: bool = Query(False, description="Force delete even with active tasks"),
-    service: AgentManagementService = Depends(get_agent_service)
+    service: AgentManagementService = Depends(get_agent_service),
 ):
     """Delete agent"""
     return await service.delete_agent(agent_id, force)
 
+
 @router.get("/{agent_id}/performance", response_model=AgentPerformanceMetrics)
 async def get_agent_performance(
-    agent_id: str,
-    service: AgentManagementService = Depends(get_agent_service)
+    agent_id: str, service: AgentManagementService = Depends(get_agent_service)
 ):
     """Get agent performance metrics"""
     return await service.get_agent_performance(agent_id)
+
 
 @router.post("/{agent_id}/assign-task")
 async def assign_task_to_agent(
     agent_id: str,
     assignment: TaskAssignmentRequest,
-    service: AgentManagementService = Depends(get_agent_service)
+    service: AgentManagementService = Depends(get_agent_service),
 ):
     """Assign task to agent"""
     return await service.assign_task(agent_id, assignment)
 
+
 @router.post("/{agent_id}/start")
-async def start_agent(
-    agent_id: str,
-    service: AgentManagementService = Depends(get_agent_service)
-):
+async def start_agent(agent_id: str, service: AgentManagementService = Depends(get_agent_service)):
     """Start/activate agent"""
     return await service.update_agent(agent_id, AgentUpdateRequest(status=AgentStatus.ACTIVE))
 
+
 @router.post("/{agent_id}/stop")
-async def stop_agent(
-    agent_id: str,
-    service: AgentManagementService = Depends(get_agent_service)
-):
+async def stop_agent(agent_id: str, service: AgentManagementService = Depends(get_agent_service)):
     """Stop/idle agent"""
     return await service.update_agent(agent_id, AgentUpdateRequest(status=AgentStatus.IDLE))
 
+
 @router.post("/{agent_id}/maintenance")
 async def set_agent_maintenance(
-    agent_id: str,
-    service: AgentManagementService = Depends(get_agent_service)
+    agent_id: str, service: AgentManagementService = Depends(get_agent_service)
 ):
     """Put agent in maintenance mode"""
     return await service.update_agent(agent_id, AgentUpdateRequest(status=AgentStatus.MAINTENANCE))
+
 
 # Health check endpoint
 @router.get("/health/check")
 async def health_check():
     """Health check for agent management system"""
     return {
         "status": "healthy",
         "timestamp": datetime.utcnow().isoformat(),
-        "service": "agent_management_api"
+        "service": "agent_management_api",
     }
would reformat /home/runner/work/ymera_y/ymera_y/agent_management_api.py
error: cannot format /home/runner/work/ymera_y/ymera_y/agent_manager_integrated (1).py: Cannot parse for target version Python 3.12: 268:0: <line number missing in source>
error: cannot format /home/runner/work/ymera_y/ymera_y/agent_manager_integrated.py: Cannot parse for target version Python 3.12: 216:50: Unexpected EOF in multi-line statement
--- /home/runner/work/ymera_y/ymera_y/agent_manager_enhancements.py	2025-10-19 22:47:02.790432+00:00
+++ /home/runner/work/ymera_y/ymera_y/agent_manager_enhancements.py	2025-10-19 23:08:50.200666+00:00
@@ -11,318 +11,330 @@
 import uuid
 
 # Structured logging setup
 logger = logging.getLogger(__name__)
 
+
 class AgentReportStatus(Enum):
     """Enhanced agent reporting status tracking"""
-    COMPLIANT = "compliant"         # Agent is reporting as required
-    WARNED = "warned"               # Agent missed reports but within grace period
-    SUSPENDED = "suspended"         # Agent missed too many reports, temporarily suspended
-    NON_COMPLIANT = "non_compliant" # Agent consistently fails to report
-    EXEMPT = "exempt"               # Agent exempt from reporting requirements
+
+    COMPLIANT = "compliant"  # Agent is reporting as required
+    WARNED = "warned"  # Agent missed reports but within grace period
+    SUSPENDED = "suspended"  # Agent missed too many reports, temporarily suspended
+    NON_COMPLIANT = "non_compliant"  # Agent consistently fails to report
+    EXEMPT = "exempt"  # Agent exempt from reporting requirements
+
 
 class AgentAction(Enum):
     """Agent lifecycle control actions"""
-    WARN = "warn"           # Send warning to agent
-    FREEZE = "freeze"       # Temporarily freeze agent activities
-    UNFREEZE = "unfreeze"   # Resume agent activities
-    DELETE = "delete"       # Permanently delete agent
-    AUDIT = "audit"         # Trigger security audit of agent
-    ISOLATE = "isolate"     # Network isolate the agent
+
+    WARN = "warn"  # Send warning to agent
+    FREEZE = "freeze"  # Temporarily freeze agent activities
+    UNFREEZE = "unfreeze"  # Resume agent activities
+    DELETE = "delete"  # Permanently delete agent
+    AUDIT = "audit"  # Trigger security audit of agent
+    ISOLATE = "isolate"  # Network isolate the agent
+
 
 class AdminApproval(Enum):
     """Admin approval status"""
+
     PENDING = "pending"
     APPROVED = "approved"
     REJECTED = "rejected"
     ESCALATED = "escalated"
 
+
 class MandatoryReportingEnforcer:
     """Enhanced mandatory reporting enforcement with escalating consequences"""
-    
+
     def __init__(self, db_manager, message_bus, notification_manager):
         self.db_manager = db_manager
         self.message_bus = message_bus
         self.notification_manager = notification_manager
-        
+
         # Reporting configuration
         self.reporting_frequency = timedelta(minutes=5)  # Required reporting interval
-        self.warning_threshold = 3    # Missed reports before warning
-        self.suspend_threshold = 5    # Missed reports before suspension
+        self.warning_threshold = 3  # Missed reports before warning
+        self.suspend_threshold = 5  # Missed reports before suspension
         self.non_compliant_threshold = 10  # Missed reports before non-compliance
-        
+
         # Track reporting status
         self.agent_reporting_status = {}  # agent_id -> AgentReportStatus
-        self.missed_reports = {}          # agent_id -> count
-        self.last_reports = {}            # agent_id -> datetime
-        
-        logger.info("Mandatory reporting enforcer initialized with thresholds: "
-                   f"warning={self.warning_threshold}, suspend={self.suspend_threshold}, "
-                   f"non_compliant={self.non_compliant_threshold}")
-    
+        self.missed_reports = {}  # agent_id -> count
+        self.last_reports = {}  # agent_id -> datetime
+
+        logger.info(
+            "Mandatory reporting enforcer initialized with thresholds: "
+            f"warning={self.warning_threshold}, suspend={self.suspend_threshold}, "
+            f"non_compliant={self.non_compliant_threshold}"
+        )
+
     async def start_monitoring(self):
         """Start background monitoring of agent reports"""
         logger.info("Starting mandatory reporting monitoring")
         while True:
             try:
                 await self._check_all_agents()
                 await asyncio.sleep(60)  # Check every minute
             except Exception as e:
                 logger.error(f"Error in reporting monitor: {e}")
                 await asyncio.sleep(300)  # Retry after 5 minutes on error
-    
+
     async def _check_all_agents(self):
         """Check reporting compliance for all active agents"""
         now = datetime.utcnow()
-        
+
         # Get all active agents
         async with self.db_manager.get_session() as session:
-            result = await session.execute("""
+            result = await session.execute(
+                """
                 SELECT id, last_heartbeat, status, reporting_exemption 
                 FROM agents
                 WHERE status != 'deleted'
-            """)
-            
+            """
+            )
+
             agents = result.fetchall()
-            
+
         for agent_id, last_heartbeat, status, exemption in agents:
             # Skip exempt agents
             if exemption:
                 self.agent_reporting_status[agent_id] = AgentReportStatus.EXEMPT
                 continue
-                
+
             # Calculate time since last report
             if last_heartbeat:
                 time_since_report = now - last_heartbeat
                 overdue = time_since_report > self.reporting_frequency
             else:
                 # No heartbeat recorded yet
                 overdue = True
-            
+
             # Update missed reports counter
             if overdue:
                 self.missed_reports[agent_id] = self.missed_reports.get(agent_id, 0) + 1
             else:
                 # Reset counter if agent reported on time
                 self.missed_reports[agent_id] = 0
                 self.agent_reporting_status[agent_id] = AgentReportStatus.COMPLIANT
-            
+
             # Apply escalating consequences
             await self._enforce_reporting_compliance(agent_id)
-    
+
     async def _enforce_reporting_compliance(self, agent_id: str):
         """Apply escalating consequences for non-reporting agents"""
         missed = self.missed_reports.get(agent_id, 0)
-        
+
         if missed >= self.non_compliant_threshold:
             # Agent is severely non-compliant
             if self.agent_reporting_status.get(agent_id) != AgentReportStatus.NON_COMPLIANT:
                 self.agent_reporting_status[agent_id] = AgentReportStatus.NON_COMPLIANT
                 await self._handle_non_compliant_agent(agent_id)
-                
+
         elif missed >= self.suspend_threshold:
             # Agent needs suspension
             if self.agent_reporting_status.get(agent_id) != AgentReportStatus.SUSPENDED:
                 self.agent_reporting_status[agent_id] = AgentReportStatus.SUSPENDED
                 await self._suspend_agent(agent_id)
-                
+
         elif missed >= self.warning_threshold:
             # Agent needs warning
             if self.agent_reporting_status.get(agent_id) != AgentReportStatus.WARNED:
                 self.agent_reporting_status[agent_id] = AgentReportStatus.WARNED
                 await self._warn_agent(agent_id)
-    
+
     async def _warn_agent(self, agent_id: str):
         """Send warning to agent about missed reports"""
-        logger.warning(f"Agent {agent_id} has missed {self.missed_reports[agent_id]} reports - sending warning")
-        
+        logger.warning(
+            f"Agent {agent_id} has missed {self.missed_reports[agent_id]} reports - sending warning"
+        )
+
         # Send warning message to agent
         await self.message_bus.publish(
             f"agent.{agent_id}.control",
             {
                 "type": "warning",
                 "reason": "missed_reports",
                 "count": self.missed_reports[agent_id],
                 "timestamp": datetime.utcnow().isoformat(),
-                "action_required": "resume_reporting"
-            }
-        )
-        
+                "action_required": "resume_reporting",
+            },
+        )
+
         # Notify admin
         await self.notification_manager.send_notification(
             "admin",
             {
                 "type": "agent_warning",
                 "title": f"Agent {agent_id} reporting warning",
                 "message": f"Agent has missed {self.missed_reports[agent_id]} required reports",
-                "severity": "medium"
-            }
-        )
-    
+                "severity": "medium",
+            },
+        )
+
     async def _suspend_agent(self, agent_id: str):
         """Temporarily suspend non-reporting agent"""
-        logger.warning(f"Agent {agent_id} has missed {self.missed_reports[agent_id]} reports - suspending")
-        
+        logger.warning(
+            f"Agent {agent_id} has missed {self.missed_reports[agent_id]} reports - suspending"
+        )
+
         # Update agent status in database
         async with self.db_manager.get_session() as session:
             await session.execute(
                 "UPDATE agents SET status = 'suspended', suspended_reason = 'missed_reports', "
                 "suspended_at = :now WHERE id = :agent_id",
-                {"now": datetime.utcnow(), "agent_id": agent_id}
-            )
-            await session.commit()
-        
+                {"now": datetime.utcnow(), "agent_id": agent_id},
+            )
+            await session.commit()
+
         # Send suspension message to agent
         await self.message_bus.publish(
             f"agent.{agent_id}.control",
             {
                 "type": "control",
                 "action": "suspend",
                 "reason": "missed_reports",
                 "count": self.missed_reports[agent_id],
-                "timestamp": datetime.utcnow().isoformat()
-            }
-        )
-        
+                "timestamp": datetime.utcnow().isoformat(),
+            },
+        )
+
         # Notify admin
         await self.notification_manager.send_notification(
             "admin",
             {
                 "type": "agent_suspended",
                 "title": f"Agent {agent_id} suspended",
                 "message": f"Agent automatically suspended after missing {self.missed_reports[agent_id]} reports",
                 "severity": "high",
-                "action_required": True
-            }
-        )
-    
+                "action_required": True,
+            },
+        )
+
     async def _handle_non_compliant_agent(self, agent_id: str):
         """Handle severely non-compliant agent - requires admin intervention"""
-        logger.error(f"Agent {agent_id} is non-compliant with {self.missed_reports[agent_id]} missed reports")
-        
+        logger.error(
+            f"Agent {agent_id} is non-compliant with {self.missed_reports[agent_id]} missed reports"
+        )
+
         # Create admin approval request for agent deletion
         approval_request_id = str(uuid.uuid4())
-        
+
         async with self.db_manager.get_session() as session:
             await session.execute(
                 "INSERT INTO admin_approvals (id, request_type, resource_id, resource_type, "
                 "action, reason, requested_at, status) "
                 "VALUES (:id, 'agent_action', :agent_id, 'agent', 'delete', "
                 "'Non-compliant with reporting requirements', :now, 'pending')",
-                {
-                    "id": approval_request_id,
-                    "agent_id": agent_id,
-                    "now": datetime.utcnow()
-                }
-            )
-            await session.commit()
-        
+                {"id": approval_request_id, "agent_id": agent_id, "now": datetime.utcnow()},
+            )
+            await session.commit()
+
         # Notify all admins of required action
         await self.notification_manager.send_notification(
             "admin",
             {
                 "type": "approval_required",
                 "title": "Agent Deletion Approval Required",
                 "message": f"Agent {agent_id} is non-compliant with reporting requirements. "
-                          f"Approval required for deletion.",
+                f"Approval required for deletion.",
                 "severity": "critical",
                 "approval_id": approval_request_id,
-                "action_required": True
-            }
-        )
-    
+                "action_required": True,
+            },
+        )
+
     async def process_agent_report(self, agent_id: str, report: dict):
         """Process incoming agent report"""
         now = datetime.utcnow()
-        
+
         # Update tracking
         self.last_reports[agent_id] = now
         self.missed_reports[agent_id] = 0
-        
+
         # If agent was previously warned or suspended, update status
         if self.agent_reporting_status.get(agent_id) in [
-            AgentReportStatus.WARNED, AgentReportStatus.SUSPENDED, AgentReportStatus.NON_COMPLIANT
+            AgentReportStatus.WARNED,
+            AgentReportStatus.SUSPENDED,
+            AgentReportStatus.NON_COMPLIANT,
         ]:
             # Reset status
             self.agent_reporting_status[agent_id] = AgentReportStatus.COMPLIANT
-            
+
             # If suspended, attempt to restore
             if report.get("status") == "suspended":
                 await self._request_unsuspend_agent(agent_id)
-        
+
         # Store report in database
         async with self.db_manager.get_session() as session:
             await session.execute(
                 "UPDATE agents SET last_heartbeat = :now, last_report = :report, "
                 "health_status = :health, resource_usage = :resources WHERE id = :agent_id",
                 {
                     "now": now,
                     "report": report,
                     "health": report.get("health_status", "unknown"),
                     "resources": report.get("resource_usage", {}),
-                    "agent_id": agent_id
-                }
-            )
-            await session.commit()
-    
+                    "agent_id": agent_id,
+                },
+            )
+            await session.commit()
+
     async def _request_unsuspend_agent(self, agent_id: str):
         """Request admin approval to unsuspend agent"""
         approval_request_id = str(uuid.uuid4())
-        
+
         async with self.db_manager.get_session() as session:
             await session.execute(
                 "INSERT INTO admin_approvals (id, request_type, resource_id, resource_type, "
                 "action, reason, requested_at, status) "
                 "VALUES (:id, 'agent_action', :agent_id, 'agent', 'unsuspend', "
                 "'Agent has resumed reporting', :now, 'pending')",
-                {
-                    "id": approval_request_id,
-                    "agent_id": agent_id,
-                    "now": datetime.utcnow()
-                }
-            )
-            await session.commit()
-        
+                {"id": approval_request_id, "agent_id": agent_id, "now": datetime.utcnow()},
+            )
+            await session.commit()
+
         # Notify admin
         await self.notification_manager.send_notification(
             "admin",
             {
                 "type": "approval_required",
                 "title": "Agent Unsuspend Approval Required",
                 "message": f"Agent {agent_id} has resumed reporting. Approve to unsuspend.",
                 "severity": "medium",
                 "approval_id": approval_request_id,
-                "action_required": True
-            }
+                "action_required": True,
+            },
         )
 
 
 class AgentLifecycleManager:
     """Enhanced agent lifecycle control with admin approval workflow"""
-    
+
     def __init__(self, db_manager, message_bus, notification_manager, security_manager):
         self.db_manager = db_manager
         self.message_bus = message_bus
         self.notification_manager = notification_manager
         self.security_manager = security_manager
-        
+
         # Track pending actions
         self.pending_actions = {}  # agent_id -> List[action_request]
-        
+
         logger.info("Agent lifecycle manager initialized")
-    
-    async def request_agent_action(self, agent_id: str, action: AgentAction, 
-                                  reason: str, requested_by: str) -> str:
+
+    async def request_agent_action(
+        self, agent_id: str, action: AgentAction, reason: str, requested_by: str
+    ) -> str:
         """Request an action on an agent - requires admin approval"""
-        
+
         # Validate action is allowed
         await self._validate_action_request(agent_id, action, requested_by)
-        
+
         # Create admin approval request
         approval_request_id = str(uuid.uuid4())
-        
+
         async with self.db_manager.get_session() as session:
             # Store request in database
             await session.execute(
                 "INSERT INTO admin_approvals (id, request_type, resource_id, resource_type, "
                 "action, reason, requested_by, requested_at, status) "
@@ -332,188 +344,192 @@
                     "id": approval_request_id,
                     "agent_id": agent_id,
                     "action": action.value,
                     "reason": reason,
                     "requested_by": requested_by,
-                    "now": datetime.utcnow()
-                }
-            )
-            await session.commit()
-        
+                    "now": datetime.utcnow(),
+                },
+            )
+            await session.commit()
+
         # Track pending action
         if agent_id not in self.pending_actions:
             self.pending_actions[agent_id] = []
         self.pending_actions[agent_id].append(approval_request_id)
-        
+
         # Notify admins
         await self.notification_manager.send_notification(
             "admin",
             {
                 "type": "approval_required",
                 "title": f"Agent {action.value.title()} Approval Required",
                 "message": f"Request to {action.value} agent {agent_id}. Reason: {reason}",
                 "severity": "high" if action == AgentAction.DELETE else "medium",
                 "approval_id": approval_request_id,
                 "action_required": True,
-                "requested_by": requested_by
-            }
-        )
-        
+                "requested_by": requested_by,
+            },
+        )
+
         # Log request
-        logger.info(f"Agent action requested: {action.value} for agent {agent_id} "
-                  f"by {requested_by}. Approval ID: {approval_request_id}")
-        
+        logger.info(
+            f"Agent action requested: {action.value} for agent {agent_id} "
+            f"by {requested_by}. Approval ID: {approval_request_id}"
+        )
+
         # Return approval request ID
         return approval_request_id
-    
+
     async def _validate_action_request(self, agent_id: str, action: AgentAction, requested_by: str):
         """Validate that the requested action is allowed"""
         # Check if agent exists
         async with self.db_manager.get_session() as session:
             result = await session.execute(
-                "SELECT status, owner_id FROM agents WHERE id = :agent_id",
-                {"agent_id": agent_id}
+                "SELECT status, owner_id FROM agents WHERE id = :agent_id", {"agent_id": agent_id}
             )
             agent = result.fetchone()
-            
+
             if not agent:
                 raise ValueError(f"Agent {agent_id} not found")
-            
+
             # Check if requester has permission (owner or admin)
             if agent[1] != requested_by:
                 # Check if user is admin
                 is_admin = await self.security_manager.user_has_role(requested_by, "admin")
                 if not is_admin:
-                    raise PermissionError(f"User {requested_by} does not have permission to {action.value} agent {agent_id}")
-            
+                    raise PermissionError(
+                        f"User {requested_by} does not have permission to {action.value} agent {agent_id}"
+                    )
+
             # Check if action makes sense for current state
             current_status = agent[0]
-            
+
             # Validate state transitions
             valid = True
             if action == AgentAction.FREEZE and current_status == "frozen":
                 valid = False
             elif action == AgentAction.UNFREEZE and current_status != "frozen":
                 valid = False
             elif action == AgentAction.DELETE and current_status == "deleted":
                 valid = False
-                
+
             if not valid:
                 raise ValueError(f"Cannot {action.value} agent with status {current_status}")
-    
-    async def process_admin_approval(self, approval_id: str, decision: AdminApproval, 
-                                   admin_id: str, notes: Optional[str] = None):
+
+    async def process_admin_approval(
+        self, approval_id: str, decision: AdminApproval, admin_id: str, notes: Optional[str] = None
+    ):
         """Process admin decision on agent action request"""
         async with self.db_manager.get_session() as session:
             # Get approval request details
             result = await session.execute(
                 "SELECT resource_id, action, requested_by FROM admin_approvals "
                 "WHERE id = :approval_id AND status = 'pending'",
-                {"approval_id": approval_id}
+                {"approval_id": approval_id},
             )
             approval = result.fetchone()
-            
+
             if not approval:
                 raise ValueError(f"Approval request {approval_id} not found or already processed")
-            
+
             agent_id, action, requested_by = approval
-            
+
             # Update approval status
             await session.execute(
                 "UPDATE admin_approvals SET status = :status, processed_by = :admin_id, "
                 "processed_at = :now, notes = :notes WHERE id = :approval_id",
                 {
                     "status": decision.value,
                     "admin_id": admin_id,
                     "now": datetime.utcnow(),
                     "notes": notes,
-                    "approval_id": approval_id
-                }
-            )
-            await session.commit()
-        
+                    "approval_id": approval_id,
+                },
+            )
+            await session.commit()
+
         # Handle decision
         if decision == AdminApproval.APPROVED:
             # Execute the approved action
             await self._execute_agent_action(agent_id, AgentAction(action), admin_id)
-            
+
             # Notify requester
             await self.notification_manager.send_notification(
                 requested_by,
                 {
                     "type": "approval_decision",
                     "title": f"Agent {action} Request Approved",
                     "message": f"Your request to {action} agent {agent_id} has been approved.",
                     "severity": "medium",
-                    "approval_id": approval_id
-                }
-            )
-        
+                    "approval_id": approval_id,
+                },
+            )
+
         elif decision == AdminApproval.REJECTED:
             # Notify requester
             await self.notification_manager.send_notification(
                 requested_by,
                 {
                     "type": "approval_decision",
                     "title": f"Agent {action} Request Rejected",
                     "message": f"Your request to {action} agent {agent_id} has been rejected. "
-                            f"Notes: {notes or 'No additional information provided.'}",
+                    f"Notes: {notes or 'No additional information provided.'}",
                     "severity": "medium",
-                    "approval_id": approval_id
-                }
-            )
-        
+                    "approval_id": approval_id,
+                },
+            )
+
         # Remove from pending actions
         if agent_id in self.pending_actions and approval_id in self.pending_actions[agent_id]:
             self.pending_actions[agent_id].remove(approval_id)
-    
+
     async def _execute_agent_action(self, agent_id: str, action: AgentAction, approved_by: str):
         """Execute an approved agent action"""
         logger.info(f"Executing approved agent action: {action.value} for agent {agent_id}")
-        
+
         # Update agent status in database
         async with self.db_manager.get_session() as session:
             if action == AgentAction.WARN:
                 # No status change, just send warning
                 pass
-                
+
             elif action == AgentAction.FREEZE:
                 await session.execute(
                     "UPDATE agents SET status = 'frozen', frozen_at = :now, "
                     "frozen_by = :approved_by WHERE id = :agent_id",
-                    {"now": datetime.utcnow(), "approved_by": approved_by, "agent_id": agent_id}
+                    {"now": datetime.utcnow(), "approved_by": approved_by, "agent_id": agent_id},
                 )
-                
+
             elif action == AgentAction.UNFREEZE:
                 await session.execute(
                     "UPDATE agents SET status = 'active', unfrozen_at = :now, "
                     "unfrozen_by = :approved_by WHERE id = :agent_id",
-                    {"now": datetime.utcnow(), "approved_by": approved_by, "agent_id": agent_id}
+                    {"now": datetime.utcnow(), "approved_by": approved_by, "agent_id": agent_id},
                 )
-                
+
             elif action == AgentAction.DELETE:
                 # Logical delete with audit trail
                 await session.execute(
                     "UPDATE agents SET status = 'deleted', deleted_at = :now, "
                     "deleted_by = :approved_by WHERE id = :agent_id",
-                    {"now": datetime.utcnow(), "approved_by": approved_by, "agent_id": agent_id}
+                    {"now": datetime.utcnow(), "approved_by": approved_by, "agent_id": agent_id},
                 )
-                
+
             elif action == AgentAction.AUDIT:
                 await session.execute(
                     "UPDATE agents SET last_audit = :now, audited_by = :approved_by, "
                     "audit_in_progress = TRUE WHERE id = :agent_id",
-                    {"now": datetime.utcnow(), "approved_by": approved_by, "agent_id": agent_id}
+                    {"now": datetime.utcnow(), "approved_by": approved_by, "agent_id": agent_id},
                 )
-                
+
             elif action == AgentAction.ISOLATE:
                 await session.execute(
                     "UPDATE agents SET status = 'isolated', isolated_at = :now, "
                     "isolated_by = :approved_by WHERE id = :agent_id",
-                    {"now": datetime.utcnow(), "approved_by": approved_by, "agent_id": agent_id}
+                    {"now": datetime.utcnow(), "approved_by": approved_by, "agent_id": agent_id},
                 )
-            
+
             # Create audit log entry
             await session.execute(
                 "INSERT INTO audit_logs (event_type, resource_type, resource_id, action, "
                 "performed_by, timestamp, details) VALUES ('agent_lifecycle', 'agent', "
                 ":agent_id, :action, :performed_by, :now, :details)",
@@ -522,125 +538,125 @@
                     "action": action.value,
                     "performed_by": approved_by,
                     "now": datetime.utcnow(),
                     "details": {
                         "approved_action": action.value,
-                    }
-                }
-            )
-            
-            await session.commit()
-        
+                    },
+                },
+            )
+
+            await session.commit()
+
         # Send control message to agent
         await self.message_bus.publish(
             f"agent.{agent_id}.control",
             {
                 "type": "control",
                 "action": action.value,
                 "timestamp": datetime.utcnow().isoformat(),
-                "approved_by": approved_by
-            }
-        )
-        
+                "approved_by": approved_by,
+            },
+        )
+
         # For deletion, perform additional cleanup
         if action == AgentAction.DELETE:
             await self._cleanup_deleted_agent(agent_id)
-    
+
     async def _cleanup_deleted_agent(self, agent_id: str):
         """Perform additional cleanup for deleted agent"""
         # Cancel any pending tasks
         async with self.db_manager.get_session() as session:
             await session.execute(
                 "UPDATE tasks SET status = 'cancelled', cancelled_reason = 'agent_deleted', "
                 "cancelled_at = :now WHERE agent_id = :agent_id AND status IN ('pending', 'running')",
-                {"now": datetime.utcnow(), "agent_id": agent_id}
-            )
-            await session.commit()
-        
+                {"now": datetime.utcnow(), "agent_id": agent_id},
+            )
+            await session.commit()
+
         # Revoke any API keys
         await self.security_manager.revoke_agent_credentials(agent_id)
-        
+
         # Unsubscribe from all topics
         await self.message_bus.unsubscribe_all(agent_id)
 
 
 class EnhancedSecurityMonitor:
     """Enhanced security monitoring and enforcement"""
-    
+
     def __init__(self, db_manager, message_bus, notification_manager, telemetry_manager):
         self.db_manager = db_manager
         self.message_bus = message_bus
         self.notification_manager = notification_manager
         self.telemetry_manager = telemetry_manager
-        
+
         # Security thresholds
         self.consecutive_auth_failures_threshold = 5
         self.suspicious_activity_score_threshold = 0.7
         self.data_volume_threshold_mb = 50  # Alert on agents transferring >50MB data
-        
+
         # Track security incidents
         self.auth_failures = {}  # agent_id -> count
         self.suspicious_activities = {}  # agent_id -> List[activity]
-        
+
         logger.info("Enhanced security monitor initialized")
-    
+
     async def start_monitoring(self):
         """Start security monitoring"""
         logger.info("Starting enhanced security monitoring")
         while True:
             try:
                 await self._scan_for_security_anomalies()
                 await asyncio.sleep(60)  # Check every minute
             except Exception as e:
                 logger.error(f"Error in security monitoring: {e}")
                 await asyncio.sleep(300)  # Retry after 5 minutes
-    
+
     async def _scan_for_security_anomalies(self):
         """Scan system for security anomalies"""
         now = datetime.utcnow()
         lookback = now - timedelta(hours=1)
-        
+
         async with self.db_manager.get_session() as session:
             # Check for authentication failures
             result = await session.execute(
                 "SELECT agent_id, COUNT(*) as failure_count FROM security_events "
                 "WHERE event_type = 'authentication_failure' AND timestamp > :lookback "
                 "GROUP BY agent_id HAVING COUNT(*) >= :threshold",
-                {"lookback": lookback, "threshold": self.consecutive_auth_failures_threshold}
-            )
-            
+                {"lookback": lookback, "threshold": self.consecutive_auth_failures_threshold},
+            )
+
             for agent_id, failure_count in result:
                 await self._handle_auth_failures(agent_id, failure_count)
-            
+
             # Check for suspicious data access patterns
             result = await session.execute(
                 "SELECT agent_id, operation_type, resource_path, COUNT(*) as access_count "
                 "FROM access_logs WHERE timestamp > :lookback "
                 "GROUP BY agent_id, operation_type, resource_path "
-                "HAVING COUNT(*) > 100", # Unusual number of accesses to same resource
-                {"lookback": lookback}
-            )
-            
+                "HAVING COUNT(*) > 100",  # Unusual number of accesses to same resource
+                {"lookback": lookback},
+            )
+
             for agent_id, op_type, resource, count in result:
                 await self._handle_suspicious_access(agent_id, op_type, resource, count)
-            
+
             # Check for large data transfers
             result = await session.execute(
                 "SELECT agent_id, SUM(data_size_bytes)/1048576 as total_mb FROM data_transfer_logs "
                 "WHERE timestamp > :lookback GROUP BY agent_id "
                 "HAVING SUM(data_size_bytes)/1048576 > :threshold",
-                {"lookback": lookback, "threshold": self.data_volume_threshold_mb}
-            )
-            
+                {"lookback": lookback, "threshold": self.data_volume_threshold_mb},
+            )
+
             for agent_id, volume_mb in result:
                 await self._handle_large_data_transfer(agent_id, volume_mb)
-    
+
     async def _handle_auth_failures(self, agent_id: str, failure_count: int):
         """Handle suspicious authentication failures"""
         # Log security incident
         incident_id = str(uuid.uuid4())
-        
+
         async with self.db_manager.get_session() as session:
             await session.execute(
                 "INSERT INTO security_incidents (id, agent_id, incident_type, severity, "
                 "detected_at, details, status) VALUES (:id, :agent_id, 'auth_failure', "
                 "'high', :now, :details, 'open')",
@@ -648,42 +664,43 @@
                     "id": incident_id,
                     "agent_id": agent_id,
                     "now": datetime.utcnow(),
                     "details": {
                         "failure_count": failure_count,
-                        "detection": "consecutive_failures"
-                    }
-                }
-            )
-            await session.commit()
-        
+                        "detection": "consecutive_failures",
+                    },
+                },
+            )
+            await session.commit()
+
         # Notify security team
         await self.notification_manager.send_notification(
             "security",
             {
                 "type": "security_incident",
                 "title": f"Multiple Authentication Failures for Agent {agent_id}",
                 "message": f"Detected {failure_count} consecutive authentication failures",
                 "severity": "high",
                 "incident_id": incident_id,
-                "action_required": True
-            }
-        )
-        
+                "action_required": True,
+            },
+        )
+
         # Request automatic agent freeze
         await self.agent_lifecycle_manager.request_agent_action(
             agent_id,
             AgentAction.FREEZE,
             f"Security incident: {failure_count} consecutive auth failures",
-            "system"
-        )
-    
-    async def _handle_suspicious_access(self, agent_id: str, op_type: str, 
-                                      resource: str, access_count: int):
+            "system",
+        )
+
+    async def _handle_suspicious_access(
+        self, agent_id: str, op_type: str, resource: str, access_count: int
+    ):
         """Handle suspicious resource access patterns"""
         incident_id = str(uuid.uuid4())
-        
+
         async with self.db_manager.get_session() as session:
             await session.execute(
                 "INSERT INTO security_incidents (id, agent_id, incident_type, severity, "
                 "detected_at, details, status) VALUES (:id, :agent_id, 'suspicious_access', "
                 "'medium', :now, :details, 'open')",
@@ -692,248 +709,254 @@
                     "agent_id": agent_id,
                     "now": datetime.utcnow(),
                     "details": {
                         "operation_type": op_type,
                         "resource": resource,
-                        "access_count": access_count
-                    }
-                }
-            )
-            await session.commit()
-        
+                        "access_count": access_count,
+                    },
+                },
+            )
+            await session.commit()
+
         # Notify security team
         await self.notification_manager.send_notification(
             "security",
             {
                 "type": "security_incident",
                 "title": f"Suspicious Access Pattern for Agent {agent_id}",
                 "message": f"Detected {access_count} {op_type} operations on {resource}",
                 "severity": "medium",
                 "incident_id": incident_id,
-                "action_required": True
-            }
-        )
-    
+                "action_required": True,
+            },
+        )
+
     async def _handle_large_data_transfer(self, agent_id: str, volume_mb: float):
         """Handle suspiciously large data transfers"""
         incident_id = str(uuid.uuid4())
-        
+
         async with self.db_manager.get_session() as session:
             await session.execute(
                 "INSERT INTO security_incidents (id, agent_id, incident_type, severity, "
                 "detected_at, details, status) VALUES (:id, :agent_id, 'data_transfer', "
                 "'high', :now, :details, 'open')",
                 {
                     "id": incident_id,
                     "agent_id": agent_id,
                     "now": datetime.utcnow(),
-                    "details": {
-                        "volume_mb": volume_mb,
-                        "threshold": self.data_volume_threshold_mb
-                    }
-                }
-            )
-            await session.commit()
-        
+                    "details": {"volume_mb": volume_mb, "threshold": self.data_volume_threshold_mb},
+                },
+            )
+            await session.commit()
+
         # Notify security team
         await self.notification_manager.send_notification(
             "security",
             {
                 "type": "security_incident",
                 "title": f"Large Data Transfer for Agent {agent_id}",
                 "message": f"Agent transferred {volume_mb:.2f} MB of data (threshold: {self.data_volume_threshold_mb} MB)",
                 "severity": "high",
                 "incident_id": incident_id,
-                "action_required": True
-            }
-        )
-        
+                "action_required": True,
+            },
+        )
+
         # Request automatic agent audit
         await self.agent_lifecycle_manager.request_agent_action(
             agent_id,
             AgentAction.AUDIT,
             f"Security incident: Large data transfer ({volume_mb:.2f} MB)",
-            "system"
-        )
+            "system",
+        )
+
 
 class DataFlowValidator:
     """Enhanced data flow validation and security"""
-    
+
     def __init__(self, db_manager, security_manager):
         self.db_manager = db_manager
         self.security_manager = security_manager
-        
+
         # Data flow rules
         self.allowed_patterns = {}  # endpoint -> allowed patterns
         self.sensitive_data_patterns = {}  # regex patterns for sensitive data
-        
+
         # Load validation rules
         self._load_validation_rules()
-        
+
         logger.info("Data flow validator initialized")
-    
+
     def _load_validation_rules(self):
         """Load validation rules from configuration"""
         # Example validation rules - would load from secure storage in production
         self.allowed_patterns = {
             "agent.register": {
                 "name": r"^[a-zA-Z0-9_\-\.]{3,50}$",
-                "capabilities": r"^[a-zA-Z0-9_\-\.]{1,20}$"  # List items pattern
+                "capabilities": r"^[a-zA-Z0-9_\-\.]{1,20}$",  # List items pattern
             },
             "task.create": {
                 "name": r"^[a-zA-Z0-9_\-\.]{3,100}$",
-                "parameters": None  # Complex validation in code
-            }
+                "parameters": None,  # Complex validation in code
+            },
         }
-        
+
         # Sensitive data patterns
         self.sensitive_data_patterns = {
             "api_key": r"[a-zA-Z0-9]{20,64}",
             "password": r"password[\"']?\s*[:=]\s*[\"'][^\"']{8,}[\"']",
             "token": r"(bearer|jwt|auth|access)[_\-\s]?token[\"']?\s*[:=]\s*[\"'][a-zA-Z0-9\._\-]{20,}[\"']",
             "ssn": r"\d{3}[-\s]?\d{2}[-\s]?\d{4}",
-            "credit_card": r"\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}"
+            "credit_card": r"\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}",
         }
-    
-    async def validate_data_flow(self, endpoint: str, data: Dict[str, Any], 
-                               context: Dict[str, Any]) -> Tuple[bool, Optional[str]]:
+
+    async def validate_data_flow(
+        self, endpoint: str, data: Dict[str, Any], context: Dict[str, Any]
+    ) -> Tuple[bool, Optional[str]]:
         """Validate data flow against rules"""
         # Check for allowed patterns
         if endpoint in self.allowed_patterns:
             patterns = self.allowed_patterns[endpoint]
             for field, pattern in patterns.items():
                 if field in data:
                     if pattern and not self._validate_pattern(data[field], pattern):
                         return False, f"Invalid format for field: {field}"
-        
+
         # Check for sensitive data leakage
         sensitive_data = self._detect_sensitive_data(data)
         if sensitive_data and not self._is_sensitive_data_allowed(endpoint, context):
             return False, f"Sensitive data detected: {', '.join(sensitive_data)}"
-        
+
         # Apply custom validations
         result, message = await self._apply_custom_validations(endpoint, data, context)
         if not result:
             return False, message
-        
+
         # Data passed validation
         return True, None
-    
+
     def _validate_pattern(self, value: Any, pattern: str) -> bool:
         """Validate value against regex pattern"""
         import re
-        
+
         if isinstance(value, str):
             return bool(re.match(pattern, value))
         elif isinstance(value, list):
             return all(bool(re.match(pattern, item)) for item in value if isinstance(item, str))
         elif isinstance(value, dict):
             # For dictionaries, patterns would need to be more complex
             return True
         else:
             return True  # Skip validation for other types
-    
+
     def _detect_sensitive_data(self, data: Any) -> List[str]:
         """Detect sensitive data in input"""
         import re
-        
+
         # Convert data to string for regex matching
         data_str = str(data)
-        
+
         detected = []
         for data_type, pattern in self.sensitive_data_patterns.items():
             if re.search(pattern, data_str):
                 detected.append(data_type)
-        
+
         return detected
-    
+
     def _is_sensitive_data_allowed(self, endpoint: str, context: Dict[str, Any]) -> bool:
         """Check if sensitive data is allowed for this endpoint"""
         # Allow sensitive data for authentication endpoints
         if endpoint.startswith("auth."):
             return True
-        
+
         # Allow sensitive data for encrypted connections with proper authorization
         if context.get("connection_encrypted", False) and context.get("authenticated", False):
             return True
-        
+
         return False
-    
-    async def _apply_custom_validations(self, endpoint: str, data: Dict[str, Any], 
-                                      context: Dict[str, Any]) -> Tuple[bool, Optional[str]]:
+
+    async def _apply_custom_validations(
+        self, endpoint: str, data: Dict[str, Any], context: Dict[str, Any]
+    ) -> Tuple[bool, Optional[str]]:
         """Apply custom validations for specific endpoints"""
-        
+
         # Task creation validation
         if endpoint == "task.create":
             # Validate task parameters don't contain executable code
             if "parameters" in data:
                 has_code = self._contains_executable_code(data["parameters"])
                 if has_code:
                     return False, "Task parameters contain executable code"
-                    
+
             # Validate task priority is appropriate for user role
             if "priority" in data and data["priority"] == "critical":
                 user_id = context.get("user_id")
                 if user_id:
                     has_permission = await self.security_manager.user_has_permission(
                         user_id, "create_critical_tasks"
                     )
                     if not has_permission:
                         return False, "User does not have permission to create critical tasks"
-        
+
         # Agent registration validation
         elif endpoint == "agent.register":
             # Validate capabilities match allowed list
             if "capabilities" in data:
                 for capability in data["capabilities"]:
                     if not await self.security_manager.is_capability_allowed(capability):
                         return False, f"Capability not allowed: {capability}"
-        
+
         # All custom validations passed
         return True, None
-    
+
     def _contains_executable_code(self, data: Any) -> bool:
         """Check if data contains patterns that might be executable code"""
         import re
-        
+
         # Convert to string for pattern matching
         data_str = str(data)
-        
+
         # Patterns that might indicate executable code
         code_patterns = [
-            r"exec\s*\(", 
-            r"eval\s*\(", 
+            r"exec\s*\(",
+            r"eval\s*\(",
             r"subprocess\.",
             r"os\.(system|popen|exec)",
             r"import\s+[a-zA-Z_][a-zA-Z0-9_]*",
             r"require\s*\(",
             r"<script>",
             r"function\s*\(",
             r"setTimeout\s*\(",
         ]
-        
+
         for pattern in code_patterns:
             if re.search(pattern, data_str):
                 return True
-        
+
         return False
 
 
 class EnhancedAuditSystem:
     """Enhanced audit logging and compliance tracking"""
-    
+
     def __init__(self, db_manager, telemetry_manager):
         self.db_manager = db_manager
         self.telemetry_manager = telemetry_manager
         logger.info("Enhanced audit system initialized")
-    
-    async def log_audit_event(self, event_type: str, resource_type: str, 
-                           resource_id: str, action: str, performed_by: str, 
-                           details: Dict[str, Any]) -> str:
+
+    async def log_audit_event(
+        self,
+        event_type: str,
+        resource_type: str,
+        resource_id: str,
+        action: str,
+        performed_by: str,
+        details: Dict[str, Any],
+    ) -> str:
         """Log a comprehensive audit event with full context"""
         audit_id = str(uuid.uuid4())
-        
+
         async with self.db_manager.get_session() as session:
             await session.execute(
                 "INSERT INTO audit_logs (id, event_type, resource_type, resource_id, "
                 "action, performed_by, timestamp, details) "
                 "VALUES (:id, :event_type, :resource_type, :resource_id, :action, "
@@ -944,70 +967,77 @@
                     "resource_type": resource_type,
                     "resource_id": resource_id,
                     "action": action,
                     "performed_by": performed_by,
                     "timestamp": datetime.utcnow(),
-                    "details": details
-                }
-            )
-            await session.commit()
-        
+                    "details": details,
+                },
+            )
+            await session.commit()
+
         # Send audit event to telemetry for real-time monitoring
-        await self.telemetry_manager.record_audit_event({
-            "audit_id": audit_id,
-            "event_type": event_type,
-            "resource_type": resource_type,
-            "resource_id": resource_id,
-            "action": action,
-            "performed_by": performed_by,
-            "timestamp": datetime.utcnow().isoformat()
-        })
-        
+        await self.telemetry_manager.record_audit_event(
+            {
+                "audit_id": audit_id,
+                "event_type": event_type,
+                "resource_type": resource_type,
+                "resource_id": resource_id,
+                "action": action,
+                "performed_by": performed_by,
+                "timestamp": datetime.utcnow().isoformat(),
+            }
+        )
+
         return audit_id
-    
+
     async def get_audit_trail(self, resource_type: str, resource_id: str) -> List[Dict[str, Any]]:
         """Get complete audit trail for a resource"""
         async with self.db_manager.get_session() as session:
             result = await session.execute(
                 "SELECT id, event_type, action, performed_by, timestamp, details "
                 "FROM audit_logs WHERE resource_type = :resource_type AND "
                 "resource_id = :resource_id ORDER BY timestamp DESC",
-                {"resource_type": resource_type, "resource_id": resource_id}
-            )
-            
+                {"resource_type": resource_type, "resource_id": resource_id},
+            )
+
             audit_logs = []
             for log in result:
-                audit_logs.append({
-                    "id": log[0],
-                    "event_type": log[1],
-                    "action": log[2],
-                    "performed_by": log[3],
-                    "timestamp": log[4],
-                    "details": log[5]
-                })
-            
+                audit_logs.append(
+                    {
+                        "id": log[0],
+                        "event_type": log[1],
+                        "action": log[2],
+                        "performed_by": log[3],
+                        "timestamp": log[4],
+                        "details": log[5],
+                    }
+                )
+
             return audit_logs
-    
-    async def get_user_activity(self, user_id: str, start_time: datetime, 
-                             end_time: datetime) -> List[Dict[str, Any]]:
+
+    async def get_user_activity(
+        self, user_id: str, start_time: datetime, end_time: datetime
+    ) -> List[Dict[str, Any]]:
         """Get all audit logs for a user's activity"""
         async with self.db_manager.get_session() as session:
             result = await session.execute(
                 "SELECT id, event_type, resource_type, resource_id, action, timestamp, "
                 "details FROM audit_logs WHERE performed_by = :user_id AND "
                 "timestamp BETWEEN :start_time AND :end_time ORDER BY timestamp DESC",
-                {"user_id": user_id, "start_time": start_time, "end_time": end_time}
-            )
-            
+                {"user_id": user_id, "start_time": start_time, "end_time": end_time},
+            )
+
             activities = []
             for log in result:
-                activities.append({
-                    "id": log[0],
-                    "event_type": log[1],
-                    "resource_type": log[2],
-                    "resource_id": log[3],
-                    "action": log[4],
-                    "timestamp": log[5],
-                    "details": log[6]
-                })
-            
-            return activities
\ No newline at end of file
+                activities.append(
+                    {
+                        "id": log[0],
+                        "event_type": log[1],
+                        "resource_type": log[2],
+                        "resource_id": log[3],
+                        "action": log[4],
+                        "timestamp": log[5],
+                        "details": log[6],
+                    }
+                )
+
+            return activities
would reformat /home/runner/work/ymera_y/ymera_y/agent_manager_enhancements.py
--- /home/runner/work/ymera_y/ymera_y/agent_manager_integration.py	2025-10-19 22:47:02.790432+00:00
+++ /home/runner/work/ymera_y/ymera_y/agent_manager_integration.py	2025-10-19 23:08:50.951247+00:00
@@ -24,20 +24,21 @@
 logger = logging.getLogger(__name__)
 
 
 class AgentManagerOperationResult(Enum):
     """Operation result status"""
+
     SUCCESS = "success"
     PARTIAL_SUCCESS = "partial_success"
     FAILURE = "failure"
     PENDING = "pending"
 
 
 class AgentManager:
     """
     Unified Agent Management System
-    
+
     Provides a single interface for all agent management operations including:
     - Agent lifecycle management
     - Real-time surveillance and monitoring
     - Intelligent task orchestration
     - Performance analytics
@@ -48,830 +49,746 @@
         self,
         db_manager: SecureDatabaseManager,
         rbac_manager: RBACManager,
         telemetry_manager: TelemetryManager,
         alert_manager: AlertManager,
-        ai_service: MultiModalAIService
+        ai_service: MultiModalAIService,
     ):
         # Initialize sub-systems
         self.lifecycle_manager = AgentLifecycleManager(
             db_manager, rbac_manager, telemetry_manager, alert_manager
         )
-        
-        self.surveillance_system = AgentSurveillanceSystem(
-            db_manager, ai_service, alert_manager
-        )
-        
+
+        self.surveillance_system = AgentSurveillanceSystem(db_manager, ai_service, alert_manager)
+
         self.orchestrator = IntelligentAgentOrchestrator(ai_service)
-        
+
         # Core dependencies
         self.db_manager = db_manager
         self.telemetry_manager = telemetry_manager
         self.alert_manager = alert_manager
-        
+
         # State management
         self.active_agents: Dict[str, Dict] = {}
         self.agent_conversations: Dict[str, List[Dict]] = {}
-        
+
         logger.info("AgentManager initialized successfully")
 
     async def start(self):
         """Start all agent management sub-systems"""
         logger.info("Starting Agent Management System...")
-        
+
         tasks = [
             asyncio.create_task(self.surveillance_system.start_monitoring(), name="surveillance"),
-            asyncio.create_task(self.orchestrator.start_performance_monitoring(), name="orchestration"),
+            asyncio.create_task(
+                self.orchestrator.start_performance_monitoring(), name="orchestration"
+            ),
             asyncio.create_task(self._sync_agents_periodically(), name="sync"),
-            asyncio.create_task(self._cleanup_stale_data(), name="cleanup")
+            asyncio.create_task(self._cleanup_stale_data(), name="cleanup"),
         ]
-        
+
         logger.info("Agent Management System started successfully")
-        
+
         try:
             await asyncio.gather(*tasks)
         except asyncio.CancelledError:
             logger.info("Agent Management System shutdown requested")
             for task in tasks:
                 task.cancel()
 
     async def register_agent(
-        self,
-        tenant_id: str,
-        registration_data: Dict[str, Any],
-        requester_id: str
+        self, tenant_id: str, registration_data: Dict[str, Any], requester_id: str
     ) -> Dict[str, Any]:
         """
         Register a new agent
-        
+
         This method:
         1. Validates registration data
         2. Creates agent record
         3. Provisions resources
         4. Initializes monitoring
         """
         try:
             from agent_lifecycle_manager import AgentRegistrationRequest
-            
+
             # Validate and create registration request
             registration = AgentRegistrationRequest(**registration_data)
-            
+
             # Register agent
             agent = await self.lifecycle_manager.register_agent(
                 tenant_id, registration, requester_id
             )
-            
+
             # Initialize orchestrator profile
             from agent_orchestrator import AgentProfile, AgentCapability, AgentPerformanceLevel
-            
+
             profile = AgentProfile(
                 agent_id=agent.id,
-                capabilities=[AgentCapability(c) for c in registration.capabilities.get('types', [])],
+                capabilities=[
+                    AgentCapability(c) for c in registration.capabilities.get("types", [])
+                ],
                 performance_metrics={
-                    'success_rate': 1.0,
-                    'avg_response_time': 100.0,
-                    'throughput': 0,
-                    'error_rate': 0.0
+                    "success_rate": 1.0,
+                    "avg_response_time": 100.0,
+                    "throughput": 0,
+                    "error_rate": 0.0,
                 },
                 current_load=0.0,
-                max_capacity=registration.capabilities.get('max_concurrent_tasks', 10),
+                max_capacity=registration.capabilities.get("max_concurrent_tasks", 10),
                 performance_level=AgentPerformanceLevel.GOOD,
-                last_updated=datetime.utcnow()
+                last_updated=datetime.utcnow(),
             )
-            
+
             self.orchestrator.update_agent_profile(profile)
-            
+
             # Track active agent
             self.active_agents[agent.id] = {
-                'agent': agent,
-                'profile': profile,
-                'registered_at': datetime.utcnow()
-            }
-            
+                "agent": agent,
+                "profile": profile,
+                "registered_at": datetime.utcnow(),
+            }
+
             return {
-                'status': AgentManagerOperationResult.SUCCESS.value,
-                'agent_id': agent.id,
-                'message': 'Agent registered and provisioning started',
-                'agent': {
-                    'id': agent.id,
-                    'name': agent.name,
-                    'type': agent.type,
-                    'status': agent.status,
-                    'capabilities': agent.capabilities
-                }
-            }
-            
+                "status": AgentManagerOperationResult.SUCCESS.value,
+                "agent_id": agent.id,
+                "message": "Agent registered and provisioning started",
+                "agent": {
+                    "id": agent.id,
+                    "name": agent.name,
+                    "type": agent.type,
+                    "status": agent.status,
+                    "capabilities": agent.capabilities,
+                },
+            }
+
         except Exception as e:
             logger.error(f"Agent registration failed: {e}", exc_info=True)
-            return {
-                'status': AgentManagerOperationResult.FAILURE.value,
-                'error': str(e)
-            }
+            return {"status": AgentManagerOperationResult.FAILURE.value, "error": str(e)}
 
     async def update_agent_metrics(
-        self,
-        agent_id: str,
-        metrics_data: Dict[str, Any]
+        self, agent_id: str, metrics_data: Dict[str, Any]
     ) -> Dict[str, Any]:
         """
         Update agent metrics from heartbeat
-        
+
         This method:
         1. Updates lifecycle metrics
         2. Feeds surveillance system
         3. Updates orchestrator profile
         """
         try:
             # Create metrics snapshot
             metrics = AgentMetricsSnapshot(**metrics_data)
-            
+
             # Update lifecycle manager
             await self.lifecycle_manager.update_agent_metrics(agent_id, metrics)
-            
+
             # Update orchestrator profile
             if agent_id in self.active_agents:
-                profile = self.active_agents[agent_id]['profile']
-                profile.performance_metrics.update({
-                    'avg_response_time': metrics.response_time_avg,
-                    'error_rate': metrics.error_rate,
-                    'throughput': metrics.completed_tasks
-                })
+                profile = self.active_agents[agent_id]["profile"]
+                profile.performance_metrics.update(
+                    {
+                        "avg_response_time": metrics.response_time_avg,
+                        "error_rate": metrics.error_rate,
+                        "throughput": metrics.completed_tasks,
+                    }
+                )
                 profile.last_updated = datetime.utcnow()
                 self.orchestrator.update_agent_profile(profile)
-            
+
             return {
-                'status': AgentManagerOperationResult.SUCCESS.value,
-                'message': 'Metrics updated successfully'
-            }
-            
+                "status": AgentManagerOperationResult.SUCCESS.value,
+                "message": "Metrics updated successfully",
+            }
+
         except Exception as e:
             logger.error(f"Metrics update failed for {agent_id}: {e}", exc_info=True)
-            return {
-                'status': AgentManagerOperationResult.FAILURE.value,
-                'error': str(e)
-            }
-
-    async def assign_task(
-        self,
-        task_data: Dict[str, Any]
-    ) -> Dict[str, Any]:
+            return {"status": AgentManagerOperationResult.FAILURE.value, "error": str(e)}
+
+    async def assign_task(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
         """
         Assign task to optimal agent using intelligent orchestration
         """
         try:
             from agent_orchestrator import TaskRequirement, TaskPriority, AgentCapability
-            
+
             # Create task requirement
             task = TaskRequirement(
-                task_id=task_data['task_id'],
-                required_capabilities=[AgentCapability(c) for c in task_data.get('required_capabilities', [])],
-                priority=TaskPriority(task_data.get('priority', 'normal')),
-                deadline=task_data.get('deadline'),
-                estimated_duration=task_data.get('estimated_duration', 60.0),
-                resource_requirements=task_data.get('resource_requirements', {}),
-                tenant_id=task_data.get('tenant_id')
+                task_id=task_data["task_id"],
+                required_capabilities=[
+                    AgentCapability(c) for c in task_data.get("required_capabilities", [])
+                ],
+                priority=TaskPriority(task_data.get("priority", "normal")),
+                deadline=task_data.get("deadline"),
+                estimated_duration=task_data.get("estimated_duration", 60.0),
+                resource_requirements=task_data.get("resource_requirements", {}),
+                tenant_id=task_data.get("tenant_id"),
             )
-            
+
             # Assign using orchestrator
             assignment = await self.orchestrator.assign_task(task)
-            
+
             return {
-                'status': AgentManagerOperationResult.SUCCESS.value,
-                'assignment': {
-                    'task_id': assignment.task_id,
-                    'agent_id': assignment.agent_id,
-                    'confidence': assignment.confidence_score,
-                    'estimated_completion': assignment.estimated_completion_time.isoformat(),
-                    'alternatives': assignment.alternative_agents,
-                    'reason': assignment.assignment_reason
-                }
-            }
-            
+                "status": AgentManagerOperationResult.SUCCESS.value,
+                "assignment": {
+                    "task_id": assignment.task_id,
+                    "agent_id": assignment.agent_id,
+                    "confidence": assignment.confidence_score,
+                    "estimated_completion": assignment.estimated_completion_time.isoformat(),
+                    "alternatives": assignment.alternative_agents,
+                    "reason": assignment.assignment_reason,
+                },
+            }
+
         except Exception as e:
             logger.error(f"Task assignment failed: {e}", exc_info=True)
-            return {
-                'status': AgentManagerOperationResult.FAILURE.value,
-                'error': str(e)
-            }
+            return {"status": AgentManagerOperationResult.FAILURE.value, "error": str(e)}
 
     async def record_task_completion(
-        self,
-        task_id: str,
-        agent_id: str,
-        completion_data: Dict[str, Any]
+        self, task_id: str, agent_id: str, completion_data: Dict[str, Any]
     ) -> Dict[str, Any]:
         """
         Record task completion for learning and analytics
         """
         try:
-            success = completion_data.get('success', False)
-            actual_duration = completion_data.get('duration', 0.0)
-            metrics = completion_data.get('metrics', {})
-            
+            success = completion_data.get("success", False)
+            actual_duration = completion_data.get("duration", 0.0)
+            metrics = completion_data.get("metrics", {})
+
             # Update orchestrator
             await self.orchestrator.record_task_completion(
                 task_id, agent_id, success, actual_duration, metrics
             )
-            
+
             # Record telemetry
             await self.telemetry_manager.record_event(
                 "task_completed",
                 {
-                    'task_id': task_id,
-                    'agent_id': agent_id,
-                    'success': success,
-                    'duration': actual_duration
-                }
+                    "task_id": task_id,
+                    "agent_id": agent_id,
+                    "success": success,
+                    "duration": actual_duration,
+                },
             )
-            
+
             return {
-                'status': AgentManagerOperationResult.SUCCESS.value,
-                'message': 'Task completion recorded'
-            }
-            
+                "status": AgentManagerOperationResult.SUCCESS.value,
+                "message": "Task completion recorded",
+            }
+
         except Exception as e:
             logger.error(f"Failed to record task completion: {e}", exc_info=True)
-            return {
-                'status': AgentManagerOperationResult.FAILURE.value,
-                'error': str(e)
-            }
+            return {"status": AgentManagerOperationResult.FAILURE.value, "error": str(e)}
 
     async def get_agent_status(self, agent_id: str) -> Dict[str, Any]:
         """Get comprehensive agent status"""
         try:
             # Get from lifecycle manager
             agent = await self.lifecycle_manager.get_agent(agent_id)
             if not agent:
-                return {'status': 'not_found'}
-            
+                return {"status": "not_found"}
+
             # Get surveillance report
-            surveillance_report = await self.surveillance_system.get_agent_surveillance_report(agent_id)
-            
+            surveillance_report = await self.surveillance_system.get_agent_surveillance_report(
+                agent_id
+            )
+
             # Get analytics
             analytics = await self.lifecycle_manager.get_agent_analytics(agent_id, 24)
-            
+
             # Get orchestration info
             orchestration_data = {}
             if agent_id in self.active_agents:
-                profile = self.active_agents[agent_id]['profile']
+                profile = self.active_agents[agent_id]["profile"]
                 orchestration_data = {
-                    'current_load': profile.current_load,
-                    'max_capacity': profile.max_capacity,
-                    'utilization': profile.current_load / profile.max_capacity,
-                    'performance_level': profile.performance_level.value
+                    "current_load": profile.current_load,
+                    "max_capacity": profile.max_capacity,
+                    "utilization": profile.current_load / profile.max_capacity,
+                    "performance_level": profile.performance_level.value,
                 }
-            
+
             return {
-                'agent_id': agent_id,
-                'basic_info': {
-                    'name': agent.name,
-                    'type': agent.type,
-                    'status': agent.status,
-                    'version': agent.version,
-                    'security_score': agent.security_score
+                "agent_id": agent_id,
+                "basic_info": {
+                    "name": agent.name,
+                    "type": agent.type,
+                    "status": agent.status,
+                    "version": agent.version,
+                    "security_score": agent.security_score,
                 },
-                'surveillance': surveillance_report,
-                'analytics': analytics,
-                'orchestration': orchestration_data,
-                'last_updated': datetime.utcnow().isoformat()
-            }
-            
+                "surveillance": surveillance_report,
+                "analytics": analytics,
+                "orchestration": orchestration_data,
+                "last_updated": datetime.utcnow().isoformat(),
+            }
+
         except Exception as e:
             logger.error(f"Failed to get agent status: {e}", exc_info=True)
-            return {'status': 'error', 'error': str(e)}
+            return {"status": "error", "error": str(e)}
 
     async def get_system_health(self) -> Dict[str, Any]:
         """Get overall system health"""
         try:
             lifecycle_health = await self.lifecycle_manager.health_check()
             surveillance_health = await self.surveillance_system.health_check()
             orchestration_health = await self.orchestrator.health_check()
-            
+
             orchestration_analytics = await self.orchestrator.get_orchestration_analytics()
-            
+
             return {
-                'timestamp': datetime.utcnow().isoformat(),
-                'overall_status': 'healthy' if all(h == 'healthy' for h in [lifecycle_health, surveillance_health, orchestration_health]) else 'degraded',
-                'components': {
-                    'lifecycle_manager': lifecycle_health,
-                    'surveillance_system': surveillance_health,
-                    'orchestrator': orchestration_health
+                "timestamp": datetime.utcnow().isoformat(),
+                "overall_status": (
+                    "healthy"
+                    if all(
+                        h == "healthy"
+                        for h in [lifecycle_health, surveillance_health, orchestration_health]
+                    )
+                    else "degraded"
+                ),
+                "components": {
+                    "lifecycle_manager": lifecycle_health,
+                    "surveillance_system": surveillance_health,
+                    "orchestrator": orchestration_health,
                 },
-                'metrics': {
-                    'total_agents': len(self.active_agents),
-                    'orchestration': orchestration_analytics
-                }
-            }
-            
+                "metrics": {
+                    "total_agents": len(self.active_agents),
+                    "orchestration": orchestration_analytics,
+                },
+            }
+
         except Exception as e:
             logger.error(f"Health check failed: {e}", exc_info=True)
-            return {'overall_status': 'unhealthy', 'error': str(e)}
+            return {"overall_status": "unhealthy", "error": str(e)}
 
     async def quarantine_agent(
-        self,
-        agent_id: str,
-        reason: str,
-        requester_id: str
+        self, agent_id: str, reason: str, requester_id: str
     ) -> Dict[str, Any]:
         """Quarantine an agent for security reasons"""
         try:
-            success = await self.lifecycle_manager.quarantine_agent(
-                agent_id, reason, requester_id
-            )
-            
+            success = await self.lifecycle_manager.quarantine_agent(agent_id, reason, requester_id)
+
             if success:
                 # Remove from active orchestration
                 if agent_id in self.active_agents:
                     del self.active_agents[agent_id]
-                
+
                 return {
-                    'status': AgentManagerOperationResult.SUCCESS.value,
-                    'message': f'Agent {agent_id} quarantined successfully'
+                    "status": AgentManagerOperationResult.SUCCESS.value,
+                    "message": f"Agent {agent_id} quarantined successfully",
                 }
             else:
                 return {
-                    'status': AgentManagerOperationResult.FAILURE.value,
-                    'message': 'Failed to quarantine agent'
+                    "status": AgentManagerOperationResult.FAILURE.value,
+                    "message": "Failed to quarantine agent",
                 }
-                
+
         except Exception as e:
             logger.error(f"Quarantine failed: {e}", exc_info=True)
-            return {
-                'status': AgentManagerOperationResult.FAILURE.value,
-                'error': str(e)
-            }
+            return {"status": AgentManagerOperationResult.FAILURE.value, "error": str(e)}
 
     async def decommission_agent(
-        self,
-        agent_id: str,
-        reason: str,
-        requester_id: str
+        self, agent_id: str, reason: str, requester_id: str
     ) -> Dict[str, Any]:
         """Gracefully decommission an agent"""
         try:
             success = await self.lifecycle_manager.decommission_agent(
                 agent_id, reason, requester_id
             )
-            
+
             if success:
                 # Remove from active tracking
                 if agent_id in self.active_agents:
                     del self.active_agents[agent_id]
-                
+
                 return {
-                    'status': AgentManagerOperationResult.SUCCESS.value,
-                    'message': f'Agent {agent_id} decommissioning initiated'
+                    "status": AgentManagerOperationResult.SUCCESS.value,
+                    "message": f"Agent {agent_id} decommissioning initiated",
                 }
             else:
                 return {
-                    'status': AgentManagerOperationResult.FAILURE.value,
-                    'message': 'Failed to initiate decommissioning'
+                    "status": AgentManagerOperationResult.FAILURE.value,
+                    "message": "Failed to initiate decommissioning",
                 }
-                
+
         except Exception as e:
             logger.error(f"Decommission failed: {e}", exc_info=True)
-            return {
-                'status': AgentManagerOperationResult.FAILURE.value,
-                'error': str(e)
-            }
-
-    async def get_capacity_forecast(
-        self,
-        forecast_days: int = 30
-    ) -> Dict[str, Any]:
+            return {"status": AgentManagerOperationResult.FAILURE.value, "error": str(e)}
+
+    async def get_capacity_forecast(self, forecast_days: int = 30) -> Dict[str, Any]:
         """Get capacity planning forecast"""
         try:
             forecast = await self.orchestrator.capacity_planning(forecast_days)
-            return {
-                'status': AgentManagerOperationResult.SUCCESS.value,
-                'forecast': forecast
-            }
+            return {"status": AgentManagerOperationResult.SUCCESS.value, "forecast": forecast}
         except Exception as e:
             logger.error(f"Capacity forecast failed: {e}", exc_info=True)
-            return {
-                'status': AgentManagerOperationResult.FAILURE.value,
-                'error': str(e)
-            }
+            return {"status": AgentManagerOperationResult.FAILURE.value, "error": str(e)}
 
     async def optimize_load_distribution(self) -> Dict[str, Any]:
         """Optimize load distribution across agents"""
         try:
             result = await self.orchestrator.intelligent_load_balancing()
-            return {
-                'status': AgentManagerOperationResult.SUCCESS.value,
-                'optimization': result
-            }
+            return {"status": AgentManagerOperationResult.SUCCESS.value, "optimization": result}
         except Exception as e:
             logger.error(f"Load optimization failed: {e}", exc_info=True)
-            return {
-                'status': AgentManagerOperationResult.FAILURE.value,
-                'error': str(e)
-            }
+            return {"status": AgentManagerOperationResult.FAILURE.value, "error": str(e)}
 
     async def get_maintenance_schedule(self) -> Dict[str, Any]:
         """Get predictive maintenance schedule"""
         try:
             schedule = await self.orchestrator.predictive_maintenance()
-            return {
-                'status': AgentManagerOperationResult.SUCCESS.value,
-                'schedule': schedule
-            }
+            return {"status": AgentManagerOperationResult.SUCCESS.value, "schedule": schedule}
         except Exception as e:
             logger.error(f"Maintenance schedule failed: {e}", exc_info=True)
-            return {
-                'status': AgentManagerOperationResult.FAILURE.value,
-                'error': str(e)
-            }
+            return {"status": AgentManagerOperationResult.FAILURE.value, "error": str(e)}
 
     async def track_conversation(
-        self,
-        conversation_id: str,
-        agent_id: str,
-        user_id: str,
-        message_data: Dict[str, Any]
+        self, conversation_id: str, agent_id: str, user_id: str, message_data: Dict[str, Any]
     ) -> None:
         """Track agent-user conversation for quality monitoring"""
         if agent_id not in self.agent_conversations:
             self.agent_conversations[agent_id] = []
-        
-        self.agent_conversations[agent_id].append({
-            'conversation_id': conversation_id,
-            'user_id': user_id,
-            'timestamp': datetime.utcnow(),
-            'message': message_data
-        })
-        
+
+        self.agent_conversations[agent_id].append(
+            {
+                "conversation_id": conversation_id,
+                "user_id": user_id,
+                "timestamp": datetime.utcnow(),
+                "message": message_data,
+            }
+        )
+
         # Keep only recent conversations
         max_conversations = 1000
         if len(self.agent_conversations[agent_id]) > max_conversations:
-            self.agent_conversations[agent_id] = self.agent_conversations[agent_id][-max_conversations:]
-
-    async def get_agent_conversations(
-        self,
-        agent_id: str,
-        limit: int = 100
-    ) -> List[Dict]:
+            self.agent_conversations[agent_id] = self.agent_conversations[agent_id][
+                -max_conversations:
+            ]
+
+    async def get_agent_conversations(self, agent_id: str, limit: int = 100) -> List[Dict]:
         """Get recent conversations for an agent"""
         conversations = self.agent_conversations.get(agent_id, [])
         return conversations[-limit:]
 
     async def _sync_agents_periodically(self):
         """Periodically sync agent states between subsystems"""
         while True:
             try:
                 await asyncio.sleep(300)  # Every 5 minutes
-                
+
                 # Get all active agents from database
                 agents = await self.lifecycle_manager.list_agents(
-                    tenant_id=None,  # Get all tenants
-                    status='active'
+                    tenant_id=None, status="active"  # Get all tenants
                 )
-                
+
                 # Sync with orchestrator
                 for agent in agents:
                     if agent.id in self.active_agents:
-                        profile = self.active_agents[agent.id]['profile']
-                        
+                        profile = self.active_agents[agent.id]["profile"]
+
                         # Update from database
                         profile.performance_metrics.update(agent.performance_metrics or {})
                         profile.last_updated = agent.last_heartbeat or datetime.utcnow()
-                        
+
                         # Re-register with orchestrator
                         self.orchestrator.update_agent_profile(profile)
-                
+
                 logger.debug(f"Synced {len(agents)} agents")
-                
+
             except asyncio.CancelledError:
                 break
             except Exception as e:
                 logger.error(f"Agent sync error: {e}", exc_info=True)
 
     async def _cleanup_stale_data(self):
         """Clean up stale data periodically"""
         while True:
             try:
                 await asyncio.sleep(3600)  # Every hour
-                
+
                 # Clean up old conversations
                 cutoff = datetime.utcnow() - timedelta(days=7)
                 for agent_id in list(self.agent_conversations.keys()):
                     conversations = self.agent_conversations[agent_id]
                     self.agent_conversations[agent_id] = [
-                        c for c in conversations 
-                        if c['timestamp'] > cutoff
+                        c for c in conversations if c["timestamp"] > cutoff
                     ]
-                
+
                 # Clean up inactive agents from active tracking
                 inactive_agents = []
                 for agent_id, data in self.active_agents.items():
-                    if (datetime.utcnow() - data['profile'].last_updated).total_seconds() > 3600:
+                    if (datetime.utcnow() - data["profile"].last_updated).total_seconds() > 3600:
                         inactive_agents.append(agent_id)
-                
+
                 for agent_id in inactive_agents:
                     del self.active_agents[agent_id]
-                
+
                 if inactive_agents:
                     logger.info(f"Cleaned up {len(inactive_agents)} inactive agents from tracking")
-                
+
             except asyncio.CancelledError:
                 break
             except Exception as e:
                 logger.error(f"Cleanup error: {e}", exc_info=True)
 
     async def generate_comprehensive_report(
-        self,
-        tenant_id: Optional[str] = None,
-        report_type: str = 'full'
+        self, tenant_id: Optional[str] = None, report_type: str = "full"
     ) -> Dict[str, Any]:
         """
         Generate comprehensive management report
-        
+
         Report types:
         - full: Complete system report
         - performance: Performance metrics only
         - security: Security posture only
         - capacity: Capacity and forecasting only
         """
         try:
             report = {
-                'generated_at': datetime.utcnow().isoformat(),
-                'report_type': report_type,
-                'tenant_id': tenant_id
-            }
-            
-            if report_type in ['full', 'performance']:
+                "generated_at": datetime.utcnow().isoformat(),
+                "report_type": report_type,
+                "tenant_id": tenant_id,
+            }
+
+            if report_type in ["full", "performance"]:
                 # Get orchestration analytics
                 orchestration = await self.orchestrator.get_orchestration_analytics()
-                report['performance'] = orchestration
-            
-            if report_type in ['full', 'security']:
+                report["performance"] = orchestration
+
+            if report_type in ["full", "security"]:
                 # Get surveillance summary
-                surveillance_summary = await self.surveillance_system._generate_surveillance_summary()
-                report['security'] = surveillance_summary
-            
-            if report_type in ['full', 'capacity']:
+                surveillance_summary = (
+                    await self.surveillance_system._generate_surveillance_summary()
+                )
+                report["security"] = surveillance_summary
+
+            if report_type in ["full", "capacity"]:
                 # Get capacity forecast
                 capacity = await self.orchestrator.capacity_planning()
-                report['capacity'] = capacity
-            
-            if report_type == 'full':
+                report["capacity"] = capacity
+
+            if report_type == "full":
                 # Get system health
                 health = await self.get_system_health()
-                report['system_health'] = health
-                
+                report["system_health"] = health
+
                 # Get maintenance schedule
                 maintenance = await self.orchestrator.predictive_maintenance()
-                report['maintenance_schedule'] = maintenance
-            
-            return {
-                'status': AgentManagerOperationResult.SUCCESS.value,
-                'report': report
-            }
-            
+                report["maintenance_schedule"] = maintenance
+
+            return {"status": AgentManagerOperationResult.SUCCESS.value, "report": report}
+
         except Exception as e:
             logger.error(f"Report generation failed: {e}", exc_info=True)
-            return {
-                'status': AgentManagerOperationResult.FAILURE.value,
-                'error': str(e)
-            }
+            return {"status": AgentManagerOperationResult.FAILURE.value, "error": str(e)}
 
     async def bulk_operations(
-        self,
-        operation: str,
-        agent_ids: List[str],
-        parameters: Dict[str, Any]
+        self, operation: str, agent_ids: List[str], parameters: Dict[str, Any]
     ) -> Dict[str, Any]:
         """
         Perform bulk operations on multiple agents
-        
+
         Supported operations:
         - update_status
         - restart
         - upgrade
         - maintenance
         """
-        results = {
-            'total': len(agent_ids),
-            'successful': 0,
-            'failed': 0,
-            'details': []
-        }
-        
+        results = {"total": len(agent_ids), "successful": 0, "failed": 0, "details": []}
+
         for agent_id in agent_ids:
             try:
-                if operation == 'update_status':
+                if operation == "update_status":
                     # Update status operation
                     from agent_lifecycle_manager import AgentStatus
-                    new_status = AgentStatus(parameters['status'])
-                    success = await self.lifecycle_manager.update_agent_status(
-                        agent_id, new_status
-                    )
-                    
+
+                    new_status = AgentStatus(parameters["status"])
+                    success = await self.lifecycle_manager.update_agent_status(agent_id, new_status)
+
                     if success:
-                        results['successful'] += 1
-                        results['details'].append({
-                            'agent_id': agent_id,
-                            'status': 'success'
-                        })
+                        results["successful"] += 1
+                        results["details"].append({"agent_id": agent_id, "status": "success"})
                     else:
-                        results['failed'] += 1
-                        results['details'].append({
-                            'agent_id': agent_id,
-                            'status': 'failed',
-                            'reason': 'Status update failed'
-                        })
-                
-                elif operation == 'restart':
+                        results["failed"] += 1
+                        results["details"].append(
+                            {
+                                "agent_id": agent_id,
+                                "status": "failed",
+                                "reason": "Status update failed",
+                            }
+                        )
+
+                elif operation == "restart":
                     # Trigger restart via remediation
                     success = await self.lifecycle_manager._execute_remediation_step(
-                        agent_id, 'restart'
+                        agent_id, "restart"
                     )
-                    
+
                     if success:
-                        results['successful'] += 1
-                        results['details'].append({
-                            'agent_id': agent_id,
-                            'status': 'success'
-                        })
+                        results["successful"] += 1
+                        results["details"].append({"agent_id": agent_id, "status": "success"})
                     else:
-                        results['failed'] += 1
-                        results['details'].append({
-                            'agent_id': agent_id,
-                            'status': 'failed',
-                            'reason': 'Restart failed'
-                        })
-                
+                        results["failed"] += 1
+                        results["details"].append(
+                            {"agent_id": agent_id, "status": "failed", "reason": "Restart failed"}
+                        )
+
                 # Add more operations as needed
-                
+
             except Exception as e:
-                results['failed'] += 1
-                results['details'].append({
-                    'agent_id': agent_id,
-                    'status': 'failed',
-                    'reason': str(e)
-                })
-        
+                results["failed"] += 1
+                results["details"].append(
+                    {"agent_id": agent_id, "status": "failed", "reason": str(e)}
+                )
+
         return {
-            'status': AgentManagerOperationResult.SUCCESS.value if results['failed'] == 0 else AgentManagerOperationResult.PARTIAL_SUCCESS.value,
-            'results': results
+            "status": (
+                AgentManagerOperationResult.SUCCESS.value
+                if results["failed"] == 0
+                else AgentManagerOperationResult.PARTIAL_SUCCESS.value
+            ),
+            "results": results,
         }
 
     async def search_agents(
-        self,
-        query: Dict[str, Any],
-        tenant_id: Optional[str] = None
+        self, query: Dict[str, Any], tenant_id: Optional[str] = None
     ) -> Dict[str, Any]:
         """
         Advanced agent search with filtering
-        
+
         Query parameters:
         - status: Agent status
         - capabilities: Required capabilities
         - performance_level: Performance level
         - min_security_score: Minimum security score
         - tags: Agent tags
         """
         try:
             # Get base list
             agents = await self.lifecycle_manager.list_agents(
-                tenant_id=tenant_id,
-                status=query.get('status'),
-                agent_type=query.get('type')
+                tenant_id=tenant_id, status=query.get("status"), agent_type=query.get("type")
             )
-            
+
             # Apply additional filters
             filtered_agents = []
-            
+
             for agent in agents:
                 # Check capabilities
-                if 'capabilities' in query:
-                    required_caps = set(query['capabilities'])
-                    agent_caps = set(agent.capabilities.get('types', []))
+                if "capabilities" in query:
+                    required_caps = set(query["capabilities"])
+                    agent_caps = set(agent.capabilities.get("types", []))
                     if not required_caps.issubset(agent_caps):
                         continue
-                
+
                 # Check security score
-                if 'min_security_score' in query:
-                    if agent.security_score < query['min_security_score']:
+                if "min_security_score" in query:
+                    if agent.security_score < query["min_security_score"]:
                         continue
-                
+
                 # Check performance level
-                if 'performance_level' in query:
+                if "performance_level" in query:
                     if agent.id in self.active_agents:
-                        profile = self.active_agents[agent.id]['profile']
-                        if profile.performance_level.value != query['performance_level']:
+                        profile = self.active_agents[agent.id]["profile"]
+                        if profile.performance_level.value != query["performance_level"]:
                             continue
-                
-                filtered_agents.append({
-                    'id': agent.id,
-                    'name': agent.name,
-                    'type': agent.type,
-                    'status': agent.status,
-                    'security_score': agent.security_score,
-                    'capabilities': agent.capabilities
-                })
-            
+
+                filtered_agents.append(
+                    {
+                        "id": agent.id,
+                        "name": agent.name,
+                        "type": agent.type,
+                        "status": agent.status,
+                        "security_score": agent.security_score,
+                        "capabilities": agent.capabilities,
+                    }
+                )
+
             return {
-                'status': AgentManagerOperationResult.SUCCESS.value,
-                'total_results': len(filtered_agents),
-                'agents': filtered_agents
-            }
-            
+                "status": AgentManagerOperationResult.SUCCESS.value,
+                "total_results": len(filtered_agents),
+                "agents": filtered_agents,
+            }
+
         except Exception as e:
             logger.error(f"Agent search failed: {e}", exc_info=True)
-            return {
-                'status': AgentManagerOperationResult.FAILURE.value,
-                'error': str(e)
-            }
-
-    async def export_agent_data(
-        self,
-        agent_id: str,
-        format: str = 'json'
-    ) -> Dict[str, Any]:
+            return {"status": AgentManagerOperationResult.FAILURE.value, "error": str(e)}
+
+    async def export_agent_data(self, agent_id: str, format: str = "json") -> Dict[str, Any]:
         """
         Export complete agent data for backup or transfer
-        
+
         Formats: json, yaml, csv
         """
         try:
             # Get complete agent data
             agent = await self.lifecycle_manager.get_agent(agent_id)
             if not agent:
                 return {
-                    'status': AgentManagerOperationResult.FAILURE.value,
-                    'error': 'Agent not found'
+                    "status": AgentManagerOperationResult.FAILURE.value,
+                    "error": "Agent not found",
                 }
-            
+
             # Get analytics
             analytics = await self.lifecycle_manager.get_agent_analytics(agent_id)
-            
+
             # Get surveillance report
             surveillance = await self.surveillance_system.get_agent_surveillance_report(agent_id)
-            
+
             # Get conversation history
             conversations = await self.get_agent_conversations(agent_id)
-            
+
             export_data = {
-                'agent_id': agent_id,
-                'exported_at': datetime.utcnow().isoformat(),
-                'basic_info': {
-                    'name': agent.name,
-                    'type': agent.type,
-                    'version': agent.version,
-                    'status': agent.status,
-                    'created_at': agent.created_at.isoformat(),
-                    'capabilities': agent.capabilities
+                "agent_id": agent_id,
+                "exported_at": datetime.utcnow().isoformat(),
+                "basic_info": {
+                    "name": agent.name,
+                    "type": agent.type,
+                    "version": agent.version,
+                    "status": agent.status,
+                    "created_at": agent.created_at.isoformat(),
+                    "capabilities": agent.capabilities,
                 },
-                'analytics': analytics,
-                'surveillance': surveillance,
-                'conversations': conversations
-            }
-            
-            if format == 'json':
+                "analytics": analytics,
+                "surveillance": surveillance,
+                "conversations": conversations,
+            }
+
+            if format == "json":
                 import json
+
                 formatted_data = json.dumps(export_data, indent=2)
-            elif format == 'yaml':
+            elif format == "yaml":
                 import yaml
+
                 formatted_data = yaml.dump(export_data, default_flow_style=False)
             else:
                 formatted_data = str(export_data)
-            
+
             return {
-                'status': AgentManagerOperationResult.SUCCESS.value,
-                'format': format,
-                'data': formatted_data
-            }
-            
+                "status": AgentManagerOperationResult.SUCCESS.value,
+                "format": format,
+                "data": formatted_data,
+            }
+
         except Exception as e:
             logger.error(f"Agent data export failed: {e}", exc_info=True)
-            return {
-                'status': AgentManagerOperationResult.FAILURE.value,
-                'error': str(e)
-            }
+            return {"status": AgentManagerOperationResult.FAILURE.value, "error": str(e)}
 
     async def health_check(self) -> str:
         """Overall health check"""
         try:
             health = await self.get_system_health()
-            return health['overall_status']
+            return health["overall_status"]
         except Exception as e:
             logger.error(f"Health check failed: {e}")
-            return 'unhealthy'
+            return "unhealthy"
 
 
 # Factory function for easy initialization
 def create_agent_manager(
     db_manager: SecureDatabaseManager,
     rbac_manager: RBACManager,
     telemetry_manager: TelemetryManager,
     alert_manager: AlertManager,
-    ai_service: MultiModalAIService
+    ai_service: MultiModalAIService,
 ) -> AgentManager:
     """Factory function to create AgentManager instance"""
-    return AgentManager(
-        db_manager,
-        rbac_manager,
-        telemetry_manager,
-        alert_manager,
-        ai_service
-    )
+    return AgentManager(db_manager, rbac_manager, telemetry_manager, alert_manager, ai_service)
would reformat /home/runner/work/ymera_y/ymera_y/agent_manager_integration.py
error: cannot format /home/runner/work/ymera_y/ymera_y/agent_manager_production.py: Cannot parse for target version Python 3.12: 1500:13:         await
--- /home/runner/work/ymera_y/ymera_y/agent_orchestrator.py	2025-10-19 22:47:02.791432+00:00
+++ /home/runner/work/ymera_y/ymera_y/agent_orchestrator.py	2025-10-19 23:08:51.349279+00:00
@@ -16,288 +16,268 @@
 
 
 class AgentOrchestrator:
     """
     Agent Orchestration System
-    
+
     Features:
     - Agent registry and discovery
     - Health monitoring
     - Request routing
     - Circuit breaker pattern
     - Retry logic with exponential backoff
     """
-    
+
     def __init__(self, settings, database: ProjectDatabase):
         self.settings = settings
         self.database = database
         self.is_initialized = False
         self.agents = {}
         self.circuit_breakers = {}
         self.http_client = None
         self.health_check_task = None
-    
+
     async def initialize(self):
         """Initialize agent orchestrator"""
         # Initialize HTTP client
         self.http_client = httpx.AsyncClient(
             timeout=httpx.Timeout(self.settings.agent_request_timeout),
-            limits=httpx.Limits(max_keepalive_connections=100)
+            limits=httpx.Limits(max_keepalive_connections=100),
         )
-        
+
         # Register agents
         self._register_agents()
-        
+
         self.is_initialized = True
         logger.info(" Agent orchestrator initialized")
-    
+
     def _register_agents(self):
         """Register all agents from configuration"""
         self.agents = {
             "manager": {
                 "url": self.settings.manager_agent_url,
                 "capabilities": ["task_delegation", "workflow_management"],
                 "priority": 1,
-                "timeout": 30
+                "timeout": 30,
             },
             "coding": {
                 "url": self.settings.coding_agent_url,
                 "capabilities": ["code_generation", "refactoring"],
                 "priority": 2,
-                "timeout": 60
+                "timeout": 60,
             },
             "examination": {
                 "url": self.settings.examination_agent_url,
                 "capabilities": ["testing", "qa", "validation"],
                 "priority": 3,
-                "timeout": 90
+                "timeout": 90,
             },
             "enhancement": {
                 "url": self.settings.enhancement_agent_url,
                 "capabilities": ["optimization", "refactoring"],
                 "priority": 4,
-                "timeout": 60
-            }
+                "timeout": 60,
+            },
         }
-        
+
         # Initialize circuit breakers
         for agent_id in self.agents:
             self.circuit_breakers[agent_id] = {
                 "failures": 0,
                 "state": "closed",  # closed, open, half-open
-                "last_failure": None
+                "last_failure": None,
             }
-    
+
     async def send_to_agent(
-        self,
-        agent_id: str,
-        endpoint: str,
-        data: Dict,
-        method: str = "POST"
+        self, agent_id: str, endpoint: str, data: Dict, method: str = "POST"
     ) -> Optional[Dict]:
         """
         Send request to agent with retry logic and circuit breaker
-        
+
         Args:
             agent_id: Agent identifier
             endpoint: API endpoint path
             data: Request data
             method: HTTP method
-        
+
         Returns:
             Response data or None if failed
         """
         if agent_id not in self.agents:
             logger.error(f"Unknown agent: {agent_id}")
             return None
-        
+
         # Check circuit breaker
         if not self._check_circuit_breaker(agent_id):
             logger.warning(f"Circuit breaker open for agent {agent_id}")
             return None
-        
+
         agent = self.agents[agent_id]
         url = f"{agent['url']}{endpoint}"
-        
+
         # Retry logic with exponential backoff
         max_retries = self.settings.agent_max_retries
         backoff = 1
-        
+
         for attempt in range(max_retries):
             try:
                 if method == "POST":
-                    response = await self.http_client.post(
-                        url,
-                        json=data,
-                        timeout=agent["timeout"]
-                    )
+                    response = await self.http_client.post(url, json=data, timeout=agent["timeout"])
                 elif method == "GET":
                     response = await self.http_client.get(
-                        url,
-                        params=data,
-                        timeout=agent["timeout"]
+                        url, params=data, timeout=agent["timeout"]
                     )
                 else:
                     raise ValueError(f"Unsupported method: {method}")
-                
+
                 response.raise_for_status()
-                
+
                 # Reset circuit breaker on success
                 self._reset_circuit_breaker(agent_id)
-                
+
                 return response.json()
-                
+
             except httpx.HTTPError as e:
                 logger.warning(
                     f"Request to {agent_id} failed (attempt {attempt + 1}/{max_retries}): {e}"
                 )
-                
+
                 # Record failure
                 self._record_failure(agent_id)
-                
+
                 if attempt < max_retries - 1:
                     await asyncio.sleep(backoff)
                     backoff *= 2  # Exponential backoff
                 else:
                     logger.error(f"All retry attempts failed for agent {agent_id}")
                     return None
-        
+
         return None
-    
+
     def _check_circuit_breaker(self, agent_id: str) -> bool:
         """Check if circuit breaker allows requests"""
         breaker = self.circuit_breakers[agent_id]
-        
+
         if breaker["state"] == "closed":
             return True
-        
+
         if breaker["state"] == "open":
             # Check if enough time has passed to try again
             if breaker["last_failure"]:
-                time_since_failure = (
-                    datetime.utcnow() - breaker["last_failure"]
-                ).seconds
-                
+                time_since_failure = (datetime.utcnow() - breaker["last_failure"]).seconds
+
                 if time_since_failure > 60:  # 60 seconds timeout
                     breaker["state"] = "half-open"
                     return True
-            
+
             return False
-        
+
         if breaker["state"] == "half-open":
             return True
-        
+
         return False
-    
+
     def _record_failure(self, agent_id: str):
         """Record agent failure for circuit breaker"""
         breaker = self.circuit_breakers[agent_id]
         breaker["failures"] += 1
         breaker["last_failure"] = datetime.utcnow()
-        
+
         # Open circuit if threshold reached
         threshold = 5  # Open after 5 failures
         if breaker["failures"] >= threshold and breaker["state"] == "closed":
             breaker["state"] = "open"
             logger.warning(f"Circuit breaker opened for agent {agent_id}")
-    
+
     def _reset_circuit_breaker(self, agent_id: str):
         """Reset circuit breaker on successful request"""
         breaker = self.circuit_breakers[agent_id]
         breaker["failures"] = 0
         breaker["state"] = "closed"
         breaker["last_failure"] = None
-    
+
     async def check_agent_health(self, agent_id: str) -> Dict:
         """Check health of specific agent"""
         if agent_id not in self.agents:
             return {"status": "unknown", "error": "Agent not found"}
-        
+
         agent = self.agents[agent_id]
-        
+
         try:
-            response = await self.http_client.get(
-                f"{agent['url']}/health",
-                timeout=5.0
-            )
-            
+            response = await self.http_client.get(f"{agent['url']}/health", timeout=5.0)
+
             if response.status_code == 200:
                 return {
                     "status": "healthy",
                     "agent_id": agent_id,
-                    "response_time_ms": response.elapsed.total_seconds() * 1000
+                    "response_time_ms": response.elapsed.total_seconds() * 1000,
                 }
             else:
                 return {
                     "status": "unhealthy",
                     "agent_id": agent_id,
-                    "status_code": response.status_code
+                    "status_code": response.status_code,
                 }
-                
+
         except Exception as e:
-            return {
-                "status": "unreachable",
-                "agent_id": agent_id,
-                "error": str(e)
-            }
-    
+            return {"status": "unreachable", "agent_id": agent_id, "error": str(e)}
+
     async def list_agents(self) -> List[Dict]:
         """List all registered agents"""
         agents_list = []
-        
+
         for agent_id, agent_info in self.agents.items():
             breaker = self.circuit_breakers[agent_id]
-            
-            agents_list.append({
-                "id": agent_id,
-                "url": agent_info["url"],
-                "capabilities": agent_info["capabilities"],
-                "priority": agent_info["priority"],
-                "circuit_breaker_state": breaker["state"],
-                "failures": breaker["failures"]
-            })
-        
+
+            agents_list.append(
+                {
+                    "id": agent_id,
+                    "url": agent_info["url"],
+                    "capabilities": agent_info["capabilities"],
+                    "priority": agent_info["priority"],
+                    "circuit_breaker_state": breaker["state"],
+                    "failures": breaker["failures"],
+                }
+            )
+
         return agents_list
-    
+
     async def start_health_monitoring(self):
         """Start background health monitoring"""
         logger.info("Agent health monitoring started")
-        
+
         async def monitor():
             while True:
                 try:
                     for agent_id in self.agents:
                         health = await self.check_agent_health(agent_id)
-                        
+
                         if health["status"] != "healthy":
-                            logger.warning(
-                                f"Agent {agent_id} health check failed: {health}"
-                            )
-                    
+                            logger.warning(f"Agent {agent_id} health check failed: {health}")
+
                     await asyncio.sleep(30)  # Check every 30 seconds
-                    
+
                 except asyncio.CancelledError:
                     break
                 except Exception as e:
                     logger.error(f"Health monitoring error: {e}")
                     await asyncio.sleep(60)
-        
+
         self.health_check_task = asyncio.create_task(monitor())
-    
+
     async def health_check(self) -> bool:
         """Check orchestrator health"""
         return self.is_initialized and self.http_client is not None
-    
+
     async def shutdown(self):
         """Shutdown agent orchestrator"""
         if self.health_check_task:
             self.health_check_task.cancel()
             try:
                 await self.health_check_task
             except asyncio.CancelledError:
                 pass
-        
+
         if self.http_client:
             await self.http_client.aclose()
-        
+
         logger.info("Agent orchestrator shutdown complete")
-
would reformat /home/runner/work/ymera_y/ymera_y/agent_orchestrator.py
--- /home/runner/work/ymera_y/ymera_y/agent_routes.py	2025-10-19 22:47:02.791432+00:00
+++ /home/runner/work/ymera_y/ymera_y/agent_routes.py	2025-10-19 23:08:51.587040+00:00
@@ -9,203 +9,217 @@
 from datetime import datetime, timedelta
 import uuid
 import json
 
 from models import (
-    Agent, AgentCreate, AgentUpdate, AgentResponse, AgentStatus,
-    Task, TaskCreate, TaskUpdate, TaskResponse, TaskStatus, TaskPriority,
-    AdminApproval, AdminApprovalCreate, AdminApprovalUpdate, AdminApprovalResponse
+    Agent,
+    AgentCreate,
+    AgentUpdate,
+    AgentResponse,
+    AgentStatus,
+    Task,
+    TaskCreate,
+    TaskUpdate,
+    TaskResponse,
+    TaskStatus,
+    TaskPriority,
+    AdminApproval,
+    AdminApprovalCreate,
+    AdminApprovalUpdate,
+    AdminApprovalResponse,
 )
 
 from services import (
-    auth_service, agent_manager, task_manager, 
-    reporting_enforcer, security_monitor, audit_system,
-    notification_manager, connection_manager
+    auth_service,
+    agent_manager,
+    task_manager,
+    reporting_enforcer,
+    security_monitor,
+    audit_system,
+    notification_manager,
+    connection_manager,
 )
 
 router = APIRouter(prefix="/agents", tags=["agents"])
+
 
 # Agent management routes
 @router.post("/", response_model=AgentResponse)
 async def create_agent(
     agent_data: AgentCreate,
     background_tasks: BackgroundTasks,
-    current_user = Depends(auth_service.get_current_user)
+    current_user=Depends(auth_service.get_current_user),
 ):
     """Register a new agent"""
-    agent_result = await agent_manager.register_agent({
-        "name": agent_data.name,
-        "description": agent_data.description,
-        "owner_id": current_user.id,
-        "capabilities": agent_data.capabilities,
-        "configuration": agent_data.configuration
-    })
-    
+    agent_result = await agent_manager.register_agent(
+        {
+            "name": agent_data.name,
+            "description": agent_data.description,
+            "owner_id": current_user.id,
+            "capabilities": agent_data.capabilities,
+            "configuration": agent_data.configuration,
+        }
+    )
+
     # Log audit event
     background_tasks.add_task(
         audit_system.log_event,
         "agent_creation",
         "agent",
         agent_result["id"],
         "create",
         current_user.id,
-        {"name": agent_data.name}
+        {"name": agent_data.name},
     )
-    
+
     return agent_result
+
 
 @router.get("/", response_model=List[AgentResponse])
 async def list_agents(
     status: Optional[str] = None,
     limit: int = Query(100, gt=0, le=1000),
     offset: int = Query(0, ge=0),
-    current_user = Depends(auth_service.get_current_user)
+    current_user=Depends(auth_service.get_current_user),
 ):
     """List user's agents with optional filtering"""
     return await agent_manager.list_agents(current_user.id, status, limit, offset)
 
+
 @router.get("/{agent_id}", response_model=AgentResponse)
-async def get_agent(
-    agent_id: str = Path(...),
-    current_user = Depends(auth_service.get_current_user)
-):
+async def get_agent(agent_id: str = Path(...), current_user=Depends(auth_service.get_current_user)):
     """Get agent details"""
     agent = await agent_manager.get_agent(agent_id, current_user.id)
     if not agent:
         raise HTTPException(status_code=404, detail="Agent not found")
     return agent
 
+
 @router.put("/{agent_id}", response_model=AgentResponse)
 async def update_agent(
     agent_id: str = Path(...),
     agent_data: AgentUpdate = Body(...),
     background_tasks: BackgroundTasks = None,
-    current_user = Depends(auth_service.get_current_user)
+    current_user=Depends(auth_service.get_current_user),
 ):
     """Update agent details"""
     agent = await agent_manager.update_agent(agent_id, agent_data.dict(), current_user.id)
     if not agent:
         raise HTTPException(status_code=404, detail="Agent not found")
-    
+
     # Log audit event
     if background_tasks:
         background_tasks.add_task(
             audit_system.log_event,
             "agent_update",
             "agent",
             agent_id,
             "update",
             current_user.id,
-            {"updated_fields": [k for k, v in agent_data.dict(exclude_unset=True).items()]}
+            {"updated_fields": [k for k, v in agent_data.dict(exclude_unset=True).items()]},
         )
-    
+
     return agent
+
 
 @router.post("/{agent_id}/action", response_model=Dict[str, Any])
 async def request_agent_action(
     agent_id: str = Path(...),
     action_data: Dict[str, Any] = Body(...),
     background_tasks: BackgroundTasks = None,
-    current_user = Depends(auth_service.get_current_user)
+    current_user=Depends(auth_service.get_current_user),
 ):
     """Request action on agent (freeze, unfreeze, delete, etc.)"""
     action = action_data.get("action")
     reason = action_data.get("reason", "No reason provided")
-    
-    result = await agent_manager.request_agent_action(
-        agent_id, action, reason, current_user.id
-    )
-    
+
+    result = await agent_manager.request_agent_action(agent_id, action, reason, current_user.id)
+
     # Log audit event
     if background_tasks:
         background_tasks.add_task(
             audit_system.log_event,
             "agent_action_request",
             "agent",
             agent_id,
             f"request_{action}",
             current_user.id,
-            {"reason": reason, "approval_id": result.get("approval_id")}
+            {"reason": reason, "approval_id": result.get("approval_id")},
         )
-    
+
     return result
+
 
 @router.post("/{agent_id}/report", response_model=Dict[str, Any])
 async def submit_agent_report(
     agent_id: str = Path(...),
     report_data: Dict[str, Any] = Body(...),
     background_tasks: BackgroundTasks = None,
     request_ip: str = None,
-    current_agent = Depends(auth_service.get_current_agent)
+    current_agent=Depends(auth_service.get_current_agent),
 ):
     """Submit agent report/heartbeat"""
     # Verify agent identity
     if current_agent["id"] != agent_id:
         raise HTTPException(status_code=403, detail="Not authorized to report for this agent")
-    
+
     # Process report
-    result = await agent_manager.handle_agent_report(
-        agent_id, report_data, ip_address=request_ip
-    )
-    
+    result = await agent_manager.handle_agent_report(agent_id, report_data, ip_address=request_ip)
+
     # Schedule security analysis in background
     if background_tasks:
-        background_tasks.add_task(
-            security_monitor.analyze_agent_report,
-            agent_id, report_data
-        )
-    
+        background_tasks.add_task(security_monitor.analyze_agent_report, agent_id, report_data)
+
     return result
+
 
 # WebSocket endpoint
 @router.websocket("/{agent_id}/ws")
 async def agent_websocket(
-    websocket: WebSocket,
-    agent_id: str = Path(...),
-    api_key: str = Query(...)
+    websocket: WebSocket, agent_id: str = Path(...), api_key: str = Query(...)
 ):
     """WebSocket connection for real-time agent communication"""
     try:
         # Authenticate agent
         valid = await agent_manager.verify_agent_api_key(agent_id, api_key)
         if not valid:
             await websocket.close(code=status.WS_1008_POLICY_VIOLATION)
             return
-        
+
         # Validate that the agent isn't deleted or suspended
         agent_status = await agent_manager.get_agent_status(agent_id)
         if agent_status in [AgentStatus.DELETED.value, AgentStatus.SUSPENDED.value]:
             await websocket.close(code=status.WS_1008_POLICY_VIOLATION)
             return
-        
+
         # Register connection
         connection_id = await connection_manager.connect(
             websocket, agent_id, "agent", {"agent_id": agent_id}
         )
-        
+
         # Register agent as connected
         await agent_manager.register_ws_connection(agent_id, websocket)
-        
+
         try:
             while True:
                 # Receive message
                 data = await websocket.receive_json()
-                
+
                 # Handle message
                 await connection_manager.handle_message(connection_id, data)
-                
+
         except WebSocketDisconnect:
             # Normal disconnect
             pass
-            
+
         except Exception as e:
             # Error handling
             logger.error(f"Agent WebSocket error: {e}")
-            
+
         finally:
             # Unregister connection
             await agent_manager.unregister_ws_connection(agent_id, websocket)
             await connection_manager.disconnect(connection_id)
-            
+
     except Exception as e:
         logger.error(f"Agent WebSocket connection error: {e}")
-        await websocket.close(code=status.WS_1011_INTERNAL_ERROR)
\ No newline at end of file
+        await websocket.close(code=status.WS_1011_INTERNAL_ERROR)
would reformat /home/runner/work/ymera_y/ymera_y/agent_routes.py
--- /home/runner/work/ymera_y/ymera_y/agent_registry.py	2025-10-19 22:47:02.791432+00:00
+++ /home/runner/work/ymera_y/ymera_y/agent_registry.py	2025-10-19 23:08:51.748257+00:00
@@ -14,10 +14,11 @@
 logger = structlog.get_logger(__name__)
 
 
 class AgentStatus(Enum):
     """Agent status enumeration"""
+
     REGISTERING = "registering"
     ACTIVE = "active"
     INACTIVE = "inactive"
     DEGRADED = "degraded"
     FAILED = "failed"
@@ -25,120 +26,121 @@
 
 
 @dataclass
 class AgentRecord:
     """Agent registration record"""
+
     agent_id: str
     agent_name: str
     agent_type: str
     capabilities: List[str]
     version: str
     host: str
     port: int
     status: AgentStatus = AgentStatus.REGISTERING
     metadata: Dict[str, Any] = field(default_factory=dict)
     tags: Dict[str, str] = field(default_factory=dict)
-    
+
     # Health tracking
     last_heartbeat: float = field(default_factory=time.time)
     last_health_check: float = field(default_factory=time.time)
     health_score: float = 1.0
     consecutive_failures: int = 0
-    
+
     # Metrics
     registered_at: float = field(default_factory=time.time)
     tasks_processed: int = 0
     tasks_failed: int = 0
     average_response_time_ms: float = 0.0
-    
+
     # Load balancing
     current_load: int = 0
     max_load: int = 10
-    
+
     def to_dict(self) -> Dict[str, Any]:
         data = asdict(self)
-        data['status'] = self.status.value
+        data["status"] = self.status.value
         return data
-    
+
     @property
     def is_healthy(self) -> bool:
         """Check if agent is healthy"""
         return (
-            self.status == AgentStatus.ACTIVE and
-            self.health_score >= 0.5 and
-            self.consecutive_failures < 3 and
-            time.time() - self.last_heartbeat < 90  # 90 second heartbeat timeout
+            self.status == AgentStatus.ACTIVE
+            and self.health_score >= 0.5
+            and self.consecutive_failures < 3
+            and time.time() - self.last_heartbeat < 90  # 90 second heartbeat timeout
         )
-    
+
     @property
     def is_available(self) -> bool:
         """Check if agent is available for work"""
         return self.is_healthy and self.current_load < self.max_load
 
 
 class AgentRegistry:
     """
     Central Agent Registry
-    
+
     Features:
     - Agent registration and deregistration
     - Health tracking and monitoring
     - Version management
     - Capability-based discovery
     - Load-aware agent selection
     - Automatic cleanup of dead agents
     """
-    
+
     def __init__(
         self,
         heartbeat_timeout: int = 90,
         health_check_interval: int = 30,
-        cleanup_interval: int = 300
+        cleanup_interval: int = 300,
     ):
         self.heartbeat_timeout = heartbeat_timeout
         self.health_check_interval = health_check_interval
         self.cleanup_interval = cleanup_interval
-        
+
         # Registry storage
         self._agents: Dict[str, AgentRecord] = {}
         self._agents_by_type: Dict[str, Set[str]] = {}
         self._agents_by_capability: Dict[str, Set[str]] = {}
-        
+
         # Locks
         self._lock = asyncio.Lock()
-        
+
         # Background tasks
         self._cleanup_task: Optional[asyncio.Task] = None
         self._monitoring_task: Optional[asyncio.Task] = None
         self._shutdown_event = asyncio.Event()
-        
+
         logger.info("Agent Registry initialized")
-    
+
     async def start(self):
         """Start background tasks"""
         self._cleanup_task = asyncio.create_task(self._cleanup_loop())
         self._monitoring_task = asyncio.create_task(self._monitoring_loop())
         logger.info("Agent Registry background tasks started")
-    
+
     async def stop(self):
         """Stop background tasks"""
         self._shutdown_event.set()
-        
+
         if self._cleanup_task:
             self._cleanup_task.cancel()
             await asyncio.gather(self._cleanup_task, return_exceptions=True)
-        
+
         if self._monitoring_task:
             self._monitoring_task.cancel()
             await asyncio.gather(self._monitoring_task, return_exceptions=True)
-        
+
         logger.info("Agent Registry stopped")
-    
+
     # =========================================================================
     # REGISTRATION
     # =========================================================================
-    
+
     async def register_agent(
         self,
         agent_id: str,
         agent_name: str,
         agent_type: str,
@@ -146,18 +148,18 @@
         version: str,
         host: str,
         port: int,
         metadata: Optional[Dict[str, Any]] = None,
         tags: Optional[Dict[str, str]] = None,
-        max_load: int = 10
+        max_load: int = 10,
     ) -> AgentRecord:
         """Register a new agent"""
         async with self._lock:
             if agent_id in self._agents:
                 logger.warning(f"Agent {agent_id} already registered, updating record")
                 return await self._update_agent(agent_id, metadata, tags)
-            
+
             record = AgentRecord(
                 agent_id=agent_id,
                 agent_name=agent_name,
                 agent_type=agent_type,
                 capabilities=capabilities,
@@ -165,303 +167,291 @@
                 host=host,
                 port=port,
                 metadata=metadata or {},
                 tags=tags or {},
                 max_load=max_load,
-                status=AgentStatus.ACTIVE
+                status=AgentStatus.ACTIVE,
             )
-            
+
             self._agents[agent_id] = record
-            
+
             # Index by type
             if agent_type not in self._agents_by_type:
                 self._agents_by_type[agent_type] = set()
             self._agents_by_type[agent_type].add(agent_id)
-            
+
             # Index by capabilities
             for capability in capabilities:
                 if capability not in self._agents_by_capability:
                     self._agents_by_capability[capability] = set()
                 self._agents_by_capability[capability].add(agent_id)
-            
+
             logger.info(
                 f"Agent registered",
                 agent_id=agent_id,
                 agent_type=agent_type,
-                capabilities=capabilities
+                capabilities=capabilities,
             )
-            
+
             return record
-    
+
     async def deregister_agent(self, agent_id: str) -> bool:
         """Deregister an agent"""
         async with self._lock:
             if agent_id not in self._agents:
                 logger.warning(f"Agent {agent_id} not found for deregistration")
                 return False
-            
+
             record = self._agents[agent_id]
             record.status = AgentStatus.DEREGISTERED
-            
+
             # Remove from indices
             self._agents_by_type.get(record.agent_type, set()).discard(agent_id)
             for capability in record.capabilities:
                 self._agents_by_capability.get(capability, set()).discard(agent_id)
-            
+
             # Remove from registry
             del self._agents[agent_id]
-            
+
             logger.info(f"Agent deregistered", agent_id=agent_id)
             return True
-    
+
     async def _update_agent(
         self,
         agent_id: str,
         metadata: Optional[Dict[str, Any]] = None,
-        tags: Optional[Dict[str, str]] = None
+        tags: Optional[Dict[str, str]] = None,
     ) -> AgentRecord:
         """Update agent record"""
         record = self._agents[agent_id]
-        
+
         if metadata:
             record.metadata.update(metadata)
         if tags:
             record.tags.update(tags)
-        
+
         record.last_heartbeat = time.time()
         record.status = AgentStatus.ACTIVE
         record.consecutive_failures = 0
-        
+
         logger.debug(f"Agent updated", agent_id=agent_id)
         return record
-    
+
     # =========================================================================
     # HEARTBEAT AND HEALTH
     # =========================================================================
-    
-    async def heartbeat(
-        self,
-        agent_id: str,
-        metrics: Optional[Dict[str, Any]] = None
-    ) -> bool:
+
+    async def heartbeat(self, agent_id: str, metrics: Optional[Dict[str, Any]] = None) -> bool:
         """Record agent heartbeat"""
         async with self._lock:
             if agent_id not in self._agents:
                 logger.warning(f"Heartbeat from unknown agent {agent_id}")
                 return False
-            
+
             record = self._agents[agent_id]
             record.last_heartbeat = time.time()
-            
+
             if metrics:
-                record.current_load = metrics.get('current_load', record.current_load)
-                record.tasks_processed = metrics.get('tasks_processed', record.tasks_processed)
-                record.tasks_failed = metrics.get('tasks_failed', record.tasks_failed)
-                record.average_response_time_ms = metrics.get('avg_response_time', record.average_response_time_ms)
-            
+                record.current_load = metrics.get("current_load", record.current_load)
+                record.tasks_processed = metrics.get("tasks_processed", record.tasks_processed)
+                record.tasks_failed = metrics.get("tasks_failed", record.tasks_failed)
+                record.average_response_time_ms = metrics.get(
+                    "avg_response_time", record.average_response_time_ms
+                )
+
             # Update health status
             if record.status != AgentStatus.ACTIVE:
                 record.status = AgentStatus.ACTIVE
                 record.consecutive_failures = 0
                 logger.info(f"Agent recovered", agent_id=agent_id)
-            
+
             return True
-    
+
     async def update_health(
-        self,
-        agent_id: str,
-        health_score: float,
-        status: Optional[AgentStatus] = None
+        self, agent_id: str, health_score: float, status: Optional[AgentStatus] = None
     ) -> bool:
         """Update agent health"""
         async with self._lock:
             if agent_id not in self._agents:
                 return False
-            
+
             record = self._agents[agent_id]
             record.health_score = max(0.0, min(1.0, health_score))
             record.last_health_check = time.time()
-            
+
             if status:
                 old_status = record.status
                 record.status = status
-                
+
                 if old_status != status:
                     logger.info(
                         f"Agent status changed",
                         agent_id=agent_id,
                         old_status=old_status.value,
-                        new_status=status.value
+                        new_status=status.value,
                     )
-            
+
             return True
-    
+
     async def record_failure(self, agent_id: str):
         """Record agent failure"""
         async with self._lock:
             if agent_id not in self._agents:
                 return
-            
+
             record = self._agents[agent_id]
             record.consecutive_failures += 1
             record.health_score = max(0.0, record.health_score - 0.2)
-            
+
             if record.consecutive_failures >= 3:
                 record.status = AgentStatus.FAILED
                 logger.error(f"Agent marked as failed", agent_id=agent_id)
             elif record.consecutive_failures >= 2:
                 record.status = AgentStatus.DEGRADED
                 logger.warning(f"Agent degraded", agent_id=agent_id)
-    
+
     # =========================================================================
     # DISCOVERY
     # =========================================================================
-    
+
     async def get_agent(self, agent_id: str) -> Optional[AgentRecord]:
         """Get agent by ID"""
         return self._agents.get(agent_id)
-    
+
     async def get_agents_by_type(
-        self,
-        agent_type: str,
-        only_healthy: bool = True
+        self, agent_type: str, only_healthy: bool = True
     ) -> List[AgentRecord]:
         """Get all agents of a specific type"""
         agent_ids = self._agents_by_type.get(agent_type, set())
         agents = [self._agents[aid] for aid in agent_ids if aid in self._agents]
-        
+
         if only_healthy:
             agents = [a for a in agents if a.is_healthy]
-        
+
         return agents
-    
+
     async def get_agents_by_capability(
-        self,
-        capability: str,
-        only_available: bool = True
+        self, capability: str, only_available: bool = True
     ) -> List[AgentRecord]:
         """Get all agents with a specific capability"""
         agent_ids = self._agents_by_capability.get(capability, set())
         agents = [self._agents[aid] for aid in agent_ids if aid in self._agents]
-        
+
         if only_available:
             agents = [a for a in agents if a.is_available]
-        
+
         # Sort by load (least loaded first)
         agents.sort(key=lambda a: (a.current_load / a.max_load, -a.health_score))
-        
+
         return agents
-    
-    async def get_all_agents(
-        self,
-        only_healthy: bool = False
-    ) -> List[AgentRecord]:
+
+    async def get_all_agents(self, only_healthy: bool = False) -> List[AgentRecord]:
         """Get all registered agents"""
         agents = list(self._agents.values())
-        
+
         if only_healthy:
             agents = [a for a in agents if a.is_healthy]
-        
+
         return agents
-    
+
     async def find_best_agent(
-        self,
-        capability: str,
-        exclude_agents: Optional[Set[str]] = None
+        self, capability: str, exclude_agents: Optional[Set[str]] = None
     ) -> Optional[AgentRecord]:
         """Find best available agent for a capability"""
         agents = await self.get_agents_by_capability(capability, only_available=True)
-        
+
         if exclude_agents:
             agents = [a for a in agents if a.agent_id not in exclude_agents]
-        
+
         return agents[0] if agents else None
-    
+
     # =========================================================================
     # LOAD MANAGEMENT
     # =========================================================================
-    
+
     async def increment_load(self, agent_id: str) -> bool:
         """Increment agent load"""
         async with self._lock:
             if agent_id not in self._agents:
                 return False
-            
+
             record = self._agents[agent_id]
             record.current_load += 1
             return True
-    
+
     async def decrement_load(self, agent_id: str) -> bool:
         """Decrement agent load"""
         async with self._lock:
             if agent_id not in self._agents:
                 return False
-            
+
             record = self._agents[agent_id]
             record.current_load = max(0, record.current_load - 1)
             return True
-    
+
     # =========================================================================
     # STATISTICS
     # =========================================================================
-    
+
     async def get_statistics(self) -> Dict[str, Any]:
         """Get registry statistics"""
         agents = list(self._agents.values())
-        
+
         return {
-            'total_agents': len(agents),
-            'active_agents': sum(1 for a in agents if a.status == AgentStatus.ACTIVE),
-            'degraded_agents': sum(1 for a in agents if a.status == AgentStatus.DEGRADED),
-            'failed_agents': sum(1 for a in agents if a.status == AgentStatus.FAILED),
-            'healthy_agents': sum(1 for a in agents if a.is_healthy),
-            'available_agents': sum(1 for a in agents if a.is_available),
-            'total_load': sum(a.current_load for a in agents),
-            'agent_types': len(self._agents_by_type),
-            'capabilities': len(self._agents_by_capability),
-            'average_health_score': sum(a.health_score for a in agents) / len(agents) if agents else 0,
-            'timestamp': datetime.utcnow().isoformat()
+            "total_agents": len(agents),
+            "active_agents": sum(1 for a in agents if a.status == AgentStatus.ACTIVE),
+            "degraded_agents": sum(1 for a in agents if a.status == AgentStatus.DEGRADED),
+            "failed_agents": sum(1 for a in agents if a.status == AgentStatus.FAILED),
+            "healthy_agents": sum(1 for a in agents if a.is_healthy),
+            "available_agents": sum(1 for a in agents if a.is_available),
+            "total_load": sum(a.current_load for a in agents),
+            "agent_types": len(self._agents_by_type),
+            "capabilities": len(self._agents_by_capability),
+            "average_health_score": (
+                sum(a.health_score for a in agents) / len(agents) if agents else 0
+            ),
+            "timestamp": datetime.utcnow().isoformat(),
         }
-    
+
     # =========================================================================
     # BACKGROUND TASKS
     # =========================================================================
-    
+
     async def _cleanup_loop(self):
         """Cleanup dead agents"""
         while not self._shutdown_event.is_set():
             try:
                 await asyncio.sleep(self.cleanup_interval)
                 await self._cleanup_dead_agents()
             except asyncio.CancelledError:
                 break
             except Exception as e:
                 logger.error(f"Cleanup loop error: {e}")
-    
+
     async def _cleanup_dead_agents(self):
         """Remove agents that haven't sent heartbeat"""
         current_time = time.time()
         dead_agents = []
-        
+
         async with self._lock:
             for agent_id, record in list(self._agents.items()):
                 time_since_heartbeat = current_time - record.last_heartbeat
-                
+
                 if time_since_heartbeat > self.heartbeat_timeout:
                     logger.warning(
                         f"Agent timeout, removing",
                         agent_id=agent_id,
-                        timeout_seconds=time_since_heartbeat
+                        timeout_seconds=time_since_heartbeat,
                     )
                     dead_agents.append(agent_id)
-        
+
         for agent_id in dead_agents:
             await self.deregister_agent(agent_id)
-        
+
         if dead_agents:
             logger.info(f"Cleaned up {len(dead_agents)} dead agents")
-    
+
     async def _monitoring_loop(self):
         """Monitor agent health"""
         while not self._shutdown_event.is_set():
             try:
                 await asyncio.sleep(self.health_check_interval)
would reformat /home/runner/work/ymera_y/ymera_y/agent_registry.py
--- /home/runner/work/ymera_y/ymera_y/agent_surveillance.py	2025-10-19 22:47:02.791432+00:00
+++ /home/runner/work/ymera_y/ymera_y/agent_surveillance.py	2025-10-19 23:08:52.483086+00:00
@@ -11,117 +11,156 @@
 from monitoring.alert_manager import AlertManager, AlertCategory, AlertSeverity
 from ai.multi_modal_ai import MultiModalAIService, AIRequest, AIModelType
 
 logger = logging.getLogger(__name__)
 
+
 class SurveillanceConfig:
     """Configuration for agent surveillance with safe defaults"""
+
     def __init__(self):
         self.anomaly_threshold = getattr(
-            settings.monitoring.agent_surveillance if hasattr(settings.monitoring, 'agent_surveillance') else type('obj', (), {}),
-            'anomaly_threshold',
-            0.7
+            (
+                settings.monitoring.agent_surveillance
+                if hasattr(settings.monitoring, "agent_surveillance")
+                else type("obj", (), {})
+            ),
+            "anomaly_threshold",
+            0.7,
         )
         self.monitoring_interval_seconds = getattr(
-            settings.monitoring.agent_surveillance if hasattr(settings.monitoring, 'agent_surveillance') else type('obj', (), {}),
-            'monitoring_interval_seconds',
-            60
+            (
+                settings.monitoring.agent_surveillance
+                if hasattr(settings.monitoring, "agent_surveillance")
+                else type("obj", (), {})
+            ),
+            "monitoring_interval_seconds",
+            60,
         )
         self.stale_agent_timeout_minutes = getattr(
-            settings.monitoring.agent_surveillance if hasattr(settings.monitoring, 'agent_surveillance') else type('obj', (), {}),
-            'stale_agent_timeout_minutes',
-            10
+            (
+                settings.monitoring.agent_surveillance
+                if hasattr(settings.monitoring, "agent_surveillance")
+                else type("obj", (), {})
+            ),
+            "stale_agent_timeout_minutes",
+            10,
         )
         self.security_score_reduction_critical = getattr(
-            settings.monitoring.agent_surveillance if hasattr(settings.monitoring, 'agent_surveillance') else type('obj', (), {}),
-            'security_score_reduction_critical',
-            30
+            (
+                settings.monitoring.agent_surveillance
+                if hasattr(settings.monitoring, "agent_surveillance")
+                else type("obj", (), {})
+            ),
+            "security_score_reduction_critical",
+            30,
         )
         self.security_score_reduction_warning = getattr(
-            settings.monitoring.agent_surveillance if hasattr(settings.monitoring, 'agent_surveillance') else type('obj', (), {}),
-            'security_score_reduction_warning',
-            15
-        )
-        
+            (
+                settings.monitoring.agent_surveillance
+                if hasattr(settings.monitoring, "agent_surveillance")
+                else type("obj", (), {})
+            ),
+            "security_score_reduction_warning",
+            15,
+        )
+
         # Performance thresholds
         thresholds_obj = getattr(
-            settings.monitoring.agent_surveillance if hasattr(settings.monitoring, 'agent_surveillance') else type('obj', (), {}),
-            'performance_thresholds',
-            type('obj', (), {
-                'cpu_usage': 85.0,
-                'memory_usage': 85.0,
-                'response_time': 1000,  # ms
-                'error_rate': 0.05  # 5%
-            })()
-        )
-        
+            (
+                settings.monitoring.agent_surveillance
+                if hasattr(settings.monitoring, "agent_surveillance")
+                else type("obj", (), {})
+            ),
+            "performance_thresholds",
+            type(
+                "obj",
+                (),
+                {
+                    "cpu_usage": 85.0,
+                    "memory_usage": 85.0,
+                    "response_time": 1000,  # ms
+                    "error_rate": 0.05,  # 5%
+                },
+            )(),
+        )
+
         self.performance_thresholds = {
-            'cpu_usage': getattr(thresholds_obj, 'cpu_usage', 85.0),
-            'memory_usage': getattr(thresholds_obj, 'memory_usage', 85.0),
-            'response_time': getattr(thresholds_obj, 'response_time', 1000),
-            'error_rate': getattr(thresholds_obj, 'error_rate', 0.05)
+            "cpu_usage": getattr(thresholds_obj, "cpu_usage", 85.0),
+            "memory_usage": getattr(thresholds_obj, "memory_usage", 85.0),
+            "response_time": getattr(thresholds_obj, "response_time", 1000),
+            "error_rate": getattr(thresholds_obj, "error_rate", 0.05),
         }
-        
+
         # Behavioral analysis settings
         self.enable_ai_behavior_analysis = getattr(
-            settings.monitoring.agent_surveillance if hasattr(settings.monitoring, 'agent_surveillance') else type('obj', (), {}),
-            'enable_ai_behavior_analysis',
-            True
+            (
+                settings.monitoring.agent_surveillance
+                if hasattr(settings.monitoring, "agent_surveillance")
+                else type("obj", (), {})
+            ),
+            "enable_ai_behavior_analysis",
+            True,
         )
         self.max_concurrent_analyses = getattr(
-            settings.monitoring.agent_surveillance if hasattr(settings.monitoring, 'agent_surveillance') else type('obj', (), {}),
-            'max_concurrent_analyses',
-            10
-        )
+            (
+                settings.monitoring.agent_surveillance
+                if hasattr(settings.monitoring, "agent_surveillance")
+                else type("obj", (), {})
+            ),
+            "max_concurrent_analyses",
+            10,
+        )
+
 
 class AgentSurveillanceSystem:
     """Real-time agent monitoring, anomaly detection, and performance analysis.
-    
+
     This system acts as the Agent Manager's eyes and ears, continuously monitoring:
     - Agent health and performance metrics
     - Behavioral anomalies using AI
     - Security violations and suspicious activities
     - API access patterns and data flow
     - Compliance with security policies
-    
+
     Reports findings to Agent Manager for enforcement actions.
     """
-    
+
     def __init__(
-        self, 
-        db_manager: SecureDatabaseManager, 
-        ai_service: MultiModalAIService, 
-        alert_manager: AlertManager
+        self,
+        db_manager: SecureDatabaseManager,
+        ai_service: MultiModalAIService,
+        alert_manager: AlertManager,
     ):
         self.db_manager = db_manager
         self.ai_service = ai_service
         self.alert_manager = alert_manager
-        
+
         # Load configuration with safe defaults
         self.config = SurveillanceConfig()
-        
+
         # Runtime state
         self.agent_metrics: Dict[str, Dict] = {}
         self.monitoring_active = False
         self.analysis_semaphore = asyncio.Semaphore(self.config.max_concurrent_analyses)
         self.monitored_agents: Set[str] = set()
-        
+
         logger.info(
             f"AgentSurveillanceSystem initialized. "
             f"AI analysis: {self.config.enable_ai_behavior_analysis}, "
             f"Monitoring interval: {self.config.monitoring_interval_seconds}s"
         )
-    
+
     async def start_monitoring(self):
         """Start background monitoring of all agents"""
         if self.monitoring_active:
             logger.warning("Monitoring already active, skipping start")
             return
-        
+
         self.monitoring_active = True
         logger.info("Starting agent surveillance system")
-        
+
         while self.monitoring_active:
             try:
                 await self._monitor_all_agents()
                 await asyncio.sleep(self.config.monitoring_interval_seconds)
             except asyncio.CancelledError:
@@ -129,196 +168,204 @@
                 self.monitoring_active = False
                 break
             except Exception as e:
                 logger.error(f"Monitoring loop failed: {e}", exc_info=True)
                 await asyncio.sleep(60)  # Wait longer on error
-    
+
     async def stop_monitoring(self):
         """Gracefully stop monitoring"""
         logger.info("Stopping agent surveillance system")
         self.monitoring_active = False
-    
+
     async def _monitor_all_agents(self):
         """Monitor all active agents for anomalies and performance issues"""
         try:
             async with self.db_manager.get_session() as session:
                 stale_cutoff = datetime.utcnow() - timedelta(
                     minutes=self.config.stale_agent_timeout_minutes
                 )
-                
+
                 result = await session.execute(
                     select(Agent).where(
-                        and_(
-                            Agent.status == 'active',
-                            Agent.last_heartbeat >= stale_cutoff
-                        )
+                        and_(Agent.status == "active", Agent.last_heartbeat >= stale_cutoff)
                     )
                 )
                 agents = result.scalars().all()
-                
+
                 # Track which agents we're monitoring
                 current_agent_ids = {str(agent.id) for agent in agents}
                 self.monitored_agents = current_agent_ids
-                
+
                 # Process agents with concurrency control
                 monitoring_tasks = []
                 for agent in agents:
                     monitoring_tasks.append(self._analyze_agent_with_semaphore(agent))
-                
+
                 # Gather results, capturing exceptions
                 results = await asyncio.gather(*monitoring_tasks, return_exceptions=True)
-                
+
                 # Log any exceptions
                 for i, result in enumerate(results):
                     if isinstance(result, Exception):
                         logger.error(
-                            f"Error analyzing agent {agents[i].id}: {result}",
-                            exc_info=result
+                            f"Error analyzing agent {agents[i].id}: {result}", exc_info=result
                         )
-        
+
         except Exception as e:
             logger.error(f"Error in _monitor_all_agents: {e}", exc_info=True)
-    
+
     async def _analyze_agent_with_semaphore(self, agent):
         """Wrapper to enforce concurrency limits"""
         async with self.analysis_semaphore:
             return await self._analyze_agent(agent)
-    
+
     async def _analyze_agent(self, agent):
         """Comprehensive agent analysis including performance and security"""
         try:
             analysis_start = datetime.utcnow()
-            
+
             # Check basic health metrics
             health_issues = await self._check_health_metrics(agent)
             if health_issues:
                 await self._handle_health_issues(agent, health_issues)
-            
+
             # Perform AI-powered behavior analysis if enabled
             if self.config.enable_ai_behavior_analysis and self.ai_service:
                 try:
                     agent_data = self._prepare_agent_data(agent)
                     analysis = await self._analyze_behavior_with_ai(agent_data)
-                    
+
                     # Store metrics for historical analysis
                     self.agent_metrics[str(agent.id)] = {
                         **analysis,
-                        'last_analyzed': analysis_start.isoformat(),
-                        'analysis_duration': (datetime.utcnow() - analysis_start).total_seconds()
+                        "last_analyzed": analysis_start.isoformat(),
+                        "analysis_duration": (datetime.utcnow() - analysis_start).total_seconds(),
                     }
-                    
+
                     # Handle anomalies
-                    if analysis.get('is_anomaly', False):
+                    if analysis.get("is_anomaly", False):
                         await self._handle_anomaly(agent, analysis)
-                
+
                 except Exception as e:
                     logger.error(f"AI behavior analysis failed for agent {agent.id}: {e}")
-            
+
             # Check for stale agents
             if await self._is_agent_stale(agent):
                 await self._handle_stale_agent(agent)
-            
+
             # Check for suspicious API access patterns
             await self._check_api_access_patterns(agent)
-            
+
         except Exception as e:
             logger.error(f"Error analyzing agent {agent.id}: {e}", exc_info=True)
-    
+
     async def _analyze_behavior_with_ai(self, agent_data: Dict) -> Dict:
         """Use AI to analyze agent behavior patterns"""
         try:
             # Create AI request for behavior analysis
             ai_request = AIRequest(
                 request_id=f"surveillance_{agent_data['id']}_{datetime.utcnow().timestamp()}",
                 model_type=AIModelType.LLM,
                 input_data={
                     "task": "analyze_agent_behavior",
                     "agent_data": agent_data,
-                    "thresholds": self.config.performance_thresholds
+                    "thresholds": self.config.performance_thresholds,
                 },
                 parameters={
                     "model": "gpt-3.5-turbo",
                     "max_tokens": 500,
-                    "temperature": 0.3  # Lower temperature for more deterministic analysis
-                }
+                    "temperature": 0.3,  # Lower temperature for more deterministic analysis
+                },
             )
-            
+
             response = await self.ai_service.process_request(ai_request)
-            
+
             if response.success:
                 # Parse AI response
-                result = response.output if isinstance(response.output, dict) else {
-                    'is_anomaly': False,
-                    'anomaly_score': 0.0,
-                    'confidence': 0.5,
-                    'ai_explanation': str(response.output)
-                }
+                result = (
+                    response.output
+                    if isinstance(response.output, dict)
+                    else {
+                        "is_anomaly": False,
+                        "anomaly_score": 0.0,
+                        "confidence": 0.5,
+                        "ai_explanation": str(response.output),
+                    }
+                )
                 return result
             else:
                 logger.warning(f"AI behavior analysis failed: {response.metadata}")
-                return {'is_anomaly': False, 'anomaly_score': 0.0, 'confidence': 0.0}
-        
+                return {"is_anomaly": False, "anomaly_score": 0.0, "confidence": 0.0}
+
         except Exception as e:
             logger.error(f"Error in AI behavior analysis: {e}")
-            return {'is_anomaly': False, 'anomaly_score': 0.0, 'confidence': 0.0}
-    
+            return {"is_anomaly": False, "anomaly_score": 0.0, "confidence": 0.0}
+
     async def _check_health_metrics(self, agent) -> List[Dict]:
         """Check agent health metrics against thresholds"""
         issues = []
         metrics = agent.performance_metrics or {}
-        
+
         # CPU usage check
-        cpu_usage = metrics.get('cpu_usage', 0)
-        if cpu_usage > self.config.performance_thresholds['cpu_usage']:
-            issues.append({
-                'type': 'high_cpu',
-                'metric': 'cpu_usage',
-                'value': cpu_usage,
-                'threshold': self.config.performance_thresholds['cpu_usage']
-            })
-        
+        cpu_usage = metrics.get("cpu_usage", 0)
+        if cpu_usage > self.config.performance_thresholds["cpu_usage"]:
+            issues.append(
+                {
+                    "type": "high_cpu",
+                    "metric": "cpu_usage",
+                    "value": cpu_usage,
+                    "threshold": self.config.performance_thresholds["cpu_usage"],
+                }
+            )
+
         # Memory usage check
-        memory_usage = metrics.get('memory_usage', 0)
-        if memory_usage > self.config.performance_thresholds['memory_usage']:
-            issues.append({
-                'type': 'high_memory',
-                'metric': 'memory_usage',
-                'value': memory_usage,
-                'threshold': self.config.performance_thresholds['memory_usage']
-            })
-        
+        memory_usage = metrics.get("memory_usage", 0)
+        if memory_usage > self.config.performance_thresholds["memory_usage"]:
+            issues.append(
+                {
+                    "type": "high_memory",
+                    "metric": "memory_usage",
+                    "value": memory_usage,
+                    "threshold": self.config.performance_thresholds["memory_usage"],
+                }
+            )
+
         # Response time check
-        response_time = metrics.get('response_time', 0)
-        if response_time > self.config.performance_thresholds['response_time']:
-            issues.append({
-                'type': 'high_response_time',
-                'metric': 'response_time',
-                'value': response_time,
-                'threshold': self.config.performance_thresholds['response_time']
-            })
-        
+        response_time = metrics.get("response_time", 0)
+        if response_time > self.config.performance_thresholds["response_time"]:
+            issues.append(
+                {
+                    "type": "high_response_time",
+                    "metric": "response_time",
+                    "value": response_time,
+                    "threshold": self.config.performance_thresholds["response_time"],
+                }
+            )
+
         # Error rate check
-        error_rate = metrics.get('error_rate', 0)
-        if error_rate > self.config.performance_thresholds['error_rate']:
-            issues.append({
-                'type': 'high_error_rate',
-                'metric': 'error_rate',
-                'value': error_rate,
-                'threshold': self.config.performance_thresholds['error_rate']
-            })
-        
+        error_rate = metrics.get("error_rate", 0)
+        if error_rate > self.config.performance_thresholds["error_rate"]:
+            issues.append(
+                {
+                    "type": "high_error_rate",
+                    "metric": "error_rate",
+                    "value": error_rate,
+                    "threshold": self.config.performance_thresholds["error_rate"],
+                }
+            )
+
         return issues
-    
+
     async def _handle_health_issues(self, agent, issues: List[Dict]):
         """Handle detected health issues"""
         for issue in issues:
             severity = AlertSeverity.WARNING
-            if issue['type'] in ['high_cpu', 'high_memory']:
-                severity = AlertSeverity.CRITICAL if issue['value'] > 95 else AlertSeverity.WARNING
-            elif issue['type'] == 'high_error_rate':
-                severity = AlertSeverity.CRITICAL if issue['value'] > 0.1 else AlertSeverity.WARNING
-            
+            if issue["type"] in ["high_cpu", "high_memory"]:
+                severity = AlertSeverity.CRITICAL if issue["value"] > 95 else AlertSeverity.WARNING
+            elif issue["type"] == "high_error_rate":
+                severity = AlertSeverity.CRITICAL if issue["value"] > 0.1 else AlertSeverity.WARNING
+
             await self.alert_manager.create_alert(
                 category=AlertCategory.PERFORMANCE,
                 severity=severity,
                 title=f"Agent Health Issue: {issue['type'].replace('_', ' ').title()}",
                 description=(
@@ -327,27 +374,27 @@
                 ),
                 source="AgentSurveillanceSystem",
                 metadata={
                     "agent_id": str(agent.id),
                     "tenant_id": agent.tenant_id,
-                    "metric": issue['metric'],
-                    "value": issue['value'],
-                    "threshold": issue['threshold']
-                }
+                    "metric": issue["metric"],
+                    "value": issue["value"],
+                    "threshold": issue["threshold"],
+                },
             )
-    
+
     async def _handle_anomaly(self, agent, analysis: Dict):
         """Handle AI-detected anomalies"""
-        anomaly_score = analysis.get('anomaly_score', 0)
-        confidence = analysis.get('confidence', 0)
-        
+        anomaly_score = analysis.get("anomaly_score", 0)
+        confidence = analysis.get("confidence", 0)
+
         severity = (
-            AlertSeverity.CRITICAL 
+            AlertSeverity.CRITICAL
             if anomaly_score > self.config.anomaly_threshold and confidence > 0.8
             else AlertSeverity.WARNING
         )
-        
+
         await self.alert_manager.create_alert(
             category=AlertCategory.SECURITY,
             severity=severity,
             title="Agent Anomalous Behavior Detected",
             description=(
@@ -360,42 +407,42 @@
                 "agent_id": str(agent.id),
                 "tenant_id": agent.tenant_id,
                 "anomaly_score": anomaly_score,
                 "confidence": confidence,
                 "ai_analysis": analysis,
-                "requires_manager_action": severity == AlertSeverity.CRITICAL
-            }
-        )
-        
+                "requires_manager_action": severity == AlertSeverity.CRITICAL,
+            },
+        )
+
         # Update agent security score based on anomaly
         async with self.db_manager.get_session() as session:
             db_agent = await session.get(Agent, agent.id)
             if db_agent:
                 score_reduction = (
-                    self.config.security_score_reduction_critical 
-                    if severity == AlertSeverity.CRITICAL 
+                    self.config.security_score_reduction_critical
+                    if severity == AlertSeverity.CRITICAL
                     else self.config.security_score_reduction_warning
                 )
                 new_score = max(0, db_agent.security_score - score_reduction)
                 db_agent.security_score = new_score
                 await session.commit()
-                
+
                 logger.warning(
                     f"Agent {agent.id} security score reduced to {new_score} "
                     f"due to anomaly (severity: {severity.value})"
                 )
-    
+
     async def _is_agent_stale(self, agent) -> bool:
         """Check if agent hasn't sent heartbeat recently"""
         if not agent.last_heartbeat:
             return True
-        
+
         stale_threshold = datetime.utcnow() - timedelta(
             minutes=self.config.stale_agent_timeout_minutes
         )
         return agent.last_heartbeat < stale_threshold
-    
+
     async def _handle_stale_agent(self, agent):
         """Handle agents that haven't sent recent heartbeats"""
         await self.alert_manager.create_alert(
             category=AlertCategory.SYSTEM,
             severity=AlertSeverity.WARNING,
@@ -406,21 +453,23 @@
             ),
             source="AgentSurveillanceSystem",
             metadata={
                 "agent_id": str(agent.id),
                 "tenant_id": agent.tenant_id,
-                "last_heartbeat": agent.last_heartbeat.isoformat() if agent.last_heartbeat else "N/A"
-            }
-        )
-    
+                "last_heartbeat": (
+                    agent.last_heartbeat.isoformat() if agent.last_heartbeat else "N/A"
+                ),
+            },
+        )
+
     async def _check_api_access_patterns(self, agent):
         """Check for suspicious API access patterns"""
         try:
             metrics = agent.performance_metrics or {}
-            
+
             # Check for unusual API call frequency
-            api_calls = metrics.get('api_calls_per_minute', 0)
+            api_calls = metrics.get("api_calls_per_minute", 0)
             if api_calls > 1000:  # Threshold for suspicious activity
                 await self.alert_manager.create_alert(
                     category=AlertCategory.SECURITY,
                     severity=AlertSeverity.WARNING,
                     title="Suspicious API Access Pattern",
@@ -430,16 +479,16 @@
                     ),
                     source="AgentSurveillanceSystem",
                     metadata={
                         "agent_id": str(agent.id),
                         "tenant_id": agent.tenant_id,
-                        "api_calls_per_minute": api_calls
-                    }
-                )
-            
+                        "api_calls_per_minute": api_calls,
+                    },
+                )
+
             # Check for unauthorized endpoint access attempts
-            failed_auth_attempts = metrics.get('failed_auth_attempts', 0)
+            failed_auth_attempts = metrics.get("failed_auth_attempts", 0)
             if failed_auth_attempts > 5:
                 await self.alert_manager.create_alert(
                     category=AlertCategory.SECURITY,
                     severity=AlertSeverity.CRITICAL,
                     title="Multiple Failed Authentication Attempts",
@@ -450,79 +499,81 @@
                     source="AgentSurveillanceSystem",
                     metadata={
                         "agent_id": str(agent.id),
                         "tenant_id": agent.tenant_id,
                         "failed_auth_attempts": failed_auth_attempts,
-                        "requires_immediate_action": True
-                    }
-                )
-        
+                        "requires_immediate_action": True,
+                    },
+                )
+
         except Exception as e:
             logger.error(f"Error checking API access patterns for agent {agent.id}: {e}")
-    
+
     def _prepare_agent_data(self, agent) -> Dict:
         """Prepare agent data for AI analysis"""
         metrics = agent.performance_metrics or {}
-        
+
         return {
-            'id': str(agent.id),
-            'name': agent.name,
-            'type': agent.type,
-            'cpu_usage': metrics.get('cpu_usage', 0),
-            'memory_usage': metrics.get('memory_usage', 0),
-            'response_time': metrics.get('response_time', 0),
-            'error_rate': metrics.get('error_rate', 0),
-            'api_calls_per_minute': metrics.get('api_calls_per_minute', 0),
-            'failed_auth_attempts': metrics.get('failed_auth_attempts', 0),
-            'last_heartbeat': agent.last_heartbeat.isoformat() if agent.last_heartbeat else None,
-            'security_score': agent.security_score,
-            'uptime_seconds': (datetime.utcnow() - agent.created_at).total_seconds() if agent.created_at else 0,
-            'status': agent.status,
-            'capabilities': agent.capabilities,
-            'recent_tasks_completed': metrics.get('recent_tasks_completed', 0),
-            'recent_tasks_failed': metrics.get('recent_tasks_failed', 0)
+            "id": str(agent.id),
+            "name": agent.name,
+            "type": agent.type,
+            "cpu_usage": metrics.get("cpu_usage", 0),
+            "memory_usage": metrics.get("memory_usage", 0),
+            "response_time": metrics.get("response_time", 0),
+            "error_rate": metrics.get("error_rate", 0),
+            "api_calls_per_minute": metrics.get("api_calls_per_minute", 0),
+            "failed_auth_attempts": metrics.get("failed_auth_attempts", 0),
+            "last_heartbeat": agent.last_heartbeat.isoformat() if agent.last_heartbeat else None,
+            "security_score": agent.security_score,
+            "uptime_seconds": (
+                (datetime.utcnow() - agent.created_at).total_seconds() if agent.created_at else 0
+            ),
+            "status": agent.status,
+            "capabilities": agent.capabilities,
+            "recent_tasks_completed": metrics.get("recent_tasks_completed", 0),
+            "recent_tasks_failed": metrics.get("recent_tasks_failed", 0),
         }
-    
+
     async def get_agent_metrics(self, agent_id: str) -> Optional[Dict]:
         """Get historical metrics for an agent"""
         return self.agent_metrics.get(agent_id)
-    
+
     async def get_all_metrics(self) -> Dict:
         """Get metrics for all monitored agents"""
         return self.agent_metrics.copy()
-    
+
     async def get_surveillance_report(self) -> Dict:
         """Generate comprehensive surveillance report"""
         return {
             "monitoring_active": self.monitoring_active,
             "monitored_agents_count": len(self.monitored_agents),
             "agents_with_metrics": len(self.agent_metrics),
             "configuration": {
                 "anomaly_threshold": self.config.anomaly_threshold,
                 "monitoring_interval_seconds": self.config.monitoring_interval_seconds,
-                "ai_analysis_enabled": self.config.enable_ai_behavior_analysis
+                "ai_analysis_enabled": self.config.enable_ai_behavior_analysis,
             },
-            "timestamp": datetime.utcnow().isoformat()
+            "timestamp": datetime.utcnow().isoformat(),
         }
 
     async def health_check(self) -> str:
         """Perform a health check on the AgentSurveillanceSystem."""
         try:
             # Check if DB is accessible
             async with self.db_manager.get_session() as session:
                 await session.execute(select(Agent).limit(1))
-            
+
             # Check AI service health
             if self.ai_service:
                 ai_health = await self.ai_service.health_check()
                 if ai_health != "healthy":
                     return f"degraded: AI service is {ai_health}"
-            
+
             # Check Alert Manager health
             alert_manager_health = await self.alert_manager.health_check()
             if alert_manager_health != "healthy":
                 return f"degraded: Alert Manager is {alert_manager_health}"
-            
+
             return "healthy"
         except Exception as e:
             logger.error(f"AgentSurveillanceSystem health check failed: {e}", exc_info=True)
             return "unhealthy"
would reformat /home/runner/work/ymera_y/ymera_y/agent_surveillance.py
--- /home/runner/work/ymera_y/ymera_y/agent_system.py	2025-10-19 22:47:02.791432+00:00
+++ /home/runner/work/ymera_y/ymera_y/agent_system.py	2025-10-19 23:08:52.790857+00:00
@@ -28,133 +28,144 @@
 from prometheus_client import Counter, Histogram, generate_latest
 import uvicorn
 
 # Configure logging
 logging.basicConfig(
-    level=logging.INFO,
-    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
+    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
 )
 logger = logging.getLogger(__name__)
 
 # =============================================================================
 # MODELS & SCHEMAS
 # =============================================================================
 
 Base = declarative_base()
+
 
 class AgentStatus(str, Enum):
     ACTIVE = "active"
     INACTIVE = "inactive"
     BUSY = "busy"
     ERROR = "error"
 
+
 class TaskStatus(str, Enum):
     PENDING = "pending"
     RUNNING = "running"
     COMPLETED = "completed"
     FAILED = "failed"
 
+
 class TaskPriority(str, Enum):
     LOW = "low"
     NORMAL = "normal"
     HIGH = "high"
     CRITICAL = "critical"
 
+
 # Database Models
 class User(Base):
     __tablename__ = "users"
-    
+
     id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
     username = Column(String(50), unique=True, nullable=False, index=True)
     email = Column(String(255), unique=True, nullable=False, index=True)
     password_hash = Column(String(255), nullable=False)
     is_active = Column(Boolean, default=True)
     created_at = Column(DateTime, default=datetime.utcnow)
     updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
-    
+
     # Relationships
     agents = relationship("Agent", back_populates="owner")
     tasks = relationship("Task", back_populates="user")
 
+
 class Agent(Base):
     __tablename__ = "agents"
-    
+
     id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
     name = Column(String(100), nullable=False)
     description = Column(Text)
     capabilities = Column(JSON, default=list)
     status = Column(String(20), default=AgentStatus.INACTIVE)
     owner_id = Column(String, ForeignKey("users.id"), nullable=False)
     config = Column(JSON, default=dict)
     last_heartbeat = Column(DateTime)
     created_at = Column(DateTime, default=datetime.utcnow)
     updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
-    
+
     # Relationships
     owner = relationship("User", back_populates="agents")
     tasks = relationship("Task", back_populates="agent")
 
+
 class Task(Base):
     __tablename__ = "tasks"
-    
+
     id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
     name = Column(String(200), nullable=False)
     description = Column(Text)
     task_type = Column(String(50), nullable=False)
     parameters = Column(JSON, default=dict)
     priority = Column(String(20), default=TaskPriority.NORMAL)
     status = Column(String(20), default=TaskStatus.PENDING)
     result = Column(JSON)
     error_message = Column(Text)
-    
+
     user_id = Column(String, ForeignKey("users.id"), nullable=False)
     agent_id = Column(String, ForeignKey("agents.id"))
-    
+
     created_at = Column(DateTime, default=datetime.utcnow)
     updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
     started_at = Column(DateTime)
     completed_at = Column(DateTime)
-    
+
     # Relationships
     user = relationship("User", back_populates="tasks")
     agent = relationship("Agent", back_populates="tasks")
 
+
 # Pydantic Schemas
 class UserCreate(BaseModel):
     username: str = Field(..., min_length=3, max_length=50)
-    email: str = Field(..., regex=r'^[^@]+@[^@]+\.[^@]+$')
+    email: str = Field(..., regex=r"^[^@]+@[^@]+\.[^@]+$")
     password: str = Field(..., min_length=8)
+
 
 class UserResponse(BaseModel):
     id: str
     username: str
     email: str
     is_active: bool
     created_at: datetime
 
+
 class AgentCreate(BaseModel):
     name: str = Field(..., min_length=1, max_length=100)
     description: Optional[str] = None
     capabilities: List[str] = Field(default_factory=list)
     config: Dict[str, Any] = Field(default_factory=dict)
+
 
 class AgentResponse(BaseModel):
     id: str
     name: str
     description: Optional[str]
     capabilities: List[str]
     status: AgentStatus
     last_heartbeat: Optional[datetime]
     created_at: datetime
 
+
 class TaskCreate(BaseModel):
     name: str = Field(..., min_length=1, max_length=200)
     description: Optional[str] = None
     task_type: str = Field(..., min_length=1, max_length=50)
     parameters: Dict[str, Any] = Field(default_factory=dict)
     priority: TaskPriority = TaskPriority.NORMAL
     agent_id: Optional[str] = None
+
 
 class TaskResponse(BaseModel):
     id: str
     name: str
     description: Optional[str]
@@ -165,149 +176,156 @@
     result: Optional[Dict[str, Any]]
     error_message: Optional[str]
     created_at: datetime
     completed_at: Optional[datetime]
 
+
 # =============================================================================
 # CORE SERVICES
 # =============================================================================
+
 
 class DatabaseManager:
     def __init__(self, database_url: str):
         self.engine = create_async_engine(database_url, echo=False)
         self.async_session = async_sessionmaker(
             self.engine, class_=AsyncSession, expire_on_commit=False
         )
-    
+
     async def get_session(self) -> AsyncSession:
         async with self.async_session() as session:
             yield session
-    
+
     async def create_tables(self):
         async with self.engine.begin() as conn:
             await conn.run_sync(Base.metadata.create_all)
+
 
 class AuthService:
     def __init__(self, secret_key: str):
         self.secret_key = secret_key
         self.algorithm = "HS256"
         self.access_token_expire_minutes = 30
-    
+
     def hash_password(self, password: str) -> str:
-        return bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt()).decode('utf-8')
-    
+        return bcrypt.hashpw(password.encode("utf-8"), bcrypt.gensalt()).decode("utf-8")
+
     def verify_password(self, password: str, hashed_password: str) -> bool:
-        return bcrypt.checkpw(password.encode('utf-8'), hashed_password.encode('utf-8'))
-    
+        return bcrypt.checkpw(password.encode("utf-8"), hashed_password.encode("utf-8"))
+
     def create_access_token(self, data: dict) -> str:
         to_encode = data.copy()
         expire = datetime.utcnow() + timedelta(minutes=self.access_token_expire_minutes)
         to_encode.update({"exp": expire})
         return jwt.encode(to_encode, self.secret_key, algorithm=self.algorithm)
-    
+
     async def verify_token(self, token: str) -> dict:
         try:
             payload = jwt.decode(token, self.secret_key, algorithms=[self.algorithm])
             return payload
         except JWTError:
             raise HTTPException(
                 status_code=status.HTTP_401_UNAUTHORIZED,
-                detail="Invalid authentication credentials"
+                detail="Invalid authentication credentials",
             )
+
 
 class TaskQueue:
     def __init__(self, redis_client):
         self.redis = redis_client
         self.queue_name = "agent_tasks"
-    
+
     async def enqueue_task(self, task_data: dict, priority: TaskPriority = TaskPriority.NORMAL):
         priority_score = {
             TaskPriority.CRITICAL: 4,
             TaskPriority.HIGH: 3,
             TaskPriority.NORMAL: 2,
-            TaskPriority.LOW: 1
+            TaskPriority.LOW: 1,
         }[priority]
-        
-        await self.redis.zadd(
-            self.queue_name, 
-            {json.dumps(task_data): priority_score}
-        )
-    
+
+        await self.redis.zadd(self.queue_name, {json.dumps(task_data): priority_score})
+
     async def dequeue_task(self) -> Optional[dict]:
         task_data = await self.redis.zpopmax(self.queue_name)
         if task_data:
             return json.loads(task_data[0][0])
         return None
 
+
 class AgentManager:
     def __init__(self, db_session_factory, task_queue: TaskQueue):
         self.db_session_factory = db_session_factory
         self.task_queue = task_queue
         self.active_agents = {}
-    
+
     async def register_agent(self, agent_data: dict) -> str:
         async with self.db_session_factory() as session:
             agent = Agent(**agent_data)
             session.add(agent)
             await session.commit()
             await session.refresh(agent)
             return agent.id
-    
+
     async def assign_task_to_agent(self, task_id: str, agent_id: Optional[str] = None) -> str:
         async with self.db_session_factory() as session:
             # Get task
             task = await session.get(Task, task_id)
             if not task:
                 raise HTTPException(status_code=404, detail="Task not found")
-            
+
             # Find best agent if not specified
             if not agent_id:
                 agent_id = await self._find_best_agent(task, session)
-            
+
             if not agent_id:
                 raise HTTPException(status_code=400, detail="No suitable agent available")
-            
+
             # Assign task
             task.agent_id = agent_id
             task.status = TaskStatus.RUNNING
             task.started_at = datetime.utcnow()
-            
+
             await session.commit()
-            
+
             # Add to task queue
-            await self.task_queue.enqueue_task({
-                "task_id": task_id,
-                "agent_id": agent_id,
-                "task_type": task.task_type,
-                "parameters": task.parameters
-            }, TaskPriority(task.priority))
-            
+            await self.task_queue.enqueue_task(
+                {
+                    "task_id": task_id,
+                    "agent_id": agent_id,
+                    "task_type": task.task_type,
+                    "parameters": task.parameters,
+                },
+                TaskPriority(task.priority),
+            )
+
             return agent_id
-    
+
     async def _find_best_agent(self, task: Task, session: AsyncSession) -> Optional[str]:
         # Simple agent selection based on availability and capabilities
         result = await session.execute(
             select(Agent).where(
-                Agent.status == AgentStatus.ACTIVE,
-                Agent.capabilities.contains([task.task_type])
+                Agent.status == AgentStatus.ACTIVE, Agent.capabilities.contains([task.task_type])
             )
         )
         agents = result.scalars().all()
-        
+
         if agents:
             # Return first available agent (can be enhanced with load balancing)
             return agents[0].id
         return None
 
+
 # =============================================================================
 # METRICS & MONITORING
 # =============================================================================
 
 # Prometheus Metrics
-REQUEST_COUNT = Counter('http_requests_total', 'Total HTTP requests', ['method', 'endpoint', 'status'])
-REQUEST_DURATION = Histogram('http_request_duration_seconds', 'HTTP request duration')
-TASK_COUNT = Counter('tasks_total', 'Total tasks', ['status', 'type'])
+REQUEST_COUNT = Counter(
+    "http_requests_total", "Total HTTP requests", ["method", "endpoint", "status"]
+)
+REQUEST_DURATION = Histogram("http_request_duration_seconds", "HTTP request duration")
+TASK_COUNT = Counter("tasks_total", "Total tasks", ["status", "type"])
 
 # =============================================================================
 # APPLICATION SETUP
 # =============================================================================
 
@@ -316,46 +334,48 @@
 auth_service = None
 task_queue = None
 agent_manager = None
 redis_client = None
 
+
 @asynccontextmanager
 async def lifespan(app: FastAPI):
     global db_manager, auth_service, task_queue, agent_manager, redis_client
-    
+
     # Initialize services
     database_url = os.getenv("DATABASE_URL", "postgresql+asyncpg://user:pass@localhost/agentdb")
     redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
     jwt_secret = os.getenv("JWT_SECRET", "your-secret-key-change-in-production")
-    
+
     # Setup database
     db_manager = DatabaseManager(database_url)
     await db_manager.create_tables()
-    
+
     # Setup Redis
     redis_client = await aioredis.from_url(redis_url)
-    
+
     # Initialize services
     auth_service = AuthService(jwt_secret)
     task_queue = TaskQueue(redis_client)
     agent_manager = AgentManager(db_manager.async_session, task_queue)
-    
+
     # Start background task processor
     asyncio.create_task(process_tasks())
-    
+
     logger.info("Agent Management System started")
     yield
-    
+
     # Cleanup
     await redis_client.close()
     await db_manager.engine.dispose()
+
 
 app = FastAPI(
     title="Agent Management System",
     description="Production-ready enterprise agent management platform",
     version="1.0.0",
-    lifespan=lifespan
+    lifespan=lifespan,
 )
 
 # Add CORS middleware
 app.add_middleware(
     CORSMiddleware,
@@ -366,29 +386,29 @@
 )
 
 # Security
 security = HTTPBearer()
 
+
 async def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security)):
     payload = await auth_service.verify_token(credentials.credentials)
     user_id = payload.get("sub")
-    
+
     async with db_manager.async_session() as session:
         result = await session.execute(select(User).where(User.id == user_id))
         user = result.scalar_one_or_none()
-        
+
         if not user:
-            raise HTTPException(
-                status_code=status.HTTP_401_UNAUTHORIZED,
-                detail="User not found"
-            )
-        
+            raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="User not found")
+
         return user
 
+
 # =============================================================================
 # API ROUTES
 # =============================================================================
+
 
 @app.post("/auth/register", response_model=dict, status_code=201)
 async def register(user_data: UserCreate):
     async with db_manager.async_session() as session:
         # Check if user exists
@@ -396,258 +416,231 @@
             select(User).where(
                 (User.username == user_data.username) | (User.email == user_data.email)
             )
         )
         if result.scalar_one_or_none():
-            raise HTTPException(
-                status_code=400, 
-                detail="Username or email already registered"
-            )
-        
+            raise HTTPException(status_code=400, detail="Username or email already registered")
+
         # Create user
         hashed_password = auth_service.hash_password(user_data.password)
         user = User(
-            username=user_data.username,
-            email=user_data.email,
-            password_hash=hashed_password
+            username=user_data.username, email=user_data.email, password_hash=hashed_password
         )
-        
+
         session.add(user)
         await session.commit()
         await session.refresh(user)
-        
+
         # Create access token
         access_token = auth_service.create_access_token(data={"sub": user.id})
-        
-        return {
-            "access_token": access_token,
-            "token_type": "bearer",
-            "user_id": user.id
-        }
+
+        return {"access_token": access_token, "token_type": "bearer", "user_id": user.id}
+
 
 @app.post("/auth/login")
 async def login(username: str, password: str):
     async with db_manager.async_session() as session:
-        result = await session.execute(
-            select(User).where(User.username == username)
-        )
+        result = await session.execute(select(User).where(User.username == username))
         user = result.scalar_one_or_none()
-        
+
         if not user or not auth_service.verify_password(password, user.password_hash):
             raise HTTPException(
-                status_code=status.HTTP_401_UNAUTHORIZED,
-                detail="Invalid credentials"
+                status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid credentials"
             )
-        
+
         access_token = auth_service.create_access_token(data={"sub": user.id})
-        
-        return {
-            "access_token": access_token,
-            "token_type": "bearer",
-            "user_id": user.id
-        }
+
+        return {"access_token": access_token, "token_type": "bearer", "user_id": user.id}
+
 
 @app.get("/users/me", response_model=UserResponse)
 async def get_current_user_info(current_user: User = Depends(get_current_user)):
     return current_user
 
+
 @app.post("/agents", response_model=dict, status_code=201)
-async def create_agent(
-    agent_data: AgentCreate,
-    current_user: User = Depends(get_current_user)
-):
+async def create_agent(agent_data: AgentCreate, current_user: User = Depends(get_current_user)):
     async with db_manager.async_session() as session:
         agent = Agent(
             name=agent_data.name,
             description=agent_data.description,
             capabilities=agent_data.capabilities,
             config=agent_data.config,
-            owner_id=current_user.id
+            owner_id=current_user.id,
         )
-        
+
         session.add(agent)
         await session.commit()
         await session.refresh(agent)
-        
+
         return {"agent_id": agent.id, "status": "created"}
+
 
 @app.get("/agents", response_model=List[AgentResponse])
 async def list_agents(current_user: User = Depends(get_current_user)):
     async with db_manager.async_session() as session:
-        result = await session.execute(
-            select(Agent).where(Agent.owner_id == current_user.id)
-        )
+        result = await session.execute(select(Agent).where(Agent.owner_id == current_user.id))
         agents = result.scalars().all()
         return agents
 
+
 @app.get("/agents/{agent_id}", response_model=AgentResponse)
-async def get_agent(
-    agent_id: str,
-    current_user: User = Depends(get_current_user)
-):
+async def get_agent(agent_id: str, current_user: User = Depends(get_current_user)):
     async with db_manager.async_session() as session:
         agent = await session.get(Agent, agent_id)
-        
+
         if not agent or agent.owner_id != current_user.id:
             raise HTTPException(status_code=404, detail="Agent not found")
-        
+
         return agent
+
 
 @app.post("/agents/{agent_id}/heartbeat")
 async def agent_heartbeat(
-    agent_id: str,
-    status_data: dict,
-    current_user: User = Depends(get_current_user)
+    agent_id: str, status_data: dict, current_user: User = Depends(get_current_user)
 ):
     async with db_manager.async_session() as session:
         agent = await session.get(Agent, agent_id)
-        
+
         if not agent or agent.owner_id != current_user.id:
             raise HTTPException(status_code=404, detail="Agent not found")
-        
+
         agent.last_heartbeat = datetime.utcnow()
         agent.status = status_data.get("status", AgentStatus.ACTIVE)
-        
+
         await session.commit()
-        
+
         return {"status": "heartbeat_received"}
+
 
 @app.post("/tasks", response_model=dict, status_code=201)
 async def create_task(
     task_data: TaskCreate,
     background_tasks: BackgroundTasks,
-    current_user: User = Depends(get_current_user)
+    current_user: User = Depends(get_current_user),
 ):
     async with db_manager.async_session() as session:
         task = Task(
             name=task_data.name,
             description=task_data.description,
             task_type=task_data.task_type,
             parameters=task_data.parameters,
             priority=task_data.priority,
             user_id=current_user.id,
-            agent_id=task_data.agent_id
+            agent_id=task_data.agent_id,
         )
-        
+
         session.add(task)
         await session.commit()
         await session.refresh(task)
-        
+
         # Assign to agent and queue for processing
-        background_tasks.add_task(
-            agent_manager.assign_task_to_agent, 
-            task.id, 
-            task_data.agent_id
-        )
-        
+        background_tasks.add_task(agent_manager.assign_task_to_agent, task.id, task_data.agent_id)
+
         TASK_COUNT.labels(status="created", type=task.task_type).inc()
-        
+
         return {"task_id": task.id, "status": "created"}
+
 
 @app.get("/tasks", response_model=List[TaskResponse])
 async def list_tasks(current_user: User = Depends(get_current_user)):
     async with db_manager.async_session() as session:
         result = await session.execute(
             select(Task).where(Task.user_id == current_user.id).order_by(Task.created_at.desc())
         )
         tasks = result.scalars().all()
         return tasks
 
+
 @app.get("/tasks/{task_id}", response_model=TaskResponse)
-async def get_task(
-    task_id: str,
-    current_user: User = Depends(get_current_user)
-):
+async def get_task(task_id: str, current_user: User = Depends(get_current_user)):
     async with db_manager.async_session() as session:
         task = await session.get(Task, task_id)
-        
+
         if not task or task.user_id != current_user.id:
             raise HTTPException(status_code=404, detail="Task not found")
-        
+
         return task
+
 
 @app.get("/health")
 async def health_check():
-    return {
-        "status": "healthy",
-        "timestamp": datetime.utcnow().isoformat(),
-        "version": "1.0.0"
-    }
+    return {"status": "healthy", "timestamp": datetime.utcnow().isoformat(), "version": "1.0.0"}
+
 
 @app.get("/metrics")
 async def get_metrics():
     from prometheus_client import generate_latest, CONTENT_TYPE_LATEST
+
     return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)
 
+
 # =============================================================================
 # BACKGROUND TASK PROCESSOR
 # =============================================================================
+
 
 async def process_tasks():
     """Background task processor"""
     logger.info("Task processor started")
-    
+
     while True:
         try:
             task_data = await task_queue.dequeue_task()
-            
+
             if task_data:
                 logger.info(f"Processing task: {task_data['task_id']}")
-                
+
                 # Process the task (implement your task processing logic here)
                 await process_single_task(task_data)
             else:
                 # No tasks, wait a bit
                 await asyncio.sleep(1)
-                
+
         except Exception as e:
             logger.error(f"Task processing error: {e}")
             await asyncio.sleep(5)
 
+
 async def process_single_task(task_data: dict):
     """Process a single task"""
     task_id = task_data["task_id"]
-    
+
     async with db_manager.async_session() as session:
         task = await session.get(Task, task_id)
-        
+
         if not task:
             logger.error(f"Task {task_id} not found")
             return
-        
+
         try:
             # Simulate task processing
             logger.info(f"Processing task {task_id} of type {task.task_type}")
-            
+
             # Here you would implement actual task processing logic
             # For demo purposes, we'll simulate some work
             await asyncio.sleep(2)
-            
+
             # Mark task as completed
             task.status = TaskStatus.COMPLETED
             task.completed_at = datetime.utcnow()
             task.result = {"status": "success", "message": "Task completed successfully"}
-            
+
             TASK_COUNT.labels(status="completed", type=task.task_type).inc()
-            
+
         except Exception as e:
             logger.error(f"Task {task_id} failed: {e}")
             task.status = TaskStatus.FAILED
             task.error_message = str(e)
             task.completed_at = datetime.utcnow()
-            
+
             TASK_COUNT.labels(status="failed", type=task.task_type).inc()
-        
+
         await session.commit()
 
+
 # =============================================================================
 # MAIN
 # =============================================================================
 
 if __name__ == "__main__":
-    uvicorn.run(
-        "main:app",
-        host="0.0.0.0",
-        port=8000,
-        reload=True,
-        log_level="info"
-    )
+    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True, log_level="info")
would reformat /home/runner/work/ymera_y/ymera_y/agent_system.py
error: cannot format /home/runner/work/ymera_y/ymera_y/agents_management_api.py: Cannot parse for target version Python 3.12: 787:0:     try:
--- /home/runner/work/ymera_y/ymera_y/alembic_setup.py	2025-10-19 22:47:02.792432+00:00
+++ /home/runner/work/ymera_y/ymera_y/alembic_setup.py	2025-10-19 23:08:53.105395+00:00
@@ -1,8 +1,9 @@
 """
 alembic/env.py - Production-ready migration configuration
 """
+
 from logging.config import fileConfig
 from sqlalchemy import engine_from_config, pool, text
 from alembic import context
 import os
 import sys
@@ -50,29 +51,33 @@
 
 def run_migrations_online() -> None:
     """Run migrations in 'online' mode with connection pooling"""
     configuration = config.get_section(config.config_ini_section)
     configuration["sqlalchemy.url"] = get_url()
-    
+
     connectable = engine_from_config(
         configuration,
         prefix="sqlalchemy.",
         poolclass=pool.NullPool,  # Don't use connection pooling for migrations
     )
 
     with connectable.connect() as connection:
         # Create schema if it doesn't exist
-        connection.execute(text("IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'dbo') BEGIN EXEC('CREATE SCHEMA dbo') END"))
+        connection.execute(
+            text(
+                "IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'dbo') BEGIN EXEC('CREATE SCHEMA dbo') END"
+            )
+        )
         connection.commit()
-        
+
         context.configure(
             connection=connection,
             target_metadata=target_metadata,
             compare_type=True,
             compare_server_default=True,
             include_schemas=True,
-            version_table_schema='dbo',
+            version_table_schema="dbo",
         )
 
         with context.begin_transaction():
             context.run_migrations()
 
would reformat /home/runner/work/ymera_y/ymera_y/alembic_setup.py
--- /home/runner/work/ymera_y/ymera_y/analytics.engine.py	2025-10-19 22:47:02.792432+00:00
+++ /home/runner/work/ymera_y/ymera_y/analytics.engine.py	2025-10-19 23:08:53.965871+00:00
@@ -12,418 +12,424 @@
 from sklearn.linear_model import LinearRegression
 from sklearn.cluster import KMeans
 from sklearn.preprocessing import StandardScaler
 from sklearn.metrics import silhouette_score
 import warnings
-warnings.filterwarnings('ignore')
+
+warnings.filterwarnings("ignore")
 
 logger = logging.getLogger(__name__)
+
 
 class AdvancedAnalyticsEngine:
     """Advanced analytics engine with ML capabilities for business intelligence"""
-    
+
     def __init__(self):
         self.db = DatabaseUtils()
         self.cache = CacheManager()
         self.ml_models = {}
         self._init_ml_models()
-    
+
     def _init_ml_models(self):
         """Initialize machine learning models"""
         # Project risk prediction model
-        self.ml_models['project_risk'] = RandomForestClassifier(
-            n_estimators=100,
-            random_state=42,
-            class_weight='balanced'
+        self.ml_models["project_risk"] = RandomForestClassifier(
+            n_estimators=100, random_state=42, class_weight="balanced"
         )
-        
+
         # Resource optimization model
-        self.ml_models['resource_optimization'] = LinearRegression()
-        
+        self.ml_models["resource_optimization"] = LinearRegression()
+
         # Anomaly detection model
-        self.ml_models['anomaly_detection'] = IsolationForest(
-            contamination=0.1,
-            random_state=42
-        )
-        
+        self.ml_models["anomaly_detection"] = IsolationForest(contamination=0.1, random_state=42)
+
         # Team clustering model
-        self.ml_models['team_clustering'] = KMeans(
-            n_clusters=4,
-            random_state=42
-        )
-        
+        self.ml_models["team_clustering"] = KMeans(n_clusters=4, random_state=42)
+
         # Model training status
         self.model_training_status = {
-            'project_risk': False,
-            'resource_optimization': False,
-            'anomaly_detection': False,
-            'team_clustering': False
-        }
-    
+            "project_risk": False,
+            "resource_optimization": False,
+            "anomaly_detection": False,
+            "team_clustering": False,
+        }
+
     async def generate_project_insights(self, project_id: str) -> Dict[str, Any]:
         """Generate comprehensive insights for a project"""
         cache_key = f"project_insights:{project_id}"
         cached_insights = await self.cache.get(cache_key)
-        
+
         if cached_insights:
             return cached_insights
-        
+
         # Get project data
         project_data = await self._get_project_data(project_id)
         if not project_data:
             return {"error": "Project not found"}
-        
+
         # Calculate various insights
         insights = {
             "project_id": project_id,
             "timestamp": datetime.utcnow().isoformat(),
             "performance_metrics": await self._calculate_performance_metrics(project_data),
             "risk_assessment": await self._assess_project_risks(project_data),
             "team_analysis": await self._analyze_team_performance(project_data),
             "timeline_analysis": await self._analyze_project_timeline(project_data),
             "resource_utilization": await self._analyze_resource_utilization(project_data),
             "financial_metrics": await self._calculate_financial_metrics(project_data),
-            "recommendations": await self._generate_recommendations(project_data)
-        }
-        
+            "recommendations": await self._generate_recommendations(project_data),
+        }
+
         # Cache insights for 1 hour
         await self.cache.set(cache_key, insights, expire=3600)
-        
+
         return insights
-    
+
     async def predict_project_risks(self, project_data: Dict[str, Any]) -> Dict[str, Any]:
         """Predict project risks using ML model"""
-        if not self.model_training_status['project_risk']:
+        if not self.model_training_status["project_risk"]:
             await self._train_risk_prediction_model()
-        
+
         # Prepare features for prediction
         features = self._extract_risk_features(project_data)
-        
+
         if not features:
             return {"error": "Insufficient data for prediction"}
-        
+
         # Make prediction
         try:
-            risk_probabilities = self.ml_models['project_risk'].predict_proba([features])[0]
-            risk_level = self.ml_models['project_risk'].predict([features])[0]
-            
+            risk_probabilities = self.ml_models["project_risk"].predict_proba([features])[0]
+            risk_level = self.ml_models["project_risk"].predict([features])[0]
+
             return {
                 "risk_level": risk_level,
                 "probabilities": {
                     "low": float(risk_probabilities[0]),
                     "medium": float(risk_probabilities[1]),
-                    "high": float(risk_probabilities[2])
+                    "high": float(risk_probabilities[2]),
                 },
                 "key_risk_factors": self._identify_risk_factors(features),
-                "confidence_score": float(max(risk_probabilities))
+                "confidence_score": float(max(risk_probabilities)),
             }
         except Exception as e:
             logger.error(f"Risk prediction failed: {e}")
             return {"error": "Prediction failed"}
-    
+
     async def optimize_resource_allocation(self, projects: List[Dict[str, Any]]) -> Dict[str, Any]:
         """Optimize resource allocation across projects"""
-        if not self.model_training_status['resource_optimization']:
+        if not self.model_training_status["resource_optimization"]:
             await self._train_resource_optimization_model()
-        
+
         # Analyze current resource allocation
         current_allocation = await self._analyze_current_allocation(projects)
-        
+
         # Predict optimal allocation
         optimal_allocation = await self._predict_optimal_allocation(projects)
-        
+
         # Calculate optimization recommendations
         recommendations = await self._generate_allocation_recommendations(
             current_allocation, optimal_allocation
         )
-        
+
         return {
             "current_allocation": current_allocation,
             "optimal_allocation": optimal_allocation,
             "recommendations": recommendations,
             "expected_improvement": await self._calculate_expected_improvement(
                 current_allocation, optimal_allocation
-            )
-        }
-    
+            ),
+        }
+
     async def calculate_team_productivity_metrics(self, team_id: str = None) -> Dict[str, Any]:
         """Calculate comprehensive team productivity metrics"""
         # Get team data
         team_data = await self._get_team_data(team_id)
-        
+
         if not team_data:
             return {"error": "No team data available"}
-        
+
         # Calculate various productivity metrics
         metrics = {
             "team_id": team_id or "all_teams",
             "timestamp": datetime.utcnow().isoformat(),
             "overall_productivity": await self._calculate_overall_productivity(team_data),
             "task_completion_rates": await self._calculate_completion_rates(team_data),
             "quality_metrics": await self._calculate_quality_metrics(team_data),
             "efficiency_metrics": await self._calculate_efficiency_metrics(team_data),
             "collaboration_metrics": await self._calculate_collaboration_metrics(team_data),
             "trend_analysis": await self._analyze_productivity_trends(team_data),
-            "benchmark_comparison": await self._compare_to_benchmarks(team_data)
-        }
-        
+            "benchmark_comparison": await self._compare_to_benchmarks(team_data),
+        }
+
         return metrics
-    
+
     async def _train_risk_prediction_model(self):
         """Train the project risk prediction model"""
         try:
             # Get historical project data for training
             training_data = await self._get_training_data()
-            
+
             if not training_data or len(training_data) < 100:
                 logger.warning("Insufficient training data for risk prediction model")
                 return
-            
+
             # Prepare features and labels
             X = []
             y = []
-            
+
             for project in training_data:
                 features = self._extract_risk_features(project)
-                if features and 'risk_level' in project:
+                if features and "risk_level" in project:
                     X.append(features)
-                    y.append(project['risk_level'])
-            
+                    y.append(project["risk_level"])
+
             if len(X) > 50:  # Minimum samples for training
                 # Train the model
-                self.ml_models['project_risk'].fit(X, y)
-                self.model_training_status['project_risk'] = True
-                
+                self.ml_models["project_risk"].fit(X, y)
+                self.model_training_status["project_risk"] = True
+
                 logger.info("Project risk prediction model trained successfully")
-        
+
         except Exception as e:
             logger.error(f"Failed to train risk prediction model: {e}")
-    
+
     async def _train_resource_optimization_model(self):
         """Train the resource optimization model"""
         try:
             # Get historical resource allocation data
             allocation_data = await self._get_allocation_training_data()
-            
+
             if not allocation_data or len(allocation_data) < 50:
                 logger.warning("Insufficient training data for resource optimization model")
                 return
-            
+
             # Prepare features and targets
             X = []
             y = []
-            
+
             for allocation in allocation_data:
                 features = self._extract_allocation_features(allocation)
-                if features and 'efficiency_score' in allocation:
+                if features and "efficiency_score" in allocation:
                     X.append(features)
-                    y.append(allocation['efficiency_score'])
-            
+                    y.append(allocation["efficiency_score"])
+
             if len(X) > 20:  # Minimum samples for regression
                 # Train the model
-                self.ml_models['resource_optimization'].fit(X, y)
-                self.model_training_status['resource_optimization'] = True
-                
+                self.ml_models["resource_optimization"].fit(X, y)
+                self.model_training_status["resource_optimization"] = True
+
                 logger.info("Resource optimization model trained successfully")
-        
+
         except Exception as e:
             logger.error(f"Failed to train resource optimization model: {e}")
-    
+
     def _extract_risk_features(self, project_data: Dict[str, Any]) -> List[float]:
         """Extract features for risk prediction"""
         features = []
-        
+
         # Project complexity features
-        features.append(project_data.get('team_size', 0))
-        features.append(project_data.get('task_count', 0))
-        features.append(project_data.get('dependency_count', 0))
-        
+        features.append(project_data.get("team_size", 0))
+        features.append(project_data.get("task_count", 0))
+        features.append(project_data.get("dependency_count", 0))
+
         # Timeline features
-        if 'deadline' in project_data and 'start_date' in project_data:
-            timeline_days = (project_data['deadline'] - project_data['start_date']).days
+        if "deadline" in project_data and "start_date" in project_data:
+            timeline_days = (project_data["deadline"] - project_data["start_date"]).days
             features.append(timeline_days)
         else:
             features.append(0)
-        
+
         # Budget features
-        features.append(project_data.get('budget', 0))
-        features.append(project_data.get('actual_spend', 0))
-        
+        features.append(project_data.get("budget", 0))
+        features.append(project_data.get("actual_spend", 0))
+
         # Historical performance features
-        features.append(project_data.get('historical_success_rate', 0.5))
-        features.append(project_data.get('avg_task_completion_time', 0))
-        
+        features.append(project_data.get("historical_success_rate", 0.5))
+        features.append(project_data.get("avg_task_completion_time", 0))
+
         return features
-    
+
     def _identify_risk_factors(self, features: List[float]) -> List[Dict[str, Any]]:
         """Identify key risk factors from features"""
         risk_factors = []
         feature_names = [
-            'team_size', 'task_count', 'dependency_count', 'timeline_days',
-            'budget', 'actual_spend', 'historical_success_rate', 'avg_completion_time'
+            "team_size",
+            "task_count",
+            "dependency_count",
+            "timeline_days",
+            "budget",
+            "actual_spend",
+            "historical_success_rate",
+            "avg_completion_time",
         ]
-        
+
         # Define risk thresholds (these would be tuned based on historical data)
         thresholds = {
-            'team_size': 10,
-            'task_count': 50,
-            'dependency_count': 15,
-            'timeline_days': 30,
-            'budget': 100000,
-            'actual_spend': 0.8,  # Percentage of budget spent
-            'historical_success_rate': 0.7,
-            'avg_completion_time': 14  # Days
-        }
-        
+            "team_size": 10,
+            "task_count": 50,
+            "dependency_count": 15,
+            "timeline_days": 30,
+            "budget": 100000,
+            "actual_spend": 0.8,  # Percentage of budget spent
+            "historical_success_rate": 0.7,
+            "avg_completion_time": 14,  # Days
+        }
+
         for i, (feature_name, value) in enumerate(zip(feature_names, features)):
             threshold = thresholds.get(feature_name)
-            
+
             if threshold is not None:
-                if feature_name == 'historical_success_rate' and value < threshold:
-                    risk_factors.append({
-                        'factor': feature_name,
-                        'value': value,
-                        'risk': 'Low historical success rate'
-                    })
-                elif feature_name == 'actual_spend' and value > threshold:
-                    risk_factors.append({
-                        'factor': feature_name,
-                        'value': value,
-                        'risk': 'High budget utilization'
-                    })
+                if feature_name == "historical_success_rate" and value < threshold:
+                    risk_factors.append(
+                        {
+                            "factor": feature_name,
+                            "value": value,
+                            "risk": "Low historical success rate",
+                        }
+                    )
+                elif feature_name == "actual_spend" and value > threshold:
+                    risk_factors.append(
+                        {"factor": feature_name, "value": value, "risk": "High budget utilization"}
+                    )
                 elif value > threshold:
-                    risk_factors.append({
-                        'factor': feature_name,
-                        'value': value,
-                        'risk': f'High {feature_name.replace("_", " ")}'
-                    })
-        
+                    risk_factors.append(
+                        {
+                            "factor": feature_name,
+                            "value": value,
+                            "risk": f'High {feature_name.replace("_", " ")}',
+                        }
+                    )
+
         return risk_factors
-    
+
     async def _get_project_data(self, project_id: str) -> Optional[Dict[str, Any]]:
         """Get comprehensive project data for analysis"""
         async with self.db.get_session() as session:
             # Get project details
             project = await session.get(ProjectRecord, project_id)
             if not project:
                 return None
-            
+
             # Get related data
             tasks = await session.execute(
                 select(TaskRecord).where(TaskRecord.project_id == project_id)
             )
             tasks = tasks.scalars().all()
-            
+
             team_members = await session.execute(
                 select(UserRecord).where(UserRecord.id.in_(project.team_members or []))
             )
             team_members = team_members.scalars().all()
-            
+
             # Compile comprehensive data
             project_data = {
                 **project.to_dict(),
-                'tasks': [task.to_dict() for task in tasks],
-                'team_members': [member.to_dict() for member in team_members],
-                'task_count': len(tasks),
-                'completed_tasks': len([t for t in tasks if t.status == 'completed']),
-                'team_size': len(team_members)
+                "tasks": [task.to_dict() for task in tasks],
+                "team_members": [member.to_dict() for member in team_members],
+                "task_count": len(tasks),
+                "completed_tasks": len([t for t in tasks if t.status == "completed"]),
+                "team_size": len(team_members),
             }
-            
+
             return project_data
-    
+
     async def _calculate_performance_metrics(self, project_data: Dict[str, Any]) -> Dict[str, Any]:
         """Calculate project performance metrics"""
-        tasks = project_data.get('tasks', [])
-        completed_tasks = [t for t in tasks if t.get('status') == 'completed']
-        
+        tasks = project_data.get("tasks", [])
+        completed_tasks = [t for t in tasks if t.get("status") == "completed"]
+
         # Basic metrics
         completion_rate = len(completed_tasks) / len(tasks) if tasks else 0
-        
+
         # Time metrics
         task_durations = [
-            (t.get('updated_at') - t.get('created_at')).total_seconds() / 3600
-            for t in completed_tasks if t.get('updated_at') and t.get('created_at')
+            (t.get("updated_at") - t.get("created_at")).total_seconds() / 3600
+            for t in completed_tasks
+            if t.get("updated_at") and t.get("created_at")
         ]
         avg_duration = sum(task_durations) / len(task_durations) if task_durations else 0
-        
+
         # Quality metrics
-        quality_scores = [t.get('quality_score', 0) for t in completed_tasks]
+        quality_scores = [t.get("quality_score", 0) for t in completed_tasks]
         avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
-        
+
         return {
             "completion_rate": completion_rate,
             "avg_task_duration_hours": avg_duration,
             "avg_quality_score": avg_quality,
             "on_time_performance": await self._calculate_on_time_performance(project_data),
-            "budget_adherence": await self._calculate_budget_adherence(project_data)
-        }
-    
+            "budget_adherence": await self._calculate_budget_adherence(project_data),
+        }
+
     async def _assess_project_risks(self, project_data: Dict[str, Any]) -> Dict[str, Any]:
         """Assess project risks using multiple methods"""
         # ML-based risk prediction
         ml_prediction = await self.predict_project_risks(project_data)
-        
+
         # Rule-based risk assessment
         rule_based_risks = await self._rule_based_risk_assessment(project_data)
-        
+
         # Historical comparison
         historical_risks = await self._historical_risk_assessment(project_data)
-        
+
         return {
             "ml_prediction": ml_prediction,
             "rule_based_assessment": rule_based_risks,
             "historical_comparison": historical_risks,
             "overall_risk_score": self._calculate_overall_risk_score(
                 ml_prediction, rule_based_risks, historical_risks
-            )
-        }
-    
+            ),
+        }
+
     async def _analyze_team_performance(self, project_data: Dict[str, Any]) -> Dict[str, Any]:
         """Analyze team performance metrics"""
-        team_members = project_data.get('team_members', [])
-        tasks = project_data.get('tasks', [])
-        
+        team_members = project_data.get("team_members", [])
+        tasks = project_data.get("tasks", [])
+
         performance_metrics = {}
-        
+
         for member in team_members:
-            member_id = member.get('id')
-            member_tasks = [t for t in tasks if t.get('assigned_to') == member_id]
-            completed_tasks = [t for t in member_tasks if t.get('status') == 'completed']
-            
+            member_id = member.get("id")
+            member_tasks = [t for t in tasks if t.get("assigned_to") == member_id]
+            completed_tasks = [t for t in member_tasks if t.get("status") == "completed"]
+
             if member_tasks:
                 performance_metrics[member_id] = {
                     "completion_rate": len(completed_tasks) / len(member_tasks),
                     "avg_task_duration": await self._calculate_avg_duration(completed_tasks),
                     "quality_score": await self._calculate_avg_quality(completed_tasks),
                     "task_complexity": await self._calculate_avg_complexity(member_tasks),
-                    "collaboration_score": await self._calculate_collaboration_score(member_id, tasks)
+                    "collaboration_score": await self._calculate_collaboration_score(
+                        member_id, tasks
+                    ),
                 }
-        
+
         return {
             "individual_performance": performance_metrics,
             "team_metrics": await self._calculate_team_level_metrics(performance_metrics),
             "skill_gaps": await self._identify_skill_gaps(team_members, tasks),
-            "recommendations": await self._generate_team_recommendations(performance_metrics)
-        }
+            "recommendations": await self._generate_team_recommendations(performance_metrics),
+        }
+
 
 # Real-time analytics dashboard
 class RealTimeAnalyticsDashboard:
     """Real-time analytics dashboard for business intelligence"""
-    
+
     def __init__(self):
         self.analytics_engine = AdvancedAnalyticsEngine()
         self.data_visualization = DataVisualization()
         self.cache = CacheManager()
-    
+
     async def get_dashboard_data(self, timeframe: str = "7d") -> Dict[str, Any]:
         """Get comprehensive dashboard data"""
         cache_key = f"dashboard:{timeframe}"
         cached_data = await self.cache.get(cache_key)
-        
+
         if cached_data:
             return cached_data
-        
+
         # Calculate various dashboard metrics
         dashboard_data = {
             "timestamp": datetime.utcnow().isoformat(),
             "timeframe": timeframe,
             "overview_metrics": await self._get_overview_metrics(timeframe),
@@ -431,139 +437,140 @@
             "team_analytics": await self._get_team_analytics(timeframe),
             "financial_analytics": await self._get_financial_analytics(timeframe),
             "risk_analytics": await self._get_risk_analytics(timeframe),
             "trend_analysis": await self._get_trend_analysis(timeframe),
             "top_performers": await self._get_top_performers(timeframe),
-            "alerts_and_insights": await self._get_alerts_and_insights()
-        }
-        
+            "alerts_and_insights": await self._get_alerts_and_insights(),
+        }
+
         # Cache for 5 minutes for real-time dashboards
         await self.cache.set(cache_key, dashboard_data, expire=300)
-        
+
         return dashboard_data
-    
+
     async def _get_overview_metrics(self, timeframe: str) -> Dict[str, Any]:
         """Get overview metrics for dashboard"""
         return {
             "total_projects": await self._count_projects(timeframe),
             "active_projects": await self._count_active_projects(),
             "completed_projects": await self._count_completed_projects(timeframe),
             "total_tasks": await self._count_tasks(timeframe),
             "completed_tasks": await self._count_completed_tasks(timeframe),
             "active_users": await self._count_active_users(timeframe),
             "total_teams": await self._count_teams(),
-            "overall_productivity": await self._calculate_overall_productivity(timeframe)
-        }
-    
+            "overall_productivity": await self._calculate_overall_productivity(timeframe),
+        }
+
     async def _get_project_analytics(self, timeframe: str) -> Dict[str, Any]:
         """Get project analytics"""
         return {
             "project_status_distribution": await self._get_project_status_distribution(),
             "project_completion_rates": await self._get_project_completion_rates(timeframe),
             "project_timeline_analysis": await self._analyze_project_timelines(timeframe),
             "project_budget_analysis": await self._analyze_project_budgets(timeframe),
-            "project_risk_distribution": await self._get_project_risk_distribution()
-        }
-    
+            "project_risk_distribution": await self._get_project_risk_distribution(),
+        }
+
     async def _get_team_analytics(self, timeframe: str) -> Dict[str, Any]:
         """Get team analytics"""
         return {
             "team_performance_comparison": await self._compare_team_performance(timeframe),
             "team_capacity_utilization": await self._analyze_team_capacity(timeframe),
             "team_skill_distribution": await self._analyze_team_skills(),
-            "team_collaboration_metrics": await self._analyze_team_collaboration(timeframe)
-        }
+            "team_collaboration_metrics": await self._analyze_team_collaboration(timeframe),
+        }
+
 
 # Predictive analytics for forecasting
 class PredictiveAnalytics:
     """Predictive analytics for forecasting and trend prediction"""
-    
+
     def __init__(self):
         self.analytics_engine = AdvancedAnalyticsEngine()
         self.time_series_models = {}
-    
+
     async def forecast_project_completion(self, project_id: str) -> Dict[str, Any]:
         """Forecast project completion date and likelihood"""
         project_data = await self.analytics_engine._get_project_data(project_id)
         if not project_data:
             return {"error": "Project not found"}
-        
+
         # Use multiple forecasting methods
         monte_carlo_forecast = await self._monte_carlo_simulation(project_data)
         regression_forecast = await self._regression_forecast(project_data)
         historical_forecast = await self._historical_comparison_forecast(project_data)
-        
+
         # Combine forecasts
         combined_forecast = self._combine_forecasts(
             monte_carlo_forecast, regression_forecast, historical_forecast
         )
-        
+
         return {
             "project_id": project_id,
             "forecasts": {
                 "monte_carlo": monte_carlo_forecast,
                 "regression": regression_forecast,
                 "historical": historical_forecast,
-                "combined": combined_forecast
+                "combined": combined_forecast,
             },
             "confidence_interval": await self._calculate_confidence_interval(combined_forecast),
-            "risk_factors": await self._identify_forecast_risks(project_data)
-        }
-    
+            "risk_factors": await self._identify_forecast_risks(project_data),
+        }
+
     async def predict_resource_demand(self, timeframe: str = "30d") -> Dict[str, Any]:
         """Predict future resource demand"""
         historical_data = await self._get_historical_resource_data(timeframe)
-        
+
         if not historical_data or len(historical_data) < 30:
             return {"error": "Insufficient historical data"}
-        
+
         # Use time series forecasting
         demand_forecast = await self._time_series_forecast(historical_data)
-        
+
         return {
             "timeframe": timeframe,
             "historical_demand": historical_data,
             "predicted_demand": demand_forecast,
             "confidence_level": await self._calculate_demand_confidence(demand_forecast),
-            "recommendations": await self._generate_resource_recommendations(demand_forecast)
-        }
-    
+            "recommendations": await self._generate_resource_recommendations(demand_forecast),
+        }
+
     async def _monte_carlo_simulation(self, project_data: Dict[str, Any]) -> Dict[str, Any]:
         """Perform Monte Carlo simulation for project completion"""
-        tasks = project_data.get('tasks', [])
-        completed_tasks = [t for t in tasks if t.get('status') == 'completed']
-        pending_tasks = [t for t in tasks if t.get('status') != 'completed']
-        
+        tasks = project_data.get("tasks", [])
+        completed_tasks = [t for t in tasks if t.get("status") == "completed"]
+        pending_tasks = [t for t in tasks if t.get("status") != "completed"]
+
         if not pending_tasks:
             return {
-                "expected_completion": project_data.get('deadline'),
+                "expected_completion": project_data.get("deadline"),
                 "confidence": 1.0,
-                "simulation_runs": 0
+                "simulation_runs": 0,
             }
-        
+
         # Get task duration distributions from historical data
         task_durations = await self._get_task_duration_distributions()
-        
+
         # Run Monte Carlo simulation
         num_simulations = 1000
         completion_dates = []
-        
+
         for _ in range(num_simulations):
             simulated_completion = await self._run_single_simulation(
                 pending_tasks, task_durations, project_data
             )
             completion_dates.append(simulated_completion)
-        
+
         # Calculate statistics
         expected_completion = np.percentile(completion_dates, 50)
         confidence_80 = np.percentile(completion_dates, 80)
         confidence_20 = np.percentile(completion_dates, 20)
-        
+
         return {
             "expected_completion": expected_completion,
             "confidence_80": confidence_80,
             "confidence_20": confidence_20,
             "simulation_runs": num_simulations,
             "completion_likelihood": await self._calculate_completion_likelihood(
-                completion_dates, project_data.get('deadline')
-            )
-        }
\ No newline at end of file
+                completion_dates, project_data.get("deadline")
+            ),
+        }
would reformat /home/runner/work/ymera_y/ymera_y/analytics.engine.py
--- /home/runner/work/ymera_y/ymera_y/analytics.py	2025-10-19 22:47:02.792432+00:00
+++ /home/runner/work/ymera_y/ymera_y/analytics.py	2025-10-19 23:08:54.051913+00:00
@@ -15,41 +15,36 @@
 logger = structlog.get_logger(__name__)
 
 
 class ReportingAnalytics:
     """Analyzes agent reporting data"""
-    
+
     def __init__(self, db_session: AsyncSession, cache_manager: CacheManager):
         self.db = db_session
         self.cache = cache_manager
-    
-    async def get_agent_report_stats(
-        self,
-        agent_id: str,
-        days: int = 7
-    ) -> Dict[str, Any]:
+
+    async def get_agent_report_stats(self, agent_id: str, days: int = 7) -> Dict[str, Any]:
         """Get reporting statistics for an agent"""
         try:
             cutoff = datetime.utcnow() - timedelta(days=days)
-            
+
             stmt = select(func.count(AgentReportModel.report_id)).where(
-                AgentReportModel.agent_id == agent_id,
-                AgentReportModel.timestamp >= cutoff
+                AgentReportModel.agent_id == agent_id, AgentReportModel.timestamp >= cutoff
             )
             result = await self.db.execute(stmt)
             report_count = result.scalar() or 0
-            
+
             expected_reports = (days * 24 * 60 * 60) // 60  # Assuming 60s interval
             compliance_rate = (report_count / expected_reports * 100) if expected_reports > 0 else 0
-            
+
             return {
                 "agent_id": agent_id,
                 "period_days": days,
                 "total_reports": report_count,
                 "expected_reports": expected_reports,
                 "compliance_rate": round(compliance_rate, 2),
-                "timestamp": datetime.utcnow().isoformat()
+                "timestamp": datetime.utcnow().isoformat(),
             }
-            
+
         except Exception as e:
             logger.error("Failed to get report stats", agent_id=agent_id, error=str(e))
             return {}
would reformat /home/runner/work/ymera_y/ymera_y/analytics.py
error: cannot format /home/runner/work/ymera_y/ymera_y/analytics__init__.py: Cannot parse for target version Python 3.12: 3:0:         relations = []
--- /home/runner/work/ymera_y/ymera_y/ai_agents_production.py	2025-10-19 22:47:02.791432+00:00
+++ /home/runner/work/ymera_y/ymera_y/ai_agents_production.py	2025-10-19 23:08:54.159413+00:00
@@ -25,44 +25,49 @@
 import uuid
 from collections import defaultdict, deque
 
 try:
     import anthropic
+
     ANTHROPIC_AVAILABLE = True
 except ImportError:
     ANTHROPIC_AVAILABLE = False
     logging.warning("Anthropic SDK not available")
 
 
 class AgentType(Enum):
     """Types of AI agents"""
+
     CODE_ANALYZER = "code_analyzer"
     SECURITY_SCANNER = "security_scanner"
     QUALITY_ASSURANCE = "quality_assurance"
     MODULE_MANAGER = "module_manager"
     PERFORMANCE_MONITOR = "performance_monitor"
     GENERAL_ASSISTANT = "general_assistant"
 
 
 class ConfidenceLevel(Enum):
     """Confidence levels for agent responses"""
+
     HIGH = "high"
     MEDIUM = "medium"
     LOW = "low"
 
 
 class PriorityLevel(Enum):
     """Priority levels for recommendations"""
+
     CRITICAL = "critical"
     HIGH = "high"
     MEDIUM = "medium"
     LOW = "low"
 
 
 @dataclass
 class AgentResponse:
     """Structured response from AI agents"""
+
     agent_id: str
     agent_type: str
     timestamp: str
     confidence_level: str
     executive_summary: str
@@ -70,57 +75,58 @@
     recommendations: List[Dict[str, Any]]
     learning_insights: Dict[str, Any]
     next_steps: List[str]
     processing_time: float
     tokens_used: int
-    
+
     def to_dict(self) -> Dict[str, Any]:
         return asdict(self)
 
 
 @dataclass
 class LearningPattern:
     """Learning pattern for agent improvement"""
+
     pattern_id: str
     agent_type: str
     pattern_data: Dict[str, Any]
     success_rate: float
     usage_count: int
     effectiveness_score: float
     context_tags: List[str]
     last_updated: float
-    
+
     def to_dict(self) -> Dict[str, Any]:
         return asdict(self)
 
 
 class AgentLearningManager:
     """Manages learning and pattern recognition using PostgreSQL"""
-    
+
     def __init__(self, db_pool=None, logger=None):
         self.db_pool = db_pool
         self.logger = logger or logging.getLogger(__name__)
         self.patterns_cache: Dict[str, LearningPattern] = {}
         self.performance_cache: Dict[str, Dict] = {}
         self.cache_ttl = 300  # 5 minutes
         self.last_cache_update = 0
-    
+
     async def initialize(self):
         """Initialize learning manager with database"""
         if not self.db_pool:
             self.logger.warning("No database pool - learning disabled")
             return False
-        
+
         try:
             await self._create_tables()
             await self._load_patterns_cache()
             self.logger.info("Learning manager initialized")
             return True
         except Exception as e:
             self.logger.error(f"Learning manager init failed: {e}")
             return False
-    
+
     async def _create_tables(self):
         """Create learning tables if they don't exist"""
         queries = [
             """
             CREATE TABLE IF NOT EXISTS ai_learning_patterns (
@@ -163,141 +169,142 @@
                 user_rating INTEGER,
                 outcome_success BOOLEAN,
                 feedback_text TEXT,
                 timestamp TIMESTAMP DEFAULT NOW()
             )
-            """
+            """,
         ]
-        
+
         async with self.db_pool.acquire() as conn:
             for query in queries:
                 await conn.execute(query)
-    
+
     async def record_pattern(
-        self,
-        agent_type: str,
-        pattern_data: Dict,
-        success: bool,
-        context_tags: List[str]
+        self, agent_type: str, pattern_data: Dict, success: bool, context_tags: List[str]
     ):
         """Record a learning pattern"""
         try:
             pattern_str = json.dumps(pattern_data, sort_keys=True)
             pattern_id = hashlib.md5(pattern_str.encode()).hexdigest()
-            
+
             if not self.db_pool:
                 return
-            
+
             async with self.db_pool.acquire() as conn:
                 # Check if pattern exists
                 existing = await conn.fetchrow(
-                    "SELECT * FROM ai_learning_patterns WHERE pattern_id = $1",
-                    pattern_id
+                    "SELECT * FROM ai_learning_patterns WHERE pattern_id = $1", pattern_id
                 )
-                
+
                 if existing:
                     # Update existing pattern
-                    old_success_rate = existing['success_rate']
-                    old_usage_count = existing['usage_count']
-                    
+                    old_success_rate = existing["success_rate"]
+                    old_usage_count = existing["usage_count"]
+
                     new_usage_count = old_usage_count + 1
                     new_success_rate = (
-                        (old_success_rate * old_usage_count + (1 if success else 0)) / 
-                        new_usage_count
-                    )
+                        old_success_rate * old_usage_count + (1 if success else 0)
+                    ) / new_usage_count
                     effectiveness_score = new_success_rate * min(new_usage_count / 10, 1.0)
-                    
+
                     await conn.execute(
                         """
                         UPDATE ai_learning_patterns 
                         SET success_rate = $1, usage_count = $2, 
                             effectiveness_score = $3, context_tags = $4, 
                             last_updated = NOW()
                         WHERE pattern_id = $5
                         """,
-                        new_success_rate, new_usage_count, effectiveness_score,
-                        json.dumps(context_tags), pattern_id
+                        new_success_rate,
+                        new_usage_count,
+                        effectiveness_score,
+                        json.dumps(context_tags),
+                        pattern_id,
                     )
                 else:
                     # Insert new pattern
                     await conn.execute(
                         """
                         INSERT INTO ai_learning_patterns 
                         (pattern_id, agent_type, pattern_data, success_rate, 
                          usage_count, effectiveness_score, context_tags)
                         VALUES ($1, $2, $3, $4, $5, $6, $7)
                         """,
-                        pattern_id, agent_type, pattern_str,
-                        1.0 if success else 0.0, 1,
-                        1.0 if success else 0.0, json.dumps(context_tags)
+                        pattern_id,
+                        agent_type,
+                        pattern_str,
+                        1.0 if success else 0.0,
+                        1,
+                        1.0 if success else 0.0,
+                        json.dumps(context_tags),
                     )
-                
+
                 # Update cache
                 self.patterns_cache[pattern_id] = LearningPattern(
                     pattern_id=pattern_id,
                     agent_type=agent_type,
                     pattern_data=pattern_data,
                     success_rate=new_success_rate if existing else (1.0 if success else 0.0),
                     usage_count=new_usage_count if existing else 1,
-                    effectiveness_score=effectiveness_score if existing else (1.0 if success else 0.0),
+                    effectiveness_score=(
+                        effectiveness_score if existing else (1.0 if success else 0.0)
+                    ),
                     context_tags=context_tags,
-                    last_updated=time.time()
+                    last_updated=time.time(),
                 )
-                
+
         except Exception as e:
             self.logger.error(f"Failed to record pattern: {e}")
-    
+
     async def get_relevant_patterns(
-        self,
-        agent_type: str,
-        context_tags: List[str],
-        limit: int = 5
+        self, agent_type: str, context_tags: List[str], limit: int = 5
     ) -> List[Dict]:
         """Get relevant patterns for decision making"""
         try:
             # Check cache freshness
             if time.time() - self.last_cache_update > self.cache_ttl:
                 await self._load_patterns_cache()
-            
+
             # Filter patterns from cache
             relevant = []
             for pattern in self.patterns_cache.values():
                 if pattern.agent_type != agent_type:
                     continue
-                
+
                 if pattern.effectiveness_score < 0.5:
                     continue
-                
+
                 # Calculate tag overlap
                 tag_overlap = len(set(context_tags) & set(pattern.context_tags))
                 relevance = tag_overlap / max(len(context_tags), 1)
-                
+
                 if tag_overlap > 0 or not context_tags:
-                    relevant.append({
-                        'pattern_id': pattern.pattern_id,
-                        'pattern_data': pattern.pattern_data,
-                        'effectiveness_score': pattern.effectiveness_score,
-                        'relevance_score': relevance
-                    })
-            
+                    relevant.append(
+                        {
+                            "pattern_id": pattern.pattern_id,
+                            "pattern_data": pattern.pattern_data,
+                            "effectiveness_score": pattern.effectiveness_score,
+                            "relevance_score": relevance,
+                        }
+                    )
+
             # Sort by combined score
             relevant.sort(
-                key=lambda x: x['effectiveness_score'] * (x['relevance_score'] + 0.1),
-                reverse=True
+                key=lambda x: x["effectiveness_score"] * (x["relevance_score"] + 0.1), reverse=True
             )
-            
+
             return relevant[:limit]
-            
+
         except Exception as e:
             self.logger.error(f"Failed to get patterns: {e}")
             return []
-    
+
     async def _load_patterns_cache(self):
         """Load patterns into cache"""
         if not self.db_pool:
             return
-        
+
         try:
             async with self.db_pool.acquire() as conn:
                 records = await conn.fetch(
                     """
                     SELECT pattern_id, agent_type, pattern_data, success_rate,
@@ -307,91 +314,91 @@
                     WHERE effectiveness_score > 0.3
                     ORDER BY effectiveness_score DESC
                     LIMIT 1000
                     """
                 )
-                
+
                 self.patterns_cache.clear()
                 for record in records:
-                    self.patterns_cache[record['pattern_id']] = LearningPattern(
-                        pattern_id=record['pattern_id'],
-                        agent_type=record['agent_type'],
-                        pattern_data=json.loads(record['pattern_data']),
-                        success_rate=record['success_rate'],
-                        usage_count=record['usage_count'],
-                        effectiveness_score=record['effectiveness_score'],
-                        context_tags=json.loads(record['context_tags']),
-                        last_updated=record['last_updated']
+                    self.patterns_cache[record["pattern_id"]] = LearningPattern(
+                        pattern_id=record["pattern_id"],
+                        agent_type=record["agent_type"],
+                        pattern_data=json.loads(record["pattern_data"]),
+                        success_rate=record["success_rate"],
+                        usage_count=record["usage_count"],
+                        effectiveness_score=record["effectiveness_score"],
+                        context_tags=json.loads(record["context_tags"]),
+                        last_updated=record["last_updated"],
                     )
-                
+
                 self.last_cache_update = time.time()
                 self.logger.debug(f"Loaded {len(self.patterns_cache)} patterns into cache")
-                
+
         except Exception as e:
             self.logger.error(f"Failed to load patterns cache: {e}")
-    
+
     async def update_performance(
-        self,
-        agent_id: str,
-        agent_type: str,
-        response_time: float,
-        confidence: float,
-        success: bool
+        self, agent_id: str, agent_type: str, response_time: float, confidence: float, success: bool
     ):
         """Update agent performance metrics"""
         if not self.db_pool:
             return
-        
+
         try:
             async with self.db_pool.acquire() as conn:
                 existing = await conn.fetchrow(
-                    "SELECT * FROM ai_agent_performance WHERE agent_id = $1",
-                    agent_id
+                    "SELECT * FROM ai_agent_performance WHERE agent_id = $1", agent_id
                 )
-                
+
                 if existing:
-                    total_requests = existing['total_requests'] + 1
-                    successful_requests = existing['successful_requests'] + (1 if success else 0)
+                    total_requests = existing["total_requests"] + 1
+                    successful_requests = existing["successful_requests"] + (1 if success else 0)
                     avg_response_time = (
-                        (existing['average_response_time'] * existing['total_requests'] + response_time) /
-                        total_requests
-                    )
+                        existing["average_response_time"] * existing["total_requests"]
+                        + response_time
+                    ) / total_requests
                     avg_confidence = (
-                        (existing['average_confidence'] * existing['total_requests'] + confidence) /
-                        total_requests
-                    )
-                    
+                        existing["average_confidence"] * existing["total_requests"] + confidence
+                    ) / total_requests
+
                     await conn.execute(
                         """
                         UPDATE ai_agent_performance 
                         SET total_requests = $1, successful_requests = $2,
                             average_response_time = $3, average_confidence = $4,
                             last_update = NOW()
                         WHERE agent_id = $5
                         """,
-                        total_requests, successful_requests, avg_response_time,
-                        avg_confidence, agent_id
+                        total_requests,
+                        successful_requests,
+                        avg_response_time,
+                        avg_confidence,
+                        agent_id,
                     )
                 else:
                     await conn.execute(
                         """
                         INSERT INTO ai_agent_performance 
                         (agent_id, agent_type, total_requests, successful_requests,
                          average_response_time, average_confidence)
                         VALUES ($1, $2, $3, $4, $5, $6)
                         """,
-                        agent_id, agent_type, 1, 1 if success else 0,
-                        response_time, confidence
+                        agent_id,
+                        agent_type,
+                        1,
+                        1 if success else 0,
+                        response_time,
+                        confidence,
                     )
-                    
+
         except Exception as e:
             self.logger.error(f"Failed to update performance: {e}")
 
 
 class AIAgent:
     """Production-ready AI Agent with Claude integration"""
-    
+
     # System prompt template
     SYSTEM_PROMPT_TEMPLATE = """# AI Agent System Prompt
 
 You are an elite {agent_specialization} AI agent within an enterprise system.
 
@@ -441,192 +448,189 @@
 
     AGENT_SPECIALIZATIONS = {
         AgentType.CODE_ANALYZER: {
             "title": "Code Analysis Specialist",
             "domain": "static analysis, code quality, architecture patterns",
-            "expertise": "software architecture, design patterns, best practices"
+            "expertise": "software architecture, design patterns, best practices",
         },
         AgentType.SECURITY_SCANNER: {
             "title": "Security Expert",
             "domain": "vulnerability assessment, threat modeling, security best practices",
-            "expertise": "OWASP Top 10, penetration testing, security compliance"
+            "expertise": "OWASP Top 10, penetration testing, security compliance",
         },
         AgentType.QUALITY_ASSURANCE: {
             "title": "Quality Assurance Specialist",
             "domain": "testing strategies, quality metrics, continuous integration",
-            "expertise": "test automation, QA methodologies, quality standards"
+            "expertise": "test automation, QA methodologies, quality standards",
         },
         AgentType.MODULE_MANAGER: {
             "title": "Architecture Expert",
             "domain": "component architecture, dependency management, modular design",
-            "expertise": "software architecture, microservices, system design"
+            "expertise": "software architecture, microservices, system design",
         },
         AgentType.PERFORMANCE_MONITOR: {
             "title": "Performance Engineer",
             "domain": "performance optimization, monitoring, system health",
-            "expertise": "profiling, optimization, observability"
+            "expertise": "profiling, optimization, observability",
         },
         AgentType.GENERAL_ASSISTANT: {
             "title": "General AI Assistant",
             "domain": "full-stack development, operations, project management",
-            "expertise": "broad technical knowledge across domains"
-        }
+            "expertise": "broad technical knowledge across domains",
+        },
     }
-    
+
     def __init__(
         self,
         agent_type: AgentType,
         api_key: str,
         learning_manager: AgentLearningManager,
         logger: Optional[logging.Logger] = None,
-        model: str = "claude-sonnet-4-20250514"
+        model: str = "claude-sonnet-4-20250514",
     ):
         self.agent_type = agent_type
         self.agent_id = f"{agent_type.value}_{uuid.uuid4().hex[:8]}"
         self.learning_manager = learning_manager
         self.logger = logger or logging.getLogger(__name__)
         self.model = model
-        
+
         if not ANTHROPIC_AVAILABLE:
             raise RuntimeError("Anthropic SDK required but not available")
-        
+
         self.client = anthropic.Anthropic(api_key=api_key)
-        
+
         # Response cache with TTL
         self.response_cache: Dict[str, Tuple[AgentResponse, float]] = {}
         self.cache_ttl = 3600  # 1 hour
-        
+
         # Metrics
         self.metrics = {
-            'total_requests': 0,
-            'successful_requests': 0,
-            'failed_requests': 0,
-            'cache_hits': 0,
-            'total_tokens': 0,
-            'avg_processing_time': 0.0
+            "total_requests": 0,
+            "successful_requests": 0,
+            "failed_requests": 0,
+            "cache_hits": 0,
+            "total_tokens": 0,
+            "avg_processing_time": 0.0,
         }
-        
+
         # Build system prompt
         self.system_prompt = self._build_system_prompt()
-    
+
     def _build_system_prompt(self) -> str:
         """Build specialized system prompt"""
         spec = self.AGENT_SPECIALIZATIONS[self.agent_type]
-        
+
         return self.SYSTEM_PROMPT_TEMPLATE.format(
-            agent_specialization=spec['title'],
-            domain=spec['domain'],
-            agent_id=self.agent_id
+            agent_specialization=spec["title"], domain=spec["domain"], agent_id=self.agent_id
         )
-    
+
     def _generate_cache_key(self, task: str, context: Dict) -> str:
         """Generate cache key"""
         content = f"{task}:{json.dumps(context, sort_keys=True)}"
         return hashlib.sha256(content.encode()).hexdigest()
-    
+
     def _extract_context_tags(self, task: str, context: Dict) -> List[str]:
         """Extract context tags for pattern matching"""
         tags = [self.agent_type.value]
-        
+
         task_lower = task.lower()
         keywords = {
-            'security': ['security', 'vulnerability', 'attack', 'exploit'],
-            'performance': ['performance', 'optimization', 'slow', 'memory'],
-            'testing': ['test', 'quality', 'qa', 'bug'],
-            'code_analysis': ['code', 'function', 'class', 'method'],
-            'debugging': ['error', 'bug', 'issue', 'problem']
+            "security": ["security", "vulnerability", "attack", "exploit"],
+            "performance": ["performance", "optimization", "slow", "memory"],
+            "testing": ["test", "quality", "qa", "bug"],
+            "code_analysis": ["code", "function", "class", "method"],
+            "debugging": ["error", "bug", "issue", "problem"],
         }
-        
+
         for tag, words in keywords.items():
             if any(word in task_lower for word in words):
                 tags.append(tag)
-        
+
         # Add context-based tags
         if context:
-            if 'file_type' in context:
+            if "file_type" in context:
                 tags.append(f"file_{context['file_type']}")
-            if 'complexity' in context:
+            if "complexity" in context:
                 tags.append(f"complexity_{context['complexity']}")
-        
+
         return list(set(tags))
-    
+
     async def analyze(
         self,
         task: str,
         context: Optional[Dict[str, Any]] = None,
         use_cache: bool = True,
-        timeout: float = 60.0
+        timeout: float = 60.0,
     ) -> AgentResponse:
         """Main analysis method with learning integration"""
         start_time = time.time()
         context = context or {}
-        
-        self.metrics['total_requests'] += 1
-        
+
+        self.metrics["total_requests"] += 1
+
         try:
             # Check cache
             cache_key = self._generate_cache_key(task, context)
             if use_cache:
                 cached = self._get_from_cache(cache_key)
                 if cached:
-                    self.metrics['cache_hits'] += 1
+                    self.metrics["cache_hits"] += 1
                     self.logger.debug(f"Cache hit for {self.agent_id}")
                     return cached
-            
+
             # Apply learning patterns
             context_tags = self._extract_context_tags(task, context)
             learning_patterns = await self.learning_manager.get_relevant_patterns(
                 self.agent_type.value, context_tags, limit=3
             )
-            
+
             # Prepare enhanced context
             enhanced_context = {
                 **context,
-                'learning_patterns': len(learning_patterns),
-                'pattern_suggestions': [
-                    p['pattern_data'].get('suggestion', '')
-                    for p in learning_patterns
-                ]
+                "learning_patterns": len(learning_patterns),
+                "pattern_suggestions": [
+                    p["pattern_data"].get("suggestion", "") for p in learning_patterns
+                ],
             }
-            
+
             # Prepare message
             user_message = f"""Task: {task}
 
 Context: {json.dumps(enhanced_context, indent=2)}
 
 Available Learning Patterns: {len(learning_patterns)}
 
 Analyze this task using your expertise as a {self.agent_type.value.replace('_', ' ').title()} and provide response in required JSON format."""
-            
+
             # Call Claude with timeout
             try:
                 response = await asyncio.wait_for(
                     asyncio.to_thread(
                         self.client.messages.create,
                         model=self.model,
                         max_tokens=4000,
                         temperature=0.1,
                         system=self.system_prompt,
-                        messages=[{"role": "user", "content": user_message}]
+                        messages=[{"role": "user", "content": user_message}],
                     ),
-                    timeout=timeout
+                    timeout=timeout,
                 )
             except asyncio.TimeoutError:
                 raise TimeoutError(f"Analysis timeout after {timeout}s")
-            
+
             processing_time = time.time() - start_time
-            
+
             # Parse response
             try:
                 response_text = response.content[0].text if response.content else "{}"
                 # Remove markdown code blocks if present
-                response_text = response_text.replace('```json\n', '').replace('\n```', '').strip()
+                response_text = response_text.replace("```json\n", "").replace("\n```", "").strip()
                 response_data = json.loads(response_text)
             except json.JSONDecodeError as e:
                 self.logger.warning(f"JSON parse error: {e}")
                 response_data = self._create_fallback_response(task)
-            
+
             # Create structured response
             agent_response = AgentResponse(
                 agent_id=self.agent_id,
                 agent_type=self.agent_type.value,
                 timestamp=response_data.get("timestamp", datetime.now().isoformat()),
@@ -635,84 +639,90 @@
                 detailed_analysis=response_data.get("detailed_analysis", {}),
                 recommendations=response_data.get("recommendations", []),
                 learning_insights=response_data.get("learning_insights", {}),
                 next_steps=response_data.get("next_steps", []),
                 processing_time=processing_time,
-                tokens_used=(response.usage.input_tokens + response.usage.output_tokens) if response.usage else 0
+                tokens_used=(
+                    (response.usage.input_tokens + response.usage.output_tokens)
+                    if response.usage
+                    else 0
+                ),
             )
-            
+
             # Record learning pattern
             pattern_data = {
-                'task_preview': task[:100],
-                'confidence': agent_response.confidence_level,
-                'processing_time': processing_time,
-                'recommendations_count': len(agent_response.recommendations)
+                "task_preview": task[:100],
+                "confidence": agent_response.confidence_level,
+                "processing_time": processing_time,
+                "recommendations_count": len(agent_response.recommendations),
             }
-            
+
             success = agent_response.confidence_level in ["high", "medium"]
             await self.learning_manager.record_pattern(
                 self.agent_type.value, pattern_data, success, context_tags
             )
-            
+
             # Update performance
-            confidence_score = {"high": 1.0, "medium": 0.7, "low": 0.4}[agent_response.confidence_level]
+            confidence_score = {"high": 1.0, "medium": 0.7, "low": 0.4}[
+                agent_response.confidence_level
+            ]
             await self.learning_manager.update_performance(
                 self.agent_id, self.agent_type.value, processing_time, confidence_score, success
             )
-            
+
             # Cache response
             if use_cache:
                 self._add_to_cache(cache_key, agent_response)
-            
+
             # Update metrics
-            self.metrics['successful_requests'] += 1
-            self.metrics['total_tokens'] += agent_response.tokens_used
+            self.metrics["successful_requests"] += 1
+            self.metrics["total_tokens"] += agent_response.tokens_used
             self._update_avg_time(processing_time)
-            
+
             self.logger.info(
                 f"Analysis completed",
                 agent=self.agent_id,
-                time_ms=processing_time*1000,
-                tokens=agent_response.tokens_used
+                time_ms=processing_time * 1000,
+                tokens=agent_response.tokens_used,
             )
-            
+
             return agent_response
-            
+
         except Exception as e:
-            self.metrics['failed_requests'] += 1
+            self.metrics["failed_requests"] += 1
             processing_time = time.time() - start_time
-            
+
             self.logger.error(f"Analysis failed: {e}", agent=self.agent_id)
-            
+
             # Update performance with failure
             await self.learning_manager.update_performance(
                 self.agent_id, self.agent_type.value, processing_time, 0.0, False
             )
-            
+
             return self._create_error_response(str(e), processing_time)
-    
+
     def _create_fallback_response(self, task: str) -> Dict:
         """Create fallback response when JSON parsing fails"""
         return {
             "agent_id": self.agent_id,
             "timestamp": datetime.now().isoformat(),
             "confidence_level": "low",
             "executive_summary": "Analysis completed with parsing issues",
             "detailed_analysis": {
                 "findings": ["Response format error occurred"],
                 "metrics": {},
-                "patterns_identified": []
+                "patterns_identified": [],
             },
             "recommendations": [],
             "learning_insights": {
                 "new_patterns": [],
                 "success_factors": [],
-                "areas_for_improvement": ["Response formatting"]
+                "areas_for_improvement": ["Response formatting"],
             },
-            "next_steps": ["Review task requirements", "Retry analysis"]
+            "next_steps": ["Review task requirements", "Retry analysis"],
         }
-    
+
     def _create_error_response(self, error: str, processing_time: float) -> AgentResponse:
         """Create error response"""
         return AgentResponse(
             agent_id=self.agent_id,
             agent_type=self.agent_type.value,
@@ -720,206 +730,194 @@
             confidence_level="low",
             executive_summary=f"Analysis failed: {error}",
             detailed_analysis={
                 "findings": [f"Error: {error}"],
                 "metrics": {"processing_time": processing_time},
-                "patterns_identified": []
+                "patterns_identified": [],
             },
-            recommendations=[{
-                "priority": "high",
-                "action": "Review error and retry",
-                "rationale": "Analysis failed",
-                "implementation": "Check logs and retry",
-                "estimated_effort": "5 minutes"
-            }],
+            recommendations=[
+                {
+                    "priority": "high",
+                    "action": "Review error and retry",
+                    "rationale": "Analysis failed",
+                    "implementation": "Check logs and retry",
+                    "estimated_effort": "5 minutes",
+                }
+            ],
             learning_insights={
                 "new_patterns": [],
                 "success_factors": [],
-                "areas_for_improvement": ["Error handling"]
+                "areas_for_improvement": ["Error handling"],
             },
             next_steps=["Review error", "Retry analysis"],
             processing_time=processing_time,
-            tokens_used=0
+            tokens_used=0,
         )
-    
+
     def _get_from_cache(self, cache_key: str) -> Optional[AgentResponse]:
         """Get response from cache if valid"""
         if cache_key in self.response_cache:
             response, timestamp = self.response_cache[cache_key]
             if time.time() - timestamp < self.cache_ttl:
                 return response
             del self.response_cache[cache_key]
         return None
-    
+
     def _add_to_cache(self, cache_key: str, response: AgentResponse):
         """Add response to cache"""
         self.response_cache[cache_key] = (response, time.time())
-        
+
         # Cleanup old entries
         if len(self.response_cache) > 100:
             oldest = min(self.response_cache.items(), key=lambda x: x[1][1])
             del self.response_cache[oldest[0]]
-    
+
     def _update_avg_time(self, processing_time: float):
         """Update average processing time"""
-        total = self.metrics['successful_requests']
-        current_avg = self.metrics['avg_processing_time']
-        self.metrics['avg_processing_time'] = (
-            (current_avg * (total - 1) + processing_time) / total
-        )
-    
+        total = self.metrics["successful_requests"]
+        current_avg = self.metrics["avg_processing_time"]
+        self.metrics["avg_processing_time"] = (current_avg * (total - 1) + processing_time) / total
+
     def get_metrics(self) -> Dict[str, Any]:
         """Get agent metrics"""
         return {
             "agent_id": self.agent_id,
             "agent_type": self.agent_type.value,
             **self.metrics,
-            "cache_size": len(self.response_cache)
+            "cache_size": len(self.response_cache),
         }
 
 
 class AIAgentOrchestrator:
     """Orchestrates multiple AI agents"""
-    
-    def __init__(
-        self,
-        api_key: str,
-        db_pool=None,
-        logger: Optional[logging.Logger] = None
-    ):
+
+    def __init__(self, api_key: str, db_pool=None, logger: Optional[logging.Logger] = None):
         self.api_key = api_key
         self.db_pool = db_pool
         self.logger = logger or logging.getLogger(__name__)
-        
+
         # Initialize learning manager
         self.learning_manager = AgentLearningManager(db_pool, logger)
-        
+
         # Agent registry
         self.agents: Dict[AgentType, AIAgent] = {}
-        
+
         # Metrics
         self.metrics = {
-            'total_requests': 0,
-            'agent_selections': defaultdict(int),
-            'avg_routing_time': 0.0
+            "total_requests": 0,
+            "agent_selections": defaultdict(int),
+            "avg_routing_time": 0.0,
         }
-    
+
     async def initialize(self):
         """Initialize orchestrator"""
         try:
             await self.learning_manager.initialize()
-            
+
             # Initialize all agent types
             for agent_type in AgentType:
                 self.agents[agent_type] = AIAgent(
                     agent_type=agent_type,
                     api_key=self.api_key,
                     learning_manager=self.learning_manager,
-                    logger=self.logger
+                    logger=self.logger,
                 )
-            
+
             self.logger.info(f"Orchestrator initialized with {len(self.agents)} agents")
             return True
-            
+
         except Exception as e:
             self.logger.error(f"Orchestrator init failed: {e}")
             return False
-    
+
     def get_agent(self, agent_type: AgentType) -> AIAgent:
         """Get specific agent"""
         return self.agents[agent_type]
-    
+
     async def analyze_with_best_agent(
-        self,
-        task: str,
-        context: Optional[Dict] = None
+        self, task: str, context: Optional[Dict] = None
     ) -> AgentResponse:
         """Automatically select and use best agent"""
         start_time = time.time()
-        
+
         # Simple task classification
         task_lower = task.lower()
-        
+
         keywords_map = {
-            AgentType.SECURITY_SCANNER: ['security', 'vulnerability', 'exploit', 'attack'],
-            AgentType.QUALITY_ASSURANCE: ['test', 'quality', 'bug', 'defect'],
-            AgentType.PERFORMANCE_MONITOR: ['performance', 'optimization', 'slow', 'memory'],
-            AgentType.MODULE_MANAGER: ['module', 'dependency', 'architecture', 'design'],
-            AgentType.CODE_ANALYZER: ['code', 'function', 'class', 'method']
+            AgentType.SECURITY_SCANNER: ["security", "vulnerability", "exploit", "attack"],
+            AgentType.QUALITY_ASSURANCE: ["test", "quality", "bug", "defect"],
+            AgentType.PERFORMANCE_MONITOR: ["performance", "optimization", "slow", "memory"],
+            AgentType.MODULE_MANAGER: ["module", "dependency", "architecture", "design"],
+            AgentType.CODE_ANALYZER: ["code", "function", "class", "method"],
         }
-        
+
         selected_type = AgentType.GENERAL_ASSISTANT
         for agent_type, keywords in keywords_map.items():
             if any(kw in task_lower for kw in keywords):
                 selected_type = agent_type
                 break
-        
+
         agent = self.agents[selected_type]
-        
+
         # Update metrics
-        self.metrics['total_requests'] += 1
-        self.metrics['agent_selections'][selected_type.value] += 1
-        
+        self.metrics["total_requests"] += 1
+        self.metrics["agent_selections"][selected_type.value] += 1
+
         routing_time = time.time() - start_time
         self._update_routing_time(routing_time)
-        
+
         self.logger.info(f"Selected {selected_type.value} for task")
-        
+
         return await agent.analyze(task, context)
-    
+
     async def collaborative_analysis(
-        self,
-        task: str,
-        agent_types: List[AgentType],
-        context: Optional[Dict] = None
+        self, task: str, agent_types: List[AgentType], context: Optional[Dict] = None
     ) -> Dict[AgentType, AgentResponse]:
         """Run collaborative analysis with multiple agents"""
         tasks = []
         for agent_type in agent_types:
             agent = self.agents[agent_type]
             tasks.append(agent.analyze(task, context))
-        
+
         responses = await asyncio.gather(*tasks, return_exceptions=True)
-        
+
         result = {}
         for i, agent_type in enumerate(agent_types):
             if isinstance(responses[i], Exception):
                 self.logger.error(f"Agent {agent_type.value} failed: {responses[i]}")
             else:
                 result[agent_type] = responses[i]
-        
+
         return result
-    
+
     def _update_routing_time(self, routing_time: float):
         """Update average routing time"""
-        total = self.metrics['total_requests']
-        current_avg = self.metrics['avg_routing_time']
-        self.metrics['avg_routing_time'] = (
-            (current_avg * (total - 1) + routing_time) / total
-        )
-    
+        total = self.metrics["total_requests"]
+        current_avg = self.metrics["avg_routing_time"]
+        self.metrics["avg_routing_time"] = (current_avg * (total - 1) + routing_time) / total
+
     def get_system_metrics(self) -> Dict[str, Any]:
         """Get system-wide metrics"""
         agent_metrics = {}
         for agent_type, agent in self.agents.items():
             agent_metrics[agent_type.value] = agent.get_metrics()
-        
+
         return {
             "orchestrator": self.metrics,
             "agents": agent_metrics,
             "total_agents": len(self.agents),
-            "timestamp": datetime.now().isoformat()
+            "timestamp": datetime.now().isoformat(),
         }
-    
+
     async def cleanup(self):
         """Cleanup resources"""
         self.logger.info("Cleaning up AI Agent Orchestrator")
-        
+
         # Clear caches
         for agent in self.agents.values():
             agent.response_cache.clear()
-        
+
         self.agents.clear()
 
 
 # Database schema
 AI_AGENTS_DB_SCHEMA = """
@@ -970,81 +968,73 @@
 
 
 # Example usage
 async def example_usage():
     """Example of using the AI agents"""
-    
+
     # Mock database pool for example
     db_pool = None  # In production, use actual asyncpg pool
-    
-    orchestrator = AIAgentOrchestrator(
-        api_key="your-anthropic-api-key",
-        db_pool=db_pool
-    )
-    
+
+    orchestrator = AIAgentOrchestrator(api_key="your-anthropic-api-key", db_pool=db_pool)
+
     await orchestrator.initialize()
-    
+
     # Example 1: Automatic agent selection
     print("=== Automatic Agent Selection ===")
     response = await orchestrator.analyze_with_best_agent(
         task="Analyze this Python code for security vulnerabilities",
-        context={
-            "file_type": "python",
-            "complexity": "medium"
-        }
+        context={"file_type": "python", "complexity": "medium"},
     )
-    
+
     print(f"Agent: {response.agent_id}")
     print(f"Confidence: {response.confidence_level}")
     print(f"Summary: {response.executive_summary}")
     print(f"Processing time: {response.processing_time:.2f}s")
     print()
-    
+
     # Example 2: Specific agent usage
     print("=== Specific Agent Usage ===")
     code_agent = orchestrator.get_agent(AgentType.CODE_ANALYZER)
     code_response = await code_agent.analyze(
-        task="Review this function for performance optimization",
-        context={"language": "python"}
+        task="Review this function for performance optimization", context={"language": "python"}
     )
-    
+
     print(f"Summary: {code_response.executive_summary}")
     print(f"Recommendations: {len(code_response.recommendations)}")
     print()
-    
+
     # Example 3: Collaborative analysis
     print("=== Collaborative Analysis ===")
     collaborative = await orchestrator.collaborative_analysis(
         task="Analyze authentication function",
         agent_types=[
             AgentType.CODE_ANALYZER,
             AgentType.SECURITY_SCANNER,
-            AgentType.PERFORMANCE_MONITOR
-        ]
+            AgentType.PERFORMANCE_MONITOR,
+        ],
     )
-    
+
     for agent_type, response in collaborative.items():
         print(f"{agent_type.value}: {response.confidence_level} confidence")
     print()
-    
+
     # Example 4: System metrics
     print("=== System Metrics ===")
     metrics = orchestrator.get_system_metrics()
     print(f"Total requests: {metrics['orchestrator']['total_requests']}")
     print(f"Agents active: {metrics['total_agents']}")
-    
+
     await orchestrator.cleanup()
 
 
 if __name__ == "__main__":
     logging.basicConfig(
-        level=logging.INFO,
-        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
+        level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
     )
-    
+
     try:
         asyncio.run(example_usage())
     except KeyboardInterrupt:
         print("\nShutdown by user")
     except Exception as e:
         print(f"Error: {e}")
-        logging.exception(e)
\ No newline at end of file
+        logging.exception(e)
would reformat /home/runner/work/ymera_y/ymera_y/ai_agents_production.py
--- /home/runner/work/ymera_y/ymera_y/api.gateway.py	2025-10-19 22:47:02.792432+00:00
+++ /home/runner/work/ymera_y/ymera_y/api.gateway.py	2025-10-19 23:08:54.957751+00:00
@@ -17,175 +17,163 @@
 from opentelemetry import trace
 from opentelemetry.trace.propagation.tracecontext import TraceContextTextMapPropagator
 
 logger = logging.getLogger(__name__)
 
+
 class EnterpriseAPIGateway:
     """Enterprise API Gateway with advanced management features"""
-    
+
     def __init__(self, app: FastAPI):
         self.app = app
         self.version_manager = APIVersionManager()
         self.rate_limiter = RateLimiter()
         self.transformer = RequestResponseTransformer()
         self.monetization = APIMonetization()
         self.developer_portal = DeveloperPortal()
         self.cache = TTLCache(maxsize=1000, ttl=300)
         self._setup_gateway()
-    
+
     def _setup_gateway(self):
         """Setup API gateway middleware and routes"""
         # Add gateway middleware
         self.app.middleware("http")(self._gateway_middleware)
-        
+
         # Add custom routes for gateway functionality
         self._add_gateway_routes()
-        
+
         # Custom OpenAPI documentation
         self._customize_openapi()
-    
+
     async def _gateway_middleware(self, request: Request, call_next):
         """Main gateway middleware"""
         # Start timing for performance metrics
         start_time = datetime.utcnow()
-        
+
         # Extract API version from request
         api_version = self._extract_api_version(request)
-        
+
         # Check rate limits
         if not await self.rate_limiter.check_rate_limit(request, api_version):
             return Response(
                 content=json.dumps({"error": "Rate limit exceeded"}),
                 status_code=429,
-                headers={"Retry-After": "60"}
+                headers={"Retry-After": "60"},
             )
-        
+
         # Apply API version transformations
         transformed_request = await self.transformer.transform_request(request, api_version)
-        
+
         # Check monetization (for paid APIs)
         if await self.monetization.requires_payment(request):
             if not await self.monetization.validate_payment(request):
-                return Response(
-                    content=json.dumps({"error": "Payment required"}),
-                    status_code=402
-                )
-        
+                return Response(content=json.dumps({"error": "Payment required"}), status_code=402)
+
         # Process request through chain
         response = await call_next(transformed_request)
-        
+
         # Apply response transformations
         transformed_response = await self.transformer.transform_response(response, api_version)
-        
+
         # Log analytics
         await self._log_api_analytics(request, transformed_response, start_time, api_version)
-        
+
         return transformed_response
-    
+
     def _extract_api_version(self, request: Request) -> str:
         """Extract API version from request"""
         # Check URL path first
-        path_parts = request.url.path.split('/')
-        if len(path_parts) > 2 and path_parts[1].startswith('v'):
+        path_parts = request.url.path.split("/")
+        if len(path_parts) > 2 and path_parts[1].startswith("v"):
             return path_parts[1]
-        
+
         # Check headers
-        version_header = request.headers.get('X-API-Version')
-        if version_header and version_header.startswith('v'):
+        version_header = request.headers.get("X-API-Version")
+        if version_header and version_header.startswith("v"):
             return version_header
-        
+
         # Check query parameter
-        version_param = request.query_params.get('api_version')
-        if version_param and version_param.startswith('v'):
+        version_param = request.query_params.get("api_version")
+        if version_param and version_param.startswith("v"):
             return version_param
-        
+
         # Default to latest stable version
         return self.version_manager.get_latest_stable_version()
-    
+
     def _add_gateway_routes(self):
         """Add gateway-specific routes"""
-        
+
         @self.app.get("/api/versions", tags=["API Gateway"])
         async def get_api_versions():
             """Get available API versions"""
             return self.version_manager.get_available_versions()
-        
+
         @self.app.get("/api/docs", include_in_schema=False)
         async def custom_swagger_ui_html():
             """Custom Swagger UI with versioning"""
             return get_swagger_ui_html(
                 openapi_url=f"/api/openapi.json",
                 title="YMERA Enterprise API - Developer Portal",
                 swagger_js_url="https://cdn.jsdelivr.net/npm/swagger-ui-dist@5/swagger-ui-bundle.js",
                 swagger_css_url="https://cdn.jsdelivr.net/npm/swagger-ui-dist@5/swagger-ui.css",
             )
-        
+
         @self.app.get("/api/openapi.json", include_in_schema=False)
         async def get_openapi_schema():
             """Custom OpenAPI schema with versioning"""
             return self._generate_openapi_schema()
-        
+
         @self.app.get("/api/health/gateway", tags=["API Gateway"])
         async def gateway_health():
             """Gateway health check"""
             return {
                 "status": "healthy",
                 "timestamp": datetime.utcnow().isoformat(),
                 "components": {
                     "rate_limiter": await self.rate_limiter.health_check(),
                     "version_manager": await self.version_manager.health_check(),
-                    "transformer": await self.transformer.health_check()
-                }
+                    "transformer": await self.transformer.health_check(),
+                },
             }
-    
+
     def _generate_openapi_schema(self):
         """Generate customized OpenAPI schema"""
         openapi_schema = get_openapi(
             title="YMERA Enterprise API",
             version="2.0.0",
             description="Enterprise-grade project management and analytics API",
             routes=self.app.routes,
         )
-        
+
         # Add customizations
         openapi_schema["info"]["x-logo"] = {
             "url": "https://ymera.example.com/logo.png",
-            "backgroundColor": "#FFFFFF"
-        }
-        
+            "backgroundColor": "#FFFFFF",
+        }
+
         openapi_schema["servers"] = [
-            {
-                "url": "https://api.ymera.example.com/v2",
-                "description": "Production API v2"
-            },
+            {"url": "https://api.ymera.example.com/v2", "description": "Production API v2"},
             {
                 "url": "https://api.ymera.example.com/v1",
-                "description": "Production API v1 (deprecated)"
-            }
+                "description": "Production API v1 (deprecated)",
+            },
         ]
-        
+
         # Add security schemes
         openapi_schema["components"]["securitySchemes"] = {
-            "BearerAuth": {
-                "type": "http",
-                "scheme": "bearer",
-                "bearerFormat": "JWT"
-            },
-            "ApiKeyAuth": {
-                "type": "apiKey",
-                "in": "header",
-                "name": "X-API-Key"
-            }
-        }
-        
+            "BearerAuth": {"type": "http", "scheme": "bearer", "bearerFormat": "JWT"},
+            "ApiKeyAuth": {"type": "apiKey", "in": "header", "name": "X-API-Key"},
+        }
+
         return openapi_schema
-    
-    async def _log_api_analytics(self, request: Request, response: Response, 
-                               start_time: datetime, api_version: str):
+
+    async def _log_api_analytics(
+        self, request: Request, response: Response, start_time: datetime, api_version: str
+    ):
         """Log API analytics for monitoring and billing"""
         duration = (datetime.utcnow() - start_time).total_seconds() * 1000  # ms
-        
+
         analytics_data = {
             "request_id": str(uuid.uuid4()),
             "timestamp": start_time.isoformat(),
             "method": request.method,
             "path": request.url.path,
@@ -195,426 +183,443 @@
             "duration_ms": duration,
             "client_ip": request.client.host if request.client else None,
             "user_agent": request.headers.get("user-agent"),
             "content_length": response.headers.get("content-length", 0),
             "rate_limit_info": await self.rate_limiter.get_rate_limit_info(request),
-            "monetization_data": await self.monetization.get_billing_data(request)
-        }
-        
+            "monetization_data": await self.monetization.get_billing_data(request),
+        }
+
         # Send to analytics pipeline
         await self._send_to_analytics(analytics_data)
 
+
 class APIVersionManager:
     """API version management with deprecation support"""
-    
+
     def __init__(self):
         self.versions = {
             "v2": {
                 "status": "current",
                 "release_date": "2024-01-15",
                 "end_of_life": "2025-01-15",
-                "docs_url": "https://docs.ymera.example.com/v2"
+                "docs_url": "https://docs.ymera.example.com/v2",
             },
             "v1": {
                 "status": "deprecated",
                 "release_date": "2023-06-10",
                 "end_of_life": "2024-06-10",
                 "docs_url": "https://docs.ymera.example.com/v1",
-                "deprecation_notice": "Migrate to v2 by June 10, 2024"
-            }
-        }
-    
+                "deprecation_notice": "Migrate to v2 by June 10, 2024",
+            },
+        }
+
     def get_available_versions(self) -> Dict[str, Any]:
         """Get available API versions"""
         return self.versions
-    
+
     def get_latest_stable_version(self) -> str:
         """Get latest stable version"""
         for version, info in self.versions.items():
             if info["status"] == "current":
                 return version
         return "v2"  # Fallback
-    
+
     def is_version_supported(self, version: str) -> bool:
         """Check if version is supported"""
         return version in self.versions
-    
+
     def is_version_deprecated(self, version: str) -> bool:
         """Check if version is deprecated"""
         return self.versions.get(version, {}).get("status") == "deprecated"
-    
+
     async def health_check(self) -> Dict[str, Any]:
         """Health check for version manager"""
         return {
             "status": "healthy",
             "supported_versions": list(self.versions.keys()),
-            "latest_version": self.get_latest_stable_version()
-        }
+            "latest_version": self.get_latest_stable_version(),
+        }
+
 
 class RateLimiter:
     """Advanced rate limiting with multiple strategies"""
-    
+
     def __init__(self):
         self.redis = await DatabaseUtils.get_redis()
         self.rate_limits = {
             "free": {"requests": 100, "period": 3600},  # 100 req/hour
             "basic": {"requests": 1000, "period": 3600},  # 1000 req/hour
             "professional": {"requests": 10000, "period": 3600},  # 10k req/hour
             "enterprise": {"requests": 100000, "period": 3600},  # 100k req/hour
-            "unlimited": {"requests": 0, "period": 0}  # Unlimited
-        }
-    
+            "unlimited": {"requests": 0, "period": 0},  # Unlimited
+        }
+
     async def check_rate_limit(self, request: Request, api_version: str) -> bool:
         """Check rate limit for request"""
         client_id = self._get_client_id(request)
         plan = await self._get_client_plan(client_id)
-        
+
         rate_limit = self.rate_limits.get(plan, self.rate_limits["free"])
-        
+
         if rate_limit["requests"] == 0:  # Unlimited
             return True
-        
+
         key = f"rate_limit:{client_id}:{api_version}"
         current = await self.redis.get(key)
-        
+
         if current and int(current) >= rate_limit["requests"]:
             return False
-        
+
         # Increment counter
         await self.redis.incr(key)
         if not current:
             await self.redis.expire(key, rate_limit["period"])
-        
+
         return True
-    
+
     def _get_client_id(self, request: Request) -> str:
         """Get client identifier for rate limiting"""
         # API key takes precedence
         api_key = request.headers.get("X-API-Key")
         if api_key:
             return f"api_key:{api_key}"
-        
+
         # Then JWT token
         auth_header = request.headers.get("Authorization")
         if auth_header and auth_header.startswith("Bearer "):
             token = auth_header[7:]
             try:
                 payload = SecurityUtils.verify_jwt(token)
                 return f"user:{payload.get('sub')}"
             except:
                 pass
-        
+
         # Fallback to IP address (least preferred)
         return f"ip:{request.client.host}" if request.client else "anonymous"
-    
+
     async def _get_client_plan(self, client_id: str) -> str:
         """Get client's rate limit plan"""
         # Implementation would check database or cache
         # For now, return based on client type
         if client_id.startswith("api_key:"):
             return "professional"
         elif client_id.startswith("user:"):
             return "basic"
         else:
             return "free"
-    
+
     async def get_rate_limit_info(self, request: Request) -> Dict[str, Any]:
         """Get rate limit information for client"""
         client_id = self._get_client_id(request)
         plan = await self._get_client_plan(client_id)
         rate_limit = self.rate_limits[plan]
-        
+
         key = f"rate_limit:{client_id}:v2"  # Using latest version
         current = int(await self.redis.get(key) or 0)
-        
+
         return {
             "plan": plan,
             "limit": rate_limit["requests"],
             "remaining": max(0, rate_limit["requests"] - current),
             "reset_in": await self.redis.ttl(key),
-            "period": rate_limit["period"]
-        }
-    
+            "period": rate_limit["period"],
+        }
+
     async def health_check(self) -> Dict[str, Any]:
         """Health check for rate limiter"""
         try:
             await self.redis.ping()
             return {"status": "healthy", "redis": "connected"}
         except Exception as e:
             return {"status": "unhealthy", "redis": "disconnected", "error": str(e)}
 
+
 class RequestResponseTransformer:
     """Request/response transformation for API versioning"""
-    
+
     async def transform_request(self, request: Request, api_version: str) -> Request:
         """Transform request based on API version"""
         # Store original request for reference
         request.state.original_url = str(request.url)
         request.state.api_version = api_version
-        
+
         # Apply version-specific transformations
         if api_version == "v1":
             request = await self._transform_v1_request(request)
         elif api_version == "v2":
             request = await self._transform_v2_request(request)
-        
+
         return request
-    
+
     async def transform_response(self, response: Response, api_version: str) -> Response:
         """Transform response based on API version"""
         # Apply version-specific response transformations
         if api_version == "v1":
             response = await self._transform_v1_response(response)
         elif api_version == "v2":
             response = await self._transform_v2_response(response)
-        
+
         # Add API version headers
         response.headers["X-API-Version"] = api_version
-        response.headers["X-API-Deprecated"] = "true" if self._is_deprecated(api_version) else "false"
-        
+        response.headers["X-API-Deprecated"] = (
+            "true" if self._is_deprecated(api_version) else "false"
+        )
+
         return response
-    
+
     async def _transform_v1_request(self, request: Request) -> Request:
         """Transform requests for v1 API"""
         # Example: Convert old field names to new ones
         if request.method in ["POST", "PUT", "PATCH"]:
             body = await request.json()
             transformed_body = self._transform_v1_body(body)
-            
+
             # Create new request with transformed body
             request._body = json.dumps(transformed_body).encode()
             request.headers.__dict__["_list"] = [
                 (b"content-length", str(len(request._body)).encode()),
-                *[(k, v) for k, v in request.headers.items() if k.lower() != b"content-length"]
+                *[(k, v) for k, v in request.headers.items() if k.lower() != b"content-length"],
             ]
-        
+
         return request
-    
+
     async def _transform_v2_request(self, request: Request) -> Request:
         """Transform requests for v2 API"""
         # v2 is current, so minimal transformations
         return request
-    
+
     def _transform_v1_body(self, body: Dict[str, Any]) -> Dict[str, Any]:
         """Transform v1 request body to current format"""
         transformations = {
             "project_name": "name",
             "project_desc": "description",
             "task_assignee": "assigned_to",
-            "task_priority_level": "priority"
-        }
-        
+            "task_priority_level": "priority",
+        }
+
         transformed = {}
         for key, value in body.items():
             new_key = transformations.get(key, key)
             transformed[new_key] = value
-        
+
         return transformed
-    
+
     async def _transform_v1_response(self, response: Response) -> Response:
         """Transform responses for v1 API"""
         # Convert modern response format to v1 format
         if response.headers.get("content-type") == "application/json":
             body = json.loads(response.body)
             transformed_body = self._transform_v1_response_body(body)
-            
+
             response.body = json.dumps(transformed_body).encode()
             response.headers["content-length"] = str(len(response.body))
-        
+
         return response
-    
+
     def _transform_v1_response_body(self, body: Dict[str, Any]) -> Dict[str, Any]:
         """Transform response body to v1 format"""
         transformations = {
             "name": "project_name",
             "description": "project_desc",
             "assigned_to": "task_assignee",
-            "priority": "task_priority_level"
-        }
-        
+            "priority": "task_priority_level",
+        }
+
         transformed = {}
         for key, value in body.items():
             new_key = transformations.get(key, key)
             transformed[new_key] = value
-        
+
         return transformed
-    
+
     def _is_deprecated(self, api_version: str) -> bool:
         """Check if API version is deprecated"""
         return api_version == "v1"  # Example
-    
+
     async def health_check(self) -> Dict[str, Any]:
         """Health check for transformer"""
         return {"status": "healthy", "transformations_available": ["v1", "v2"]}
 
+
 class APIMonetization:
     """API monetization and billing management"""
-    
+
     def __init__(self):
         self.plans = {
             "free": {
                 "price": 0,
                 "features": ["100 requests/hour", "Basic analytics", "Community support"],
-                "rate_limit": "free"
+                "rate_limit": "free",
             },
             "basic": {
                 "price": 49,
                 "features": ["1,000 requests/hour", "Advanced analytics", "Email support"],
-                "rate_limit": "basic"
+                "rate_limit": "basic",
             },
             "professional": {
                 "price": 199,
-                "features": ["10,000 requests/hour", "Premium analytics", "Priority support", "API keys"],
-                "rate_limit": "professional"
+                "features": [
+                    "10,000 requests/hour",
+                    "Premium analytics",
+                    "Priority support",
+                    "API keys",
+                ],
+                "rate_limit": "professional",
             },
             "enterprise": {
                 "price": 999,
-                "features": ["100,000 requests/hour", "Custom analytics", "24/7 support", "SLA guarantee"],
-                "rate_limit": "enterprise"
-            }
-        }
-    
+                "features": [
+                    "100,000 requests/hour",
+                    "Custom analytics",
+                    "24/7 support",
+                    "SLA guarantee",
+                ],
+                "rate_limit": "enterprise",
+            },
+        }
+
     async def requires_payment(self, request: Request) -> bool:
         """Check if request requires payment"""
         # Free endpoints don't require payment
         if self._is_free_endpoint(request):
             return False
-        
+
         # Check if client has a paid plan
         client_id = self._get_client_id(request)
         plan = await self._get_client_plan(client_id)
-        
+
         return plan != "free"
-    
+
     async def validate_payment(self, request: Request) -> bool:
         """Validate payment for request"""
         client_id = self._get_client_id(request)
-        
+
         # Check if client has active subscription
         has_active_sub = await self._check_active_subscription(client_id)
-        
+
         # Check credit balance for pay-as-you-go
         has_sufficient_credit = await self._check_credit_balance(client_id)
-        
+
         return has_active_sub or has_sufficient_credit
-    
+
     def _is_free_endpoint(self, request: Request) -> bool:
         """Check if endpoint is free"""
         free_endpoints = [
             ("GET", "/api/health"),
             ("GET", "/api/versions"),
             ("GET", "/api/docs"),
-            ("GET", "/api/openapi.json")
+            ("GET", "/api/openapi.json"),
         ]
-        
+
         return (request.method, request.url.path) in free_endpoints
-    
+
     def _get_client_id(self, request: Request) -> str:
         """Get client identifier"""
         # Similar to rate limiter implementation
         api_key = request.headers.get("X-API-Key")
         if api_key:
             return f"api_key:{api_key}"
-        
+
         auth_header = request.headers.get("Authorization")
         if auth_header and auth_header.startswith("Bearer "):
             token = auth_header[7:]
             try:
                 payload = SecurityUtils.verify_jwt(token)
                 return f"user:{payload.get('sub')}"
             except:
                 pass
-        
+
         return f"ip:{request.client.host}" if request.client else "anonymous"
-    
+
     async def _get_client_plan(self, client_id: str) -> str:
         """Get client's plan"""
         # Implementation would check database
         if client_id.startswith("api_key:"):
             return "professional"
         elif client_id.startswith("user:"):
             return "basic"
         else:
             return "free"
-    
+
     async def _check_active_subscription(self, client_id: str) -> bool:
         """Check if client has active subscription"""
         # Implementation would check subscription database
         return True  # Placeholder
-    
+
     async def _check_credit_balance(self, client_id: str) -> bool:
         """Check if client has sufficient credit"""
         # Implementation would check credit balance
         return True  # Placeholder
-    
+
     async def get_billing_data(self, request: Request) -> Dict[str, Any]:
         """Get billing data for analytics"""
         client_id = self._get_client_id(request)
         plan = await self._get_client_plan(client_id)
-        
+
         return {
             "client_id": client_id,
             "plan": plan,
             "price": self.plans[plan]["price"],
             "endpoint": request.url.path,
-            "method": request.method
-        }
+            "method": request.method,
+        }
+
 
 class DeveloperPortal:
     """Developer portal for API consumers"""
-    
+
     def __init__(self):
         self.documentation = self._load_documentation()
         self.sdk_repositories = {
             "python": "https://github.com/ymera/ymera-python-sdk",
             "javascript": "https://github.com/ymera/ymera-js-sdk",
             "java": "https://github.com/ymera/ymera-java-sdk",
             "go": "https://github.com/ymera/ymera-go-sdk",
-            "csharp": "https://github.com/ymera/ymera-csharp-sdk"
-        }
-    
+            "csharp": "https://github.com/ymera/ymera-csharp-sdk",
+        }
+
     def _load_documentation(self) -> Dict[str, Any]:
         """Load API documentation"""
         return {
             "quickstart": {
                 "title": "Getting Started",
-                "content": self._load_markdown_file("docs/quickstart.md")
+                "content": self._load_markdown_file("docs/quickstart.md"),
             },
             "authentication": {
                 "title": "Authentication",
-                "content": self._load_markdown_file("docs/authentication.md")
+                "content": self._load_markdown_file("docs/authentication.md"),
             },
             "examples": {
                 "title": "Code Examples",
-                "content": self._load_markdown_file("docs/examples.md")
+                "content": self._load_markdown_file("docs/examples.md"),
             },
             "best_practices": {
                 "title": "Best Practices",
-                "content": self._load_markdown_file("docs/best_practices.md")
-            }
-        }
-    
+                "content": self._load_markdown_file("docs/best_practices.md"),
+            },
+        }
+
     def _load_markdown_file(self, path: str) -> str:
         """Load markdown file content"""
         try:
-            with open(path, 'r') as f:
+            with open(path, "r") as f:
                 return f.read()
         except FileNotFoundError:
             return f"Documentation not found: {path}"
-    
+
     async def get_portal_data(self) -> Dict[str, Any]:
         """Get developer portal data"""
         return {
             "documentation": self.documentation,
             "sdks": self.sdk_repositories,
             "api_reference": "/api/docs",
             "support": {
                 "email": "api-support@ymera.example.com",
                 "slack": "https://ymera.slack.com/archives/api-support",
-                "forum": "https://community.ymera.example.com"
+                "forum": "https://community.ymera.example.com",
             },
             "status": {
                 "api_status": "https://status.ymera.example.com",
                 "uptime": "99.99%",
-                "incidents": []
-            }
-        }
\ No newline at end of file
+                "incidents": [],
+            },
+        }
would reformat /home/runner/work/ymera_y/ymera_y/api.gateway.py
error: cannot format /home/runner/work/ymera_y/ymera_y/api_gateway.py: Cannot parse for target version Python 3.12: 1:0: Failed to parse: UnterminatedString
--- /home/runner/work/ymera_y/ymera_y/api_gateway_init.py	2025-10-19 22:47:02.792432+00:00
+++ /home/runner/work/ymera_y/ymera_y/api_gateway_init.py	2025-10-19 23:08:55.113937+00:00
@@ -20,32 +20,33 @@
 # ===============================================================================
 
 try:
     # Database wrapper - CRITICAL FIX
     from . import database
-    
+
     # API Gateway components
     from .ymera_api_gateway import (
         APIGateway,
         GatewayConfig,
         APIRequest,
         APIResponse,
         GatewayStats,
         LoadBalancer,
         RateLimitManager,
-        router as gateway_router
+        router as gateway_router,
     )
-    
+
     # Route modules with proper error handling
     from .ymera_auth_routes import router as auth_router
     from .ymera_agent_routes import router as agent_router
     from .ymera_file_routes import router as file_router
     from .project_routes import router as project_router
     from .websocket_routes import router as websocket_router
-    
+
 except ImportError as e:
     import logging
+
     logging.error(f"CRITICAL: Failed to import API Gateway components: {e}")
     logging.error(f"Import error details: {type(e).__name__}: {str(e)}")
     raise ImportError(
         f"Failed to initialize API Gateway Core Routes. "
         f"Missing dependency: {e}. "
@@ -65,40 +66,38 @@
 # ===============================================================================
 
 __all__ = [
     # Database wrapper - CRITICAL
     "database",
-    
     # API Gateway core
     "APIGateway",
     "GatewayConfig",
     "APIRequest",
     "APIResponse",
     "GatewayStats",
     "LoadBalancer",
     "RateLimitManager",
-    
     # Routers
     "gateway_router",
     "auth_router",
     "agent_router",
     "file_router",
     "project_router",
     "websocket_router",
-    
     # Module info
     "__version__",
 ]
 
 # ===============================================================================
 # HEALTH CHECK FUNCTION
 # ===============================================================================
 
+
 def verify_imports() -> Dict[str, Any]:
     """
     Verify all critical imports are working.
-    
+
     Returns:
         Dict with import status for each component
     """
     import_status = {
         "database": False,
@@ -107,30 +106,32 @@
         "agent_routes": False,
         "file_routes": False,
         "project_routes": False,
         "websocket_routes": False,
     }
-    
+
     try:
         # Check database
-        import_status["database"] = hasattr(database, 'get_db_session')
-        
+        import_status["database"] = hasattr(database, "get_db_session")
+
         # Check API Gateway
         import_status["api_gateway"] = APIGateway is not None
-        
+
         # Check routers
         import_status["auth_routes"] = auth_router is not None
         import_status["agent_routes"] = agent_router is not None
         import_status["file_routes"] = file_router is not None
         import_status["project_routes"] = project_router is not None
         import_status["websocket_routes"] = websocket_router is not None
-        
+
     except Exception as e:
         import logging
+
         logging.error(f"Import verification failed: {e}")
-    
+
     return import_status
+
 
 # ===============================================================================
 # INITIALIZATION CHECK
 # ===============================================================================
 
@@ -138,11 +139,13 @@
 _import_status = verify_imports()
 _failed_imports = [k for k, v in _import_status.items() if not v]
 
 if _failed_imports:
     import logging
+
     logging.warning(
         f"API Gateway initialized with missing components: {', '.join(_failed_imports)}"
     )
 else:
     import logging
+
     logging.info("API Gateway Core Routes initialized successfully - All components loaded")
would reformat /home/runner/work/ymera_y/ymera_y/api_gateway_init.py
--- /home/runner/work/ymera_y/ymera_y/api_extensions.py	2025-10-19 22:47:02.792432+00:00
+++ /home/runner/work/ymera_y/ymera_y/api_extensions.py	2025-10-19 23:08:55.119322+00:00
@@ -14,311 +14,340 @@
 import io
 import csv
 
 # Import our advanced features
 from advanced_features import (
-    connection_manager, cache_manager, security_manager, 
-    task_scheduler, health_monitor, notification_manager, 
-    analytics_engine, CacheManager, SecurityManager,
-    TaskScheduler, HealthMonitor, NotificationManager, AnalyticsEngine
+    connection_manager,
+    cache_manager,
+    security_manager,
+    task_scheduler,
+    health_monitor,
+    notification_manager,
+    analytics_engine,
+    CacheManager,
+    SecurityManager,
+    TaskScheduler,
+    HealthMonitor,
+    NotificationManager,
+    AnalyticsEngine,
 )
 
 # Additional API routes to add to main.py
 
 # =============================================================================
 # WEBSOCKET ENDPOINTS
 # =============================================================================
+
 
 @app.websocket("/ws/{user_id}")
 async def websocket_endpoint(
-    websocket: WebSocket, 
-    user_id: str = Path(...),
-    token: str = Query(...)
+    websocket: WebSocket, user_id: str = Path(...), token: str = Query(...)
 ):
     """WebSocket endpoint for real-time communication"""
     try:
         # Verify token
         payload = await auth_service.verify_token(token)
         if payload.get("sub") != user_id:
             await websocket.close(code=1008, reason="Invalid token")
             return
-        
+
         connection_id = str(uuid.uuid4())
         await connection_manager.connect(websocket, user_id, connection_id)
-        
+
         try:
             while True:
                 # Receive message from client
                 data = await websocket.receive_text()
                 message = json.loads(data)
-                
+
                 # Handle different message types
                 if message["type"] == "ping":
-                    await websocket.send_text(json.dumps({
-                        "type": "pong",
-                        "timestamp": datetime.utcnow().isoformat()
-                    }))
-                
+                    await websocket.send_text(
+                        json.dumps({"type": "pong", "timestamp": datetime.utcnow().isoformat()})
+                    )
+
                 elif message["type"] == "task_update":
                     # Handle task updates from agents
                     await handle_task_update(message, user_id)
-                
+
                 elif message["type"] == "agent_status":
                     # Handle agent status updates
                     await handle_agent_status(message, user_id)
-                
+
         except WebSocketDisconnect:
             pass
         except Exception as e:
             logger.error(f"WebSocket error: {e}")
         finally:
             connection_manager.disconnect(user_id, connection_id)
-    
+
     except Exception as e:
         logger.error(f"WebSocket connection error: {e}")
         await websocket.close(code=1011, reason="Server error")
+
 
 async def handle_task_update(message: dict, user_id: str):
     """Handle task update from agent"""
     task_id = message.get("task_id")
     status = message.get("status")
     result = message.get("result")
-    
+
     if task_id:
         async with db_manager.async_session() as session:
             task = await session.get(Task, task_id)
             if task and task.user_id == user_id:
                 task.status = TaskStatus(status)
                 if result:
                     task.result = result
                 if status in [TaskStatus.COMPLETED, TaskStatus.FAILED]:
                     task.completed_at = datetime.utcnow()
-                
+
                 await session.commit()
-                
+
                 # Send notification
-                await notification_manager.send_notification(user_id, {
-                    "type": "task_update",
-                    "title": "Task Update",
-                    "message": f"Task {task.name} is now {status}",
-                    "task_id": task_id
-                })
+                await notification_manager.send_notification(
+                    user_id,
+                    {
+                        "type": "task_update",
+                        "title": "Task Update",
+                        "message": f"Task {task.name} is now {status}",
+                        "task_id": task_id,
+                    },
+                )
+
 
 async def handle_agent_status(message: dict, user_id: str):
     """Handle agent status update"""
     agent_id = message.get("agent_id")
     status = message.get("status")
-    
+
     if agent_id:
         async with db_manager.async_session() as session:
             agent = await session.get(Agent, agent_id)
             if agent and agent.owner_id == user_id:
                 agent.status = AgentStatus(status)
                 agent.last_heartbeat = datetime.utcnow()
                 await session.commit()
 
+
 # =============================================================================
 # ADVANCED TASK MANAGEMENT
 # =============================================================================
+
 
 @app.post("/tasks/batch", response_model=dict)
 async def create_batch_tasks(
-    tasks: List[TaskCreate],
-    current_user: User = Depends(get_current_user)
+    tasks: List[TaskCreate], current_user: User = Depends(get_current_user)
 ):
     """Create multiple tasks in batch"""
     task_ids = []
-    
+
     async with db_manager.async_session() as session:
         for task_data in tasks:
             task = Task(
                 name=task_data.name,
                 description=task_data.description,
                 task_type=task_data.task_type,
                 parameters=task_data.parameters,
                 priority=task_data.priority,
                 user_id=current_user.id,
-                agent_id=task_data.agent_id
+                agent_id=task_data.agent_id,
             )
             session.add(task)
             await session.flush()  # Get ID without committing
             task_ids.append(task.id)
-        
+
         await session.commit()
-    
+
     # Schedule all tasks
     for i, task_id in enumerate(task_ids):
-        await task_scheduler.schedule_task({
-            "task_id": task_id,
-            "task_type": tasks[i].task_type,
-            "parameters": tasks[i].parameters
-        }, priority=tasks[i].priority.value)
-    
+        await task_scheduler.schedule_task(
+            {
+                "task_id": task_id,
+                "task_type": tasks[i].task_type,
+                "parameters": tasks[i].parameters,
+            },
+            priority=tasks[i].priority.value,
+        )
+
     return {"task_ids": task_ids, "count": len(task_ids)}
+
 
 @app.get("/tasks/stats", response_model=dict)
 async def get_task_statistics(
-    current_user: User = Depends(get_current_user),
-    days: int = Query(7, ge=1, le=90)
+    current_user: User = Depends(get_current_user), days: int = Query(7, ge=1, le=90)
 ):
     """Get task statistics for user"""
     cache_key = f"task_stats:{current_user.id}:{days}"
-    
+
     # Try cache first
     cached_stats = await cache_manager.get(cache_key)
     if cached_stats:
         return cached_stats
-    
+
     # Calculate statistics
     start_date = datetime.utcnow() - timedelta(days=days)
-    
+
     async with db_manager.async_session() as session:
         # Get task counts by status
-        result = await session.execute("""
+        result = await session.execute(
+            """
             SELECT status, COUNT(*) as count 
             FROM tasks 
             WHERE user_id = :user_id AND created_at >= :start_date
             GROUP BY status
-        """, {"user_id": current_user.id, "start_date": start_date})
-        
+        """,
+            {"user_id": current_user.id, "start_date": start_date},
+        )
+
         status_counts = dict(result.fetchall())
-        
+
         # Get task counts by type
-        result = await session.execute("""
+        result = await session.execute(
+            """
             SELECT task_type, COUNT(*) as count 
             FROM tasks 
             WHERE user_id = :user_id AND created_at >= :start_date
             GROUP BY task_type
-        """, {"user_id": current_user.id, "start_date": start_date})
-        
+        """,
+            {"user_id": current_user.id, "start_date": start_date},
+        )
+
         type_counts = dict(result.fetchall())
-        
+
         # Calculate average completion time
-        result = await session.execute("""
+        result = await session.execute(
+            """
             SELECT AVG(EXTRACT(EPOCH FROM (completed_at - created_at))) as avg_time
             FROM tasks 
             WHERE user_id = :user_id AND status = 'completed' AND created_at >= :start_date
-        """, {"user_id": current_user.id, "start_date": start_date})
-        
+        """,
+            {"user_id": current_user.id, "start_date": start_date},
+        )
+
         avg_completion_time = result.scalar() or 0
-    
+
     stats = {
         "period_days": days,
         "status_counts": status_counts,
         "type_counts": type_counts,
         "average_completion_time_seconds": float(avg_completion_time),
-        "total_tasks": sum(status_counts.values())
+        "total_tasks": sum(status_counts.values()),
     }
-    
+
     # Cache for 1 hour
     await cache_manager.set(cache_key, stats, ttl=3600)
-    
+
     return stats
 
+
 # =============================================================================
 # AGENT MANAGEMENT EXTENSIONS
 # =============================================================================
+
 
 @app.post("/agents/{agent_id}/execute", response_model=dict)
 async def execute_agent_command(
-    agent_id: str,
-    command: dict = Body(...),
-    current_user: User = Depends(get_current_user)
+    agent_id: str, command: dict = Body(...), current_user: User = Depends(get_current_user)
 ):
     """Execute a command on an agent"""
     async with db_manager.async_session() as session:
         agent = await session.get(Agent, agent_id)
-        
+
         if not agent or agent.owner_id != current_user.id:
             raise HTTPException(status_code=404, detail="Agent not found")
-        
+
         if agent.status != AgentStatus.ACTIVE:
             raise HTTPException(status_code=400, detail="Agent is not active")
-        
+
         # Execute command via WebSocket or queue
         command_id = str(uuid.uuid4())
         command_data = {
             "command_id": command_id,
             "agent_id": agent_id,
             "command": command,
-            "timestamp": datetime.utcnow().isoformat()
+            "timestamp": datetime.utcnow().isoformat(),
         }
-        
+
         # Try to send via WebSocket first
-        await connection_manager.send_to_user(current_user.id, {
-            "type": "agent_command",
-            "agent_id": agent_id,
-            "data": command_data
-        })
-        
+        await connection_manager.send_to_user(
+            current_user.id, {"type": "agent_command", "agent_id": agent_id, "data": command_data}
+        )
+
         # Also queue for processing
         await task_scheduler.schedule_task(command_data, priority=2)
-        
+
         return {"command_id": command_id, "status": "sent"}
+
 
 @app.get("/agents/available", response_model=List[dict])
 async def get_available_agents(
-    task_type: Optional[str] = Query(None),
-    current_user: User = Depends(get_current_user)
+    task_type: Optional[str] = Query(None), current_user: User = Depends(get_current_user)
 ):
     """Get available agents for task assignment"""
     async with db_manager.async_session() as session:
         query = select(Agent).where(
-            Agent.owner_id == current_user.id,
-            Agent.status == AgentStatus.ACTIVE
-        )
-        
+            Agent.owner_id == current_user.id, Agent.status == AgentStatus.ACTIVE
+        )
+
         if task_type:
             query = query.where(Agent.capabilities.contains([task_type]))
-        
+
         result = await session.execute(query)
         agents = result.scalars().all()
-        
-        return [{
-            "id": agent.id,
-            "name": agent.name,
-            "capabilities": agent.capabilities,
-            "last_heartbeat": agent.last_heartbeat,
-            "load": await _calculate_agent_load(agent.id)
-        } for agent in agents]
+
+        return [
+            {
+                "id": agent.id,
+                "name": agent.name,
+                "capabilities": agent.capabilities,
+                "last_heartbeat": agent.last_heartbeat,
+                "load": await _calculate_agent_load(agent.id),
+            }
+            for agent in agents
+        ]
+
 
 async def _calculate_agent_load(agent_id: str) -> float:
     """Calculate current load for an agent"""
     async with db_manager.async_session() as session:
         result = await session.execute(
             select(func.count(Task.id)).where(
-                Task.agent_id == agent_id,
-                Task.status == TaskStatus.RUNNING
+                Task.agent_id == agent_id, Task.status == TaskStatus.RUNNING
             )
         )
         running_tasks = result.scalar() or 0
         return min(running_tasks / 5.0, 1.0)  # Assume max 5 concurrent tasks
 
+
 # =============================================================================
 # MONITORING & ANALYTICS ENDPOINTS
 # =============================================================================
+
 
 @app.get("/monitoring/health", response_model=dict)
 async def detailed_health_check():
     """Detailed health check with component status"""
     health_data = await health_monitor.check_system_health()
     return health_data
+
 
 @app.get("/monitoring/metrics/live", response_model=dict)
 async def get_live_metrics():
     """Get live system metrics"""
     metrics = {
         "timestamp": datetime.utcnow().isoformat(),
         "system": {
             "active_websocket_connections": len(connection_manager.active_connections),
             "cache_stats": cache_manager.cache_stats,
-            "task_queue_size": await _get_total_queue_size()
+            "task_queue_size": await _get_total_queue_size(),
         },
         "database": await _get_db_metrics(),
-        "redis": await _get_redis_metrics()
+        "redis": await _get_redis_metrics(),
     }
     return metrics
+
 
 async def _get_total_queue_size() -> int:
     """Get total tasks in all queues"""
     try:
         queues = await redis_client.keys("task_queue:*")
@@ -328,153 +357,174 @@
             total_size += size
         return total_size
     except:
         return 0
 
+
 async def _get_db_metrics() -> dict:
     """Get database performance metrics"""
     try:
         async with db_manager.async_session() as session:
             # Count active connections (this would be database-specific)
-            result = await session.execute("SELECT COUNT(*) as total_tasks FROM tasks WHERE status = 'running'")
+            result = await session.execute(
+                "SELECT COUNT(*) as total_tasks FROM tasks WHERE status = 'running'"
+            )
             running_tasks = result.scalar()
-            
-            return {
-                "running_tasks": running_tasks,
-                "connection_status": "healthy"
-            }
+
+            return {"running_tasks": running_tasks, "connection_status": "healthy"}
     except Exception as e:
         return {"error": str(e), "connection_status": "unhealthy"}
+
 
 async def _get_redis_metrics() -> dict:
     """Get Redis performance metrics"""
     try:
         info = await redis_client.info()
         return {
             "connected_clients": info.get("connected_clients", 0),
             "used_memory_human": info.get("used_memory_human", "0B"),
             "keyspace_hits": info.get("keyspace_hits", 0),
-            "keyspace_misses": info.get("keyspace_misses", 0)
+            "keyspace_misses": info.get("keyspace_misses", 0),
         }
     except Exception as e:
         return {"error": str(e)}
 
+
 @app.get("/analytics/summary", response_model=dict)
 async def get_analytics_summary(
-    hours: int = Query(24, ge=1, le=168),
-    current_user: User = Depends(get_current_user)
+    hours: int = Query(24, ge=1, le=168), current_user: User = Depends(get_current_user)
 ):
     """Get analytics summary"""
     summary = await analytics_engine.get_analytics_summary(hours * 3600)
-    
+
     # Add user-specific analytics
     user_tasks = await _get_user_task_analytics(current_user.id, hours)
     summary["user_analytics"] = user_tasks
-    
+
     return summary
+
 
 async def _get_user_task_analytics(user_id: str, hours: int) -> dict:
     """Get user-specific task analytics"""
     start_time = datetime.utcnow() - timedelta(hours=hours)
-    
-    async with db_manager.async_session() as session:
-        result = await session.execute("""
+
+    async with db_manager.async_session() as session:
+        result = await session.execute(
+            """
             SELECT 
                 status,
                 task_type,
                 COUNT(*) as count,
                 AVG(CASE WHEN completed_at IS NOT NULL 
                     THEN EXTRACT(EPOCH FROM (completed_at - created_at)) 
                     ELSE NULL END) as avg_duration
             FROM tasks 
             WHERE user_id = :user_id AND created_at >= :start_time
             GROUP BY status, task_type
-        """, {"user_id": user_id, "start_time": start_time})
-        
+        """,
+            {"user_id": user_id, "start_time": start_time},
+        )
+
         analytics_data = {}
         for row in result.fetchall():
             key = f"{row.status}_{row.task_type}"
             analytics_data[key] = {
                 "count": row.count,
-                "avg_duration": float(row.avg_duration) if row.avg_duration else None
+                "avg_duration": float(row.avg_duration) if row.avg_duration else None,
             }
-        
+
         return analytics_data
 
+
 # =============================================================================
 # NOTIFICATION ENDPOINTS
 # =============================================================================
+
 
 @app.get("/notifications", response_model=List[dict])
 async def get_notifications(
-    limit: int = Query(50, ge=1, le=100),
-    current_user: User = Depends(get_current_user)
+    limit: int = Query(50, ge=1, le=100), current_user: User = Depends(get_current_user)
 ):
     """Get user notifications"""
-    notifications = await notification_manager.get_user_notifications(
-        current_user.id, limit
-    )
+    notifications = await notification_manager.get_user_notifications(current_user.id, limit)
     return notifications
+
 
 @app.post("/notifications/{notification_id}/read")
 async def mark_notification_read(
-    notification_id: str,
-    current_user: User = Depends(get_current_user)
+    notification_id: str, current_user: User = Depends(get_current_user)
 ):
     """Mark notification as read"""
     # Implementation would update notification status
     return {"status": "marked_as_read"}
 
+
 # =============================================================================
 # EXPORT & REPORTING ENDPOINTS
 # =============================================================================
+
 
 @app.get("/export/tasks")
 async def export_tasks(
     format: str = Query("csv", regex="^(csv|json)$"),
     days: int = Query(30, ge=1, le=365),
-    current_user: User = Depends(get_current_user)
+    current_user: User = Depends(get_current_user),
 ):
     """Export tasks data"""
     start_date = datetime.utcnow() - timedelta(days=days)
-    
+
     async with db_manager.async_session() as session:
         result = await session.execute(
-            select(Task).where(
-                Task.user_id == current_user.id,
-                Task.created_at >= start_date
-            ).order_by(Task.created_at.desc())
+            select(Task)
+            .where(Task.user_id == current_user.id, Task.created_at >= start_date)
+            .order_by(Task.created_at.desc())
         )
         tasks = result.scalars().all()
-    
+
     if format == "csv":
         output = io.StringIO()
         writer = csv.writer(output)
-        
+
         # Write header
-        writer.writerow([
-            "ID", "Name", "Type", "Status", "Priority", 
-            "Created", "Completed", "Duration (seconds)"
-        ])
-        
+        writer.writerow(
+            [
+                "ID",
+                "Name",
+                "Type",
+                "Status",
+                "Priority",
+                "Created",
+                "Completed",
+                "Duration (seconds)",
+            ]
+        )
+
         # Write data
         for task in tasks:
             duration = None
             if task.completed_at and task.created_at:
                 duration = (task.completed_at - task.created_at).total_seconds()
-            
-            writer.writerow([
-                task.id, task.name, task.task_type, task.status, 
-                task.priority, task.created_at, task.completed_at, duration
-            ])
-        
+
+            writer.writerow(
+                [
+                    task.id,
+                    task.name,
+                    task.task_type,
+                    task.status,
+                    task.priority,
+                    task.created_at,
+                    task.completed_at,
+                    duration,
+                ]
+            )
+
         return StreamingResponse(
             io.BytesIO(output.getvalue().encode()),
             media_type="text/csv",
-            headers={"Content-Disposition": f"attachment; filename=tasks_{days}days.csv"}
-        )
-    
+            headers={"Content-Disposition": f"attachment; filename=tasks_{days}days.csv"},
+        )
+
     else:  # JSON format
         task_data = []
         for task in tasks:
             task_dict = {
                 "id": task.id,
@@ -482,22 +532,24 @@
                 "task_type": task.task_type,
                 "status": task.status,
                 "priority": task.priority,
                 "created_at": task.created_at.isoformat(),
                 "completed_at": task.completed_at.isoformat() if task.completed_at else None,
-                "result": task.result
+                "result": task.result,
             }
             task_data.append(task_dict)
-        
+
         return JSONResponse(
             content={"tasks": task_data, "exported_at": datetime.utcnow().isoformat()},
-            headers={"Content-Disposition": f"attachment; filename=tasks_{days}days.json"}
-        )
+            headers={"Content-Disposition": f"attachment; filename=tasks_{days}days.json"},
+        )
+
 
 # =============================================================================
 # ADMIN ENDPOINTS
 # =============================================================================
+
 
 @app.get("/admin/system-info")
 async def get_system_info(current_user: User = Depends(get_current_user)):
     """Get system information (admin only)"""
     # In production, add proper admin role checking
@@ -505,58 +557,64 @@
         "version": "1.0.0",
         "uptime": "calculated_uptime_here",
         "total_users": await _count_total_users(),
         "total_agents": await _count_total_agents(),
         "total_tasks": await _count_total_tasks(),
-        "system_load": await health_monitor.check_system_health()
+        "system_load": await health_monitor.check_system_health(),
     }
+
 
 async def _count_total_users() -> int:
     async with db_manager.async_session() as session:
         result = await session.execute(select(func.count(User.id)))
         return result.scalar()
 
+
 async def _count_total_agents() -> int:
     async with db_manager.async_session() as session:
         result = await session.execute(select(func.count(Agent.id)))
         return result.scalar()
 
+
 async def _count_total_tasks() -> int:
     async with db_manager.async_session() as session:
         result = await session.execute(select(func.count(Task.id)))
         return result.scalar()
 
+
 # =============================================================================
 # RATE-LIMITED ENDPOINTS
 # =============================================================================
+
 
 @app.middleware("http")
 async def rate_limiting_middleware(request: Request, call_next):
     """Global rate limiting middleware"""
     # Skip rate limiting for health checks
     if request.url.path in ["/health", "/metrics"]:
         return await call_next(request)
-    
+
     # Get client identifier
     client_id = request.client.host
     if hasattr(request.state, "user"):
         client_id = f"user:{request.state.user.id}"
-    
+
     # Check rate limit
     allowed = await security_manager.rate_limit(client_id, limit=1000, window=3600)  # 1000/hour
-    
+
     if not allowed:
         return JSONResponse(
-            status_code=429,
-            content={"detail": "Rate limit exceeded. Try again later."}
-        )
-    
+            status_code=429, content={"detail": "Rate limit exceeded. Try again later."}
+        )
+
     return await call_next(request)
 
+
 # =============================================================================
 # ERROR HANDLERS
 # =============================================================================
+
 
 @app.exception_handler(HTTPException)
 async def http_exception_handler(request: Request, exc: HTTPException):
     """Custom HTTP exception handler with logging"""
     await analytics_engine.record_event(
@@ -564,39 +622,32 @@
         getattr(request.state, "user_id", "anonymous"),
         {
             "status_code": exc.status_code,
             "detail": exc.detail,
             "path": request.url.path,
-            "method": request.method
-        }
+            "method": request.method,
+        },
     )
-    
+
     return JSONResponse(
         status_code=exc.status_code,
         content={
             "error": exc.detail,
             "timestamp": datetime.utcnow().isoformat(),
-            "path": request.url.path
-        }
+            "path": request.url.path,
+        },
     )
+
 
 @app.exception_handler(Exception)
 async def general_exception_handler(request: Request, exc: Exception):
     """General exception handler for unhandled errors"""
     logger.error(f"Unhandled error: {exc}", exc_info=True)
-    
+
     await security_manager.log_security_event(
-        "system_error",
-        {
-            "error": str(exc),
-            "path": request.url.path,
-            "method": request.method
-        }
+        "system_error", {"error": str(exc), "path": request.url.path, "method": request.method}
     )
-    
+
     return JSONResponse(
         status_code=500,
-        content={
-            "error": "Internal server error",
-            "timestamp": datetime.utcnow().isoformat()
-        }
+        content={"error": "Internal server error", "timestamp": datetime.utcnow().isoformat()},
     )
would reformat /home/runner/work/ymera_y/ymera_y/api_extensions.py
--- /home/runner/work/ymera_y/ymera_y/app_agent_mgmt_endpoints.py	2025-10-19 22:47:02.792432+00:00
+++ /home/runner/work/ymera_y/ymera_y/app_agent_mgmt_endpoints.py	2025-10-19 23:08:55.606104+00:00
@@ -2,307 +2,318 @@
 
 # ============================================================================
 # AGENT MANAGEMENT ENDPOINTS (Enhanced Authority)
 # ============================================================================
 
-@app.post("/agents/{agent_id}/actions", response_model=Dict, dependencies=[Depends(zero_trust_enforcement)])
+
+@app.post(
+    "/agents/{agent_id}/actions",
+    response_model=Dict,
+    dependencies=[Depends(zero_trust_enforcement)],
+)
 @require_permission(Permission.MANAGE_AGENTS)
 async def execute_agent_action(
     agent_id: str,
     action_request: AgentActionRequest,
-    current_user: Dict = Depends(get_rbac_manager_instance_dependency().get_current_user)
+    current_user: Dict = Depends(get_rbac_manager_instance_dependency().get_current_user),
 ):
     """Execute administrative action on an agent (suspend, freeze, resume, decommission).
-    
+
     Requires MANAGE_AGENTS permission. Critical actions may require additional approval.
     """
     global agent_lifecycle_manager, telemetry_manager
-    
+
     if not agent_lifecycle_manager:
         raise HTTPException(
             status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
-            detail="Agent lifecycle manager not initialized"
-        )
-    
+            detail="Agent lifecycle manager not initialized",
+        )
+
     try:
         # Verify the admin has authority
         action_request.admin_id = current_user["id"]
-        
-        result = await agent_lifecycle_manager.execute_agent_action(
-            agent_id, 
-            action_request
-        )
-        
+
+        result = await agent_lifecycle_manager.execute_agent_action(agent_id, action_request)
+
         if telemetry_manager:
             await telemetry_manager.record_event(
                 "agent_action_executed",
                 {
                     "agent_id": agent_id,
                     "action": action_request.action.value,
                     "admin_id": current_user["id"],
-                    "result": result["status"]
-                }
+                    "result": result["status"],
+                },
             )
-        
+
         return result
-    
+
     except HTTPException:
         raise
     except Exception as e:
         logger.error(f"Failed to execute action on agent {agent_id}: {e}", exc_info=True)
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to execute action: {str(e)}"
-        )
-
-
-@app.post("/agents/{agent_id}/security-violation", response_model=Dict, dependencies=[Depends(zero_trust_enforcement)])
+            detail=f"Failed to execute action: {str(e)}",
+        )
+
+
+@app.post(
+    "/agents/{agent_id}/security-violation",
+    response_model=Dict,
+    dependencies=[Depends(zero_trust_enforcement)],
+)
 @require_permission(Permission.MANAGE_AGENTS)
 async def report_security_violation(
     agent_id: str,
     violation_type: str,
     severity: str,
     details: Dict[str, Any],
-    current_user: Dict = Depends(get_rbac_manager_instance_dependency().get_current_user)
+    current_user: Dict = Depends(get_rbac_manager_instance_dependency().get_current_user),
 ):
     """Report a security violation by an agent (called by surveillance system or manually).
-    
+
     This endpoint allows the Agent Manager to handle security violations with appropriate
     enforcement actions including automatic suspension if configured.
     """
     global agent_lifecycle_manager
-    
+
     if not agent_lifecycle_manager:
         raise HTTPException(
             status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
-            detail="Agent lifecycle manager not initialized"
-        )
-    
+            detail="Agent lifecycle manager not initialized",
+        )
+
     try:
         # Convert string severity to enum
         severity_enum = AlertSeverity(severity.lower())
-        
+
         result = await agent_lifecycle_manager.handle_security_violation(
             agent_id=agent_id,
             violation_type=violation_type,
             severity=severity_enum,
-            details={**details, "reported_by": current_user["id"]}
-        )
-        
+            details={**details, "reported_by": current_user["id"]},
+        )
+
         return result
-    
+
     except ValueError as e:
         raise HTTPException(
-            status_code=status.HTTP_400_BAD_REQUEST,
-            detail=f"Invalid severity level: {severity}"
-        )
-    except Exception as e:
-        logger.error(f"Failed to handle security violation for agent {agent_id}: {e}", exc_info=True)
-        raise HTTPException(
-            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to handle violation: {str(e)}"
-        )
-
-
-@app.get("/agents/{agent_id}/surveillance-report", response_model=Dict, dependencies=[Depends(zero_trust_enforcement)])
+            status_code=status.HTTP_400_BAD_REQUEST, detail=f"Invalid severity level: {severity}"
+        )
+    except Exception as e:
+        logger.error(
+            f"Failed to handle security violation for agent {agent_id}: {e}", exc_info=True
+        )
+        raise HTTPException(
+            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
+            detail=f"Failed to handle violation: {str(e)}",
+        )
+
+
+@app.get(
+    "/agents/{agent_id}/surveillance-report",
+    response_model=Dict,
+    dependencies=[Depends(zero_trust_enforcement)],
+)
 @require_permission(Permission.VIEW_AGENTS)
 async def get_agent_surveillance_report(
     agent_id: str,
-    current_user: Dict = Depends(get_rbac_manager_instance_dependency().get_current_user)
+    current_user: Dict = Depends(get_rbac_manager_instance_dependency().get_current_user),
 ):
     """Get detailed surveillance report for a specific agent.
-    
+
     Includes performance metrics, security scores, anomaly detection results,
     and behavioral analysis from the surveillance system.
     """
     global agent_surveillance_system, agent_lifecycle_manager
-    
+
     if not agent_surveillance_system or not agent_lifecycle_manager:
         raise HTTPException(
             status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
-            detail="Surveillance system not initialized"
-        )
-    
+            detail="Surveillance system not initialized",
+        )
+
     try:
         # Get agent details
         agent = await agent_lifecycle_manager.get_agent(agent_id)
-        
+
         if not agent:
             raise HTTPException(
-                status_code=status.HTTP_404_NOT_FOUND,
-                detail=f"Agent {agent_id} not found"
+                status_code=status.HTTP_404_NOT_FOUND, detail=f"Agent {agent_id} not found"
             )
-        
+
         # Verify tenant access
         if agent.tenant_id != current_user["tenant_id"]:
             raise HTTPException(
-                status_code=status.HTTP_403_FORBIDDEN,
-                detail="Access denied to this agent"
+                status_code=status.HTTP_403_FORBIDDEN, detail="Access denied to this agent"
             )
-        
+
         # Get surveillance metrics
         metrics = await agent_surveillance_system.get_agent_metrics(agent_id)
-        
+
         return {
             "agent_id": agent_id,
             "agent_name": agent.name,
             "status": agent.status,
             "security_score": agent.security_score,
             "last_heartbeat": agent.last_heartbeat.isoformat() if agent.last_heartbeat else None,
             "surveillance_metrics": metrics or {},
             "performance_metrics": agent.performance_metrics or {},
-            "timestamp": datetime.utcnow().isoformat()
+            "timestamp": datetime.utcnow().isoformat(),
         }
-    
+
     except HTTPException:
         raise
     except Exception as e:
         logger.error(f"Failed to get surveillance report for agent {agent_id}: {e}", exc_info=True)
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to generate report: {str(e)}"
-        )
-
-
-@app.get("/surveillance/dashboard", response_model=Dict, dependencies=[Depends(zero_trust_enforcement)])
+            detail=f"Failed to generate report: {str(e)}",
+        )
+
+
+@app.get(
+    "/surveillance/dashboard", response_model=Dict, dependencies=[Depends(zero_trust_enforcement)]
+)
 @require_permission(Permission.VIEW_AGENTS)
 async def get_surveillance_dashboard(
-    current_user: Dict = Depends(get_rbac_manager_instance_dependency().get_current_user)
+    current_user: Dict = Depends(get_rbac_manager_instance_dependency().get_current_user),
 ):
     """Get comprehensive surveillance dashboard for all agents in tenant.
-    
+
     Provides overview of agent health, security status, and system-wide metrics.
     """
     global agent_surveillance_system, agent_lifecycle_manager
-    
+
     if not agent_surveillance_system or not agent_lifecycle_manager:
         raise HTTPException(
             status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
-            detail="Surveillance system not initialized"
-        )
-    
+            detail="Surveillance system not initialized",
+        )
+
     try:
         tenant_id = current_user["tenant_id"]
-        
+
         # Get all agents for tenant
         agents = await agent_lifecycle_manager.list_agents(tenant_id)
-        
+
         # Get surveillance report
         surveillance_report = await agent_surveillance_system.get_surveillance_report()
-        
+
         # Aggregate statistics
         total_agents = len(agents)
         active_agents = sum(1 for a in agents if a.status == "active")
         suspended_agents = sum(1 for a in agents if a.status == "suspended")
         frozen_agents = sum(1 for a in agents if a.status == "frozen")
         compromised_agents = sum(1 for a in agents if a.status == "compromised")
-        
+
         avg_security_score = (
-            sum(a.security_score for a in agents) / total_agents 
-            if total_agents > 0 else 100
-        )
-        
+            sum(a.security_score for a in agents) / total_agents if total_agents > 0 else 100
+        )
+
         # Agents requiring attention
         low_security_agents = [
             {
                 "id": str(a.id),
                 "name": a.name,
                 "security_score": a.security_score,
-                "status": a.status
+                "status": a.status,
             }
-            for a in agents if a.security_score < 70
+            for a in agents
+            if a.security_score < 70
         ]
-        
+
         return {
             "tenant_id": tenant_id,
             "summary": {
                 "total_agents": total_agents,
                 "active_agents": active_agents,
                 "suspended_agents": suspended_agents,
                 "frozen_agents": frozen_agents,
                 "compromised_agents": compromised_agents,
-                "average_security_score": round(avg_security_score, 2)
+                "average_security_score": round(avg_security_score, 2),
             },
             "agents_requiring_attention": low_security_agents,
             "surveillance_status": surveillance_report,
-            "timestamp": datetime.utcnow().isoformat()
+            "timestamp": datetime.utcnow().isoformat(),
         }
-    
+
     except Exception as e:
         logger.error(f"Failed to generate surveillance dashboard: {e}", exc_info=True)
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to generate dashboard: {str(e)}"
-        )
-
-
-@app.post("/agents/{agent_id}/approve-action", response_model=Dict, dependencies=[Depends(zero_trust_enforcement)])
+            detail=f"Failed to generate dashboard: {str(e)}",
+        )
+
+
+@app.post(
+    "/agents/{agent_id}/approve-action",
+    response_model=Dict,
+    dependencies=[Depends(zero_trust_enforcement)],
+)
 @require_permission(Permission.MANAGE_AGENTS)
 async def approve_agent_action(
     agent_id: str,
     action: str,
     approval_notes: str,
-    current_user: Dict = Depends(get_rbac_manager_instance_dependency().get_current_user)
+    current_user: Dict = Depends(get_rbac_manager_instance_dependency().get_current_user),
 ):
     """Approve a pending agent action (e.g., decommission).
-    
+
     Used when actions require admin approval. Only users with MANAGE_AGENTS
     permission can approve.
     """
     global agent_lifecycle_manager, telemetry_manager
-    
+
     if not agent_lifecycle_manager:
         raise HTTPException(
             status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
-            detail="Agent lifecycle manager not initialized"
-        )
-    
+            detail="Agent lifecycle manager not initialized",
+        )
+
     try:
         # Generate approval ID
         approval_id = f"approval_{current_user['id']}_{int(datetime.utcnow().timestamp())}"
-        
+
         # Execute the approved action
         action_request = AgentActionRequest(
             action=AgentAction(action),
             reason=f"Admin approved: {approval_notes}",
             admin_id=current_user["id"],
-            approval_id=approval_id
-        )
-        
-        result = await agent_lifecycle_manager.execute_agent_action(
-            agent_id,
-            action_request
-        )
-        
+            approval_id=approval_id,
+        )
+
+        result = await agent_lifecycle_manager.execute_agent_action(agent_id, action_request)
+
         if telemetry_manager:
             await telemetry_manager.record_event(
                 "agent_action_approved",
                 {
                     "agent_id": agent_id,
                     "action": action,
                     "approved_by": current_user["id"],
-                    "approval_id": approval_id
-                }
+                    "approval_id": approval_id,
+                },
             )
-        
+
         return {
             **result,
             "approval_id": approval_id,
             "approved_by": current_user["username"],
-            "approval_timestamp": datetime.utcnow().isoformat()
+            "approval_timestamp": datetime.utcnow().isoformat(),
         }
-    
+
     except ValueError as e:
         raise HTTPException(
-            status_code=status.HTTP_400_BAD_REQUEST,
-            detail=f"Invalid action: {action}"
+            status_code=status.HTTP_400_BAD_REQUEST, detail=f"Invalid action: {action}"
         )
     except Exception as e:
         logger.error(f"Failed to approve action for agent {agent_id}: {e}", exc_info=True)
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to approve action: {str(e)}"
+            detail=f"Failed to approve action: {str(e)}",
         )
 
 
 # Import the new types at the top of app.py
 from agent_lifecycle_manager import AgentStatus, AgentAction, AgentActionRequest
would reformat /home/runner/work/ymera_y/ymera_y/app_agent_mgmt_endpoints.py
--- /home/runner/work/ymera_y/ymera_y/audit_manager.py	2025-10-19 22:47:02.792432+00:00
+++ /home/runner/work/ymera_y/ymera_y/audit_manager.py	2025-10-19 23:08:56.267833+00:00
@@ -17,38 +17,46 @@
 import hmac
 from dataclasses import dataclass, field
 
 logger = logging.getLogger(__name__)
 
+
 class AuditType(str, Enum):
     """Types of audits"""
-    REGULAR = "regular"          # Scheduled, expected audit
-    SURPRISE = "surprise"        # Unannounced surprise audit
-    COMPLIANCE = "compliance"    # Compliance-focused audit
-    SECURITY = "security"        # Security-focused audit
+
+    REGULAR = "regular"  # Scheduled, expected audit
+    SURPRISE = "surprise"  # Unannounced surprise audit
+    COMPLIANCE = "compliance"  # Compliance-focused audit
+    SECURITY = "security"  # Security-focused audit
     PERFORMANCE = "performance"  # Performance-focused audit
-    BEHAVIORAL = "behavioral"    # Agent behavior audit
+    BEHAVIORAL = "behavioral"  # Agent behavior audit
+
 
 class AuditScope(str, Enum):
     """Scope of audit"""
-    SYSTEM = "system"            # Full system audit
-    TENANT = "tenant"            # Tenant-level audit
-    AGENT = "agent"              # Single agent audit
-    WORKFLOW = "workflow"        # Workflow execution audit
+
+    SYSTEM = "system"  # Full system audit
+    TENANT = "tenant"  # Tenant-level audit
+    AGENT = "agent"  # Single agent audit
+    WORKFLOW = "workflow"  # Workflow execution audit
     INTERACTION = "interaction"  # Agent interaction audit
+
 
 class AuditOutcome(str, Enum):
     """Outcome of audit"""
-    PASSED = "passed"            # Fully compliant
+
+    PASSED = "passed"  # Fully compliant
     PASSED_WITH_FINDINGS = "passed_with_findings"  # Passed with minor issues
-    FAILED = "failed"            # Failed audit
-    CRITICAL = "critical"        # Critical failures detected
-    INCOMPLETE = "incomplete"    # Audit not completed
+    FAILED = "failed"  # Failed audit
+    CRITICAL = "critical"  # Critical failures detected
+    INCOMPLETE = "incomplete"  # Audit not completed
+
 
 @dataclass
 class AuditFinding:
     """Audit finding with details"""
+
     id: str
     audit_id: str
     severity: str  # critical, high, medium, low
     title: str
     description: str
@@ -58,13 +66,15 @@
     due_date: Optional[datetime] = None
     status: str = "open"
     created_at: datetime = field(default_factory=datetime.utcnow)
     updated_at: datetime = field(default_factory=datetime.utcnow)
 
+
 @dataclass
 class AuditRecord:
     """Complete audit record"""
+
     id: str
     audit_type: AuditType
     scope: AuditScope
     target_id: str  # System, tenant, agent, workflow ID
     auditor_id: str
@@ -75,372 +85,421 @@
     findings_count: Dict[str, int] = field(default_factory=dict)
     summary: str = ""
     next_audit_date: Optional[datetime] = None
     metadata: Dict = field(default_factory=dict)
 
+
 class AuditManager:
     """
     Comprehensive auditing system for agent governance
     """
-    
+
     def __init__(self, redis_client: aioredis.Redis, config: Dict):
         self.redis = redis_client
         self.config = config
         self.audit_templates = self._load_audit_templates()
-        self.surprise_audit_probability = config.get('surprise_audit_probability', 0.2)
-        
+        self.surprise_audit_probability = config.get("surprise_audit_probability", 0.2)
+
         # Schedule audit intervals (in days)
         self.audit_intervals = {
-            AuditType.REGULAR: config.get('regular_audit_interval', 30),
-            AuditType.COMPLIANCE: config.get('compliance_audit_interval', 90),
-            AuditType.SECURITY: config.get('security_audit_interval', 60),
-            AuditType.PERFORMANCE: config.get('performance_audit_interval', 45),
-            AuditType.BEHAVIORAL: config.get('behavioral_audit_interval', 30)
+            AuditType.REGULAR: config.get("regular_audit_interval", 30),
+            AuditType.COMPLIANCE: config.get("compliance_audit_interval", 90),
+            AuditType.SECURITY: config.get("security_audit_interval", 60),
+            AuditType.PERFORMANCE: config.get("performance_audit_interval", 45),
+            AuditType.BEHAVIORAL: config.get("behavioral_audit_interval", 30),
         }
-        
+
         logger.info("AuditManager initialized")
-    
+
     def _load_audit_templates(self) -> Dict:
         """Load audit templates from configuration"""
         templates = {}
-        
+
         # Load from config, or use defaults
-        templates[AuditType.REGULAR] = self.config.get('templates', {}).get('regular', [
-            {"id": "reg-1", "check": "agent_reporting", "weight": 1.0, "critical": False},
-            {"id": "reg-2", "check": "information_flow", "weight": 1.0, "critical": True},
-            {"id": "reg-3", "check": "permission_compliance", "weight": 1.0, "critical": True}
-        ])
-        
-        templates[AuditType.SURPRISE] = self.config.get('templates', {}).get('surprise', [
-            {"id": "surp-1", "check": "unauthorized_access", "weight": 2.0, "critical": True},
-            {"id": "surp-2", "check": "data_retention", "weight": 1.5, "critical": False},
-            {"id": "surp-3", "check": "undisclosed_capabilities", "weight": 2.0, "critical": True}
-        ])
-        
-        templates[AuditType.COMPLIANCE] = self.config.get('templates', {}).get('compliance', [
-            {"id": "comp-1", "check": "regulatory_compliance", "weight": 1.5, "critical": True},
-            {"id": "comp-2", "check": "data_handling", "weight": 1.5, "critical": True},
-            {"id": "comp-3", "check": "audit_trail", "weight": 1.0, "critical": False}
-        ])
-        
-        templates[AuditType.SECURITY] = self.config.get('templates', {}).get('security', [
-            {"id": "sec-1", "check": "vulnerability_scan", "weight": 2.0, "critical": True},
-            {"id": "sec-2", "check": "encryption_verification", "weight": 1.5, "critical": True},
-            {"id": "sec-3", "check": "access_control", "weight": 1.5, "critical": True}
-        ])
-        
-        templates[AuditType.PERFORMANCE] = self.config.get('templates', {}).get('performance', [
-            {"id": "perf-1", "check": "resource_usage", "weight": 1.0, "critical": False},
-            {"id": "perf-2", "check": "response_time", "weight": 1.0, "critical": False},
-            {"id": "perf-3", "check": "throughput", "weight": 1.0, "critical": False}
-        ])
-        
-        templates[AuditType.BEHAVIORAL] = self.config.get('templates', {}).get('behavioral', [
-            {"id": "behav-1", "check": "decision_validation", "weight": 1.5, "critical": True},
-            {"id": "behav-2", "check": "bias_detection", "weight": 1.5, "critical": True},
-            {"id": "behav-3", "check": "consistency", "weight": 1.0, "critical": False}
-        ])
-        
+        templates[AuditType.REGULAR] = self.config.get("templates", {}).get(
+            "regular",
+            [
+                {"id": "reg-1", "check": "agent_reporting", "weight": 1.0, "critical": False},
+                {"id": "reg-2", "check": "information_flow", "weight": 1.0, "critical": True},
+                {"id": "reg-3", "check": "permission_compliance", "weight": 1.0, "critical": True},
+            ],
+        )
+
+        templates[AuditType.SURPRISE] = self.config.get("templates", {}).get(
+            "surprise",
+            [
+                {"id": "surp-1", "check": "unauthorized_access", "weight": 2.0, "critical": True},
+                {"id": "surp-2", "check": "data_retention", "weight": 1.5, "critical": False},
+                {
+                    "id": "surp-3",
+                    "check": "undisclosed_capabilities",
+                    "weight": 2.0,
+                    "critical": True,
+                },
+            ],
+        )
+
+        templates[AuditType.COMPLIANCE] = self.config.get("templates", {}).get(
+            "compliance",
+            [
+                {"id": "comp-1", "check": "regulatory_compliance", "weight": 1.5, "critical": True},
+                {"id": "comp-2", "check": "data_handling", "weight": 1.5, "critical": True},
+                {"id": "comp-3", "check": "audit_trail", "weight": 1.0, "critical": False},
+            ],
+        )
+
+        templates[AuditType.SECURITY] = self.config.get("templates", {}).get(
+            "security",
+            [
+                {"id": "sec-1", "check": "vulnerability_scan", "weight": 2.0, "critical": True},
+                {
+                    "id": "sec-2",
+                    "check": "encryption_verification",
+                    "weight": 1.5,
+                    "critical": True,
+                },
+                {"id": "sec-3", "check": "access_control", "weight": 1.5, "critical": True},
+            ],
+        )
+
+        templates[AuditType.PERFORMANCE] = self.config.get("templates", {}).get(
+            "performance",
+            [
+                {"id": "perf-1", "check": "resource_usage", "weight": 1.0, "critical": False},
+                {"id": "perf-2", "check": "response_time", "weight": 1.0, "critical": False},
+                {"id": "perf-3", "check": "throughput", "weight": 1.0, "critical": False},
+            ],
+        )
+
+        templates[AuditType.BEHAVIORAL] = self.config.get("templates", {}).get(
+            "behavioral",
+            [
+                {"id": "behav-1", "check": "decision_validation", "weight": 1.5, "critical": True},
+                {"id": "behav-2", "check": "bias_detection", "weight": 1.5, "critical": True},
+                {"id": "behav-3", "check": "consistency", "weight": 1.0, "critical": False},
+            ],
+        )
+
         return templates
-    
-    async def initiate_audit(self, audit_type: AuditType, 
-                           scope: AuditScope, 
-                           target_id: str,
-                           auditor_id: str,
-                           metadata: Optional[Dict] = None) -> str:
+
+    async def initiate_audit(
+        self,
+        audit_type: AuditType,
+        scope: AuditScope,
+        target_id: str,
+        auditor_id: str,
+        metadata: Optional[Dict] = None,
+    ) -> str:
         """
         Initiate a new audit
         """
         audit_id = f"audit_{uuid.uuid4().hex}"
-        
+
         # Create audit record
         audit = AuditRecord(
             id=audit_id,
             audit_type=audit_type,
             scope=scope,
             target_id=target_id,
             auditor_id=auditor_id,
             start_time=datetime.utcnow(),
-            metadata=metadata or {}
-        )
-        
+            metadata=metadata or {},
+        )
+
         # Store in Redis
         await self.redis.set(
-            f"audit:{audit_id}", 
+            f"audit:{audit_id}",
             json.dumps(self._audit_to_dict(audit)),
-            ex=86400 * 180  # Store for 180 days
-        )
-        
+            ex=86400 * 180,  # Store for 180 days
+        )
+
         # Add to indexes
         await self.redis.sadd(f"audits:type:{audit_type}", audit_id)
         await self.redis.sadd(f"audits:scope:{scope}:{target_id}", audit_id)
-        
+
         logger.info(f"Initiated {audit_type} audit {audit_id} on {scope}:{target_id}")
-        
+
         # Start audit process in background
         asyncio.create_task(self._conduct_audit(audit_id))
-        
+
         return audit_id
-    
+
     async def _conduct_audit(self, audit_id: str) -> None:
         """
         Conduct the audit process
         """
         # Get audit record
         audit_data = await self.redis.get(f"audit:{audit_id}")
         if not audit_data:
             logger.error(f"Audit {audit_id} not found")
             return
-        
+
         audit = self._dict_to_audit(json.loads(audit_data))
-        
+
         try:
             # Get audit template
             template = self.audit_templates.get(audit.audit_type, [])
-            
+
             # Conduct checks based on template
             findings = []
             for check in template:
                 check_result = await self._run_audit_check(
-                    check["check"], 
-                    audit.scope, 
-                    audit.target_id,
-                    check["critical"]
+                    check["check"], audit.scope, audit.target_id, check["critical"]
                 )
-                
+
                 if check_result["issues"]:
                     for issue in check_result["issues"]:
                         finding = AuditFinding(
                             id=f"finding_{uuid.uuid4().hex}",
                             audit_id=audit_id,
                             severity=issue["severity"],
                             title=issue["title"],
                             description=issue["description"],
                             evidence=issue["evidence"],
-                            recommendation=issue["recommendation"]
+                            recommendation=issue["recommendation"],
                         )
                         findings.append(finding)
-            
+
             # Calculate outcome based on findings
             outcome = self._determine_audit_outcome(findings)
-            
+
             # Complete audit record
             audit.end_time = datetime.utcnow()
             audit.outcome = outcome
             audit.findings = findings
             audit.findings_count = self._count_findings_by_severity(findings)
             audit.summary = self._generate_audit_summary(audit)
-            
+
             # Schedule next audit
             audit.next_audit_date = self._schedule_next_audit(audit)
-            
+
             # Update audit record
             await self.redis.set(
                 f"audit:{audit_id}",
                 json.dumps(self._audit_to_dict(audit)),
-                ex=86400 * 180  # Store for 180 days
+                ex=86400 * 180,  # Store for 180 days
             )
-            
+
             # Store findings separately
             for finding in findings:
                 await self.redis.set(
                     f"finding:{finding.id}",
                     json.dumps(self._finding_to_dict(finding)),
-                    ex=86400 * 180  # Store for 180 days
+                    ex=86400 * 180,  # Store for 180 days
                 )
                 await self.redis.sadd(f"audit:{audit_id}:findings", finding.id)
-            
+
             # Notify about audit completion
             await self._send_audit_notifications(audit)
-            
+
             logger.info(f"Completed audit {audit_id} with outcome {outcome}")
-            
+
         except Exception as e:
             logger.error(f"Error conducting audit {audit_id}: {e}")
-            
+
             # Update audit record with error status
             audit.end_time = datetime.utcnow()
             audit.outcome = AuditOutcome.INCOMPLETE
             audit.summary = f"Audit failed to complete: {str(e)}"
-            
+
             await self.redis.set(
-                f"audit:{audit_id}",
-                json.dumps(self._audit_to_dict(audit)),
-                ex=86400 * 180
+                f"audit:{audit_id}", json.dumps(self._audit_to_dict(audit)), ex=86400 * 180
             )
-    
-    async def _run_audit_check(self, check_name: str, scope: AuditScope, 
-                            target_id: str, is_critical: bool) -> Dict:
+
+    async def _run_audit_check(
+        self, check_name: str, scope: AuditScope, target_id: str, is_critical: bool
+    ) -> Dict:
         """
         Run a specific audit check
         """
         # This would integrate with specific check implementations
         # For now, simulate check execution with randomized results
-        
+
         # Map check to handler
         check_handler = getattr(self, f"_check_{check_name}", None)
-        
+
         if check_handler and callable(check_handler):
             # Execute the actual check
             return await check_handler(scope, target_id, is_critical)
         else:
             # Simulate a check result if handler not found
             return self._simulate_check_result(check_name, is_critical)
-    
-    async def _check_agent_reporting(self, scope: AuditScope, target_id: str, is_critical: bool) -> Dict:
+
+    async def _check_agent_reporting(
+        self, scope: AuditScope, target_id: str, is_critical: bool
+    ) -> Dict:
         """Check if agent is properly reporting"""
         issues = []
-        
+
         if scope == AuditScope.AGENT:
             # Get agent reporting data
             last_report = await self._get_agent_last_report(target_id)
             reporting_frequency = await self._get_agent_reporting_frequency(target_id)
-            
+
             if not last_report:
-                issues.append({
-                    "severity": "critical" if is_critical else "high",
-                    "title": "Agent not reporting",
-                    "description": f"Agent {target_id} has no reporting history",
-                    "evidence": {"last_report": None},
-                    "recommendation": "Investigate agent communication issues"
-                })
+                issues.append(
+                    {
+                        "severity": "critical" if is_critical else "high",
+                        "title": "Agent not reporting",
+                        "description": f"Agent {target_id} has no reporting history",
+                        "evidence": {"last_report": None},
+                        "recommendation": "Investigate agent communication issues",
+                    }
+                )
             elif datetime.utcnow() - last_report > timedelta(minutes=reporting_frequency * 2):
-                issues.append({
-                    "severity": "high",
-                    "title": "Agent reporting delays",
-                    "description": f"Agent {target_id} last reported {(datetime.utcnow() - last_report).total_seconds() / 60} minutes ago",
-                    "evidence": {"last_report": last_report.isoformat(), "expected_frequency": reporting_frequency},
-                    "recommendation": "Check agent connectivity and status"
-                })
-        
+                issues.append(
+                    {
+                        "severity": "high",
+                        "title": "Agent reporting delays",
+                        "description": f"Agent {target_id} last reported {(datetime.utcnow() - last_report).total_seconds() / 60} minutes ago",
+                        "evidence": {
+                            "last_report": last_report.isoformat(),
+                            "expected_frequency": reporting_frequency,
+                        },
+                        "recommendation": "Check agent connectivity and status",
+                    }
+                )
+
         return {
             "check": "agent_reporting",
             "status": "completed",
             "issues": issues,
-            "timestamp": datetime.utcnow().isoformat()
+            "timestamp": datetime.utcnow().isoformat(),
         }
-    
-    async def _check_information_flow(self, scope: AuditScope, target_id: str, is_critical: bool) -> Dict:
+
+    async def _check_information_flow(
+        self, scope: AuditScope, target_id: str, is_critical: bool
+    ) -> Dict:
         """Check information flow compliance"""
         # Implementation would check proper information routing and permissions
         issues = []
-        
+
         # Simulate finding issues
         if random.random() < 0.2:  # 20% chance of finding an issue
-            issues.append({
-                "severity": "medium",
-                "title": "Suboptimal information routing",
-                "description": "Information is not following optimal routes between components",
-                "evidence": {"flow_analysis": {"inefficiencies": ["route_A_to_B"]}},
-                "recommendation": "Optimize routing patterns between components"
-            })
-        
+            issues.append(
+                {
+                    "severity": "medium",
+                    "title": "Suboptimal information routing",
+                    "description": "Information is not following optimal routes between components",
+                    "evidence": {"flow_analysis": {"inefficiencies": ["route_A_to_B"]}},
+                    "recommendation": "Optimize routing patterns between components",
+                }
+            )
+
         if random.random() < 0.05:  # 5% chance of finding a critical issue
-            issues.append({
-                "severity": "critical",
-                "title": "Information leakage risk",
-                "description": "Potential for sensitive data to be exposed through improper routing",
-                "evidence": {"vulnerability_scan": {"risk_points": ["api_endpoint_x"]}},
-                "recommendation": "Implement additional encryption and access controls"
-            })
-        
+            issues.append(
+                {
+                    "severity": "critical",
+                    "title": "Information leakage risk",
+                    "description": "Potential for sensitive data to be exposed through improper routing",
+                    "evidence": {"vulnerability_scan": {"risk_points": ["api_endpoint_x"]}},
+                    "recommendation": "Implement additional encryption and access controls",
+                }
+            )
+
         return {
             "check": "information_flow",
             "status": "completed",
             "issues": issues,
-            "timestamp": datetime.utcnow().isoformat()
+            "timestamp": datetime.utcnow().isoformat(),
         }
-    
+
     def _simulate_check_result(self, check_name: str, is_critical: bool) -> Dict:
         """Simulate an audit check result"""
         issues = []
-        
+
         # Simulate a 15% chance of finding an issue
         if random.random() < 0.15:
-            severity = random.choice(["low", "medium", "high", "critical" if is_critical else "high"])
-            
-            issues.append({
-                "severity": severity,
-                "title": f"Simulated {severity} issue with {check_name}",
-                "description": f"This is a simulated issue for the {check_name} check",
-                "evidence": {"simulation": True},
-                "recommendation": f"Address the {severity} {check_name} issue"
-            })
-        
+            severity = random.choice(
+                ["low", "medium", "high", "critical" if is_critical else "high"]
+            )
+
+            issues.append(
+                {
+                    "severity": severity,
+                    "title": f"Simulated {severity} issue with {check_name}",
+                    "description": f"This is a simulated issue for the {check_name} check",
+                    "evidence": {"simulation": True},
+                    "recommendation": f"Address the {severity} {check_name} issue",
+                }
+            )
+
         return {
             "check": check_name,
             "status": "completed",
             "issues": issues,
-            "timestamp": datetime.utcnow().isoformat()
+            "timestamp": datetime.utcnow().isoformat(),
         }
-    
+
     async def _get_agent_last_report(self, agent_id: str) -> Optional[datetime]:
         """Get agent's last report timestamp"""
         # This would fetch from agent reporting data
         # Simulated implementation
         try:
             timestamp = await self.redis.get(f"agent:{agent_id}:last_report")
             if timestamp:
-                return datetime.fromisoformat(timestamp.decode('utf-8'))
+                return datetime.fromisoformat(timestamp.decode("utf-8"))
             return None
         except:
             return datetime.utcnow() - timedelta(minutes=random.randint(5, 60))
-    
+
     async def _get_agent_reporting_frequency(self, agent_id: str) -> int:
         """Get agent's reporting frequency in minutes"""
         # This would fetch from agent configuration
         # Simulated implementation
         return random.choice([5, 10, 15, 30])
-    
+
     def _determine_audit_outcome(self, findings: List[AuditFinding]) -> AuditOutcome:
         """Determine audit outcome based on findings"""
         if not findings:
             return AuditOutcome.PASSED
-        
+
         # Check for critical findings
         critical_findings = [f for f in findings if f.severity == "critical"]
         if critical_findings:
             return AuditOutcome.CRITICAL
-        
+
         # Check for high severity findings
         high_findings = [f for f in findings if f.severity == "high"]
         if len(high_findings) > 2:  # More than 2 high severity findings = fail
             return AuditOutcome.FAILED
-        
+
         # If we have findings but not enough to fail
         return AuditOutcome.PASSED_WITH_FINDINGS
-    
+
     def _count_findings_by_severity(self, findings: List[AuditFinding]) -> Dict[str, int]:
         """Count findings by severity"""
         counts = {"critical": 0, "high": 0, "medium": 0, "low": 0}
-        
+
         for finding in findings:
             if finding.severity in counts:
                 counts[finding.severity] += 1
-        
+
         return counts
-    
+
     def _generate_audit_summary(self, audit: AuditRecord) -> str:
         """Generate summary of audit results"""
         if not audit.findings:
             return "No issues were found during this audit."
-        
+
         summary = f"Audit completed with {len(audit.findings)} findings: "
-        summary += ", ".join(f"{count} {severity}" for severity, count in audit.findings_count.items() if count > 0)
-        
+        summary += ", ".join(
+            f"{count} {severity}" for severity, count in audit.findings_count.items() if count > 0
+        )
+
         if audit.outcome == AuditOutcome.CRITICAL:
             summary += "\n\nCRITICAL ISSUES REQUIRE IMMEDIATE ATTENTION!"
         elif audit.outcome == AuditOutcome.FAILED:
             summary += "\n\nAudit failed due to multiple significant issues."
-        
+
         return summary
-    
+
     def _schedule_next_audit(self, audit: AuditRecord) -> datetime:
         """Schedule next audit based on outcome and type"""
         # Base interval from configuration
         base_days = self.audit_intervals.get(audit.audit_type, 30)
-        
+
         # Adjust based on outcome
         if audit.outcome == AuditOutcome.CRITICAL:
             # Critical findings - re-audit quickly
             days = max(1, base_days // 10)
         elif audit.outcome == AuditOutcome.FAILED:
@@ -450,16 +509,16 @@
             # Minor issues - re-audit at 1/2 the normal interval
             days = max(15, base_days // 2)
         else:
             # Passed clean - normal interval
             days = base_days
-        
+
         # Add some randomness to prevent predictability
         days = int(days * random.uniform(0.9, 1.1))
-        
+
         return datetime.utcnow() + timedelta(days=days)
-    
+
     async def _send_audit_notifications(self, audit: AuditRecord) -> None:
         """Send notifications about audit results"""
         # This would integrate with notification system
         notification_data = {
             "type": "audit_completed",
@@ -467,23 +526,20 @@
             "audit_type": audit.audit_type,
             "scope": audit.scope,
             "target_id": audit.target_id,
             "outcome": audit.outcome,
             "findings_count": audit.findings_count,
-            "timestamp": datetime.utcnow().isoformat()
+            "timestamp": datetime.utcnow().isoformat(),
         }
-        
+
         # Publish to audit events channel
         await self.redis.publish("audit_events", json.dumps(notification_data))
-        
+
         # Store notification
-        await self.redis.lpush(
-            "audit_notifications", 
-            json.dumps(notification_data)
-        )
+        await self.redis.lpush("audit_notifications", json.dumps(notification_data))
         await self.redis.ltrim("audit_notifications", 0, 999)  # Keep last 1000
-    
+
     def _audit_to_dict(self, audit: AuditRecord) -> Dict:
         """Convert AuditRecord to dictionary for storage"""
         return {
             "id": audit.id,
             "audit_type": audit.audit_type,
@@ -495,13 +551,13 @@
             "outcome": audit.outcome,
             "findings": [self._finding_to_dict(finding) for finding in audit.findings],
             "findings_count": audit.findings_count,
             "summary": audit.summary,
             "next_audit_date": audit.next_audit_date.isoformat() if audit.next_audit_date else None,
-            "metadata": audit.metadata
+            "metadata": audit.metadata,
         }
-    
+
     def _dict_to_audit(self, data: Dict) -> AuditRecord:
         """Convert dictionary to AuditRecord"""
         return AuditRecord(
             id=data["id"],
             audit_type=data["audit_type"],
@@ -512,14 +568,18 @@
             end_time=datetime.fromisoformat(data["end_time"]) if data.get("end_time") else None,
             outcome=data.get("outcome"),
             findings=[self._dict_to_finding(finding) for finding in data.get("findings", [])],
             findings_count=data.get("findings_count", {}),
             summary=data.get("summary", ""),
-            next_audit_date=datetime.fromisoformat(data["next_audit_date"]) if data.get("next_audit_date") else None,
-            metadata=data.get("metadata", {})
-        )
-    
+            next_audit_date=(
+                datetime.fromisoformat(data["next_audit_date"])
+                if data.get("next_audit_date")
+                else None
+            ),
+            metadata=data.get("metadata", {}),
+        )
+
     def _finding_to_dict(self, finding: AuditFinding) -> Dict:
         """Convert AuditFinding to dictionary for storage"""
         return {
             "id": finding.id,
             "audit_id": finding.audit_id,
@@ -530,13 +590,13 @@
             "recommendation": finding.recommendation,
             "assigned_to": finding.assigned_to,
             "due_date": finding.due_date.isoformat() if finding.due_date else None,
             "status": finding.status,
             "created_at": finding.created_at.isoformat(),
-            "updated_at": finding.updated_at.isoformat()
+            "updated_at": finding.updated_at.isoformat(),
         }
-    
+
     def _dict_to_finding(self, data: Dict) -> AuditFinding:
         """Convert dictionary to AuditFinding"""
         return AuditFinding(
             id=data["id"],
             audit_id=data["audit_id"],
@@ -547,50 +607,50 @@
             recommendation=data["recommendation"],
             assigned_to=data.get("assigned_to"),
             due_date=datetime.fromisoformat(data["due_date"]) if data.get("due_date") else None,
             status=data.get("status", "open"),
             created_at=datetime.fromisoformat(data["created_at"]),
-            updated_at=datetime.fromisoformat(data["updated_at"])
-        )
-    
+            updated_at=datetime.fromisoformat(data["updated_at"]),
+        )
+
     async def schedule_regular_audits(self) -> None:
         """Schedule regular audits for all targets"""
         # This would run periodically to maintain audit schedule
         while True:
             try:
                 # Get all agents that need auditing
                 agents_to_audit = await self._find_agents_needing_audit()
-                
+
                 for agent_id in agents_to_audit:
                     # Randomly determine if this should be a surprise audit
                     is_surprise = random.random() < self.surprise_audit_probability
-                    
+
                     audit_type = AuditType.SURPRISE if is_surprise else AuditType.REGULAR
-                    
+
                     # Initiate audit
                     await self.initiate_audit(
                         audit_type=audit_type,
                         scope=AuditScope.AGENT,
                         target_id=agent_id,
                         auditor_id="system",
-                        metadata={"scheduled": True, "surprise": is_surprise}
+                        metadata={"scheduled": True, "surprise": is_surprise},
                     )
-                
+
                 # Check for systems and workflows needing audits
                 # Similar to agent code above
-                
+
                 # Sleep for a while before next check
                 await asyncio.sleep(3600)  # Check hourly
-                
+
             except Exception as e:
                 logger.error(f"Error scheduling regular audits: {e}")
                 await asyncio.sleep(300)  # Wait 5 minutes before retry
-    
+
     async def _find_agents_needing_audit(self) -> List[str]:
         """Find agents that need to be audited"""
         # This would check when agents were last audited
         # For now, return a simulated list
         return ["agent_123", "agent_456"]
-    
+
     async def get_audit_report(self, audit_id: str) -> Dict:
         """Get audit report by ID"""
-        audit
\ No newline at end of file
+        audit
would reformat /home/runner/work/ymera_y/ymera_y/audit_manager.py
--- /home/runner/work/ymera_y/ymera_y/audit_system.py	2025-10-19 22:47:02.794432+00:00
+++ /home/runner/work/ymera_y/ymera_y/audit_system.py	2025-10-19 23:08:56.586885+00:00
@@ -13,98 +13,109 @@
 
 from models import AuditLog
 
 logger = logging.getLogger(__name__)
 
+
 class EnhancedAuditSystem:
     """Enhanced audit logging and compliance reporting"""
-    
+
     def __init__(self, manager):
         """Initialize with reference to manager"""
         self.manager = manager
         self.retention_period = timedelta(days=self.manager.config.get("audit_retention_days", 365))
-    
-    async def log_event(self, event_type: str, resource_type: str, 
-                      resource_id: str, action: str, performed_by: str, 
-                      details: Dict[str, Any] = None, severity: str = "info",
-                      ip_address: str = None) -> str:
+
+    async def log_event(
+        self,
+        event_type: str,
+        resource_type: str,
+        resource_id: str,
+        action: str,
+        performed_by: str,
+        details: Dict[str, Any] = None,
+        severity: str = "info",
+        ip_address: str = None,
+    ) -> str:
         """Log comprehensive audit event"""
         try:
             correlation_id = str(uuid.uuid4())
-            
+
             async with self.manager.get_db_session() as session:
                 audit_log = AuditLog(
                     event_type=event_type,
                     resource_type=resource_type,
                     resource_id=resource_id,
                     action=action,
                     performed_by=performed_by,
                     details=details or {},
                     ip_address=ip_address,
                     severity=severity,
-                    correlation_id=correlation_id
+                    correlation_id=correlation_id,
                 )
-                
+
                 session.add(audit_log)
                 await session.commit()
-                
+
                 # Update metrics
                 # self.manager.AUDIT_EVENTS.labels(event_type=event_type, severity=severity).inc()
-                
+
                 return audit_log.id
-                
+
         except Exception as e:
             logger.error(f"Failed to log audit event: {e}")
             # Fall back to regular logging if database fails
-            logger.warning(f"AUDIT: {event_type} {resource_type}:{resource_id} {action} by {performed_by}")
+            logger.warning(
+                f"AUDIT: {event_type} {resource_type}:{resource_id} {action} by {performed_by}"
+            )
             return None
-    
-    async def search_audit_logs(self, filters: Dict[str, Any] = None, 
-                              limit: int = 100, offset: int = 0) -> List[Dict[str, Any]]:
+
+    async def search_audit_logs(
+        self, filters: Dict[str, Any] = None, limit: int = 100, offset: int = 0
+    ) -> List[Dict[str, Any]]:
         """Search audit logs with filtering"""
         try:
             async with self.manager.get_db_session() as session:
                 from sqlalchemy import select, and_, or_, not_
-                
+
                 # Start with base query
                 query = select(AuditLog).order_by(AuditLog.timestamp.desc())
-                
+
                 # Apply filters if provided
                 if filters:
                     conditions = []
-                    
+
                     if "event_type" in filters:
                         conditions.append(AuditLog.event_type == filters["event_type"])
-                    
+
                     if "resource_type" in filters:
                         conditions.append(AuditLog.resource_type == filters["resource_type"])
-                    
+
                     if "resource_id" in filters:
                         conditions.append(AuditLog.resource_id == filters["resource_id"])
-                    
+
                     if "performed_by" in filters:
                         conditions.append(AuditLog.performed_by == filters["performed_by"])
-                    
+
                     if "severity" in filters:
                         conditions.append(AuditLog.severity == filters["severity"])
-                    
+
                     if "start_time" in filters:
                         conditions.append(AuditLog.timestamp >= filters["start_time"])
-                    
+
                     if "end_time" in filters:
                         conditions.append(AuditLog.timestamp <= filters["end_time"])
-                    
+
                     if conditions:
                         query = query.where(and_(*conditions))
-                
+
                 # Apply pagination
                 query = query.limit(limit).offset(offset)
-                
+
                 # Execute query
                 result = await session.execute(query)
                 logs = result.scalars().all()
-                
+
                 # Format results
                 return [
                     {
                         "id": log.id,
                         "timestamp": log.timestamp.isoformat(),
@@ -114,74 +125,70 @@
                         "action": log.action,
                         "performed_by": log.performed_by,
                         "severity": log.severity,
                         "details": log.details,
                         "ip_address": log.ip_address,
-                        "correlation_id": log.correlation_id
+                        "correlation_id": log.correlation_id,
                     }
                     for log in logs
                 ]
-                
+
         except Exception as e:
             logger.error(f"Audit log search failed: {e}")
             return []
-    
-    async def generate_compliance_report(self, start_time: datetime, 
-                                      end_time: datetime, report_type: str = "summary") -> Dict[str, Any]:
+
+    async def generate_compliance_report(
+        self, start_time: datetime, end_time: datetime, report_type: str = "summary"
+    ) -> Dict[str, Any]:
         """Generate compliance report from audit logs"""
         try:
             # Get relevant audit logs
             audit_logs = await self.search_audit_logs(
-                filters={
-                    "start_time": start_time,
-                    "end_time": end_time
-                },
-                limit=10000  # Higher limit for comprehensive reports
+                filters={"start_time": start_time, "end_time": end_time},
+                limit=10000,  # Higher limit for comprehensive reports
             )
-            
+
             if report_type == "summary":
                 return self._generate_summary_report(audit_logs, start_time, end_time)
             elif report_type == "detailed":
                 return self._generate_detailed_report(audit_logs, start_time, end_time)
             else:
-                return {
-                    "error": "Invalid report type",
-                    "valid_types": ["summary", "detailed"]
-                }
-                
+                return {"error": "Invalid report type", "valid_types": ["summary", "detailed"]}
+
         except Exception as e:
             logger.error(f"Compliance report generation failed: {e}")
             return {"error": str(e)}
-    
-    def _generate_summary_report(self, audit_logs: List[Dict[str, Any]], 
-                              start_time: datetime, end_time: datetime) -> Dict[str, Any]:
+
+    def _generate_summary_report(
+        self, audit_logs: List[Dict[str, Any]], start_time: datetime, end_time: datetime
+    ) -> Dict[str, Any]:
         """Generate summary compliance report"""
         # Count events by type
         event_counts = {}
         severity_counts = {}
         user_actions = {}
         resource_counts = {}
-        
+
         for log in audit_logs:
             # Count by event type
             event_counts[log["event_type"]] = event_counts.get(log["event_type"], 0) + 1
-            
+
             # Count by severity
             severity_counts[log["severity"]] = severity_counts.get(log["severity"], 0) + 1
-            
+
             # Count by user
             user = log["performed_by"]
             if user not in user_actions:
                 user_actions[user] = {}
-            
+
             action = log["action"]
             user_actions[user][action] = user_actions[user].get(action, 0) + 1
-            
+
             # Count by resource type
             res_type = log["resource_type"]
             resource_counts[res_type] = resource_counts.get(res_type, 0) + 1
-        
+
         # Generate report
         return {
             "report_type": "summary",
             "generated_at": datetime.utcnow().isoformat(),
             "period_start": start_time.isoformat(),
@@ -189,31 +196,32 @@
             "total_events": len(audit_logs),
             "event_type_distribution": event_counts,
             "severity_distribution": severity_counts,
             "user_activity": user_actions,
             "resource_distribution": resource_counts,
-            "high_severity_count": severity_counts.get("high", 0) + severity_counts.get("critical", 0)
+            "high_severity_count": severity_counts.get("high", 0)
+            + severity_counts.get("critical", 0),
         }
-    
+
     async def clean_old_logs(self):
         """Clean up old audit logs based on retention policy"""
         try:
             cutoff_date = datetime.utcnow() - self.retention_period
-            
+
             async with self.manager.get_db_session() as session:
                 from sqlalchemy import delete
-                
+
                 # Delete logs older than retention period
                 result = await session.execute(
                     delete(AuditLog).where(AuditLog.timestamp < cutoff_date)
                 )
-                
+
                 deleted_count = result.rowcount
                 await session.commit()
-                
+
                 logger.info(f"Cleaned {deleted_count} audit logs older than {cutoff_date}")
-                
+
                 return deleted_count
-                
+
         except Exception as e:
             logger.error(f"Audit log cleanup failed: {e}")
-            return 0
\ No newline at end of file
+            return 0
would reformat /home/runner/work/ymera_y/ymera_y/audit_system.py
--- /home/runner/work/ymera_y/ymera_y/auth.py	2025-10-19 22:47:02.794432+00:00
+++ /home/runner/work/ymera_y/ymera_y/auth.py	2025-10-19 23:08:56.704482+00:00
@@ -1,6 +1,5 @@
-
 import jwt
 from datetime import datetime, timedelta
 from typing import Optional, Dict, Any
 from fastapi import HTTPException, Security
 from fastapi.security import OAuth2PasswordBearer
@@ -10,44 +9,49 @@
 from core.config import Settings
 
 pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
 oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")
 
+
 class AuthService:
     def __init__(self, settings: Settings):
         self.settings = settings
-    
+
     def verify_password(self, plain_password: str, hashed_password: str) -> bool:
         return pwd_context.verify(plain_password, hashed_password)
-    
+
     def get_password_hash(self, password: str) -> str:
         return pwd_context.hash(password)
-    
-    def create_access_token(self, data: Dict[str, Any], expires_delta: Optional[timedelta] = None) -> str:
+
+    def create_access_token(
+        self, data: Dict[str, Any], expires_delta: Optional[timedelta] = None
+    ) -> str:
         to_encode = data.copy()
         if expires_delta:
             expire = datetime.utcnow() + expires_delta
         else:
             expire = datetime.utcnow() + timedelta(minutes=self.settings.jwt_expire_minutes)
         to_encode.update({"exp": expire, "sub": "access"})
-        encoded_jwt = jwt.encode(to_encode, self.settings.jwt_secret_key, algorithm=self.settings.jwt_algorithm)
+        encoded_jwt = jwt.encode(
+            to_encode, self.settings.jwt_secret_key, algorithm=self.settings.jwt_algorithm
+        )
         return encoded_jwt
-    
+
     def decode_access_token(self, token: str) -> Optional[Dict[str, Any]]:
         try:
-            decoded_token = jwt.decode(token, self.settings.jwt_secret_key, algorithms=[self.settings.jwt_algorithm])
+            decoded_token = jwt.decode(
+                token, self.settings.jwt_secret_key, algorithms=[self.settings.jwt_algorithm]
+            )
             return decoded_token
         except jwt.ExpiredSignatureError:
             raise HTTPException(status_code=401, detail="Token has expired")
         except jwt.InvalidTokenError:
             raise HTTPException(status_code=401, detail="Invalid token")
-    
+
     async def get_current_user(self, token: str = Security(oauth2_scheme)) -> Dict[str, Any]:
         payload = self.decode_access_token(token)
         if not payload or "user_id" not in payload:
             raise HTTPException(status_code=401, detail="Invalid authentication credentials")
-        
+
         # In a real application, you would fetch the user from the database here
         # For now, we'll return a mock user
         return {"id": UUID(payload["user_id"]), "email": payload["email"], "role": payload["role"]}
-
-
would reformat /home/runner/work/ymera_y/ymera_y/auth.py
error: cannot format /home/runner/work/ymera_y/ymera_y/automl__init__.py: Cannot parse for target version Python 3.12: 2:0:             if score > best_score:
--- /home/runner/work/ymera_y/ymera_y/audit_scripts/comprehensive_audit.py	2025-10-19 22:56:36.660881+00:00
+++ /home/runner/work/ymera_y/ymera_y/audit_scripts/comprehensive_audit.py	2025-10-19 23:08:57.253087+00:00
@@ -4,11 +4,11 @@
 ==========================================
 Phase 2: Testing & Quality Audit Implementation
 
 This script executes ALL audit tasks with brutal honesty:
 - Task 2.1: Full Test Suite Execution
-- Task 2.2: Code Quality Analysis  
+- Task 2.2: Code Quality Analysis
 - Task 2.3: Dependency Audit
 - Task 2.4: Performance Profiling
 
 Author: YMERA Quality Assurance Team
 Version: 1.0.0
@@ -28,10 +28,11 @@
 
 
 @dataclass
 class TestResult:
     """Individual test result"""
+
     test: str
     file: str
     status: str
     duration: float
     error: Optional[str] = None
@@ -42,167 +43,167 @@
 
 
 @dataclass
 class CoverageGap:
     """Coverage gap information"""
+
     file: str
     coverage: float
     missing_lines: List[int]
     recommendation: str
 
 
 @dataclass
 class QualityIssue:
     """Code quality issue"""
+
     code: str
     description: str
     count: int
     severity: str = "info"
 
 
 @dataclass
 class SecurityIssue:
     """Security issue"""
+
     severity: str
     file: str
     line: int
     issue: str
     recommendation: str
 
 
 @dataclass
 class DependencyIssue:
     """Dependency issue"""
+
     package: str
     current_version: str
     issue_type: str  # vulnerability, deprecated, outdated
     severity: str
     recommendation: str
     details: Dict[str, Any]
 
 
 class PlatformAuditor:
     """Main auditor orchestrating all audit tasks"""
-    
+
     def __init__(self, project_root: Path):
         self.project_root = project_root
         self.audit_dir = project_root / "audit_reports"
         self.timestamp = datetime.now().isoformat()
         self.results = {
             "execution_timestamp": self.timestamp,
             "tasks_completed": [],
-            "tasks_failed": []
+            "tasks_failed": [],
         }
-        
+
     def run_all_audits(self):
         """Execute all audit tasks"""
         print("=" * 80)
         print("YMERA PLATFORM COMPREHENSIVE AUDIT")
         print("=" * 80)
         print(f"Timestamp: {self.timestamp}")
         print(f"Project Root: {self.project_root}")
         print("=" * 80)
-        
+
         tasks = [
             ("Task 2.1: Test Suite Execution", self.run_test_audit),
             ("Task 2.2: Code Quality Analysis", self.run_quality_audit),
             ("Task 2.3: Dependency Audit", self.run_dependency_audit),
-            ("Task 2.4: Performance Profiling", self.run_performance_audit)
+            ("Task 2.4: Performance Profiling", self.run_performance_audit),
         ]
-        
+
         for task_name, task_func in tasks:
             print(f"\n{'=' * 80}")
             print(f"Starting: {task_name}")
             print(f"{'=' * 80}")
             try:
                 task_func()
                 self.results["tasks_completed"].append(task_name)
                 print(f" {task_name} completed successfully")
             except Exception as e:
-                self.results["tasks_failed"].append({
-                    "task": task_name,
-                    "error": str(e),
-                    "traceback": traceback.format_exc()
-                })
+                self.results["tasks_failed"].append(
+                    {"task": task_name, "error": str(e), "traceback": traceback.format_exc()}
+                )
                 print(f" {task_name} failed: {e}")
                 traceback.print_exc()
-        
+
         self._generate_master_summary()
         print("\n" + "=" * 80)
         print("AUDIT COMPLETE")
         print("=" * 80)
         print(f"Completed: {len(self.results['tasks_completed'])}/4")
         print(f"Failed: {len(self.results['tasks_failed'])}/4")
         print(f"Reports saved to: {self.audit_dir}")
-        
+
     def run_test_audit(self):
         """Task 2.1: Execute full test suite with detailed reporting"""
         test_dir = self.audit_dir / "testing"
         test_dir.mkdir(exist_ok=True)
-        
+
         print("\n Running pytest with coverage...")
-        
+
         # Install pytest-json-report if needed
         try:
-            subprocess.run([sys.executable, "-m", "pip", "install", "pytest-json-report", "-q"], check=False)
+            subprocess.run(
+                [sys.executable, "-m", "pip", "install", "pytest-json-report", "-q"], check=False
+            )
         except:
             pass
-        
+
         # Run pytest with all options
         pytest_cmd = [
-            sys.executable, "-m", "pytest",
+            sys.executable,
+            "-m",
+            "pytest",
             "-v",
             "--cov=.",
             "--cov-report=html:" + str(test_dir / "htmlcov"),
             "--cov-report=json:" + str(test_dir / "coverage.json"),
             "--cov-report=term",
             "--tb=short",
             "--maxfail=999",
             "--junit-xml=" + str(test_dir / "junit.xml"),
         ]
-        
+
         # Try to add json report if plugin available
         try:
-            pytest_cmd.extend([
-                "--json-report",
-                "--json-report-file=" + str(test_dir / "pytest_report.json")
-            ])
+            pytest_cmd.extend(
+                ["--json-report", "--json-report-file=" + str(test_dir / "pytest_report.json")]
+            )
         except:
             pass
-        
-        result = subprocess.run(
-            pytest_cmd,
-            cwd=self.project_root,
-            capture_output=True,
-            text=True
-        )
-        
+
+        result = subprocess.run(pytest_cmd, cwd=self.project_root, capture_output=True, text=True)
+
         # Save raw output
         (test_dir / "pytest_output.txt").write_text(result.stdout + "\n\n" + result.stderr)
-        
+
         # Parse results
         test_summary = self._parse_pytest_output(result.stdout, result.stderr, result.returncode)
         coverage_data = self._parse_coverage_data(test_dir / "coverage.json")
-        
+
         # Generate JSON report
         test_audit_report = {
             "execution_timestamp": self.timestamp,
             "summary": test_summary,
             "failures": self._categorize_failures(result.stdout, result.stderr),
             "coverage_gaps": self._identify_coverage_gaps(coverage_data),
-            "raw_exit_code": result.returncode
+            "raw_exit_code": result.returncode,
         }
-        
-        (test_dir / "test_audit_report.json").write_text(
-            json.dumps(test_audit_report, indent=2)
-        )
-        
+
+        (test_dir / "test_audit_report.json").write_text(json.dumps(test_audit_report, indent=2))
+
         # Generate Markdown report
         self._generate_test_markdown_report(test_audit_report, test_dir)
-        
-        print(f" Test audit complete: {test_summary.get('passed', 0)}/{test_summary.get('total_tests', 0)} passed")
-        
+
+        print(
+            f" Test audit complete: {test_summary.get('passed', 0)}/{test_summary.get('total_tests', 0)} passed"
+        )
+
     def _parse_pytest_output(self, stdout: str, stderr: str, exit_code: int) -> Dict[str, Any]:
         """Parse pytest output for summary statistics"""
         summary = {
             "total_tests": 0,
             "passed": 0,
@@ -210,91 +211,99 @@
             "skipped": 0,
             "errors": 0,
             "pass_rate": 0.0,
             "total_duration": 0.0,
             "overall_coverage": 0.0,
-            "exit_code": exit_code
+            "exit_code": exit_code,
         }
-        
+
         # Extract test counts from pytest output
         output = stdout + stderr
-        
+
         # Look for summary line like "10 passed, 2 failed, 1 skipped"
-        summary_pattern = r'(\d+)\s+passed|(\d+)\s+failed|(\d+)\s+skipped|(\d+)\s+error'
+        summary_pattern = r"(\d+)\s+passed|(\d+)\s+failed|(\d+)\s+skipped|(\d+)\s+error"
         for match in re.finditer(summary_pattern, output):
-            for i, category in enumerate(['passed', 'failed', 'skipped', 'error']):
+            for i, category in enumerate(["passed", "failed", "skipped", "error"]):
                 if match.group(i + 1):
-                    summary[category + 's' if category == 'error' else category] = int(match.group(i + 1))
-        
+                    summary[category + "s" if category == "error" else category] = int(
+                        match.group(i + 1)
+                    )
+
         # Calculate totals
-        summary["total_tests"] = summary["passed"] + summary["failed"] + summary["skipped"] + summary["errors"]
+        summary["total_tests"] = (
+            summary["passed"] + summary["failed"] + summary["skipped"] + summary["errors"]
+        )
         if summary["total_tests"] > 0:
             summary["pass_rate"] = (summary["passed"] / summary["total_tests"]) * 100
-        
+
         # Extract duration
-        duration_match = re.search(r'in\s+([\d.]+)s', output)
+        duration_match = re.search(r"in\s+([\d.]+)s", output)
         if duration_match:
             summary["total_duration"] = float(duration_match.group(1))
-        
+
         # Extract coverage percentage
-        coverage_match = re.search(r'TOTAL.*?(\d+)%', output)
+        coverage_match = re.search(r"TOTAL.*?(\d+)%", output)
         if coverage_match:
             summary["overall_coverage"] = float(coverage_match.group(1))
-        
+
         return summary
-    
+
     def _parse_coverage_data(self, coverage_file: Path) -> Dict[str, Any]:
         """Parse coverage.json file"""
         if not coverage_file.exists():
             return {"files": {}}
-        
+
         try:
             return json.loads(coverage_file.read_text())
         except Exception as e:
             print(f"Warning: Could not parse coverage data: {e}")
             return {"files": {}}
-    
+
     def _categorize_failures(self, stdout: str, stderr: str) -> List[Dict[str, Any]]:
         """Extract and categorize test failures"""
         failures = []
         output = stdout + stderr
-        
+
         # Parse FAILED lines from pytest output
-        failure_pattern = r'FAILED\s+([^\s]+)\s+-\s+(.*)'
+        failure_pattern = r"FAILED\s+([^\s]+)\s+-\s+(.*)"
         for match in re.finditer(failure_pattern, output, re.MULTILINE):
             test_path = match.group(1)
             error_msg = match.group(2)
-            
+
             # Determine severity
             severity = "MEDIUM"
             if "Connection" in error_msg or "Redis" in error_msg or "Database" in error_msg:
                 severity = "HIGH"
             elif "Deprecated" in error_msg or "Warning" in error_msg:
                 severity = "LOW"
-            
-            failures.append({
-                "test": test_path.split("::")[-1] if "::" in test_path else test_path,
-                "file": test_path,
-                "error": error_msg,
-                "severity": severity,
-                "impact": self._determine_impact(error_msg),
-                "recommendation": self._suggest_fix(error_msg)
-            })
-        
+
+            failures.append(
+                {
+                    "test": test_path.split("::")[-1] if "::" in test_path else test_path,
+                    "file": test_path,
+                    "error": error_msg,
+                    "severity": severity,
+                    "impact": self._determine_impact(error_msg),
+                    "recommendation": self._suggest_fix(error_msg),
+                }
+            )
+
         return failures
-    
+
     def _determine_impact(self, error_msg: str) -> str:
         """Determine impact of a failure"""
-        if any(word in error_msg.lower() for word in ["connection", "redis", "database", "postgresql"]):
+        if any(
+            word in error_msg.lower() for word in ["connection", "redis", "database", "postgresql"]
+        ):
             return "Critical infrastructure component non-functional"
         elif any(word in error_msg.lower() for word in ["auth", "security", "token"]):
             return "Security/authentication system affected"
         elif "import" in error_msg.lower() or "module" in error_msg.lower():
             return "Module dependency or import issue"
         else:
             return "Functionality may be impaired"
-    
+
     def _suggest_fix(self, error_msg: str) -> str:
         """Suggest fix for common errors"""
         error_lower = error_msg.lower()
         if "redis" in error_lower:
             return "Verify Redis service is running or update connection config"
@@ -304,42 +313,48 @@
             return "Check requirements.txt and ensure all dependencies are installed"
         elif "deprecated" in error_lower:
             return "Update code to use non-deprecated APIs"
         else:
             return "Review test code and fix assertions or setup"
-    
+
     def _identify_coverage_gaps(self, coverage_data: Dict[str, Any]) -> List[Dict[str, Any]]:
         """Identify files with low coverage"""
         gaps = []
         files = coverage_data.get("files", {})
-        
+
         for filepath, file_data in files.items():
             summary = file_data.get("summary", {})
             percent_covered = summary.get("percent_covered", 0)
-            
+
             if percent_covered < 80:  # Threshold for reporting
                 missing_lines = file_data.get("missing_lines", [])
-                gaps.append({
-                    "file": filepath,
-                    "coverage": round(percent_covered, 2),
-                    "missing_lines": missing_lines[:10] if len(missing_lines) <= 10 else [f"{missing_lines[0]}-{missing_lines[-1]}"],
-                    "recommendation": self._coverage_recommendation(percent_covered, filepath)
-                })
-        
+                gaps.append(
+                    {
+                        "file": filepath,
+                        "coverage": round(percent_covered, 2),
+                        "missing_lines": (
+                            missing_lines[:10]
+                            if len(missing_lines) <= 10
+                            else [f"{missing_lines[0]}-{missing_lines[-1]}"]
+                        ),
+                        "recommendation": self._coverage_recommendation(percent_covered, filepath),
+                    }
+                )
+
         # Sort by coverage (worst first)
         gaps.sort(key=lambda x: x["coverage"])
         return gaps[:20]  # Top 20 worst files
-    
+
     def _coverage_recommendation(self, coverage: float, filepath: str) -> str:
         """Generate coverage improvement recommendation"""
         if coverage < 20:
             return f"CRITICAL: Add comprehensive unit tests for {filepath}"
         elif coverage < 50:
             return f"Add unit tests for error handling and edge cases"
         else:
             return f"Add tests for uncovered branches and error paths"
-    
+
     def _generate_test_markdown_report(self, report_data: Dict[str, Any], test_dir: Path):
         """Generate markdown test audit report"""
         md_lines = [
             "# Test Audit Report",
             f"\n**Execution Timestamp:** {report_data['execution_timestamp']}",
@@ -349,577 +364,659 @@
             f"- **Failed:** {report_data['summary']['failed']} ",
             f"- **Skipped:** {report_data['summary']['skipped']} ",
             f"- **Pass Rate:** {report_data['summary']['pass_rate']:.1f}%",
             f"- **Duration:** {report_data['summary']['total_duration']:.2f}s",
             f"- **Coverage:** {report_data['summary']['overall_coverage']:.1f}%",
-            "\n## Pass/Fail Breakdown by Category\n"
+            "\n## Pass/Fail Breakdown by Category\n",
         ]
-        
+
         # Status assessment
-        pass_rate = report_data['summary']['pass_rate']
+        pass_rate = report_data["summary"]["pass_rate"]
         if pass_rate >= 95:
             status = " EXCELLENT"
         elif pass_rate >= 80:
             status = " GOOD"
         elif pass_rate >= 60:
             status = " NEEDS IMPROVEMENT"
         else:
             status = " CRITICAL"
-        
+
         md_lines.append(f"\n**Overall Status:** {status}\n")
-        
+
         # Critical failures
-        if report_data['failures']:
+        if report_data["failures"]:
             md_lines.append("\n## Critical Failures (HIGH Severity)\n")
-            high_severity = [f for f in report_data['failures'] if f.get('severity') == 'HIGH']
+            high_severity = [f for f in report_data["failures"] if f.get("severity") == "HIGH"]
             if high_severity:
                 for failure in high_severity:
-                    md_lines.extend([
-                        f"\n### {failure['test']}",
-                        f"- **File:** `{failure['file']}`",
-                        f"- **Error:** {failure['error']}",
-                        f"- **Impact:** {failure['impact']}",
-                        f"- **Recommendation:** {failure['recommendation']}"
-                    ])
+                    md_lines.extend(
+                        [
+                            f"\n### {failure['test']}",
+                            f"- **File:** `{failure['file']}`",
+                            f"- **Error:** {failure['error']}",
+                            f"- **Impact:** {failure['impact']}",
+                            f"- **Recommendation:** {failure['recommendation']}",
+                        ]
+                    )
             else:
                 md_lines.append("\n*No high severity failures*\n")
-        
+
         # Coverage gaps
-        if report_data['coverage_gaps']:
+        if report_data["coverage_gaps"]:
             md_lines.append("\n## Coverage Report Summary\n")
             md_lines.append("\n### Files with Low Coverage (<80%)\n")
-            for gap in report_data['coverage_gaps'][:10]:
+            for gap in report_data["coverage_gaps"][:10]:
                 md_lines.append(
                     f"- **{gap['file']}**: {gap['coverage']:.1f}% - {gap['recommendation']}"
                 )
-        
+
         # Actionable recommendations
-        md_lines.extend([
-            "\n## Actionable Recommendations\n",
-            "\n### Immediate Actions Required\n"
-        ])
-        
-        high_prio_failures = [f for f in report_data['failures'] if f.get('severity') == 'HIGH']
+        md_lines.extend(["\n## Actionable Recommendations\n", "\n### Immediate Actions Required\n"])
+
+        high_prio_failures = [f for f in report_data["failures"] if f.get("severity") == "HIGH"]
         if high_prio_failures:
             md_lines.append(f"1. **Fix {len(high_prio_failures)} HIGH severity test failures**")
             for f in high_prio_failures[:3]:
                 md_lines.append(f"   - {f['test']}: {f['recommendation']}")
-        
-        critical_coverage = [g for g in report_data['coverage_gaps'] if g['coverage'] < 20]
+
+        critical_coverage = [g for g in report_data["coverage_gaps"] if g["coverage"] < 20]
         if critical_coverage:
-            md_lines.append(f"2. **Address {len(critical_coverage)} files with critical coverage gaps (<20%)**")
-        
-        if report_data['summary']['pass_rate'] < 80:
+            md_lines.append(
+                f"2. **Address {len(critical_coverage)} files with critical coverage gaps (<20%)**"
+            )
+
+        if report_data["summary"]["pass_rate"] < 80:
             md_lines.append("3. **Improve test pass rate to at least 80%**")
-        
+
         (test_dir / "test_audit_report.md").write_text("\n".join(md_lines))
-    
+
     def run_quality_audit(self):
         """Task 2.2: Code Quality Analysis"""
         quality_dir = self.audit_dir / "quality"
         quality_dir.mkdir(exist_ok=True)
-        
+
         print("\n Running code quality checks...")
-        
+
         quality_results = {
             "timestamp": self.timestamp,
             "style_issues": {},
             "type_coverage": {},
             "security_issues": {},
-            "formatting_issues": {}
+            "formatting_issues": {},
         }
-        
+
         # Run flake8
         print("  - Running flake8...")
         try:
             flake8_result = subprocess.run(
-                [sys.executable, "-m", "flake8", ".", "--output-file", str(quality_dir / "flake8_report.txt"), "--statistics", "--exit-zero"],
+                [
+                    sys.executable,
+                    "-m",
+                    "flake8",
+                    ".",
+                    "--output-file",
+                    str(quality_dir / "flake8_report.txt"),
+                    "--statistics",
+                    "--exit-zero",
+                ],
                 cwd=self.project_root,
                 capture_output=True,
-                text=True
-            )
-            quality_results["style_issues"] = self._parse_flake8_output(quality_dir / "flake8_report.txt")
+                text=True,
+            )
+            quality_results["style_issues"] = self._parse_flake8_output(
+                quality_dir / "flake8_report.txt"
+            )
         except Exception as e:
             print(f"    Warning: flake8 error: {e}")
-        
+
         # Run mypy
         print("  - Running mypy...")
         try:
             mypy_result = subprocess.run(
-                [sys.executable, "-m", "mypy", ".", "--html-report", str(quality_dir / "mypy_html"), "--junit-xml", str(quality_dir / "mypy_results.xml"), "--no-error-summary"],
+                [
+                    sys.executable,
+                    "-m",
+                    "mypy",
+                    ".",
+                    "--html-report",
+                    str(quality_dir / "mypy_html"),
+                    "--junit-xml",
+                    str(quality_dir / "mypy_results.xml"),
+                    "--no-error-summary",
+                ],
                 cwd=self.project_root,
                 capture_output=True,
-                text=True
-            )
-            (quality_dir / "mypy_output.txt").write_text(mypy_result.stdout + "\n" + mypy_result.stderr)
-            quality_results["type_coverage"] = self._parse_mypy_output(mypy_result.stdout + mypy_result.stderr)
+                text=True,
+            )
+            (quality_dir / "mypy_output.txt").write_text(
+                mypy_result.stdout + "\n" + mypy_result.stderr
+            )
+            quality_results["type_coverage"] = self._parse_mypy_output(
+                mypy_result.stdout + mypy_result.stderr
+            )
         except Exception as e:
             print(f"    Warning: mypy error: {e}")
-        
+
         # Run black
         print("  - Running black...")
         try:
             black_result = subprocess.run(
                 [sys.executable, "-m", "black", ".", "--check", "--diff"],
                 cwd=self.project_root,
                 capture_output=True,
-                text=True
+                text=True,
             )
             (quality_dir / "black_report.txt").write_text(black_result.stdout)
-            quality_results["formatting_issues"] = self._parse_black_output(black_result.stdout, black_result.returncode)
+            quality_results["formatting_issues"] = self._parse_black_output(
+                black_result.stdout, black_result.returncode
+            )
         except Exception as e:
             print(f"    Warning: black error: {e}")
-        
+
         # Run bandit
         print("  - Running bandit...")
         try:
             bandit_result = subprocess.run(
-                [sys.executable, "-m", "bandit", "-r", ".", "-f", "json", "-o", str(quality_dir / "security_report.json"), "-ll"],
+                [
+                    sys.executable,
+                    "-m",
+                    "bandit",
+                    "-r",
+                    ".",
+                    "-f",
+                    "json",
+                    "-o",
+                    str(quality_dir / "security_report.json"),
+                    "-ll",
+                ],
                 cwd=self.project_root,
                 capture_output=True,
-                text=True
-            )
-            quality_results["security_issues"] = self._parse_bandit_output(quality_dir / "security_report.json")
+                text=True,
+            )
+            quality_results["security_issues"] = self._parse_bandit_output(
+                quality_dir / "security_report.json"
+            )
         except Exception as e:
             print(f"    Warning: bandit error: {e}")
-        
+
         # Generate reports
         (quality_dir / "code_quality_report.json").write_text(json.dumps(quality_results, indent=2))
         self._generate_quality_markdown_report(quality_results, quality_dir)
-        
+
         print(f" Quality audit complete")
-    
+
     def _parse_flake8_output(self, flake8_file: Path) -> Dict[str, Any]:
         """Parse flake8 output"""
         if not flake8_file.exists():
             return {"total": 0, "by_severity": {}, "top_violations": []}
-        
+
         content = flake8_file.read_text()
         violations = {}
         total = 0
-        
+
         # Count violations by code
-        for line in content.split('\n'):
-            match = re.search(r'([A-Z]\d{3})', line)
+        for line in content.split("\n"):
+            match = re.search(r"([A-Z]\d{3})", line)
             if match:
                 code = match.group(1)
                 violations[code] = violations.get(code, 0) + 1
                 total += 1
-        
+
         # Get top violations
         top_violations = sorted(violations.items(), key=lambda x: x[1], reverse=True)[:10]
-        
+
         return {
             "total": total,
             "by_severity": self._categorize_flake8_severity(violations),
             "top_violations": [
                 {"code": code, "description": self._flake8_description(code), "count": count}
                 for code, count in top_violations
-            ]
+            ],
         }
-    
+
     def _categorize_flake8_severity(self, violations: Dict[str, int]) -> Dict[str, int]:
         """Categorize flake8 violations by severity"""
         severity = {"error": 0, "warning": 0, "info": 0}
         for code, count in violations.items():
-            if code.startswith('E') and int(code[1:]) >= 9:
+            if code.startswith("E") and int(code[1:]) >= 9:
                 severity["error"] += count
-            elif code.startswith('E') or code.startswith('F'):
+            elif code.startswith("E") or code.startswith("F"):
                 severity["warning"] += count
             else:
                 severity["info"] += count
         return severity
-    
+
     def _flake8_description(self, code: str) -> str:
         """Get description for flake8 code"""
         descriptions = {
             "E501": "line too long",
             "F401": "imported but unused",
             "E302": "expected 2 blank lines",
             "E305": "expected 2 blank lines after class or function",
             "W503": "line break before binary operator",
             "E303": "too many blank lines",
-            "F841": "local variable assigned but never used"
+            "F841": "local variable assigned but never used",
         }
         return descriptions.get(code, "style violation")
-    
+
     def _parse_mypy_output(self, output: str) -> Dict[str, Any]:
         """Parse mypy output"""
         # Count functions and type coverage
-        error_count = len(re.findall(r'error:', output))
-        
+        error_count = len(re.findall(r"error:", output))
+
         return {
             "total_functions": 0,  # Would need AST parsing for accurate count
             "typed_functions": 0,
             "type_coverage_percent": 0.0,
             "error_count": error_count,
-            "files_without_types": []
+            "files_without_types": [],
         }
-    
+
     def _parse_black_output(self, output: str, exit_code: int) -> Dict[str, Any]:
         """Parse black output"""
         # Count files that would be reformatted
-        files_needing_format = len(re.findall(r'would reformat', output))
-        
+        files_needing_format = len(re.findall(r"would reformat", output))
+
         return {
             "files_needing_format": files_needing_format,
             "total_lines_to_change": 0,  # black doesn't provide this
-            "all_formatted": exit_code == 0
+            "all_formatted": exit_code == 0,
         }
-    
+
     def _parse_bandit_output(self, bandit_file: Path) -> Dict[str, Any]:
         """Parse bandit security report"""
         if not bandit_file.exists():
             return {"high": 0, "medium": 0, "low": 0, "issues": []}
-        
+
         try:
             data = json.loads(bandit_file.read_text())
             results = data.get("results", [])
-            
+
             severity_counts = {"high": 0, "medium": 0, "low": 0}
             issues = []
-            
+
             for result in results:
                 severity = result.get("issue_severity", "LOW").lower()
                 severity_counts[severity] = severity_counts.get(severity, 0) + 1
-                
+
                 if severity in ["high", "medium"]:
-                    issues.append({
-                        "severity": severity.upper(),
-                        "file": result.get("filename", ""),
-                        "line": result.get("line_number", 0),
-                        "issue": result.get("issue_text", ""),
-                        "recommendation": result.get("issue_text", "")
-                    })
-            
+                    issues.append(
+                        {
+                            "severity": severity.upper(),
+                            "file": result.get("filename", ""),
+                            "line": result.get("line_number", 0),
+                            "issue": result.get("issue_text", ""),
+                            "recommendation": result.get("issue_text", ""),
+                        }
+                    )
+
             return {
                 "high": severity_counts.get("high", 0),
                 "medium": severity_counts.get("medium", 0),
                 "low": severity_counts.get("low", 0),
-                "issues": issues[:10]  # Top 10
+                "issues": issues[:10],  # Top 10
             }
         except Exception as e:
             print(f"Warning: Could not parse bandit output: {e}")
             return {"high": 0, "medium": 0, "low": 0, "issues": []}
-    
+
     def _generate_quality_markdown_report(self, quality_data: Dict[str, Any], quality_dir: Path):
         """Generate markdown quality report"""
         # Calculate quality score
         style_issues = quality_data.get("style_issues", {}).get("total", 0)
         security_high = quality_data.get("security_issues", {}).get("high", 0)
         security_medium = quality_data.get("security_issues", {}).get("medium", 0)
-        
+
         # Simple scoring (0-100)
         quality_score = 100
         quality_score -= min(50, style_issues / 10)  # -0.1 per style issue, max -50
         quality_score -= security_high * 10  # -10 per high security issue
         quality_score -= security_medium * 5  # -5 per medium security issue
         quality_score = max(0, quality_score)
-        
+
         md_lines = [
             "# Code Quality Report",
             f"\n**Timestamp:** {quality_data['timestamp']}",
             f"\n## Quality Score: {quality_score:.1f}/100\n",
         ]
-        
+
         # Status
         if quality_score >= 90:
             status = " EXCELLENT"
         elif quality_score >= 75:
             status = " GOOD"
         elif quality_score >= 50:
             status = " NEEDS IMPROVEMENT"
         else:
             status = " CRITICAL"
-        
+
         md_lines.append(f"**Status:** {status}\n")
-        
+
         # Critical issues
         md_lines.append("\n## Critical Issues Requiring Immediate Attention\n")
-        
+
         security_issues = quality_data.get("security_issues", {})
         if security_issues.get("high", 0) > 0:
             md_lines.append(f"###  HIGH Security Issues: {security_issues['high']}\n")
             for issue in security_issues.get("issues", [])[:5]:
                 if issue["severity"] == "HIGH":
-                    md_lines.extend([
-                        f"- **{issue['file']}:{issue['line']}**",
-                        f"  - {issue['issue']}",
-                        f"  - Recommendation: {issue['recommendation']}\n"
-                    ])
-        
+                    md_lines.extend(
+                        [
+                            f"- **{issue['file']}:{issue['line']}**",
+                            f"  - {issue['issue']}",
+                            f"  - Recommendation: {issue['recommendation']}\n",
+                        ]
+                    )
+
         # Style issues
         style = quality_data.get("style_issues", {})
         if style.get("total", 0) > 0:
-            md_lines.extend([
-                f"\n## Style Issues: {style['total']}\n",
-                "\n### Top Violations:\n"
-            ])
+            md_lines.extend([f"\n## Style Issues: {style['total']}\n", "\n### Top Violations:\n"])
             for violation in style.get("top_violations", [])[:5]:
-                md_lines.append(f"- **{violation['code']}** ({violation['description']}): {violation['count']} occurrences")
-        
+                md_lines.append(
+                    f"- **{violation['code']}** ({violation['description']}): {violation['count']} occurrences"
+                )
+
         # Technical debt
-        md_lines.extend([
-            "\n## Technical Debt Assessment\n",
-            f"- **Style Issues:** {style.get('total', 0)}",
-            f"- **Security Issues:** {security_issues.get('high', 0) + security_issues.get('medium', 0) + security_issues.get('low', 0)}",
-            f"- **Formatting Issues:** {quality_data.get('formatting_issues', {}).get('files_needing_format', 0)} files need formatting",
-        ])
-        
+        md_lines.extend(
+            [
+                "\n## Technical Debt Assessment\n",
+                f"- **Style Issues:** {style.get('total', 0)}",
+                f"- **Security Issues:** {security_issues.get('high', 0) + security_issues.get('medium', 0) + security_issues.get('low', 0)}",
+                f"- **Formatting Issues:** {quality_data.get('formatting_issues', {}).get('files_needing_format', 0)} files need formatting",
+            ]
+        )
+
         # Refactoring recommendations
-        md_lines.extend([
-            "\n## Refactoring Recommendations\n",
-            "1. Run `black .` to auto-format code",
-            "2. Fix HIGH severity security issues immediately",
-            "3. Address top style violations (see flake8_report.txt)",
-            "4. Add type hints to improve type coverage"
-        ])
-        
+        md_lines.extend(
+            [
+                "\n## Refactoring Recommendations\n",
+                "1. Run `black .` to auto-format code",
+                "2. Fix HIGH severity security issues immediately",
+                "3. Address top style violations (see flake8_report.txt)",
+                "4. Add type hints to improve type coverage",
+            ]
+        )
+
         (quality_dir / "code_quality_report.md").write_text("\n".join(md_lines))
-    
+
     def run_dependency_audit(self):
         """Task 2.3: Dependency Audit"""
         dep_dir = self.audit_dir / "dependencies"
         dep_dir.mkdir(exist_ok=True)
-        
+
         print("\n Running dependency audit...")
-        
+
         # Run pip-audit
         print("  - Running pip-audit...")
         try:
             audit_result = subprocess.run(
-                [sys.executable, "-m", "pip_audit", "--format", "json", "--output", str(dep_dir / "security_audit.json")],
+                [
+                    sys.executable,
+                    "-m",
+                    "pip_audit",
+                    "--format",
+                    "json",
+                    "--output",
+                    str(dep_dir / "security_audit.json"),
+                ],
                 cwd=self.project_root,
                 capture_output=True,
-                text=True
+                text=True,
             )
         except Exception as e:
             print(f"    Warning: pip-audit error: {e}")
             # Create empty file
             (dep_dir / "security_audit.json").write_text("[]")
-        
+
         # Check for outdated packages
         print("  - Checking for outdated packages...")
         try:
             outdated_result = subprocess.run(
                 [sys.executable, "-m", "pip", "list", "--outdated", "--format", "json"],
                 cwd=self.project_root,
                 capture_output=True,
-                text=True
+                text=True,
             )
             (dep_dir / "outdated_packages.json").write_text(outdated_result.stdout or "[]")
         except Exception as e:
             print(f"    Warning: pip list error: {e}")
             (dep_dir / "outdated_packages.json").write_text("[]")
-        
+
         # Generate dependency tree
         print("  - Generating dependency tree...")
         try:
             tree_result = subprocess.run(
                 [sys.executable, "-m", "pipdeptree", "--json"],
                 cwd=self.project_root,
                 capture_output=True,
-                text=True
+                text=True,
             )
             (dep_dir / "dependency_tree.json").write_text(tree_result.stdout or "[]")
         except Exception as e:
             print(f"    Warning: pipdeptree error: {e}")
             (dep_dir / "dependency_tree.json").write_text("[]")
-        
+
         # Parse and generate report
         dependency_report = self._generate_dependency_report(dep_dir)
-        (dep_dir / "dependency_audit_report.json").write_text(json.dumps(dependency_report, indent=2))
+        (dep_dir / "dependency_audit_report.json").write_text(
+            json.dumps(dependency_report, indent=2)
+        )
         self._generate_dependency_markdown_report(dependency_report, dep_dir)
-        
+
         print(f" Dependency audit complete")
-    
+
     def _generate_dependency_report(self, dep_dir: Path) -> Dict[str, Any]:
         """Generate comprehensive dependency report"""
         # Load data
         security_vulns = self._load_json(dep_dir / "security_audit.json", [])
         outdated = self._load_json(dep_dir / "outdated_packages.json", [])
-        
+
         # Check for deprecated packages
         deprecated_packages = self._check_deprecated_packages()
-        
+
         # Count total dependencies
         requirements_file = self.project_root / "requirements.txt"
         total_deps = 0
         if requirements_file.exists():
-            total_deps = len([l for l in requirements_file.read_text().split('\n') if l.strip() and not l.startswith('#')])
-        
+            total_deps = len(
+                [
+                    l
+                    for l in requirements_file.read_text().split("\n")
+                    if l.strip() and not l.startswith("#")
+                ]
+            )
+
         return {
             "total_dependencies": total_deps,
             "security_vulnerabilities": self._parse_security_vulnerabilities(security_vulns),
             "deprecated_packages": deprecated_packages,
             "outdated_packages": self._parse_outdated_packages(outdated),
-            "missing_dependencies": []  # Would need import analysis
+            "missing_dependencies": [],  # Would need import analysis
         }
-    
+
     def _load_json(self, filepath: Path, default=None):
         """Safely load JSON file"""
         if not filepath.exists():
             return default
         try:
             return json.loads(filepath.read_text())
         except:
             return default
-    
+
     def _parse_security_vulnerabilities(self, vulns: List[Dict]) -> List[Dict[str, Any]]:
         """Parse security vulnerabilities from pip-audit"""
         parsed = []
         for vuln in vulns:
             if isinstance(vuln, dict):
-                parsed.append({
-                    "package": vuln.get("name", "unknown"),
-                    "current_version": vuln.get("version", "unknown"),
-                    "vulnerability": vuln.get("id", "unknown"),
-                    "severity": vuln.get("severity", "MEDIUM"),
-                    "fixed_in": vuln.get("fix_versions", ["unknown"])[0] if vuln.get("fix_versions") else "unknown",
-                    "recommendation": f"Update to version {vuln.get('fix_versions', ['latest'])[0]}"
-                })
+                parsed.append(
+                    {
+                        "package": vuln.get("name", "unknown"),
+                        "current_version": vuln.get("version", "unknown"),
+                        "vulnerability": vuln.get("id", "unknown"),
+                        "severity": vuln.get("severity", "MEDIUM"),
+                        "fixed_in": (
+                            vuln.get("fix_versions", ["unknown"])[0]
+                            if vuln.get("fix_versions")
+                            else "unknown"
+                        ),
+                        "recommendation": f"Update to version {vuln.get('fix_versions', ['latest'])[0]}",
+                    }
+                )
         return parsed
-    
+
     def _check_deprecated_packages(self) -> List[Dict[str, Any]]:
         """Check for known deprecated packages"""
         deprecated = []
-        
+
         # Check if aioredis is installed
         try:
             result = subprocess.run(
-                [sys.executable, "-m", "pip", "show", "aioredis"],
-                capture_output=True,
-                text=True
+                [sys.executable, "-m", "pip", "show", "aioredis"], capture_output=True, text=True
             )
             if result.returncode == 0:
-                version_match = re.search(r'Version:\s+(\S+)', result.stdout)
+                version_match = re.search(r"Version:\s+(\S+)", result.stdout)
                 version = version_match.group(1) if version_match else "unknown"
-                deprecated.append({
-                    "package": "aioredis",
-                    "current_version": version,
-                    "status": "DEPRECATED",
-                    "replacement": "redis[asyncio]>=4.5.0",
-                    "used_in": ["Check codebase for 'import aioredis'"],
-                    "migration_required": True
-                })
+                deprecated.append(
+                    {
+                        "package": "aioredis",
+                        "current_version": version,
+                        "status": "DEPRECATED",
+                        "replacement": "redis[asyncio]>=4.5.0",
+                        "used_in": ["Check codebase for 'import aioredis'"],
+                        "migration_required": True,
+                    }
+                )
         except:
             pass
-        
+
         return deprecated
-    
+
     def _parse_outdated_packages(self, outdated: List[Dict]) -> List[Dict[str, Any]]:
         """Parse outdated packages"""
         parsed = []
         for pkg in outdated:
             if isinstance(pkg, dict):
-                parsed.append({
-                    "package": pkg.get("name", "unknown"),
-                    "current": pkg.get("version", "unknown"),
-                    "latest": pkg.get("latest_version", "unknown"),
-                    "breaking_changes": False  # Would need changelog analysis
-                })
+                parsed.append(
+                    {
+                        "package": pkg.get("name", "unknown"),
+                        "current": pkg.get("version", "unknown"),
+                        "latest": pkg.get("latest_version", "unknown"),
+                        "breaking_changes": False,  # Would need changelog analysis
+                    }
+                )
         return parsed[:10]  # Top 10
-    
+
     def _generate_dependency_markdown_report(self, dep_data: Dict[str, Any], dep_dir: Path):
         """Generate markdown dependency report"""
         md_lines = [
             "# Dependency Audit Report",
             f"\n## Summary\n",
             f"- **Total Dependencies:** {dep_data['total_dependencies']}",
             f"- **Security Vulnerabilities:** {len(dep_data['security_vulnerabilities'])}",
             f"- **Deprecated Packages:** {len(dep_data['deprecated_packages'])}",
             f"- **Outdated Packages:** {len(dep_data['outdated_packages'])}",
         ]
-        
+
         # Security vulnerabilities
-        if dep_data['security_vulnerabilities']:
+        if dep_data["security_vulnerabilities"]:
             md_lines.append("\n##  Security Vulnerabilities\n")
-            for vuln in dep_data['security_vulnerabilities']:
-                md_lines.extend([
-                    f"### {vuln['package']}",
-                    f"- **Current Version:** {vuln['current_version']}",
-                    f"- **Vulnerability:** {vuln['vulnerability']}",
-                    f"- **Severity:** {vuln['severity']}",
-                    f"- **Fixed In:** {vuln['fixed_in']}",
-                    f"- **Recommendation:** {vuln['recommendation']}\n"
-                ])
-        
+            for vuln in dep_data["security_vulnerabilities"]:
+                md_lines.extend(
+                    [
+                        f"### {vuln['package']}",
+                        f"- **Current Version:** {vuln['current_version']}",
+                        f"- **Vulnerability:** {vuln['vulnerability']}",
+                        f"- **Severity:** {vuln['severity']}",
+                        f"- **Fixed In:** {vuln['fixed_in']}",
+                        f"- **Recommendation:** {vuln['recommendation']}\n",
+                    ]
+                )
+
         # Deprecated packages
-        if dep_data['deprecated_packages']:
+        if dep_data["deprecated_packages"]:
             md_lines.append("\n##   Deprecated Packages\n")
-            for pkg in dep_data['deprecated_packages']:
-                md_lines.extend([
-                    f"### {pkg['package']}",
-                    f"- **Current Version:** {pkg['current_version']}",
-                    f"- **Status:** {pkg['status']}",
-                    f"- **Replacement:** {pkg['replacement']}",
-                    f"- **Migration Required:** {'Yes' if pkg['migration_required'] else 'No'}\n"
-                ])
-        
+            for pkg in dep_data["deprecated_packages"]:
+                md_lines.extend(
+                    [
+                        f"### {pkg['package']}",
+                        f"- **Current Version:** {pkg['current_version']}",
+                        f"- **Status:** {pkg['status']}",
+                        f"- **Replacement:** {pkg['replacement']}",
+                        f"- **Migration Required:** {'Yes' if pkg['migration_required'] else 'No'}\n",
+                    ]
+                )
+
         # Prioritized action items
         md_lines.append("\n## Prioritized Action Items\n")
         priority = 1
-        
-        if dep_data['security_vulnerabilities']:
-            md_lines.append(f"{priority}. **URGENT:** Fix {len(dep_data['security_vulnerabilities'])} security vulnerabilities")
+
+        if dep_data["security_vulnerabilities"]:
+            md_lines.append(
+                f"{priority}. **URGENT:** Fix {len(dep_data['security_vulnerabilities'])} security vulnerabilities"
+            )
             priority += 1
-        
-        if dep_data['deprecated_packages']:
-            md_lines.append(f"{priority}. **HIGH:** Replace {len(dep_data['deprecated_packages'])} deprecated packages")
+
+        if dep_data["deprecated_packages"]:
+            md_lines.append(
+                f"{priority}. **HIGH:** Replace {len(dep_data['deprecated_packages'])} deprecated packages"
+            )
             priority += 1
-        
-        if dep_data['outdated_packages']:
-            md_lines.append(f"{priority}. **MEDIUM:** Update {len(dep_data['outdated_packages'])} outdated packages")
-        
+
+        if dep_data["outdated_packages"]:
+            md_lines.append(
+                f"{priority}. **MEDIUM:** Update {len(dep_data['outdated_packages'])} outdated packages"
+            )
+
         (dep_dir / "dependency_audit_report.md").write_text("\n".join(md_lines))
-    
+
     def run_performance_audit(self):
         """Task 2.4: Performance Profiling"""
         perf_dir = self.audit_dir / "performance"
         perf_dir.mkdir(exist_ok=True)
-        
+
         print("\n Running performance profiling...")
-        
+
         # Create benchmark script
         self._create_benchmark_script()
-        
+
         # For now, create a basic performance report
         # In a real scenario, we would run the benchmark script
         perf_report = {
             "timestamp": self.timestamp,
             "api_endpoints": [
                 {
                     "endpoint": "System Info",
                     "avg_response_time_ms": 0,
                     "status": "NOT_TESTED",
-                    "note": "Performance tests require running server"
+                    "note": "Performance tests require running server",
                 }
             ],
             "database_queries": [
                 {
                     "query": "System check",
                     "status": "NOT_TESTED",
-                    "note": "Database performance tests require running database"
+                    "note": "Database performance tests require running database",
                 }
             ],
             "memory_usage": {
                 "baseline_mb": 0,
                 "status": "NOT_TESTED",
-                "note": "Memory profiling requires running application"
-            }
+                "note": "Memory profiling requires running application",
+            },
         }
-        
+
         (perf_dir / "performance_report.json").write_text(json.dumps(perf_report, indent=2))
         self._generate_performance_markdown_report(perf_report, perf_dir)
-        
+
         print(f" Performance audit complete (benchmark script created)")
-    
+
     def _create_benchmark_script(self):
         """Create performance benchmark script"""
         benchmark_dir = self.project_root / "benchmarks"
         benchmark_dir.mkdir(exist_ok=True)
-        
+
         benchmark_script = '''#!/usr/bin/env python3
 """
 Performance Benchmark Script
 Can be re-run to measure system performance
 """
@@ -988,13 +1085,13 @@
 
 
 if __name__ == "__main__":
     asyncio.run(main())
 '''
-        
+
         (benchmark_dir / "performance_benchmark.py").write_text(benchmark_script)
-    
+
     def _generate_performance_markdown_report(self, perf_data: Dict[str, Any], perf_dir: Path):
         """Generate markdown performance report"""
         md_lines = [
             "# Performance Report",
             f"\n**Timestamp:** {perf_data['timestamp']}",
@@ -1012,54 +1109,56 @@
             "```",
             "\n## Recommendations\n",
             "1. Set up performance testing in CI/CD pipeline",
             "2. Establish baseline performance metrics",
             "3. Add database query profiling",
-            "4. Monitor memory usage under load"
+            "4. Monitor memory usage under load",
         ]
-        
+
         (perf_dir / "performance_report.md").write_text("\n".join(md_lines))
-    
+
     def _generate_master_summary(self):
         """Generate master summary report"""
         summary_file = self.audit_dir / "AUDIT_SUMMARY.md"
-        
+
         md_lines = [
             "# YMERA Platform - Comprehensive Audit Summary",
             f"\n**Execution Date:** {self.timestamp}",
-            "\n## Tasks Completed\n"
+            "\n## Tasks Completed\n",
         ]
-        
+
         for i, task in enumerate(self.results["tasks_completed"], 1):
             md_lines.append(f"{i}.  {task}")
-        
+
         if self.results["tasks_failed"]:
             md_lines.append("\n## Tasks Failed\n")
             for task in self.results["tasks_failed"]:
                 md_lines.append(f"-  {task['task']}")
                 md_lines.append(f"  - Error: {task['error']}")
-        
-        md_lines.extend([
-            "\n## Report Locations\n",
-            "- **Testing:** `audit_reports/testing/test_audit_report.md`",
-            "- **Quality:** `audit_reports/quality/code_quality_report.md`",
-            "- **Dependencies:** `audit_reports/dependencies/dependency_audit_report.md`",
-            "- **Performance:** `audit_reports/performance/performance_report.md`",
-            "\n## Next Steps\n",
-            "1. Review all audit reports",
-            "2. Address HIGH severity issues immediately",
-            "3. Create action plan for MEDIUM severity issues",
-            "4. Schedule regular audits"
-        ])
-        
+
+        md_lines.extend(
+            [
+                "\n## Report Locations\n",
+                "- **Testing:** `audit_reports/testing/test_audit_report.md`",
+                "- **Quality:** `audit_reports/quality/code_quality_report.md`",
+                "- **Dependencies:** `audit_reports/dependencies/dependency_audit_report.md`",
+                "- **Performance:** `audit_reports/performance/performance_report.md`",
+                "\n## Next Steps\n",
+                "1. Review all audit reports",
+                "2. Address HIGH severity issues immediately",
+                "3. Create action plan for MEDIUM severity issues",
+                "4. Schedule regular audits",
+            ]
+        )
+
         summary_file.write_text("\n".join(md_lines))
 
 
 def main():
     """Main entry point"""
     project_root = Path(__file__).parent.parent.resolve()
-    
+
     auditor = PlatformAuditor(project_root)
     auditor.run_all_audits()
 
 
 if __name__ == "__main__":
would reformat /home/runner/work/ymera_y/ymera_y/audit_scripts/comprehensive_audit.py
--- /home/runner/work/ymera_y/ymera_y/backup_manager.py	2025-10-19 22:47:02.795432+00:00
+++ /home/runner/work/ymera_y/ymera_y/backup_manager.py	2025-10-19 23:08:57.372999+00:00
@@ -18,422 +18,426 @@
 
 
 @dataclass
 class BackupMetadata:
     """Backup metadata"""
+
     backup_id: str
     timestamp: datetime
     database_type: str
     database_name: str
     backup_size_bytes: int
     backup_path: Path
     compression: str
     checksum: str
     version: str = "5.0.0"
-    
+
     def to_dict(self) -> Dict[str, Any]:
         data = asdict(self)
-        data['timestamp'] = self.timestamp.isoformat()
-        data['backup_path'] = str(self.backup_path)
+        data["timestamp"] = self.timestamp.isoformat()
+        data["backup_path"] = str(self.backup_path)
         return data
-    
+
     @classmethod
-    def from_dict(cls, data: Dict[str, Any]) -> 'BackupMetadata':
-        data['timestamp'] = datetime.fromisoformat(data['timestamp'])
-        data['backup_path'] = Path(data['backup_path'])
+    def from_dict(cls, data: Dict[str, Any]) -> "BackupMetadata":
+        data["timestamp"] = datetime.fromisoformat(data["timestamp"])
+        data["backup_path"] = Path(data["backup_path"])
         return cls(**data)
 
 
 class BackupManager:
     """Manages database backups"""
-    
+
     def __init__(self, backup_dir: Optional[Path] = None):
         self.config = DatabaseConfig()
         self.backup_dir = backup_dir or Path("./database/backups")
         self.backup_dir.mkdir(parents=True, exist_ok=True)
-    
+
     async def create_backup(
-        self,
-        compress: bool = True,
-        include_data: bool = True
+        self, compress: bool = True, include_data: bool = True
     ) -> BackupMetadata:
         """Create database backup"""
         import uuid
         import hashlib
-        
+
         backup_id = str(uuid.uuid4())[:8]
         timestamp = datetime.utcnow()
         timestamp_str = timestamp.strftime("%Y%m%d_%H%M%S")
-        
+
         # Determine backup filename
         db_name = self._get_database_name()
         filename = f"backup_{db_name}_{timestamp_str}_{backup_id}"
-        
+
         if self.config.is_postgres:
             backup_path = await self._backup_postgresql(filename, include_data)
         elif self.config.is_sqlite:
             backup_path = await self._backup_sqlite(filename)
         else:
             raise NotImplementedError(f"Backup not implemented for {self.config.db_type}")
-        
+
         # Compress if requested
         if compress:
             backup_path = await self._compress_backup(backup_path)
             compression = "gzip"
         else:
             compression = "none"
-        
+
         # Calculate checksum
         checksum = self._calculate_checksum(backup_path)
-        
+
         # Get backup size
         backup_size = backup_path.stat().st_size
-        
+
         # Create metadata
         metadata = BackupMetadata(
             backup_id=backup_id,
             timestamp=timestamp,
             database_type=self.config.db_type,
             database_name=db_name,
             backup_size_bytes=backup_size,
             backup_path=backup_path,
             compression=compression,
-            checksum=checksum
+            checksum=checksum,
         )
-        
+
         # Save metadata
         self._save_metadata(metadata)
-        
+
         print(f" Backup created: {backup_path.name}")
         print(f"  Size: {backup_size / 1024 / 1024:.2f} MB")
         print(f"  Checksum: {checksum[:16]}...")
-        
+
         return metadata
-    
+
     async def _backup_postgresql(self, filename: str, include_data: bool) -> Path:
         """Backup PostgreSQL database"""
         backup_path = self.backup_dir / f"{filename}.sql"
-        
+
         # Parse connection URL
         db_url = self.config.database_url
         # postgresql+asyncpg://user:pass@host:port/dbname
         parts = db_url.replace("postgresql+asyncpg://", "").split("/")
         connection_part = parts[0]
         dbname = parts[1] if len(parts) > 1 else "postgres"
-        
+
         # Build pg_dump command
         cmd = [
             "pg_dump",
-            "--host", connection_part.split("@")[1].split(":")[0],
-            "--username", connection_part.split(":")[0],
-            "--dbname", dbname,
-            "--file", str(backup_path),
-            "--format", "plain",
-            "--verbose"
+            "--host",
+            connection_part.split("@")[1].split(":")[0],
+            "--username",
+            connection_part.split(":")[0],
+            "--dbname",
+            dbname,
+            "--file",
+            str(backup_path),
+            "--format",
+            "plain",
+            "--verbose",
         ]
-        
+
         if not include_data:
             cmd.append("--schema-only")
-        
+
         # Run pg_dump
         process = await asyncio.create_subprocess_exec(
-            *cmd,
-            stdout=asyncio.subprocess.PIPE,
-            stderr=asyncio.subprocess.PIPE
+            *cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
         )
-        
+
         stdout, stderr = await process.communicate()
-        
+
         if process.returncode != 0:
             raise Exception(f"pg_dump failed: {stderr.decode()}")
-        
+
         return backup_path
-    
+
     async def _backup_sqlite(self, filename: str) -> Path:
         """Backup SQLite database"""
         backup_path = self.backup_dir / f"{filename}.db"
-        
+
         # Get source database path
         db_path = self.config.database_url.replace("sqlite+aiosqlite:///", "").replace("./", "")
         source_path = Path(db_path)
-        
+
         if not source_path.exists():
             raise FileNotFoundError(f"Database file not found: {source_path}")
-        
+
         # Copy database file
         shutil.copy2(source_path, backup_path)
-        
+
         return backup_path
-    
+
     async def _compress_backup(self, backup_path: Path) -> Path:
         """Compress backup file"""
         compressed_path = backup_path.with_suffix(backup_path.suffix + ".gz")
-        
-        with open(backup_path, 'rb') as f_in:
-            with gzip.open(compressed_path, 'wb') as f_out:
+
+        with open(backup_path, "rb") as f_in:
+            with gzip.open(compressed_path, "wb") as f_out:
                 shutil.copyfileobj(f_in, f_out)
-        
+
         # Remove uncompressed file
         backup_path.unlink()
-        
+
         return compressed_path
-    
+
     def _calculate_checksum(self, file_path: Path) -> str:
         """Calculate SHA-256 checksum"""
         import hashlib
-        
+
         sha256_hash = hashlib.sha256()
-        
+
         with open(file_path, "rb") as f:
             for byte_block in iter(lambda: f.read(4096), b""):
                 sha256_hash.update(byte_block)
-        
+
         return sha256_hash.hexdigest()
-    
+
     def _save_metadata(self, metadata: BackupMetadata) -> None:
         """Save backup metadata"""
-        metadata_path = metadata.backup_path.with_suffix('.meta.json')
-        
-        with open(metadata_path, 'w') as f:
+        metadata_path = metadata.backup_path.with_suffix(".meta.json")
+
+        with open(metadata_path, "w") as f:
             json.dump(metadata.to_dict(), f, indent=2)
-    
+
     def _load_metadata(self, metadata_path: Path) -> BackupMetadata:
         """Load backup metadata"""
-        with open(metadata_path, 'r') as f:
+        with open(metadata_path, "r") as f:
             data = json.load(f)
-        
+
         return BackupMetadata.from_dict(data)
-    
+
     def _get_database_name(self) -> str:
         """Extract database name from URL"""
         db_url = self.config.database_url
-        
+
         if ":///" in db_url:
             # SQLite
             return Path(db_url.split("///")[1]).stem
         else:
             # PostgreSQL/MySQL
             return db_url.split("/")[-1]
-    
-    async def restore_backup(
-        self,
-        backup_path: Path,
-        verify_checksum: bool = True
-    ) -> bool:
+
+    async def restore_backup(self, backup_path: Path, verify_checksum: bool = True) -> bool:
         """Restore database from backup"""
-        
+
         # Load metadata
-        metadata_path = backup_path.with_suffix('.meta.json')
+        metadata_path = backup_path.with_suffix(".meta.json")
         if not metadata_path.exists():
             raise FileNotFoundError(f"Metadata file not found: {metadata_path}")
-        
+
         metadata = self._load_metadata(metadata_path)
-        
+
         # Verify checksum
         if verify_checksum:
             current_checksum = self._calculate_checksum(backup_path)
             if current_checksum != metadata.checksum:
                 raise ValueError("Backup checksum mismatch - file may be corrupted")
-        
+
         # Decompress if needed
         if metadata.compression == "gzip":
             backup_path = await self._decompress_backup(backup_path)
-        
+
         # Restore based on database type
         if metadata.database_type == "postgresql":
             await self._restore_postgresql(backup_path)
         elif metadata.database_type == "sqlite":
             await self._restore_sqlite(backup_path)
         else:
             raise NotImplementedError(f"Restore not implemented for {metadata.database_type}")
-        
+
         print(f" Database restored from backup: {backup_path.name}")
         return True
-    
+
     async def _decompress_backup(self, backup_path: Path) -> Path:
         """Decompress backup file"""
-        decompressed_path = backup_path.with_suffix('')
-        
-        with gzip.open(backup_path, 'rb') as f_in:
-            with open(decompressed_path, 'wb') as f_out:
+        decompressed_path = backup_path.with_suffix("")
+
+        with gzip.open(backup_path, "rb") as f_in:
+            with open(decompressed_path, "wb") as f_out:
                 shutil.copyfileobj(f_in, f_out)
-        
+
         return decompressed_path
-    
+
     async def _restore_postgresql(self, backup_path: Path) -> None:
         """Restore PostgreSQL database"""
         # Parse connection URL
         db_url = self.config.database_url
         parts = db_url.replace("postgresql+asyncpg://", "").split("/")
         connection_part = parts[0]
         dbname = parts[1] if len(parts) > 1 else "postgres"
-        
+
         # Build psql command
         cmd = [
             "psql",
-            "--host", connection_part.split("@")[1].split(":")[0],
-            "--username", connection_part.split(":")[0],
-            "--dbname", dbname,
-            "--file", str(backup_path)
+            "--host",
+            connection_part.split("@")[1].split(":")[0],
+            "--username",
+            connection_part.split(":")[0],
+            "--dbname",
+            dbname,
+            "--file",
+            str(backup_path),
         ]
-        
+
         # Run psql
         process = await asyncio.create_subprocess_exec(
-            *cmd,
-            stdout=asyncio.subprocess.PIPE,
-            stderr=asyncio.subprocess.PIPE
+            *cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
         )
-        
+
         stdout, stderr = await process.communicate()
-        
+
         if process.returncode != 0:
             raise Exception(f"psql restore failed: {stderr.decode()}")
-    
+
     async def _restore_sqlite(self, backup_path: Path) -> None:
         """Restore SQLite database"""
         # Get target database path
         db_path = self.config.database_url.replace("sqlite+aiosqlite:///", "").replace("./", "")
         target_path = Path(db_path)
-        
+
         # Backup current database if it exists
         if target_path.exists():
-            backup_current = target_path.with_suffix('.db.bak')
+            backup_current = target_path.with_suffix(".db.bak")
             shutil.copy2(target_path, backup_current)
-        
+
         # Copy backup to database location
         shutil.copy2(backup_path, target_path)
-    
+
     async def list_backups(self) -> List[BackupMetadata]:
         """List all available backups"""
         backups = []
-        
+
         for metadata_file in self.backup_dir.glob("*.meta.json"):
             try:
                 metadata = self._load_metadata(metadata_file)
                 backups.append(metadata)
             except Exception as e:
                 print(f"Warning: Could not load metadata from {metadata_file}: {e}")
-        
+
         return sorted(backups, key=lambda b: b.timestamp, reverse=True)
-    
+
     async def cleanup_old_backups(self, days_to_keep: int = 30) -> int:
         """Remove backups older than specified days"""
         cutoff_date = datetime.utcnow() - timedelta(days=days_to_keep)
         backups = await self.list_backups()
-        
+
         removed_count = 0
-        
+
         for backup in backups:
             if backup.timestamp < cutoff_date:
                 # Remove backup file
                 if backup.backup_path.exists():
                     backup.backup_path.unlink()
-                
+
                 # Remove metadata file
-                metadata_path = backup.backup_path.with_suffix('.meta.json')
+                metadata_path = backup.backup_path.with_suffix(".meta.json")
                 if metadata_path.exists():
                     metadata_path.unlink()
-                
+
                 removed_count += 1
                 print(f" Removed old backup: {backup.backup_path.name}")
-        
+
         return removed_count
-    
+
     async def verify_backup(self, backup_path: Path) -> bool:
         """Verify backup integrity"""
-        metadata_path = backup_path.with_suffix('.meta.json')
-        
+        metadata_path = backup_path.with_suffix(".meta.json")
+
         if not metadata_path.exists():
             print(" Metadata file not found")
             return False
-        
+
         metadata = self._load_metadata(metadata_path)
-        
+
         # Verify checksum
         current_checksum = self._calculate_checksum(backup_path)
         if current_checksum != metadata.checksum:
             print(" Checksum mismatch - backup may be corrupted")
             return False
-        
+
         # Verify file size
         current_size = backup_path.stat().st_size
         if current_size != metadata.backup_size_bytes:
             print(" File size mismatch")
             return False
-        
+
         print(" Backup verification passed")
         return True
 
 
 async def main():
     """CLI for backup manager"""
     import argparse
-    
+
     parser = argparse.ArgumentParser(description="YMERA Database Backup Manager")
-    subparsers = parser.add_subparsers(dest='command', help='Command to execute')
-    
+    subparsers = parser.add_subparsers(dest="command", help="Command to execute")
+
     # backup command
-    backup_parser = subparsers.add_parser('backup', help='Create database backup')
-    backup_parser.add_argument('--no-compress', action='store_true', help='Do not compress backup')
-    backup_parser.add_argument('--schema-only', action='store_true', help='Backup schema only (no data)')
-    
+    backup_parser = subparsers.add_parser("backup", help="Create database backup")
+    backup_parser.add_argument("--no-compress", action="store_true", help="Do not compress backup")
+    backup_parser.add_argument(
+        "--schema-only", action="store_true", help="Backup schema only (no data)"
+    )
+
     # restore command
-    restore_parser = subparsers.add_parser('restore', help='Restore database from backup')
-    restore_parser.add_argument('backup_file', help='Backup file path')
-    restore_parser.add_argument('--no-verify', action='store_true', help='Skip checksum verification')
-    
+    restore_parser = subparsers.add_parser("restore", help="Restore database from backup")
+    restore_parser.add_argument("backup_file", help="Backup file path")
+    restore_parser.add_argument(
+        "--no-verify", action="store_true", help="Skip checksum verification"
+    )
+
     # list command
-    subparsers.add_parser('list', help='List all backups')
-    
+    subparsers.add_parser("list", help="List all backups")
+
     # cleanup command
-    cleanup_parser = subparsers.add_parser('cleanup', help='Remove old backups')
-    cleanup_parser.add_argument('--days', type=int, default=30, help='Days to keep (default: 30)')
-    
+    cleanup_parser = subparsers.add_parser("cleanup", help="Remove old backups")
+    cleanup_parser.add_argument("--days", type=int, default=30, help="Days to keep (default: 30)")
+
     # verify command
-    verify_parser = subparsers.add_parser('verify', help='Verify backup integrity')
-    verify_parser.add_argument('backup_file', help='Backup file path')
-    
+    verify_parser = subparsers.add_parser("verify", help="Verify backup integrity")
+    verify_parser.add_argument("backup_file", help="Backup file path")
+
     args = parser.parse_args()
-    
+
     manager = BackupManager()
-    
-    if args.command == 'backup':
+
+    if args.command == "backup":
         await manager.create_backup(
-            compress=not args.no_compress,
-            include_data=not args.schema_only
+            compress=not args.no_compress, include_data=not args.schema_only
         )
-    
-    elif args.command == 'restore':
-        await manager.restore_backup(
-            Path(args.backup_file),
-            verify_checksum=not args.no_verify
-        )
-    
-    elif args.command == 'list':
+
+    elif args.command == "restore":
+        await manager.restore_backup(Path(args.backup_file), verify_checksum=not args.no_verify)
+
+    elif args.command == "list":
         backups = await manager.list_backups()
-        
+
         print("\nAvailable Backups:")
         print("=" * 100)
-        print(f"{'Timestamp':<20} {'ID':<10} {'Database':<20} {'Size (MB)':<12} {'Compression':<12} {'File':<25}")
+        print(
+            f"{'Timestamp':<20} {'ID':<10} {'Database':<20} {'Size (MB)':<12} {'Compression':<12} {'File':<25}"
+        )
         print("-" * 100)
-        
+
         for backup in backups:
             size_mb = backup.backup_size_bytes / 1024 / 1024
-            timestamp = backup.timestamp.strftime('%Y-%m-%d %H:%M:%S')
-            
-            print(f"{timestamp:<20} {backup.backup_id:<10} {backup.database_name:<20} {size_mb:>10.2f} {backup.compression:<12} {backup.backup_path.name:<25}")
-        
+            timestamp = backup.timestamp.strftime("%Y-%m-%d %H:%M:%S")
+
+            print(
+                f"{timestamp:<20} {backup.backup_id:<10} {backup.database_name:<20} {size_mb:>10.2f} {backup.compression:<12} {backup.backup_path.name:<25}"
+            )
+
         print("=" * 100)
         print()
-    
-    elif args.command == 'cleanup':
+
+    elif args.command == "cleanup":
         removed = await manager.cleanup_old_backups(args.days)
         print(f"\n Removed {removed} old backup(s)")
-    
-    elif args.command == 'verify':
+
+    elif args.command == "verify":
         await manager.verify_backup(Path(args.backup_file))
-    
+
     else:
         parser.print_help()
 
 
 if __name__ == "__main__":
would reformat /home/runner/work/ymera_y/ymera_y/backup_manager.py
--- /home/runner/work/ymera_y/ymera_y/base_agent 2.py	2025-10-19 22:47:02.795432+00:00
+++ /home/runner/work/ymera_y/ymera_y/base_agent 2.py	2025-10-19 23:08:57.609301+00:00
@@ -1,16 +1,17 @@
-
 import time
 from typing import Dict, Any, Optional
 from enum import Enum
 from dataclasses import dataclass, field
+
 
 class Priority(Enum):
     LOW = "low"
     MEDIUM = "medium"
     HIGH = "high"
     CRITICAL = "critical"
+
 
 @dataclass
 class AgentConfig:
     agent_id: str
     name: str
@@ -29,29 +30,33 @@
     anomaly_detection_interval_seconds: int = 60
     dashboard_aggregation_interval_seconds: int = 30
     cleanup_interval_seconds: int = 3600
     alert_rule_refresh_interval_seconds: int = 3000
 
+
 @dataclass
 class TaskRequest:
     task_id: str
     task_type: str
     payload: Dict[str, Any]
     priority: Priority = Priority.MEDIUM
     created_at: float = field(default_factory=time.time)
     deadline: Optional[float] = None
     metadata: Dict[str, Any] = field(default_factory=dict)
 
+
 class BaseAgent:
     def __init__(self, config: AgentConfig):
         self.config = config
         import logging
-        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
+
+        logging.basicConfig(
+            level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
+        )
         self.logger = logging.getLogger(self.config.name)
 
-
-        self.tracer = None # Placeholder, will be set by actual agent
+        self.tracer = None  # Placeholder, will be set by actual agent
 
     async def start(self):
         raise NotImplementedError
 
     async def stop(self):
@@ -70,14 +75,16 @@
         if self.nc:
             if queue_group:
                 await self.nc.subscribe(topic, cb=handler, queue=queue_group)
             else:
                 await self.nc.subscribe(topic, cb=handler)
-            self.logger.info(f"Subscribed to {topic} with queue group {queue_group}" if queue_group else f"Subscribed to {topic}")
+            self.logger.info(
+                f"Subscribed to {topic} with queue group {queue_group}"
+                if queue_group
+                else f"Subscribed to {topic}"
+            )
         else:
             self.logger.warning(f"NATS client not initialized, cannot subscribe to {topic}")
 
     async def _handle_message(self, msg):
         # Placeholder for message handling logic
         print(f"Handling message: {msg}")
-
-
would reformat /home/runner/work/ymera_y/ymera_y/base_agent 2.py
--- /home/runner/work/ymera_y/ymera_y/backup_recovery.py	2025-10-19 22:47:02.795432+00:00
+++ /home/runner/work/ymera_y/ymera_y/backup_recovery.py	2025-10-19 23:08:57.798681+00:00
@@ -1,8 +1,9 @@
 """
 Azure SQL Backup and Disaster Recovery Manager
 """
+
 from azure.mgmt.sql import SqlManagementClient
 from azure.identity import DefaultAzureCredential
 from datetime import datetime, timedelta
 import logging
 import os
@@ -11,29 +12,33 @@
 logger = logging.getLogger(__name__)
 
 
 class AzureBackupManager:
     """Manage Azure SQL Database backups and recovery"""
-    
+
     def __init__(self):
-        self.subscription_id = os.getenv('AZURE_SUBSCRIPTION_ID')
-        self.resource_group = os.getenv('AZURE_RESOURCE_GROUP', 'ymera')
-        self.server_name = os.getenv('AZURE_SQL_SERVER', 'ymera-sql').replace('.database.windows.net', '')
-        self.database_name = os.getenv('AZURE_SQL_DATABASE', 'ymeradb')
-        
+        self.subscription_id = os.getenv("AZURE_SUBSCRIPTION_ID")
+        self.resource_group = os.getenv("AZURE_RESOURCE_GROUP", "ymera")
+        self.server_name = os.getenv("AZURE_SQL_SERVER", "ymera-sql").replace(
+            ".database.windows.net", ""
+        )
+        self.database_name = os.getenv("AZURE_SQL_DATABASE", "ymeradb")
+
         # Initialize Azure client
         self.credential = DefaultAzureCredential()
         self.sql_client = SqlManagementClient(self.credential, self.subscription_id)
-    
-    def create_long_term_retention_policy(self, 
-                                         weekly_retention: str = "P4W",
-                                         monthly_retention: str = "P12M",
-                                         yearly_retention: str = "P5Y",
-                                         week_of_year: int = 1):
+
+    def create_long_term_retention_policy(
+        self,
+        weekly_retention: str = "P4W",
+        monthly_retention: str = "P12M",
+        yearly_retention: str = "P5Y",
+        week_of_year: int = 1,
+    ):
         """
         Configure Long-Term Retention (LTR) policy
-        
+
         Args:
             weekly_retention: Keep weekly backups (e.g., "P4W" = 4 weeks)
             monthly_retention: Keep monthly backups (e.g., "P12M" = 12 months)
             yearly_retention: Keep yearly backups (e.g., "P5Y" = 5 years)
             week_of_year: Which week of year to keep yearly backup
@@ -46,321 +51,300 @@
                 policy_name="default",
                 parameters={
                     "weekly_retention": weekly_retention,
                     "monthly_retention": monthly_retention,
                     "yearly_retention": yearly_retention,
-                    "week_of_year": week_of_year
-                }
-            )
-            
-            logger.info(f"LTR policy configured: Weekly={weekly_retention}, "
-                       f"Monthly={monthly_retention}, Yearly={yearly_retention}")
+                    "week_of_year": week_of_year,
+                },
+            )
+
+            logger.info(
+                f"LTR policy configured: Weekly={weekly_retention}, "
+                f"Monthly={monthly_retention}, Yearly={yearly_retention}"
+            )
             return policy
-            
+
         except Exception as e:
             logger.error(f"Failed to configure LTR policy: {e}")
             raise
-    
+
     def list_long_term_backups(self) -> List[dict]:
         """List all long-term retention backups"""
         try:
             backups = self.sql_client.long_term_retention_backups.list_by_database(
-                location_name=os.getenv('AZURE_LOCATION', 'eastus'),
+                location_name=os.getenv("AZURE_LOCATION", "eastus"),
                 long_term_retention_server_name=self.server_name,
-                long_term_retention_database_name=self.database_name
-            )
-            
+                long_term_retention_database_name=self.database_name,
+            )
+
             backup_list = []
             for backup in backups:
-                backup_list.append({
-                    "name": backup.name,
-                    "backup_time": backup.backup_time,
-                    "backup_expiration_time": backup.backup_expiration_time,
-                    "database_name": backup.database_name,
-                    "server_name": backup.server_name
-                })
-            
+                backup_list.append(
+                    {
+                        "name": backup.name,
+                        "backup_time": backup.backup_time,
+                        "backup_expiration_time": backup.backup_expiration_time,
+                        "database_name": backup.database_name,
+                        "server_name": backup.server_name,
+                    }
+                )
+
             return backup_list
-            
+
         except Exception as e:
             logger.error(f"Failed to list LTR backups: {e}")
             return []
-    
+
     def create_manual_backup(self, backup_name: Optional[str] = None) -> dict:
         """
         Create a manual backup (copy of latest automatic backup)
         Note: Azure SQL automatically creates backups, this creates a copy
         """
         try:
             if not backup_name:
                 backup_name = f"manual-backup-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
-            
+
             # Create a database copy as backup
             copy_params = {
-                "location": os.getenv('AZURE_LOCATION', 'eastus'),
+                "location": os.getenv("AZURE_LOCATION", "eastus"),
                 "create_mode": "Copy",
-                "source_database_id": f"/subscriptions/{self.subscription_id}/resourceGroups/{self.resource_group}/providers/Microsoft.Sql/servers/{self.server_name}/databases/{self.database_name}"
-            }
-            
+                "source_database_id": f"/subscriptions/{self.subscription_id}/resourceGroups/{self.resource_group}/providers/Microsoft.Sql/servers/{self.server_name}/databases/{self.database_name}",
+            }
+
             operation = self.sql_client.databases.begin_create_or_update(
                 resource_group_name=self.resource_group,
                 server_name=self.server_name,
                 database_name=backup_name,
-                parameters=copy_params
-            )
-            
+                parameters=copy_params,
+            )
+
             result = operation.result()
             logger.info(f"Manual backup created: {backup_name}")
-            
+
             return {
                 "backup_name": backup_name,
                 "created_at": datetime.now().isoformat(),
-                "status": "completed"
-            }
-            
+                "status": "completed",
+            }
+
         except Exception as e:
             logger.error(f"Failed to create manual backup: {e}")
             raise
-    
-    def restore_to_point_in_time(self, 
-                                restore_point: datetime,
-                                target_database_name: str) -> dict:
+
+    def restore_to_point_in_time(self, restore_point: datetime, target_database_name: str) -> dict:
         """
         Restore database to a specific point in time
-        
+
         Args:
             restore_point: DateTime to restore to (within retention period)
             target_database_name: Name for the restored database
         """
         try:
             restore_params = {
-                "location": os.getenv('AZURE_LOCATION', 'eastus'),
+                "location": os.getenv("AZURE_LOCATION", "eastus"),
                 "create_mode": "PointInTimeRestore",
                 "source_database_id": f"/subscriptions/{self.subscription_id}/resourceGroups/{self.resource_group}/providers/Microsoft.Sql/servers/{self.server_name}/databases/{self.database_name}",
-                "restore_point_in_time": restore_point.isoformat()
-            }
-            
+                "restore_point_in_time": restore_point.isoformat(),
+            }
+
             operation = self.sql_client.databases.begin_create_or_update(
                 resource_group_name=self.resource_group,
                 server_name=self.server_name,
                 database_name=target_database_name,
-                parameters=restore_params
-            )
-            
+                parameters=restore_params,
+            )
+
             result = operation.result()
             logger.info(f"Database restored to {restore_point} as {target_database_name}")
-            
+
             return {
                 "restored_database": target_database_name,
                 "restore_point": restore_point.isoformat(),
-                "status": "completed"
-            }
-            
+                "status": "completed",
+            }
+
         except Exception as e:
             logger.error(f"Failed to restore database: {e}")
             raise
-    
-    def restore_from_ltr_backup(self, 
-                               backup_name: str,
-                               target_database_name: str) -> dict:
+
+    def restore_from_ltr_backup(self, backup_name: str, target_database_name: str) -> dict:
         """Restore from a long-term retention backup"""
         try:
             restore_params = {
-                "location": os.getenv('AZURE_LOCATION', 'eastus'),
+                "location": os.getenv("AZURE_LOCATION", "eastus"),
                 "create_mode": "RestoreLongTermRetentionBackup",
-                "long_term_retention_backup_resource_id": f"/subscriptions/{self.subscription_id}/resourceGroups/{self.resource_group}/providers/Microsoft.Sql/locations/{os.getenv('AZURE_LOCATION', 'eastus')}/longTermRetentionServers/{self.server_name}/longTermRetentionDatabases/{self.database_name}/longTermRetentionBackups/{backup_name}"
-            }
-            
+                "long_term_retention_backup_resource_id": f"/subscriptions/{self.subscription_id}/resourceGroups/{self.resource_group}/providers/Microsoft.Sql/locations/{os.getenv('AZURE_LOCATION', 'eastus')}/longTermRetentionServers/{self.server_name}/longTermRetentionDatabases/{self.database_name}/longTermRetentionBackups/{backup_name}",
+            }
+
             operation = self.sql_client.databases.begin_create_or_update(
                 resource_group_name=self.resource_group,
                 server_name=self.server_name,
                 database_name=target_database_name,
-                parameters=restore_params
-            )
-            
+                parameters=restore_params,
+            )
+
             result = operation.result()
             logger.info(f"Database restored from LTR backup {backup_name}")
-            
+
             return {
                 "restored_database": target_database_name,
                 "backup_name": backup_name,
-                "status": "completed"
-            }
-            
+                "status": "completed",
+            }
+
         except Exception as e:
             logger.error(f"Failed to restore from LTR backup: {e}")
             raise
-    
+
     def get_retention_policy(self) -> dict:
         """Get current backup retention policies"""
         try:
             # Short-term retention policy (automated backups)
             short_term = self.sql_client.backup_short_term_retention_policies.get(
                 resource_group_name=self.resource_group,
                 server_name=self.server_name,
                 database_name=self.database_name,
-                policy_name="default"
-            )
-            
+                policy_name="default",
+            )
+
             # Long-term retention policy
             long_term = self.sql_client.long_term_retention_policies.get(
                 resource_group_name=self.resource_group,
                 server_name=self.server_name,
                 database_name=self.database_name,
-                policy_name="default"
-            )
-            
+                policy_name="default",
+            )
+
             return {
                 "short_term_retention_days": short_term.retention_days,
                 "weekly_retention": long_term.weekly_retention,
                 "monthly_retention": long_term.monthly_retention,
                 "yearly_retention": long_term.yearly_retention,
-                "week_of_year": long_term.week_of_year
-            }
-            
+                "week_of_year": long_term.week_of_year,
+            }
+
         except Exception as e:
             logger.error(f"Failed to get retention policy: {e}")
             return {}
-    
-    def configure_geo_replication(self, 
-                                 secondary_location: str = "westus2",
-                                 secondary_database_name: Optional[str] = None):
+
+    def configure_geo_replication(
+        self, secondary_location: str = "westus2", secondary_database_name: Optional[str] = None
+    ):
         """
         Configure geo-replication for disaster recovery
-        
+
         Args:
             secondary_location: Azure region for secondary database
             secondary_database_name: Name for secondary database
         """
         try:
             if not secondary_database_name:
                 secondary_database_name = f"{self.database_name}-secondary"
-            
+
             replica_params = {
                 "location": secondary_location,
                 "create_mode": "Secondary",
-                "source_database_id": f"/subscriptions/{self.subscription_id}/resourceGroups/{self.resource_group}/providers/Microsoft.Sql/servers/{self.server_name}/databases/{self.database_name}"
-            }
-            
+                "source_database_id": f"/subscriptions/{self.subscription_id}/resourceGroups/{self.resource_group}/providers/Microsoft.Sql/servers/{self.server_name}/databases/{self.database_name}",
+            }
+
             # Note: You need a secondary server in the target region
             secondary_server = f"{self.server_name}-secondary"
-            
+
             operation = self.sql_client.databases.begin_create_or_update(
                 resource_group_name=self.resource_group,
                 server_name=secondary_server,
                 database_name=secondary_database_name,
-                parameters=replica_params
-            )
-            
+                parameters=replica_params,
+            )
+
             result = operation.result()
             logger.info(f"Geo-replication configured to {secondary_location}")
-            
+
             return {
                 "secondary_location": secondary_location,
                 "secondary_database": secondary_database_name,
                 "secondary_server": secondary_server,
-                "status": "configured"
-            }
-            
+                "status": "configured",
+            }
+
         except Exception as e:
             logger.error(f"Failed to configure geo-replication: {e}")
             raise
 
 
 class DisasterRecoveryManager:
     """Disaster recovery procedures and automation"""
-    
+
     def __init__(self, backup_manager: AzureBackupManager):
         self.backup_manager = backup_manager
         self.recovery_log_file = "recovery_log.json"
-    
+
     def create_recovery_plan(self) -> dict:
         """Create a disaster recovery plan"""
         plan = {
             "rpo": "1 hour",  # Recovery Point Objective
             "rto": "4 hours",  # Recovery Time Objective
             "backup_frequency": "continuous",
             "retention": {
                 "daily": "7 days",
                 "weekly": "4 weeks",
                 "monthly": "12 months",
-                "yearly": "5 years"
+                "yearly": "5 years",
             },
             "procedures": [
-                {
-                    "step": 1,
-                    "action": "Identify failure scope",
-                    "responsible": "DevOps Team"
-                },
-                {
-                    "step": 2,
-                    "action": "Verify backup availability",
-                    "responsible": "DBA"
-                },
-                {
-                    "step": 3,
-                    "action": "Initiate point-in-time restore",
-                    "responsible": "DBA"
-                },
-                {
-                    "step": 4,
-                    "action": "Verify data integrity",
-                    "responsible": "QA Team"
-                },
+                {"step": 1, "action": "Identify failure scope", "responsible": "DevOps Team"},
+                {"step": 2, "action": "Verify backup availability", "responsible": "DBA"},
+                {"step": 3, "action": "Initiate point-in-time restore", "responsible": "DBA"},
+                {"step": 4, "action": "Verify data integrity", "responsible": "QA Team"},
                 {
                     "step": 5,
                     "action": "Switch application to restored database",
-                    "responsible": "DevOps Team"
+                    "responsible": "DevOps Team",
                 },
-                {
-                    "step": 6,
-                    "action": "Monitor and validate",
-                    "responsible": "Operations Team"
-                }
-            ]
+                {"step": 6, "action": "Monitor and validate", "responsible": "Operations Team"},
+            ],
         }
         return plan
-    
+
     def execute_failover(self, target_location: str = "westus2") -> dict:
         """Execute failover to secondary region"""
         try:
             logger.info(f"Initiating failover to {target_location}")
-            
+
             # Log failover initiation
             failover_log = {
                 "timestamp": datetime.now().isoformat(),
                 "action": "failover_initiated",
                 "target_location": target_location,
-                "status": "in_progress"
-            }
-            
+                "status": "in_progress",
+            }
+
             # In production, this would trigger actual failover
             # For now, we document the process
-            
+
             logger.info("Failover completed successfully")
             failover_log["status"] = "completed"
             failover_log["completed_at"] = datetime.now().isoformat()
-            
+
             return failover_log
-            
+
         except Exception as e:
             logger.error(f"Failover failed: {e}")
             raise
-    
+
     def test_recovery(self, test_database_name: str = "recovery-test") -> bool:
         """Test disaster recovery procedures"""
         try:
             logger.info("Starting disaster recovery test")
-            
+
             # Test point-in-time restore
             restore_point = datetime.now() - timedelta(hours=1)
             result = self.backup_manager.restore_to_point_in_time(
-                restore_point=restore_point,
-                target_database_name=test_database_name
-            )
-            
+                restore_point=restore_point, target_database_name=test_database_name
+            )
+
             logger.info("Recovery test completed successfully")
             return True
-            
+
         except Exception as e:
             logger.error(f"Recovery test failed: {e}")
-            return False
\ No newline at end of file
+            return False
would reformat /home/runner/work/ymera_y/ymera_y/backup_recovery.py
--- /home/runner/work/ymera_y/ymera_y/batch_processor.py	2025-10-19 22:47:02.795432+00:00
+++ /home/runner/work/ymera_y/ymera_y/batch_processor.py	2025-10-19 23:08:57.941481+00:00
@@ -8,140 +8,134 @@
 from dataclasses import dataclass
 import structlog
 
 logger = structlog.get_logger(__name__)
 
-T = TypeVar('T')
-R = TypeVar('R')
+T = TypeVar("T")
+R = TypeVar("R")
 
 
 @dataclass
 class BatchConfig:
     """Batch processing configuration"""
+
     batch_size: int = 100
     max_wait_time: float = 1.0  # seconds
     max_concurrent_batches: int = 5
 
 
 class BatchProcessor:
     """
     Process items in optimized batches
-    
+
     Benefits:
     - 10x faster for bulk operations
     - Reduced database round trips
     - Better resource utilization
     - Automatic batching
-    
+
     Usage:
         async def batch_insert(items: List[Dict]) -> List[str]:
             return await db.bulk_insert(items)
-        
+
         processor = BatchProcessor(batch_insert, BatchConfig(batch_size=100))
-        
+
         for item in items:
             item_id = await processor.add(item)
     """
-    
-    def __init__(
-        self,
-        processor: Callable[[List[T]], List[R]],
-        config: BatchConfig
-    ):
+
+    def __init__(self, processor: Callable[[List[T]], List[R]], config: BatchConfig):
         self.processor = processor
         self.config = config
         self.queue: List[T] = []
         self.processing = False
         self.lock = asyncio.Lock()
         self.result_futures: Dict[int, asyncio.Future] = {}
         self.current_index = 0
-    
+
     async def add(self, item: T) -> R:
         """
         Add item to batch queue
-        
+
         Args:
             item: Item to process
-            
+
         Returns:
             Processing result for this item
         """
         async with self.lock:
             self.queue.append(item)
             item_index = self.current_index
             self.current_index += 1
-            
+
             # Create future for this item's result
             future = asyncio.Future()
             self.result_futures[item_index] = future
-            
+
             # Check if we should process now
-            should_process = (
-                len(self.queue) >= self.config.batch_size and
-                not self.processing
-            )
-        
+            should_process = len(self.queue) >= self.config.batch_size and not self.processing
+
         if should_process:
             asyncio.create_task(self._process_batch())
         else:
             # Schedule batch processing after wait time
             asyncio.create_task(self._schedule_batch_processing())
-        
+
         # Wait for result
         return await future
-    
+
     async def _schedule_batch_processing(self):
         """Schedule batch processing after wait time"""
         await asyncio.sleep(self.config.max_wait_time)
-        
+
         async with self.lock:
             if self.queue and not self.processing:
                 asyncio.create_task(self._process_batch())
-    
+
     async def _process_batch(self):
         """Process accumulated items"""
         async with self.lock:
             if self.processing or not self.queue:
                 return
-            
+
             self.processing = True
-            batch = self.queue[:self.config.batch_size]
-            self.queue = self.queue[self.config.batch_size:]
+            batch = self.queue[: self.config.batch_size]
+            self.queue = self.queue[self.config.batch_size :]
             batch_size = len(batch)
-        
+
         try:
             logger.debug(f"Processing batch of {batch_size} items")
-            
+
             # Process batch
             results = await self.processor(batch)
-            
+
             # Resolve futures with results
             for i, result in enumerate(results):
                 future = self.result_futures.pop(i, None)
                 if future and not future.done():
                     future.set_result(result)
-            
+
             logger.debug(f"Batch processed successfully: {batch_size} items")
-            
+
         except Exception as e:
             logger.error(f"Batch processing error: {e}", exc_info=True)
-            
+
             # Reject all futures with error
             for future in self.result_futures.values():
                 if not future.done():
                     future.set_exception(e)
-            
+
             self.result_futures.clear()
-        
+
         finally:
             async with self.lock:
                 self.processing = False
-                
+
                 # Process next batch if queue not empty
                 if self.queue:
                     asyncio.create_task(self._process_batch())
-    
+
     async def flush(self):
         """Force process any remaining items"""
         async with self.lock:
             if self.queue and not self.processing:
                 asyncio.create_task(self._process_batch())
would reformat /home/runner/work/ymera_y/ymera_y/batch_processor.py
--- /home/runner/work/ymera_y/ymera_y/behavior_monitor.py	2025-10-19 22:47:02.795432+00:00
+++ /home/runner/work/ymera_y/ymera_y/behavior_monitor.py	2025-10-19 23:08:58.043798+00:00
@@ -13,67 +13,69 @@
 logger = structlog.get_logger(__name__)
 
 
 class AgentBehaviorMonitor:
     """Monitors and analyzes agent behavior"""
-    
+
     def __init__(self, db_session: AsyncSession, cache_manager: CacheManager):
         self.db = db_session
         self.cache = cache_manager
         self.behavior_patterns = {}
-    
+
     async def analyze_agent(self, agent_id: str) -> Dict[str, Any]:
         """Analyze agent behavior for anomalies"""
         try:
             # Get behavior history from cache
             history_key = f"agent:behavior:{agent_id}"
             history = await self.cache.get(history_key) or []
-            
+
             # Analyze patterns
             anomalies = self._detect_anomalies(history)
-            
+
             return {
                 "agent_id": agent_id,
                 "anomalies_detected": len(anomalies),
                 "anomalies": anomalies,
-                "timestamp": datetime.utcnow().isoformat()
+                "timestamp": datetime.utcnow().isoformat(),
             }
-            
+
         except Exception as e:
             logger.error("Behavior analysis failed", agent_id=agent_id, error=str(e))
             return {"agent_id": agent_id, "error": str(e)}
-    
+
     def _detect_anomalies(self, history: List[Dict]) -> List[Dict]:
         """Detect behavioral anomalies"""
         anomalies = []
-        
+
         # Simple anomaly detection - can be enhanced with ML
         if len(history) > 10:
             recent = history[-10:]
-            error_rate = sum(1 for h in recent if h.get('has_errors', False)) / len(recent)
-            
+            error_rate = sum(1 for h in recent if h.get("has_errors", False)) / len(recent)
+
             if error_rate > 0.5:
-                anomalies.append({
-                    "type": "high_error_rate",
-                    "severity": "high",
-                    "description": f"Error rate of {error_rate:.1%} detected"
-                })
-        
+                anomalies.append(
+                    {
+                        "type": "high_error_rate",
+                        "severity": "high",
+                        "description": f"Error rate of {error_rate:.1%} detected",
+                    }
+                )
+
         return anomalies
-    
+
     async def record_behavior(self, agent_id: str, behavior_data: Dict[str, Any]):
         """Record agent behavior for analysis"""
         try:
             history_key = f"agent:behavior:{agent_id}"
             history = await self.cache.get(history_key) or []
-            
-            behavior_data['timestamp'] = datetime.utcnow().isoformat()
+
+            behavior_data["timestamp"] = datetime.utcnow().isoformat()
             history.append(behavior_data)
-            
+
             # Keep last 100 records
             if len(history) > 100:
                 history = history[-100:]
-            
+
             await self.cache.set(history_key, history, ttl=86400)
-            
+
         except Exception as e:
             logger.error("Failed to record behavior", agent_id=agent_id, error=str(e))
would reformat /home/runner/work/ymera_y/ymera_y/behavior_monitor.py
--- /home/runner/work/ymera_y/ymera_y/cache_manager.py	2025-10-19 22:47:02.795432+00:00
+++ /home/runner/work/ymera_y/ymera_y/cache_manager.py	2025-10-19 23:08:58.526463+00:00
@@ -12,10 +12,11 @@
 from functools import wraps
 from enum import Enum
 
 try:
     import redis.asyncio as redis
+
     REDIS_AVAILABLE = True
 except ImportError:
     REDIS_AVAILABLE = False
 
 import structlog
@@ -23,120 +24,121 @@
 logger = structlog.get_logger(__name__)
 
 
 class CacheStrategy(Enum):
     """Caching strategies"""
+
     LRU = "lru"  # Least Recently Used
     LFU = "lfu"  # Least Frequently Used
     TTL = "ttl"  # Time To Live
     WRITE_THROUGH = "write_through"  # Write to all levels
     WRITE_BACK = "write_back"  # Write async to L2
 
 
 class MultiLevelCache:
     """
     Multi-level caching with L1 (memory), L2 (Redis)
-    
+
     Provides:
     - Automatic cache warming
     - Cache invalidation
     - TTL management
     - Cache-aside pattern
     - Write-through/write-back
     - Graceful degradation
-    
+
     Usage:
         cache = MultiLevelCache(redis_client, default_ttl=3600)
-        
+
         # Get
         value = await cache.get("my_key")
-        
+
         # Set
         await cache.set("my_key", value, ttl=300)
-        
+
         # Decorator
         @cached(ttl=300, key_prefix="user")
         async def get_user(user_id: str):
             return await db.get_user(user_id)
     """
-    
+
     def __init__(self, redis_client: Optional[redis.Redis], default_ttl: int = 3600):
         self.redis = redis_client
         self.default_ttl = default_ttl
         self.enabled = redis_client is not None and REDIS_AVAILABLE
-        
+
         # L1 cache (memory) - fast but limited
         self.l1_cache: dict[str, tuple[Any, float]] = {}
         self.l1_max_size = 1000
         self.l1_access_count: dict[str, int] = {}
-        
+
         # Cache statistics
         self.stats = {
             "l1_hits": 0,
             "l1_misses": 0,
             "l2_hits": 0,
             "l2_misses": 0,
             "total_requests": 0,
             "sets": 0,
-            "deletes": 0
+            "deletes": 0,
         }
-        
+
         if not self.enabled:
             logger.warning("Redis not available - using L1 (memory) cache only")
-    
+
     async def get(self, key: str) -> Optional[Any]:
         """
         Get value from cache (multi-level)
-        
+
         Args:
             key: Cache key
-            
+
         Returns:
             Cached value or None
         """
         self.stats["total_requests"] += 1
-        
+
         # Try L1 cache (memory) first
         value = self._get_from_l1(key)
         if value is not None:
             self.stats["l1_hits"] += 1
             return value
-        
+
         self.stats["l1_misses"] += 1
-        
+
         # Try L2 cache (Redis)
         if self.enabled:
             value = await self._get_from_l2(key)
             if value is not None:
                 self.stats["l2_hits"] += 1
                 # Warm L1 cache
                 self._set_to_l1(key, value)
                 return value
-            
+
             self.stats["l2_misses"] += 1
-        
+
         return None
-    
+
     async def set(
         self,
         key: str,
         value: Any,
         ttl: Optional[int] = None,
-        strategy: CacheStrategy = CacheStrategy.WRITE_THROUGH
+        strategy: CacheStrategy = CacheStrategy.WRITE_THROUGH,
     ):
         """
         Set value in cache
-        
+
         Args:
             key: Cache key
             value: Value to cache
             ttl: Time to live in seconds
             strategy: Caching strategy
         """
         self.stats["sets"] += 1
         ttl = ttl or self.default_ttl
-        
+
         if strategy == CacheStrategy.WRITE_THROUGH:
             # Write to all levels synchronously
             self._set_to_l1(key, value, ttl)
             if self.enabled:
                 await self._set_to_l2(key, value, ttl)
@@ -148,67 +150,67 @@
         else:
             # Default to write-through
             self._set_to_l1(key, value, ttl)
             if self.enabled:
                 await self._set_to_l2(key, value, ttl)
-    
+
     async def delete(self, key: str):
         """Delete cache entry"""
         self.stats["deletes"] += 1
         self._delete_from_l1(key)
         if self.enabled:
             await self._delete_from_l2(key)
-    
+
     async def invalidate(self, key: str):
         """Alias for delete"""
         await self.delete(key)
-    
+
     async def invalidate_pattern(self, pattern: str):
         """Invalidate all keys matching pattern"""
         # L1 invalidation
         keys_to_delete = [k for k in self.l1_cache.keys() if pattern in k]
         for key in keys_to_delete:
             del self.l1_cache[key]
             if key in self.l1_access_count:
                 del self.l1_access_count[key]
-        
+
         # L2 invalidation
         if self.enabled and self.redis:
             try:
                 keys = await self.redis.keys(f"*{pattern}*")
                 if keys:
                     await self.redis.delete(*keys)
             except Exception as e:
                 logger.error(f"Pattern invalidation error: {e}")
-    
+
     async def clear(self):
         """Clear all cache"""
         self.l1_cache.clear()
         self.l1_access_count.clear()
         if self.enabled and self.redis:
             try:
                 await self.redis.flushdb()
             except Exception as e:
                 logger.error(f"Cache clear error: {e}")
-    
+
     def _get_from_l1(self, key: str) -> Optional[Any]:
         """Get from L1 (memory) cache"""
         if key in self.l1_cache:
             value, expiry = self.l1_cache[key]
             current_time = asyncio.get_event_loop().time()
-            
+
             if expiry > current_time:
                 # Update access count for LRU
                 self.l1_access_count[key] = self.l1_access_count.get(key, 0) + 1
                 return value
             else:
                 # Expired
                 del self.l1_cache[key]
                 if key in self.l1_access_count:
                     del self.l1_access_count[key]
         return None
-    
+
     def _set_to_l1(self, key: str, value: Any, ttl: int = None):
         """Set to L1 (memory) cache"""
         # Evict if cache is full (LRU)
         if len(self.l1_cache) >= self.l1_max_size:
             # Find least recently used (lowest access count)
@@ -219,148 +221,149 @@
             else:
                 # If no access counts, remove oldest
                 if self.l1_cache:
                     oldest_key = next(iter(self.l1_cache))
                     del self.l1_cache[oldest_key]
-        
+
         ttl = ttl or self.default_ttl
         expiry = asyncio.get_event_loop().time() + ttl
         self.l1_cache[key] = (value, expiry)
         self.l1_access_count[key] = 1
-    
+
     def _delete_from_l1(self, key: str):
         """Delete from L1 cache"""
         if key in self.l1_cache:
             del self.l1_cache[key]
         if key in self.l1_access_count:
             del self.l1_access_count[key]
-    
+
     async def _get_from_l2(self, key: str) -> Optional[Any]:
         """Get from L2 (Redis) cache"""
         if not self.enabled or not self.redis:
             return None
-        
+
         try:
             data = await self.redis.get(key)
             if data:
                 return pickle.loads(data)
         except Exception as e:
             logger.error(f"L2 cache get error: {e}")
         return None
-    
+
     async def _set_to_l2(self, key: str, value: Any, ttl: int):
         """Set to L2 (Redis) cache"""
         if not self.enabled or not self.redis:
             return
-        
+
         try:
             data = pickle.dumps(value)
             await self.redis.setex(key, ttl, data)
         except Exception as e:
             logger.error(f"L2 cache set error: {e}")
-    
+
     async def _delete_from_l2(self, key: str):
         """Delete from L2 cache"""
         if not self.enabled or not self.redis:
             return
-        
+
         try:
             await self.redis.delete(key)
         except Exception as e:
             logger.error(f"L2 cache delete error: {e}")
-    
+
     def get_stats(self) -> dict:
         """Get cache statistics"""
         total = self.stats["total_requests"]
         if total == 0:
             return {**self.stats, "l1_hit_rate": 0, "l2_hit_rate": 0, "overall_hit_rate": 0}
-        
+
         l1_hit_rate = self.stats["l1_hits"] / total
         l2_requests = total - self.stats["l1_hits"]
         l2_hit_rate = self.stats["l2_hits"] / l2_requests if l2_requests > 0 else 0
         overall_hit_rate = (self.stats["l1_hits"] + self.stats["l2_hits"]) / total
-        
+
         return {
             **self.stats,
             "l1_hit_rate": l1_hit_rate,
             "l2_hit_rate": l2_hit_rate,
             "overall_hit_rate": overall_hit_rate,
             "l1_size": len(self.l1_cache),
-            "l1_max_size": self.l1_max_size
+            "l1_max_size": self.l1_max_size,
         }
-    
+
     def reset_stats(self):
         """Reset statistics"""
         self.stats = {
             "l1_hits": 0,
             "l1_misses": 0,
             "l2_hits": 0,
             "l2_misses": 0,
             "total_requests": 0,
             "sets": 0,
-            "deletes": 0
+            "deletes": 0,
         }
 
 
 def cached(
-    ttl: int = 3600,
-    key_prefix: str = "",
-    strategy: CacheStrategy = CacheStrategy.WRITE_THROUGH
+    ttl: int = 3600, key_prefix: str = "", strategy: CacheStrategy = CacheStrategy.WRITE_THROUGH
 ):
     """
     Decorator for caching function results
-    
+
     Usage:
         @cached(ttl=300, key_prefix="user_data")
         async def get_user(user_id: str):
             return await db.get_user(user_id)
-        
+
         # With different strategies
         @cached(ttl=60, strategy=CacheStrategy.WRITE_BACK)
         async def get_temporary_data():
             return expensive_operation()
     """
+
     def decorator(func: Callable):
         @wraps(func)
         async def wrapper(*args, **kwargs):
             # Generate cache key from function name and arguments
             key_parts = [key_prefix or func.__name__]
             key_parts.extend(str(arg) for arg in args)
             key_parts.extend(f"{k}={v}" for k, v in sorted(kwargs.items()))
             cache_key = ":".join(key_parts)
-            
+
             # Try to get from cache
             cache = get_cache_manager()
             if cache:
                 cached_value = await cache.get(cache_key)
                 if cached_value is not None:
                     return cached_value
-            
+
             # Execute function
             result = await func(*args, **kwargs)
-            
+
             # Cache result
             if cache:
                 await cache.set(cache_key, result, ttl, strategy)
-            
+
             return result
+
         return wrapper
+
     return decorator
 
 
 # Global cache manager instance
 _cache_manager: Optional[MultiLevelCache] = None
 
 
 def initialize_cache(redis_client: Optional[redis.Redis], default_ttl: int = 3600):
     """
     Initialize global cache manager
-    
+
     Usage:
         from shared.cache import initialize_cache
         import redis.asyncio as redis
-        
+
         redis_client = redis.from_url(settings.REDIS_URL)
         initialize_cache(redis_client, default_ttl=3600)
     """
     global _cache_manager
     _cache_manager = MultiLevelCache(redis_client, default_ttl)
would reformat /home/runner/work/ymera_y/ymera_y/cache_manager.py
--- /home/runner/work/ymera_y/ymera_y/chat_handler.py	2025-10-19 22:47:02.795432+00:00
+++ /home/runner/work/ymera_y/ymera_y/chat_handler.py	2025-10-19 23:08:58.679319+00:00
@@ -9,79 +9,67 @@
 from shared.database.connection_pool import DatabaseManager
 
 
 class ChatHandler:
     """Handles chat communication with users"""
-    
+
     def __init__(self, db_manager: DatabaseManager, settings: Settings):
         self.db_manager = db_manager
         self.settings = settings
         self.logger = structlog.get_logger(__name__)
         self.active_sessions = {}
-    
-    async def process_message(
-        self,
-        user_id: str,
-        message: str,
-        agent_name: str
-    ) -> Dict[str, Any]:
+
+    async def process_message(self, user_id: str, message: str, agent_name: str) -> Dict[str, Any]:
         """Process user message and generate response"""
         try:
             # Create or get session
             if user_id not in self.active_sessions:
-                self.active_sessions[user_id] = {
-                    'created_at': datetime.utcnow(),
-                    'messages': []
-                }
-            
+                self.active_sessions[user_id] = {"created_at": datetime.utcnow(), "messages": []}
+
             # Add message to history
-            self.active_sessions[user_id]['messages'].append({
-                'role': 'user',
-                'content': message,
-                'timestamp': datetime.utcnow().isoformat()
-            })
-            
+            self.active_sessions[user_id]["messages"].append(
+                {"role": "user", "content": message, "timestamp": datetime.utcnow().isoformat()}
+            )
+
             # Generate response based on message content
             response = await self._generate_response(message, agent_name)
-            
+
             # Add response to history
-            self.active_sessions[user_id]['messages'].append({
-                'role': 'assistant',
-                'content': response,
-                'timestamp': datetime.utcnow().isoformat()
-            })
-            
-            return {
-                'response': response,
-                'session_id': user_id,
-                'agent': agent_name
-            }
-            
+            self.active_sessions[user_id]["messages"].append(
+                {
+                    "role": "assistant",
+                    "content": response,
+                    "timestamp": datetime.utcnow().isoformat(),
+                }
+            )
+
+            return {"response": response, "session_id": user_id, "agent": agent_name}
+
         except Exception as e:
             self.logger.error(f"Chat processing error: {e}", exc_info=True)
             return {
-                'response': "I apologize, but I encountered an error processing your message.",
-                'error': str(e)
+                "response": "I apologize, but I encountered an error processing your message.",
+                "error": str(e),
             }
-    
+
     async def _generate_response(self, message: str, agent_name: str) -> str:
         """Generate response to user message"""
         message_lower = message.lower()
-        
+
         # Simple rule-based responses (in production, use AI model)
-        if 'status' in message_lower:
+        if "status" in message_lower:
             return f"The {agent_name} is currently operational and processing requests."
-        
-        elif 'help' in message_lower:
+
+        elif "help" in message_lower:
             return (
                 f"I'm the {agent_name}. I can help you with:\n"
                 "- Project status inquiries\n"
                 "- File operations\n"
                 "- Quality reports\n"
                 "- General questions about the system"
             )
-        
-        elif 'quality' in message_lower or 'score' in message_lower:
+
+        elif "quality" in message_lower or "score" in message_lower:
             return "I analyze code quality based on documentation, error handling, type hints, test coverage, and code complexity."
-        
+
         else:
             return f"Thank you for your message. The {agent_name} has received your inquiry and will process it accordingly."
would reformat /home/runner/work/ymera_y/ymera_y/chat_handler.py
--- /home/runner/work/ymera_y/ymera_y/base_agent.py	2025-10-19 22:47:02.795432+00:00
+++ /home/runner/work/ymera_y/ymera_y/base_agent.py	2025-10-19 23:08:58.925397+00:00
@@ -1,6 +1,5 @@
-
 """
 Advanced Base Agent Framework
 Combines cutting-edge features with operational simplicity
 """
 
@@ -35,70 +34,75 @@
 from prometheus_client import Counter, Histogram, Gauge, start_http_server
 import structlog
 
 # === CORE DATA STRUCTURES ===
 
+
 class AgentStatus(Enum):
     INITIALIZING = "initializing"
     ACTIVE = "active"
     DEGRADED = "degraded"
     MAINTENANCE = "maintenance"
     ERROR = "error"
     SHUTTING_DOWN = "shutting_down"
 
+
 class TaskStatus(Enum):
     PENDING = "pending"
     RUNNING = "running"
     COMPLETED = "completed"
     FAILED = "failed"
     CANCELLED = "cancelled"
     TIMEOUT = "timeout"
 
+
 class Priority(Enum):
     LOW = 1
     NORMAL = 2
     HIGH = 3
     CRITICAL = 4
+
 
 @dataclass
 class AgentConfig:
     name: str
     version: str = "1.0.0"
     agent_type: str = "generic"
     capabilities: List[str] = field(default_factory=list)
-    
+
     # Connection configs
     nats_url: str = "nats://nats:4222"
     postgres_url: str = "postgresql://agent:secure_password@postgres:5432/ymera"
     redis_url: str = "redis://redis:6379"
     consul_url: str = "http://consul:8500"
-    
+
     # Operational configs
     log_level: str = "INFO"
     max_concurrent_tasks: int = 10
-    heartbeat_interval: int = 5 # seconds
-    health_check_interval: int = 30 # seconds
-    graceful_shutdown_timeout: int = 30 # seconds
-    config_refresh_interval: int = 60 # seconds
-    
+    heartbeat_interval: int = 5  # seconds
+    health_check_interval: int = 30  # seconds
+    graceful_shutdown_timeout: int = 30  # seconds
+    config_refresh_interval: int = 60  # seconds
+
     # Observability
     metrics_port: int = 9100
     enable_tracing: bool = True
     jaeger_endpoint: str = "http://jaeger:14268/api/traces"
-    
+
     # Advanced features
     enable_circuit_breaker: bool = True
     circuit_breaker_failure_threshold: int = 5
     circuit_breaker_timeout: int = 60
     enable_rate_limiting: bool = True
     rate_limit_requests: int = 100
     rate_limit_window: int = 60
-    
+
     # Custom configurations
     custom: Dict[str, Any] = field(default_factory=dict)
 
-@dataclass 
+
+@dataclass
 class TaskRequest:
     task_id: str
     task_type: str
     payload: Dict[str, Any]
     priority: Priority = Priority.NORMAL
@@ -106,20 +110,22 @@
     retry_policy: Optional[Dict[str, Any]] = None
     metadata: Dict[str, Any] = field(default_factory=dict)
     correlation_id: Optional[str] = None
     parent_task_id: Optional[str] = None
     workflow_id: Optional[str] = None
-    sender_id: Optional[str] = None # Added sender_id
+    sender_id: Optional[str] = None  # Added sender_id
+
 
 @dataclass
 class TaskResponse:
     task_id: str
     success: bool
     result: Dict[str, Any] = field(default_factory=dict)
     error: Optional[str] = None
     execution_time_ms: Optional[float] = None
     metadata: Dict[str, Any] = field(default_factory=dict)
+
 
 @dataclass
 class HealthStatus:
     agent_id: str
     agent_name: str
@@ -131,72 +137,77 @@
     last_error: Optional[str] = None
     resource_usage: Dict[str, float] = field(default_factory=dict)
     dependencies: Dict[str, bool] = field(default_factory=dict)
     timestamp: float = field(default_factory=time.time)
 
+
 # === CIRCUIT BREAKER IMPLEMENTATION ===
+
 
 class CircuitBreakerState(Enum):
     CLOSED = "closed"
-    OPEN = "open"  
+    OPEN = "open"
     HALF_OPEN = "half_open"
+
 
 class CircuitBreaker:
     def __init__(self, failure_threshold: int = 5, timeout: int = 60):
         self.failure_threshold = failure_threshold
         self.timeout = timeout
         self.failure_count = 0
         self.last_failure_time = 0
         self.state = CircuitBreakerState.CLOSED
-    
+
     async def call(self, func: Callable, *args, **kwargs):
         if self.state == CircuitBreakerState.OPEN:
             if time.time() - self.last_failure_time > self.timeout:
                 self.state = CircuitBreakerState.HALF_OPEN
             else:
                 raise Exception("Circuit breaker is OPEN")
-        
+
         try:
             result = await func(*args, **kwargs)
             if self.state == CircuitBreakerState.HALF_OPEN:
                 self.state = CircuitBreakerState.CLOSED
                 self.failure_count = 0
             return result
         except Exception as e:
             self.failure_count += 1
             self.last_failure_time = time.time()
-            
+
             if self.failure_count >= self.failure_threshold:
                 self.state = CircuitBreakerState.OPEN
-            
+
             raise e
 
+
 # === ADVANCED BASE AGENT ===
 
 T = TypeVar("T")
+
 
 class BaseAgent(abc.ABC):
     """
     Advanced base agent with cutting-edge features:
     - Distributed tracing & metrics
-    - Circuit breakers & rate limiting  
+    - Circuit breakers & rate limiting
     - Service discovery & health checks
     - Graceful shutdown & resource management
     - Auto-retry with exponential backoff
     - Stream processing capabilities
     - Dynamic configuration management
     """
-    
+
     def __init__(self, config: AgentConfig):
         self.config = config
         self.id = f"{config.name}-{uuid.uuid4().hex[:8]}"
         self.start_time = time.time()
         self.status = AgentStatus.INITIALIZING
         self.active_tasks = 0
         self.total_tasks_processed = 0
         self.last_error: Optional[str] = None
-        
+
         # Setup structured logging
         structlog.configure(
             processors=[
                 structlog.stdlib.filter_by_level,
                 structlog.stdlib.add_logger_name,
@@ -204,280 +215,273 @@
                 structlog.stdlib.PositionalArgumentsFormatter(),
                 structlog.processors.TimeStamper(fmt="iso"),
                 structlog.processors.StackInfoRenderer(),
                 structlog.processors.format_exc_info,
                 structlog.processors.UnicodeDecoder(),
-                structlog.processors.JSONRenderer()
+                structlog.processors.JSONRenderer(),
             ],
             context_class=dict,
             logger_factory=structlog.stdlib.LoggerFactory(),
             wrapper_class=structlog.stdlib.BoundLogger,
             cache_logger_on_first_use=True,
         )
-        
+
         self.logger = structlog.get_logger(self.id)
-        
+
         # Initialize connections (will be established in connect())
         self.nc: Optional[NATS] = None
         self.js = None  # JetStream context
         self.redis: Optional[aioredis.Redis] = None
         self.db_pool: Optional[asyncpg.Pool] = None
         self.consul_client = None
-        
+
         # Observability
         self._setup_observability()
-        
+
         # Advanced features
         self.circuit_breakers: Dict[str, CircuitBreaker] = {}
         self.rate_limiters: Dict[str, Dict] = {}
-        
+
         # Task management
         self._task_semaphore = asyncio.Semaphore(config.max_concurrent_tasks)
         self._running_tasks: Dict[str, asyncio.Task] = {}
         self._subscriptions = []
-        
+
         # Shutdown handling
         self._shutdown_event = asyncio.Event()
         self._cleanup_tasks: List[Callable] = []
-        
+
         # Setup signal handlers
         self._setup_signal_handlers()
 
         # Metrics for performance tracking
         self._task_response_times: List[float] = []
         self._task_success_rates: List[bool] = []
 
     def _setup_observability(self):
         """Setup OpenTelemetry tracing and Prometheus metrics"""
         # Resource identification
-        resource = Resource.create({
-            "service.name": f"ymera-{self.config.name}",
-            "service.version": self.config.version,
-            "agent.id": self.id,
-            "agent.type": self.config.agent_type
-        })
-        
+        resource = Resource.create(
+            {
+                "service.name": f"ymera-{self.config.name}",
+                "service.version": self.config.version,
+                "agent.id": self.id,
+                "agent.type": self.config.agent_type,
+            }
+        )
+
         # Tracing
         if self.config.enable_tracing:
             tracer_provider = TracerProvider(resource=resource)
-            jaeger_exporter = JaegerExporter(
-                agent_host_name="jaeger",
-                agent_port=6831
-            )
+            jaeger_exporter = JaegerExporter(agent_host_name="jaeger", agent_port=6831)
             span_processor = BatchSpanProcessor(jaeger_exporter)
             tracer_provider.add_span_processor(span_processor)
             trace.set_tracer_provider(tracer_provider)
-            
+
         self.tracer = trace.get_tracer(__name__)
-        
+
         # Metrics
         prometheus_reader = PrometheusMetricReader()
-        meter_provider = MeterProvider(
-            resource=resource,
-            metric_readers=[prometheus_reader]
-        )
+        meter_provider = MeterProvider(resource=resource, metric_readers=[prometheus_reader])
         metrics.set_meter_provider(meter_provider)
         self.meter = metrics.get_meter(__name__)
-        
+
         # Custom metrics
         self.task_counter = self.meter.create_counter(
-            "agent_tasks_total",
-            description="Total number of tasks processed"
+            "agent_tasks_total", description="Total number of tasks processed"
         )
-        
+
         self.task_duration = self.meter.create_histogram(
-            "agent_task_duration_seconds",
-            description="Task execution duration"
+            "agent_task_duration_seconds", description="Task execution duration"
         )
-        
+
         self.active_tasks_gauge = self.meter.create_up_down_counter(
-            "agent_active_tasks",
-            description="Currently active tasks"
+            "agent_active_tasks", description="Currently active tasks"
         )
-        
+
         # Start Prometheus metrics server
         start_http_server(self.config.metrics_port)
-    
+
     def _setup_signal_handlers(self):
         """Setup graceful shutdown signal handlers"""
+
         def signal_handler(signum, frame):
             self.logger.info("Received shutdown signal", signal=signum)
             asyncio.create_task(self.shutdown())
-        
+
         signal.signal(signal.SIGINT, signal_handler)
         signal.signal(signal.SIGTERM, signal_handler)
-    
+
     async def connect(self):
         """Establish all external connections"""
         self.logger.info("Establishing connections...")
-        
+
         try:
             # NATS connection with JetStream
             self.nc = NATS()
             await self.nc.connect(
                 self.config.nats_url,
                 error_cb=self._nats_error_cb,
                 disconnected_cb=self._nats_disconnected_cb,
-                reconnected_cb=self._nats_reconnected_cb
-            )
-            
+                reconnected_cb=self._nats_reconnected_cb,
+            )
+
             # Enable JetStream
             self.js = self.nc.jetstream()
-            
+
             # Ensure required streams exist
             await self._setup_streams()
-            
+
             self.logger.info("NATS connected with JetStream enabled")
-            
+
             # Redis connection
             self.redis = aioredis.from_url(
-                self.config.redis_url,
-                encoding="utf8",
-                decode_responses=True
+                self.config.redis_url, encoding="utf8", decode_responses=True
             )
             await self.redis.ping()
             self.logger.info("Redis connected")
-            
+
             # PostgreSQL connection pool
             self.db_pool = await asyncpg.create_pool(
-                self.config.postgres_url,
-                min_size=2,
-                max_size=10,
-                command_timeout=30
+                self.config.postgres_url, min_size=2, max_size=10, command_timeout=30
             )
             self.logger.info("PostgreSQL pool created")
-            
+
             # Consul client
-            self.consul_client = consul.Consul(
-                host="consul",
-                port=8500
-            )
-            
+            self.consul_client = consul.Consul(host="consul", port=8500)
+
             # Register with service discovery
             await self._register_with_consul()
-            
+
             self.logger.info("All connections established")
-            
+
         except Exception as e:
             self.logger.error("Failed to establish connections", error=str(e))
             raise
-    
+
     async def _setup_streams(self):
         """Setup required NATS JetStream streams"""
         streams_config = [
             StreamConfig(
                 name="TASKS",
                 subjects=["task.*", "agent.*.task"],
                 retention="workqueue",
-                max_age=86400  # 24 hours
+                max_age=86400,  # 24 hours
             ),
             StreamConfig(
                 name="EVENTS",
                 subjects=["event.*", "agent.*.event"],
                 retention="limits",
-                max_age=604800  # 7 days
+                max_age=604800,  # 7 days
             ),
             StreamConfig(
-                name="METRICS", 
+                name="METRICS",
                 subjects=["metrics.*", "agent.*.metrics", "agent.heartbeat", "agent.register"],
                 retention="limits",
-                max_age=259200  # 3 days
-            )
+                max_age=259200,  # 3 days
+            ),
         ]
-        
+
         for stream_config in streams_config:
             try:
                 await self.js.add_stream(stream_config)
             except Exception as e:
                 if "stream name already in use" not in str(e):
-                    self.logger.warning("Failed to create stream", 
-                                      stream=stream_config.name, error=str(e))
-    
+                    self.logger.warning(
+                        "Failed to create stream", stream=stream_config.name, error=str(e)
+                    )
+
     async def _register_with_consul(self):
         """Register agent with Consul service discovery"""
         service_def = {
-            'Name': self.config.name,
-            'ID': self.id,
-            'Tags': [self.config.agent_type] + self.config.capabilities,
-            'Address': 'localhost',  # Will be container IP in real deployment
-            'Port': self.config.metrics_port,
-            'Check': {
-                'HTTP': f'http://localhost:{self.config.metrics_port}/health',
-                'Interval': '10s',
-                'Timeout': '5s'
+            "Name": self.config.name,
+            "ID": self.id,
+            "Tags": [self.config.agent_type] + self.config.capabilities,
+            "Address": "localhost",  # Will be container IP in real deployment
+            "Port": self.config.metrics_port,
+            "Check": {
+                "HTTP": f"http://localhost:{self.config.metrics_port}/health",
+                "Interval": "10s",
+                "Timeout": "5s",
             },
-            'Meta': {
-                'version': self.config.version,
-                'agent_type': self.config.agent_type,
-                'capabilities': json.dumps(self.config.capabilities)
-            }
+            "Meta": {
+                "version": self.config.version,
+                "agent_type": self.config.agent_type,
+                "capabilities": json.dumps(self.config.capabilities),
+            },
         }
-        
+
         try:
             self.consul_client.agent.service.register(**service_def)
             self.logger.info("Registered with Consul", service_id=self.id)
         except Exception as e:
             self.logger.warning("Failed to register with Consul", error=str(e))
-    
+
     async def _nats_error_cb(self, e):
         self.logger.error("NATS error", error=str(e))
-    
+
     async def _nats_disconnected_cb(self):
         self.logger.warning("NATS disconnected")
         self.status = AgentStatus.DEGRADED
-    
+
     async def _nats_reconnected_cb(self):
         self.logger.info("NATS reconnected")
         if self.status == AgentStatus.DEGRADED:
             self.status = AgentStatus.ACTIVE
-    
+
     @asynccontextmanager
     async def _rate_limit(self, operation: str):
         """Rate limiting context manager"""
         if not self.config.enable_rate_limiting:
             yield
             return
-            
+
         now = time.time()
         window_start = now - self.config.rate_limit_window
-        
+
         # Clean old entries
         limiter = self.rate_limiters.setdefault(operation, {"requests": [], "count": 0})
-        limiter["requests"] = [req_time for req_time in limiter["requests"] if req_time > window_start]
+        limiter["requests"] = [
+            req_time for req_time in limiter["requests"] if req_time > window_start
+        ]
         limiter["count"] = len(limiter["requests"])
-        
+
         if limiter["count"] >= self.config.rate_limit_requests:
             raise Exception(f"Rate limit exceeded for {operation}")
-        
+
         limiter["requests"].append(now)
         limiter["count"] += 1
         yield
-    
+
     def get_circuit_breaker(self, operation: str) -> CircuitBreaker:
         """Get or create circuit breaker for operation"""
         if operation not in self.circuit_breakers:
             self.circuit_breakers[operation] = CircuitBreaker(
-                self.config.circuit_breaker_failure_threshold,
-                self.config.circuit_breaker_timeout
+                self.config.circuit_breaker_failure_threshold, self.config.circuit_breaker_timeout
             )
         return self.circuit_breakers[operation]
-    
+
     async def _subscribe(self, subject: str, callback: Callable, queue_group: Optional[str] = None):
         """Enhanced subscription with error handling and metrics"""
+
         async def wrapped_callback(msg):
             with self.tracer.start_as_current_span(f"message_received_{subject}") as span:
                 span.set_attribute("subject", msg.subject)
                 span.set_attribute("agent_id", self.id)
-                
+
                 try:
                     await callback(msg)
                 except Exception as e:
-                    self.logger.error("Message processing failed", 
-                                    subject=msg.subject, error=str(e), traceback=traceback.format_exc())
+                    self.logger.error(
+                        "Message processing failed",
+                        subject=msg.subject,
+                        error=str(e),
+                        traceback=traceback.format_exc(),
+                    )
                     span.record_exception(e)
                     span.set_status(trace.Status(trace.StatusCode.ERROR))
-        
+
         if queue_group:
             sub = await self.nc.subscribe(subject, queue=queue_group, cb=wrapped_callback)
         else:
             sub = await self.nc.subscribe(subject, cb=wrapped_callback)
         self._subscriptions.append(sub)
@@ -499,15 +503,22 @@
             try:
                 js = self.nc.jetstream()
                 await js.publish(stream_name, json.dumps(data).encode())
                 self.logger.debug("Published to stream", stream_name=stream_name)
             except Exception as e:
-                self.logger.error("Failed to publish to stream", stream_name=stream_name, error=str(e), traceback=traceback.format_exc())
+                self.logger.error(
+                    "Failed to publish to stream",
+                    stream_name=stream_name,
+                    error=str(e),
+                    traceback=traceback.format_exc(),
+                )
                 span.record_exception(e)
                 span.set_status(trace.Status(trace.StatusCode.ERROR))
 
-    async def _request(self, subject: str, data: bytes, timeout: float = 1.0) -> Optional[Dict[str, Any]]:
+    async def _request(
+        self, subject: str, data: bytes, timeout: float = 1.0
+    ) -> Optional[Dict[str, Any]]:
         """Send a NATS request and wait for a response"""
         with self.tracer.start_as_current_span(f"nats_request_{subject}") as span:
             span.set_attribute("subject", subject)
             span.set_attribute("agent_id", self.id)
             try:
@@ -518,11 +529,16 @@
             except nats.errors.TimeoutError:
                 self.logger.warning("NATS request timed out", subject=subject)
                 span.set_status(trace.Status(trace.StatusCode.ERROR, "Timeout"))
                 return None
             except Exception as e:
-                self.logger.error("NATS request failed", subject=subject, error=str(e), traceback=traceback.format_exc())
+                self.logger.error(
+                    "NATS request failed",
+                    subject=subject,
+                    error=str(e),
+                    traceback=traceback.format_exc(),
+                )
                 span.record_exception(e)
                 span.set_status(trace.Status(trace.StatusCode.ERROR))
                 return None
 
     async def run(self):
@@ -530,34 +546,36 @@
         self.logger.info("Agent starting", agent_id=self.id, agent_name=self.config.name)
         try:
             await self.connect()
             await self._fetch_initial_config()
             await self._register_agent()
-            
+
             # Start agent-specific logic (to be overridden by subclasses)
             await self.start()
-            
+
             # Start background tasks
             asyncio.create_task(self._heartbeat_sender())
             asyncio.create_task(self._metrics_publisher())
             asyncio.create_task(self._config_watcher())
 
             # Subscribe to agent-specific task queue
             await self._subscribe(
                 f"agent.{self.config.name}.task",
                 self._handle_task_request,
-                queue_group=f"{self.config.name}_workers"
+                queue_group=f"{self.config.name}_workers",
             )
 
             self.status = AgentStatus.ACTIVE
             self.logger.info("Agent is active", agent_id=self.id, agent_name=self.config.name)
-            
-            await self._shutdown_event.wait() # Keep agent running until shutdown event is set
+
+            await self._shutdown_event.wait()  # Keep agent running until shutdown event is set
         except asyncio.CancelledError:
             self.logger.info("Agent received cancellation signal.", agent_id=self.id)
         except Exception as e:
-            self.logger.error("Agent encountered a fatal error", error=str(e), traceback=traceback.format_exc())
+            self.logger.error(
+                "Agent encountered a fatal error", error=str(e), traceback=traceback.format_exc()
+            )
             self.status = AgentStatus.ERROR
         finally:
             await self.shutdown()
 
     @abc.abstractmethod
@@ -585,11 +603,11 @@
 
         # Close NATS connection
         if self.nc and self.nc.is_connected:
             await self.nc.close()
             self.logger.info("NATS connection closed.")
-        
+
         # Close database pool
         if self.db_pool:
             await self.db_pool.close()
             self.logger.info("PostgreSQL connection pool closed.")
 
@@ -608,54 +626,69 @@
 
         self.logger.info("Agent stopped.", agent_id=self.id)
 
     async def _fetch_initial_config(self):
         """Fetch initial configuration from ConfigManager."""
-        self.logger.info("Fetching initial configuration from ConfigManager", agent_name=self.config.name)
+        self.logger.info(
+            "Fetching initial configuration from ConfigManager", agent_name=self.config.name
+        )
         try:
             response = await self._request(
-                "config.get",
-                json.dumps({"agent_name": self.config.name}).encode(),
-                timeout=5
+                "config.get", json.dumps({"agent_name": self.config.name}).encode(), timeout=5
             )
             if response and "config" in response:
                 # Merge fetched config with current config, prioritizing fetched values
                 for key, value in response["config"].items():
                     if hasattr(self.config, key):
                         setattr(self.config, key, value)
-                self.logger.info("Initial configuration loaded from ConfigManager.", agent_name=self.config.name)
+                self.logger.info(
+                    "Initial configuration loaded from ConfigManager.", agent_name=self.config.name
+                )
             else:
-                self.logger.warning("No specific configuration found for agent in ConfigManager. Using defaults.", agent_name=self.config.name)
+                self.logger.warning(
+                    "No specific configuration found for agent in ConfigManager. Using defaults.",
+                    agent_name=self.config.name,
+                )
         except Exception as e:
-            self.logger.error("Failed to fetch initial config", agent_name=self.config.name, error=str(e), traceback=traceback.format_exc())
+            self.logger.error(
+                "Failed to fetch initial config",
+                agent_name=self.config.name,
+                error=str(e),
+                traceback=traceback.format_exc(),
+            )
 
     async def _config_watcher(self):
         """Watch for dynamic configuration updates from ConfigManager."""
         while not self._shutdown_event.is_set():
             try:
                 # Request config periodically, or subscribe to push updates
                 # For now, let's implement a pull mechanism for simplicity and robustness
                 await asyncio.sleep(self.config.config_refresh_interval)
                 self.logger.debug("Checking for config updates", agent_name=self.config.name)
                 response = await self._request(
-                    "config.get",
-                    json.dumps({"agent_name": self.config.name}).encode(),
-                    timeout=5
+                    "config.get", json.dumps({"agent_name": self.config.name}).encode(), timeout=5
                 )
                 if response and "config" in response:
                     new_config_data = response["config"]
                     # Compare and apply changes
                     current_config_dict = asdict(self.config)
                     if any(current_config_dict.get(k) != v for k, v in new_config_data.items()):
-                        self.logger.info("Applying dynamic configuration update.", agent_name=self.config.name)
+                        self.logger.info(
+                            "Applying dynamic configuration update.", agent_name=self.config.name
+                        )
                         for key, value in new_config_data.items():
                             if hasattr(self.config, key):
                                 setattr(self.config, key, value)
-                        if hasattr(self, '_on_config_update'):
+                        if hasattr(self, "_on_config_update"):
                             await self._on_config_update(new_config_data)
             except Exception as e:
-                self.logger.error("Error in config watcher", agent_name=self.config.name, error=str(e), traceback=traceback.format_exc())
+                self.logger.error(
+                    "Error in config watcher",
+                    agent_name=self.config.name,
+                    error=str(e),
+                    traceback=traceback.format_exc(),
+                )
 
     async def _register_agent(self):
         """Register agent with the Orchestrator and persist in DB."""
         if not self.db_pool:
             self.logger.warning("DB pool not initialized, skipping agent registration persistence.")
@@ -666,144 +699,191 @@
             "agent_type": self.config.agent_type,
             "capabilities": self.config.capabilities,
             "version": self.config.version,
             "description": f"A {self.config.agent_type} agent named {self.config.name}",
             "max_concurrent_tasks": self.config.max_concurrent_tasks,
-            "status": self.status.value # Initial status
+            "status": self.status.value,  # Initial status
         }
-        
+
         # Publish registration to Orchestrator (which subscribes to agent.register on METRICS stream)
         await self._publish_to_stream("agent.register", registration_data)
         self.logger.info("Agent registered with Orchestrator.", agent_name=self.config.name)
 
     async def _heartbeat_sender(self):
         """Periodically send heartbeats to the Orchestrator."""
         while not self._shutdown_event.is_set():
             try:
-                avg_response_time = sum(self._task_response_times) / len(self._task_response_times) if self._task_response_times else 0.0
-                success_rate = sum(1 for s in self._task_success_rates if s) / len(self._task_success_rates) if self._task_success_rates else 1.0
-                
+                avg_response_time = (
+                    sum(self._task_response_times) / len(self._task_response_times)
+                    if self._task_response_times
+                    else 0.0
+                )
+                success_rate = (
+                    sum(1 for s in self._task_success_rates if s) / len(self._task_success_rates)
+                    if self._task_success_rates
+                    else 1.0
+                )
+
                 heartbeat_data = {
                     "agent_id": self.id,
                     "agent_name": self.config.name,
                     "agent_type": self.config.agent_type,
                     "capabilities": self.config.capabilities,
                     "status": self.status.value,
                     "current_load": self.active_tasks,
                     "max_load": self.config.max_concurrent_tasks,
-                    "health_score": 1.0, # TODO: Implement actual health score calculation
+                    "health_score": 1.0,  # TODO: Implement actual health score calculation
                     "average_response_time": avg_response_time,
                     "success_rate": success_rate,
-                    "timestamp": time.time()
+                    "timestamp": time.time(),
                 }
                 # Publish heartbeat to Orchestrator (which subscribes to agent.heartbeat on METRICS stream)
                 await self._publish_to_stream("agent.heartbeat", heartbeat_data)
-                self.logger.debug("Heartbeat sent.", agent_name=self.config.name, load=self.active_tasks)
-                
+                self.logger.debug(
+                    "Heartbeat sent.", agent_name=self.config.name, load=self.active_tasks
+                )
+
                 # Clear recent performance metrics to avoid unbounded growth
                 self._task_response_times.clear()
                 self._task_success_rates.clear()
 
                 await asyncio.sleep(self.config.heartbeat_interval)
             except Exception as e:
-                self.logger.error("Error in heartbeat sender", agent_name=self.config.name, error=str(e), traceback=traceback.format_exc())
+                self.logger.error(
+                    "Error in heartbeat sender",
+                    agent_name=self.config.name,
+                    error=str(e),
+                    traceback=traceback.format_exc(),
+                )
                 await asyncio.sleep(self.config.heartbeat_interval)
 
     async def _metrics_publisher(self):
         """Periodically publish agent-specific metrics."""
         while not self._shutdown_event.is_set():
             try:
-                await asyncio.sleep(self.config.heartbeat_interval) # Use heartbeat interval for metrics for now
+                await asyncio.sleep(
+                    self.config.heartbeat_interval
+                )  # Use heartbeat interval for metrics for now
                 metrics_data = self._get_agent_metrics()
                 if metrics_data:
-                    await self._publish_to_stream("metrics.agent.data", {
-                        "agent_id": self.id,
-                        "agent_name": self.config.name,
-                        "agent_type": self.config.agent_type,
-                        "timestamp": time.time(),
-                        "metrics": metrics_data
-                    })
+                    await self._publish_to_stream(
+                        "metrics.agent.data",
+                        {
+                            "agent_id": self.id,
+                            "agent_name": self.config.name,
+                            "agent_type": self.config.agent_type,
+                            "timestamp": time.time(),
+                            "metrics": metrics_data,
+                        },
+                    )
                     self.logger.debug("Published agent metrics.", agent_name=self.config.name)
             except Exception as e:
-                self.logger.error("Error in metrics publisher", agent_name=self.config.name, error=str(e), traceback=traceback.format_exc())
+                self.logger.error(
+                    "Error in metrics publisher",
+                    agent_name=self.config.name,
+                    error=str(e),
+                    traceback=traceback.format_exc(),
+                )
                 await asyncio.sleep(self.config.heartbeat_interval)
 
     def _get_agent_metrics(self) -> Dict[str, Any]:
         """Agents should override this to provide specific metrics."""
         return {
             "current_load": self.active_tasks,
             "status": self.status.value,
             "uptime_seconds": time.time() - self.start_time,
             "total_tasks_processed": self.total_tasks_processed,
-            "avg_task_response_time": sum(self._task_response_times) / len(self._task_response_times) if self._task_response_times else 0.0,
-            "task_success_rate": sum(1 for s in self._task_success_rates if s) / len(self._task_success_rates) if self._task_success_rates else 1.0,
-            "last_error": self.last_error
+            "avg_task_response_time": (
+                sum(self._task_response_times) / len(self._task_response_times)
+                if self._task_response_times
+                else 0.0
+            ),
+            "task_success_rate": (
+                sum(1 for s in self._task_success_rates if s) / len(self._task_success_rates)
+                if self._task_success_rates
+                else 1.0
+            ),
+            "last_error": self.last_error,
         }
 
     async def _handle_task_request(self, msg):
         with self.tracer.start_as_current_span("handle_task_request") as span:
             task_start_time = time.time()
             request_data = json.loads(msg.data.decode())
             request = TaskRequest(**request_data)
-            
+
             span.set_attribute("task.id", request.task_id)
             span.set_attribute("task.type", request.task_type)
             span.set_attribute("agent.name", self.config.name)
 
             self.logger.info("Received task", task_id=request.task_id, task_type=request.task_type)
-            
+
             # Check if agent is overloaded
             if self.active_tasks >= self.config.max_concurrent_tasks:
                 error_msg = f"Agent {self.config.name} is overloaded. Max concurrent tasks: {self.config.max_concurrent_tasks}"
                 self.logger.warning(error_msg, task_id=request.task_id)
-                response = TaskResponse(
-                    task_id=request.task_id,
-                    success=False,
-                    error=error_msg
-                )
+                response = TaskResponse(task_id=request.task_id, success=False, error=error_msg)
                 await self._publish(msg.reply, json.dumps(asdict(response)).encode())
                 return
 
             self.active_tasks += 1
-            self.active_tasks_gauge.add(1, {"agent_name": self.config.name, "agent_type": self.config.agent_type})
+            self.active_tasks_gauge.add(
+                1, {"agent_name": self.config.name, "agent_type": self.config.agent_type}
+            )
 
             task_response = None
             try:
                 # Execute the agent's specific task implementation
                 result = await self._execute_task_impl(request)
-                task_response = TaskResponse(
+                task_response = TaskResponse(task_id=request.task_id, success=True, result=result)
+            except Exception as e:
+                self.logger.error(
+                    "Error processing task",
                     task_id=request.task_id,
-                    success=True,
-                    result=result
-                )
-            except Exception as e:
-                self.logger.error("Error processing task", task_id=request.task_id, error=str(e), traceback=traceback.format_exc())
+                    error=str(e),
+                    traceback=traceback.format_exc(),
+                )
                 self.last_error = str(e)
-                task_response = TaskResponse(
-                    task_id=request.task_id,
-                    success=False,
-                    error=str(e)
-                )
+                task_response = TaskResponse(task_id=request.task_id, success=False, error=str(e))
             finally:
                 self.active_tasks -= 1
-                self.active_tasks_gauge.add(-1, {"agent_name": self.config.name, "agent_type": self.config.agent_type})
+                self.active_tasks_gauge.add(
+                    -1, {"agent_name": self.config.name, "agent_type": self.config.agent_type}
+                )
                 self.total_tasks_processed += 1
-                self.task_counter.add(1, {"agent_name": self.config.name, "agent_type": self.config.agent_type, "task_type": request.task_type, "success": str(task_response.success if task_response else False)})
-
-                response_time = (time.time() - task_start_time) * 1000 # in ms
+                self.task_counter.add(
+                    1,
+                    {
+                        "agent_name": self.config.name,
+                        "agent_type": self.config.agent_type,
+                        "task_type": request.task_type,
+                        "success": str(task_response.success if task_response else False),
+                    },
+                )
+
+                response_time = (time.time() - task_start_time) * 1000  # in ms
                 self._task_response_times.append(response_time)
                 self._task_success_rates.append(task_response.success if task_response else False)
-                self.task_duration.record(response_time / 1000, {"agent_name": self.config.name, "agent_type": self.config.agent_type, "task_type": request.task_type})
-
-                span.set_attribute("task.success", task_response.success if task_response else False)
+                self.task_duration.record(
+                    response_time / 1000,
+                    {
+                        "agent_name": self.config.name,
+                        "agent_type": self.config.agent_type,
+                        "task_type": request.task_type,
+                    },
+                )
+
+                span.set_attribute(
+                    "task.success", task_response.success if task_response else False
+                )
                 span.set_attribute("task.execution_time_ms", response_time)
 
                 if msg.reply:
                     # Ensure task_response.execution_time_ms is set before sending
                     task_response.execution_time_ms = response_time
                     await self._publish(msg.reply, json.dumps(asdict(task_response)).encode())
-                
+
                 # Publish task completion to a stream for Orchestrator/MetricsAgent
                 await self._publish_to_stream("task.completed", asdict(task_response))
 
     @abc.abstractmethod
     async def _execute_task_impl(self, request: TaskRequest) -> Dict[str, Any]:
@@ -820,17 +900,27 @@
         def __init__(self, config: AgentConfig):
             super().__init__(config)
             self.processed_messages = 0
 
         async def start(self):
-            self.logger.info("MySimpleAgent is ready to process tasks.", agent_name=self.config.name)
+            self.logger.info(
+                "MySimpleAgent is ready to process tasks.", agent_name=self.config.name
+            )
 
         async def _execute_task_impl(self, request: TaskRequest) -> Dict[str, Any]:
-            self.logger.info("MySimpleAgent processing task", task_type=request.task_type, payload=request.payload)
+            self.logger.info(
+                "MySimpleAgent processing task",
+                task_type=request.task_type,
+                payload=request.payload,
+            )
             self.processed_messages += 1
-            await asyncio.sleep(1) # Simulate work
-            return {"status": "processed", "agent": self.config.name, "processed_count": self.processed_messages}
+            await asyncio.sleep(1)  # Simulate work
+            return {
+                "status": "processed",
+                "agent": self.config.name,
+                "processed_count": self.processed_messages,
+            }
 
         def _get_agent_metrics(self) -> Dict[str, Any]:
             base_metrics = super()._get_agent_metrics()
             base_metrics["processed_messages_total"] = self.processed_messages
             return base_metrics
@@ -839,11 +929,10 @@
         name="simple_agent_1",
         agent_type="simple",
         capabilities=["process_data"],
         nats_url=os.getenv("NATS_URL", "nats://localhost:4222"),
         postgres_url=os.getenv("POSTGRES_URL", "postgresql://user:password@host:port/db"),
-        log_level="DEBUG"
+        log_level="DEBUG",
     )
 
     agent = MySimpleAgent(config)
     asyncio.run(agent.run())
-
error: cannot format /home/runner/work/ymera_y/ymera_y/chat_service.py: Cannot parse for target version Python 3.12: 3:0:     async def create_chat_session(self, name: str, participants: List[Dict], 
would reformat /home/runner/work/ymera_y/ymera_y/base_agent.py
--- /home/runner/work/ymera_y/ymera_y/chat_interface.py	2025-10-19 22:47:02.795432+00:00
+++ /home/runner/work/ymera_y/ymera_y/chat_interface.py	2025-10-19 23:08:59.072960+00:00
@@ -15,138 +15,130 @@
 
 
 class ChatInterface:
     """
     Chat Interface with Natural Language Processing
-    
+
     Features:
     - Context-aware responses
     - Command recognition
     - Multi-language support (placeholder)
     - History tracking
     """
-    
-    def __init__(
-        self,
-        settings,
-        database: ProjectDatabase,
-        agent_orchestrator: AgentOrchestrator
-    ):
+
+    def __init__(self, settings, database: ProjectDatabase, agent_orchestrator: AgentOrchestrator):
         self.settings = settings
         self.database = database
         self.agent_orchestrator = agent_orchestrator
         self.is_initialized = False
-    
+
     async def initialize(self):
         """Initialize chat interface"""
         self.is_initialized = True
         logger.info(" Chat interface initialized")
-    
+
     async def process_message(
-        self,
-        user_id: str,
-        message: str,
-        context: Optional[Dict] = None
+        self, user_id: str, message: str, context: Optional[Dict] = None
     ) -> Dict:
         """
         Process user message and generate response
-        
+
         Args:
             user_id: User ID
             message: User message
             context: Optional context
-        
+
         Returns:
             response: Dict with response and metadata
         """
         logger.info(f"Processing message from user {user_id}: {message[:50]}...")
-        
+
         try:
             # Detect intent
             intent = self._detect_intent(message)
-            
+
             # Generate response based on intent
             if intent == "status_query":
                 response = await self._handle_status_query(message, context)
             elif intent == "help":
                 response = await self._handle_help_request(message)
             elif intent == "command":
                 response = await self._handle_command(message, context)
             else:
                 response = await self._handle_general_query(message, context)
-            
+
             # Store message in history
             await self._store_message(user_id, message, response["response"], context)
-            
+
             return response
-            
+
         except Exception as e:
             logger.error(f"Message processing failed: {e}")
             return {
                 "response": "I apologize, but I'm having trouble understanding your request. Could you rephrase that?",
                 "suggestions": ["Try 'help' for available commands"],
                 "attachments": [],
-                "error": True
-            }
-    
+                "error": True,
+            }
+
     def _detect_intent(self, message: str) -> str:
         """Detect user intent from message"""
         message_lower = message.lower()
-        
+
         # Status queries
         if any(word in message_lower for word in ["status", "progress", "how is"]):
             return "status_query"
-        
+
         # Help requests
         if any(word in message_lower for word in ["help", "how to", "what can"]):
             return "help"
-        
+
         # Commands
         if message.startswith("/"):
             return "command"
-        
+
         return "general"
-    
+
     async def _handle_status_query(self, message: str, context: Optional[Dict]) -> Dict:
         """Handle project status queries"""
         # Extract project info from context
         project_id = context.get("project_id") if context else None
-        
+
         if not project_id:
             return {
                 "response": "Which project would you like to check? Please specify the project ID or name.",
                 "suggestions": ["Show all projects", "List recent projects"],
-                "attachments": []
-            }
-        
+                "attachments": [],
+            }
+
         # Get project status
         project = await self.database.get_project(project_id)
-        
+
         if not project:
             return {
                 "response": f"I couldn't find project {project_id}. Please check the ID.",
                 "suggestions": ["List all projects"],
-                "attachments": []
-            }
-        
+                "attachments": [],
+            }
+
         response_text = f"""
 Project: {project['name']}
 Status: {project['status']}
 Progress: {project.get('progress', 0):.1f}%
 Created: {project['created_at'].strftime('%Y-%m-%d')}
         """.strip()
-        
+
         return {
             "response": response_text,
             "suggestions": [
                 "Show detailed report",
                 "List pending submissions",
-                "Check quality metrics"
+                "Check quality metrics",
             ],
-            "attachments": []
+            "attachments": [],
         }
-    
+
     async def _handle_help_request(self, message: str) -> Dict:
         """Handle help requests"""
         help_text = """
 I can help you with:
 
@@ -170,76 +162,66 @@
   - Get reports
   - View analytics
 
 Try asking: "What's the status of project XYZ?" or "Show quality report"
         """.strip()
-        
+
         return {
             "response": help_text,
             "suggestions": ["Show project status", "List recent submissions"],
-            "attachments": []
+            "attachments": [],
         }
-    
+
     async def _handle_command(self, message: str, context: Optional[Dict]) -> Dict:
         """Handle slash commands"""
         parts = message.split()
         command = parts[0][1:]  # Remove /
-        
+
         if command == "status":
             project_id = parts[1] if len(parts) > 1 else context.get("project_id")
-            return await self._handle_status_query(f"status of {project_id}", {"project_id": project_id})
-        
+            return await self._handle_status_query(
+                f"status of {project_id}", {"project_id": project_id}
+            )
+
         elif command == "help":
             return await self._handle_help_request("")
-        
+
         else:
             return {
                 "response": f"Unknown command: /{command}",
                 "suggestions": ["/help", "/status <project_id>"],
-                "attachments": []
-            }
-    
+                "attachments": [],
+            }
+
     async def _handle_general_query(self, message: str, context: Optional[Dict]) -> Dict:
         """Handle general queries"""
         # Simplified response generation
         # In production, integrate with NLP models
-        
+
         return {
             "response": f"I understand you're asking about: {message[:100]}...\n\nI'm processing your request. How can I help you specifically?",
-            "suggestions": [
-                "Show project status",
-                "Get help",
-                "List commands"
-            ],
-            "attachments": []
+            "suggestions": ["Show project status", "Get help", "List commands"],
+            "attachments": [],
         }
-    
+
     async def _store_message(
-        self,
-        user_id: str,
-        message: str,
-        response: str,
-        context: Optional[Dict]
+        self, user_id: str, message: str, response: str, context: Optional[Dict]
     ):
         """Store message in chat history"""
         query = """
             INSERT INTO chat_messages (user_id, message, response, context)
             VALUES ($1, $2, $3, $4)
         """
-        
+
         import json
+
         await self.database.execute_command(
-            query,
-            user_id,
-            message,
-            response,
-            json.dumps(context or {})
+            query, user_id, message, response, json.dumps(context or {})
         )
-    
+
     async def health_check(self) -> bool:
         """Check chat interface health"""
         return self.is_initialized
-    
+
     async def shutdown(self):
         """Shutdown chat interface"""
         logger.info("Chat interface shutdown complete")
-
would reformat /home/runner/work/ymera_y/ymera_y/chat_interface.py
error: cannot format /home/runner/work/ymera_y/ymera_y/chatting_files_agent_api_system.py: Cannot parse for target version Python 3.12: 766:13: Unexpected EOF in multi-line statement
error: cannot format /home/runner/work/ymera_y/ymera_y/code_editor_agent_api.py: Cannot parse for target version Python 3.12: 2:6: YMERA Code Editing Agent FastAPI Application
--- /home/runner/work/ymera_y/ymera_y/circuit_breaker.py	2025-10-19 22:47:02.795432+00:00
+++ /home/runner/work/ymera_y/ymera_y/circuit_breaker.py	2025-10-19 23:08:59.541711+00:00
@@ -14,276 +14,261 @@
 logger = structlog.get_logger(__name__)
 
 
 class CircuitState(Enum):
     """Circuit breaker states"""
-    CLOSED = "closed"          # Normal operation
-    OPEN = "open"              # Blocking requests
-    HALF_OPEN = "half_open"    # Testing recovery
+
+    CLOSED = "closed"  # Normal operation
+    OPEN = "open"  # Blocking requests
+    HALF_OPEN = "half_open"  # Testing recovery
 
 
 @dataclass
 class CircuitBreakerConfig:
     """Circuit breaker configuration"""
-    failure_threshold: int = 5        # Failures before opening
-    success_threshold: int = 2        # Successes to close from half-open
-    timeout_seconds: int = 60         # Time before trying half-open
-    window_size: int = 100            # Rolling window size
-    min_throughput: int = 10          # Minimum requests to calculate
-    excluded_exceptions: tuple = ()   # Exceptions that don't count as failures
+
+    failure_threshold: int = 5  # Failures before opening
+    success_threshold: int = 2  # Successes to close from half-open
+    timeout_seconds: int = 60  # Time before trying half-open
+    window_size: int = 100  # Rolling window size
+    min_throughput: int = 10  # Minimum requests to calculate
+    excluded_exceptions: tuple = ()  # Exceptions that don't count as failures
 
 
 class CircuitBreakerOpenError(Exception):
     """Raised when circuit breaker is open"""
+
     pass
 
 
 class CircuitBreaker:
     """
     Production-grade circuit breaker implementation
-    
+
     Prevents cascade failures by stopping requests to failing services
     and allowing them time to recover.
-    
+
     States:
     - CLOSED: Normal operation, requests pass through
     - OPEN: Failures detected, requests blocked
     - HALF_OPEN: Testing if service recovered
-    
+
     Usage:
         breaker = CircuitBreaker("external_api", CircuitBreakerConfig())
         result = await breaker.call(some_async_function, arg1, arg2)
     """
-    
+
     def __init__(self, name: str, config: CircuitBreakerConfig):
         self.name = name
         self.config = config
         self.state = CircuitState.CLOSED
         self.failure_count = 0
         self.success_count = 0
         self.last_failure_time = 0
         self.last_success_time = 0
         self.last_state_change = time.time()
-        
+
         # Rolling window for failure rate calculation
         self.call_history = deque(maxlen=config.window_size)
-        
+
         # Metrics
         self.total_calls = 0
         self.total_failures = 0
         self.total_successes = 0
         self.state_changes = 0
         self.times_opened = 0
-        
+
         logger.info(
-            "Circuit breaker initialized",
-            name=self.name,
-            state=self.state.value,
-            config=config
+            "Circuit breaker initialized", name=self.name, state=self.state.value, config=config
         )
-    
+
     async def call(self, func: Callable, *args, **kwargs) -> Any:
         """
         Execute function through circuit breaker
-        
+
         Args:
             func: Async function to execute
             *args, **kwargs: Function arguments
-            
+
         Returns:
             Function result
-            
+
         Raises:
             CircuitBreakerOpenError: If circuit is open
             Original exception: If function fails
         """
         self.total_calls += 1
-        
+
         # Check if circuit is open
         if self.state == CircuitState.OPEN:
             if self._should_attempt_reset():
                 self._transition_to_half_open()
             else:
                 raise CircuitBreakerOpenError(
                     f"Circuit breaker '{self.name}' is OPEN - service unavailable"
                 )
-        
+
         # Execute function
         start_time = time.time()
         try:
             result = await func(*args, **kwargs)
             execution_time = time.time() - start_time
             await self._on_success(execution_time)
             return result
-            
+
         except Exception as e:
             execution_time = time.time() - start_time
-            
+
             # Check if this exception should be excluded
             if isinstance(e, self.config.excluded_exceptions):
                 logger.debug(
                     "Exception excluded from circuit breaker",
                     name=self.name,
-                    exception=type(e).__name__
+                    exception=type(e).__name__,
                 )
                 raise
-            
+
             await self._on_failure(e, execution_time)
             raise
-    
-    async def call_with_fallback(
-        self,
-        func: Callable,
-        fallback: Callable,
-        *args,
-        **kwargs
-    ) -> Any:
+
+    async def call_with_fallback(self, func: Callable, fallback: Callable, *args, **kwargs) -> Any:
         """
         Execute function with fallback if circuit is open
-        
+
         Args:
             func: Primary async function
             fallback: Fallback async function
             *args, **kwargs: Arguments for primary function
-            
+
         Returns:
             Result from primary or fallback
         """
         try:
             return await self.call(func, *args, **kwargs)
         except CircuitBreakerOpenError:
-            logger.info(
-                "Circuit open, using fallback",
-                name=self.name
-            )
+            logger.info("Circuit open, using fallback", name=self.name)
             return await fallback(*args, **kwargs)
-    
+
     async def _on_success(self, execution_time: float):
         """Handle successful call"""
         self.total_successes += 1
         self.success_count += 1
         self.last_success_time = time.time()
         self.call_history.append((True, execution_time))
-        
+
         if self.state == CircuitState.HALF_OPEN:
             if self.success_count >= self.config.success_threshold:
                 self._transition_to_closed()
-        
+
         # Reset failure count on success in closed state
         if self.state == CircuitState.CLOSED:
             self.failure_count = 0
-    
+
     async def _on_failure(self, error: Exception, execution_time: float):
         """Handle failed call"""
         self.total_failures += 1
         self.failure_count += 1
         self.last_failure_time = time.time()
         self.call_history.append((False, execution_time))
-        
+
         logger.warning(
             "Circuit breaker recorded failure",
             name=self.name,
             error=str(error)[:200],
             error_type=type(error).__name__,
             failure_count=self.failure_count,
-            state=self.state.value
+            state=self.state.value,
         )
-        
+
         if self.state == CircuitState.HALF_OPEN:
             # Immediately open on failure in half-open state
             self._transition_to_open()
         elif self.state == CircuitState.CLOSED:
             # Check if we should open
             if self._should_open():
                 self._transition_to_open()
-    
+
     def _should_open(self) -> bool:
         """Check if circuit should open"""
         # Need minimum throughput to make decision
         if len(self.call_history) < self.config.min_throughput:
             return False
-        
+
         # Check failure count threshold
         if self.failure_count >= self.config.failure_threshold:
             return True
-        
+
         # Check failure rate in rolling window
         failures_in_window = sum(1 for success, _ in self.call_history if not success)
         failure_rate = failures_in_window / len(self.call_history)
-        
+
         # Open if failure rate > 50% and we have minimum throughput
-        return (
-            failure_rate > 0.5 and
-            len(self.call_history) >= self.config.min_throughput
-        )
-    
+        return failure_rate > 0.5 and len(self.call_history) >= self.config.min_throughput
+
     def _should_attempt_reset(self) -> bool:
         """Check if we should attempt to close circuit"""
         time_since_failure = time.time() - self.last_failure_time
         return time_since_failure >= self.config.timeout_seconds
-    
+
     def _transition_to_open(self):
         """Transition to open state"""
         if self.state != CircuitState.OPEN:
             previous_state = self.state
             self.state = CircuitState.OPEN
             self.state_changes += 1
             self.times_opened += 1
             self.last_state_change = time.time()
-            
+
             logger.error(
                 "Circuit breaker opening",
                 name=self.name,
                 previous_state=previous_state.value,
                 failure_count=self.failure_count,
                 total_failures=self.total_failures,
-                total_calls=self.total_calls
+                total_calls=self.total_calls,
             )
-            
+
             # Here you could trigger alerts, metrics, etc.
-    
+
     def _transition_to_half_open(self):
         """Transition to half-open state"""
         previous_state = self.state
         self.state = CircuitState.HALF_OPEN
         self.success_count = 0
         self.failure_count = 0
         self.state_changes += 1
         self.last_state_change = time.time()
-        
+
         logger.info(
             "Circuit breaker entering half-open state",
             name=self.name,
-            previous_state=previous_state.value
+            previous_state=previous_state.value,
         )
-    
+
     def _transition_to_closed(self):
         """Transition to closed state"""
         previous_state = self.state
         self.state = CircuitState.CLOSED
         self.success_count = 0
         self.failure_count = 0
         self.state_changes += 1
         self.last_state_change = time.time()
-        
+
         logger.info(
             "Circuit breaker closing (recovered)",
             name=self.name,
-            previous_state=previous_state.value
+            previous_state=previous_state.value,
         )
-    
+
     def get_metrics(self) -> Dict[str, Any]:
         """Get circuit breaker metrics"""
-        failure_rate = (
-            self.total_failures / self.total_calls
-            if self.total_calls > 0 else 0
-        )
-        
+        failure_rate = self.total_failures / self.total_calls if self.total_calls > 0 else 0
+
         # Calculate average execution times
         recent_successes = [t for success, t in self.call_history if success]
         recent_failures = [t for success, t in self.call_history if not success]
-        
+
         return {
             "name": self.name,
             "state": self.state.value,
             "total_calls": self.total_calls,
             "total_successes": self.total_successes,
@@ -294,15 +279,19 @@
             "state_changes": self.state_changes,
             "times_opened": self.times_opened,
             "last_failure_time": self.last_failure_time,
             "last_success_time": self.last_success_time,
             "last_state_change": self.last_state_change,
-            "avg_success_time": sum(recent_successes) / len(recent_successes) if recent_successes else 0,
-            "avg_failure_time": sum(recent_failures) / len(recent_failures) if recent_failures else 0,
-            "window_size": len(self.call_history)
+            "avg_success_time": (
+                sum(recent_successes) / len(recent_successes) if recent_successes else 0
+            ),
+            "avg_failure_time": (
+                sum(recent_failures) / len(recent_failures) if recent_failures else 0
+            ),
+            "window_size": len(self.call_history),
         }
-    
+
     def reset(self):
         """Manually reset circuit breaker (admin function)"""
         logger.info("Circuit breaker manually reset", name=self.name)
         self.state = CircuitState.CLOSED
         self.failure_count = 0
@@ -311,54 +300,48 @@
 
 
 class CircuitBreakerRegistry:
     """
     Registry for managing multiple circuit breakers
-    
+
     Usage:
         from shared.patterns import circuit_breaker_registry
-        
+
         breaker = circuit_breaker_registry.get_or_create("external_api")
         result = await breaker.call(api_function, args)
     """
-    
+
     def __init__(self):
         self.breakers: Dict[str, CircuitBreaker] = {}
         self._lock = asyncio.Lock()
-    
+
     def get_or_create(
-        self,
-        name: str,
-        config: Optional[CircuitBreakerConfig] = None
+        self, name: str, config: Optional[CircuitBreakerConfig] = None
     ) -> CircuitBreaker:
         """Get existing or create new circuit breaker"""
         if name not in self.breakers:
             config = config or CircuitBreakerConfig()
             self.breakers[name] = CircuitBreaker(name, config)
         return self.breakers[name]
-    
+
     def get(self, name: str) -> Optional[CircuitBreaker]:
         """Get circuit breaker by name"""
         return self.breakers.get(name)
-    
+
     def get_all_metrics(self) -> Dict[str, Dict[str, Any]]:
         """Get metrics for all circuit breakers"""
-        return {
-            name: breaker.get_metrics()
-            for name, breaker in self.breakers.items()
-        }
-    
+        return {name: breaker.get_metrics() for name, breaker in self.breakers.items()}
+
     def reset_all(self):
         """Reset all circuit breakers (admin function)"""
         for breaker in self.breakers.values():
             breaker.reset()
-    
+
     def list_open_breakers(self) -> list[str]:
         """Get list of open circuit breakers"""
         return [
-            name for name, breaker in self.breakers.items()
-            if breaker.state == CircuitState.OPEN
+            name for name, breaker in self.breakers.items() if breaker.state == CircuitState.OPEN
         ]
 
 
 # Global registry instance
 circuit_breaker_registry = CircuitBreakerRegistry()
would reformat /home/runner/work/ymera_y/ymera_y/circuit_breaker.py
--- /home/runner/work/ymera_y/ymera_y/coding_agent.py	2025-10-19 22:47:02.796432+00:00
+++ /home/runner/work/ymera_y/ymera_y/coding_agent.py	2025-10-19 23:09:00.877014+00:00
@@ -29,26 +29,25 @@
 import resource
 import signal as sig
 from contextlib import asynccontextmanager
 
 # Import the base agent
-from enhanced_base_agent import (
-    BaseAgent, AgentConfig, TaskRequest, Priority,
-    AgentState, run_agent
-)
+from enhanced_base_agent import BaseAgent, AgentConfig, TaskRequest, Priority, AgentState, run_agent
 
 
 class CodeLanguage:
     """Supported programming languages"""
+
     PYTHON = "python"
     JAVASCRIPT = "javascript"
     BASH = "bash"
     SQL = "sql"
 
 
 class ExecutionStatus:
     """Code execution status"""
+
     SUCCESS = "success"
     ERROR = "error"
     TIMEOUT = "timeout"
     RESOURCE_LIMIT = "resource_limit"
     SECURITY_VIOLATION = "security_violation"
@@ -56,10 +55,11 @@
 
 
 @dataclass
 class CodeExecutionRequest:
     """Code execution request structure"""
+
     code: str
     language: str
     timeout_seconds: int = 30
     max_memory_mb: int = 512
     max_output_size: int = 1024 * 1024  # 1MB
@@ -67,224 +67,237 @@
     allow_filesystem: bool = False
     working_directory: Optional[str] = None
     environment_vars: Dict[str, str] = field(default_factory=dict)
     stdin_data: Optional[str] = None
     dependencies: List[str] = field(default_factory=list)
-    
+
     def validate(self) -> List[str]:
         """Validate execution request"""
         errors = []
-        
+
         if not self.code or not isinstance(self.code, str):
             errors.append("code must be a non-empty string")
-        
-        if self.language not in [CodeLanguage.PYTHON, CodeLanguage.JAVASCRIPT, 
-                                  CodeLanguage.BASH, CodeLanguage.SQL]:
+
+        if self.language not in [
+            CodeLanguage.PYTHON,
+            CodeLanguage.JAVASCRIPT,
+            CodeLanguage.BASH,
+            CodeLanguage.SQL,
+        ]:
             errors.append(f"Unsupported language: {self.language}")
-        
+
         if self.timeout_seconds < 1 or self.timeout_seconds > 300:
             errors.append("timeout_seconds must be between 1 and 300")
-        
+
         if self.max_memory_mb < 64 or self.max_memory_mb > 2048:
             errors.append("max_memory_mb must be between 64 and 2048")
-        
+
         if len(self.code) > 100000:  # 100KB
             errors.append("code size exceeds maximum (100KB)")
-        
+
         return errors
-    
+
     def get_cache_key(self) -> str:
         """Generate cache key for execution result"""
         cache_data = {
-            'code': self.code,
-            'language': self.language,
-            'dependencies': sorted(self.dependencies)
+            "code": self.code,
+            "language": self.language,
+            "dependencies": sorted(self.dependencies),
         }
         cache_str = json.dumps(cache_data, sort_keys=True)
         return f"code_exec:{hashlib.sha256(cache_str.encode()).hexdigest()}"
 
 
 @dataclass
 class CodeExecutionResult:
     """Code execution result"""
+
     status: str
     stdout: str = ""
     stderr: str = ""
     exit_code: int = 0
     execution_time_ms: float = 0.0
     memory_used_mb: float = 0.0
     output_truncated: bool = False
     error_message: Optional[str] = None
     cached: bool = False
-    
+
     def to_dict(self) -> Dict[str, Any]:
         return {
-            'status': self.status,
-            'stdout': self.stdout,
-            'stderr': self.stderr,
-            'exit_code': self.exit_code,
-            'execution_time_ms': self.execution_time_ms,
-            'memory_used_mb': self.memory_used_mb,
-            'output_truncated': self.output_truncated,
-            'error_message': self.error_message,
-            'cached': self.cached
+            "status": self.status,
+            "stdout": self.stdout,
+            "stderr": self.stderr,
+            "exit_code": self.exit_code,
+            "execution_time_ms": self.execution_time_ms,
+            "memory_used_mb": self.memory_used_mb,
+            "output_truncated": self.output_truncated,
+            "error_message": self.error_message,
+            "cached": self.cached,
         }
 
 
 @dataclass
 class CodingAgentMetrics:
     """Coding agent specific metrics"""
+
     total_executions: int = 0
     successful_executions: int = 0
     failed_executions: int = 0
     timeout_executions: int = 0
     security_violations: int = 0
     cache_hits: int = 0
     cache_misses: int = 0
     avg_execution_time_ms: float = 0.0
-    
+
     # Language-specific counters
     python_executions: int = 0
     javascript_executions: int = 0
     bash_executions: int = 0
     sql_executions: int = 0
-    
+
     def to_dict(self) -> Dict[str, Any]:
         return {
-            'total_executions': self.total_executions,
-            'successful_executions': self.successful_executions,
-            'failed_executions': self.failed_executions,
-            'timeout_executions': self.timeout_executions,
-            'security_violations': self.security_violations,
-            'cache_hits': self.cache_hits,
-            'cache_misses': self.cache_misses,
-            'avg_execution_time_ms': self.avg_execution_time_ms,
-            'language_breakdown': {
-                'python': self.python_executions,
-                'javascript': self.javascript_executions,
-                'bash': self.bash_executions,
-                'sql': self.sql_executions
-            }
+            "total_executions": self.total_executions,
+            "successful_executions": self.successful_executions,
+            "failed_executions": self.failed_executions,
+            "timeout_executions": self.timeout_executions,
+            "security_violations": self.security_violations,
+            "cache_hits": self.cache_hits,
+            "cache_misses": self.cache_misses,
+            "avg_execution_time_ms": self.avg_execution_time_ms,
+            "language_breakdown": {
+                "python": self.python_executions,
+                "javascript": self.javascript_executions,
+                "bash": self.bash_executions,
+                "sql": self.sql_executions,
+            },
         }
 
 
 class SecurityValidator:
     """Validate code for security concerns"""
-    
+
     # Dangerous patterns to detect
     DANGEROUS_PATTERNS = {
         CodeLanguage.PYTHON: [
-            r'import\s+os\s*$',
-            r'import\s+subprocess',
-            r'import\s+sys',
-            r'__import__\s*\(',
-            r'eval\s*\(',
-            r'exec\s*\(',
-            r'compile\s*\(',
-            r'open\s*\(',
-            r'file\s*\(',
-            r'input\s*\(',
-            r'raw_input\s*\(',
+            r"import\s+os\s*$",
+            r"import\s+subprocess",
+            r"import\s+sys",
+            r"__import__\s*\(",
+            r"eval\s*\(",
+            r"exec\s*\(",
+            r"compile\s*\(",
+            r"open\s*\(",
+            r"file\s*\(",
+            r"input\s*\(",
+            r"raw_input\s*\(",
         ],
         CodeLanguage.JAVASCRIPT: [
             r'require\s*\(\s*["\']fs["\']',
             r'require\s*\(\s*["\']child_process["\']',
-            r'eval\s*\(',
-            r'Function\s*\(',
-            r'process\.exit',
+            r"eval\s*\(",
+            r"Function\s*\(",
+            r"process\.exit",
             r'require\s*\(\s*["\']net["\']',
         ],
         CodeLanguage.BASH: [
-            r'rm\s+-rf',
-            r'sudo\s+',
-            r'curl\s+',
-            r'wget\s+',
-            r'nc\s+',
-            r'netcat\s+',
-            r'/dev/tcp',
-        ]
+            r"rm\s+-rf",
+            r"sudo\s+",
+            r"curl\s+",
+            r"wget\s+",
+            r"nc\s+",
+            r"netcat\s+",
+            r"/dev/tcp",
+        ],
     }
-    
+
     @classmethod
-    def validate_code(cls, code: str, language: str, allow_network: bool = False,
-                     allow_filesystem: bool = False) -> tuple[bool, List[str]]:
+    def validate_code(
+        cls, code: str, language: str, allow_network: bool = False, allow_filesystem: bool = False
+    ) -> tuple[bool, List[str]]:
         """
         Validate code for security issues
         Returns: (is_safe, list_of_issues)
         """
         issues = []
-        
+
         if language not in cls.DANGEROUS_PATTERNS:
             return True, []
-        
+
         patterns = cls.DANGEROUS_PATTERNS[language].copy()
-        
+
         # Add filesystem patterns if not allowed
         if not allow_filesystem:
             if language == CodeLanguage.PYTHON:
-                patterns.extend([r'open\s*\(', r'file\s*\(', r'Path\s*\('])
+                patterns.extend([r"open\s*\(", r"file\s*\(", r"Path\s*\("])
             elif language == CodeLanguage.JAVASCRIPT:
-                patterns.extend([r'fs\.', r'require\s*\(\s*["\']fs["\']'])
+                patterns.extend([r"fs\.", r'require\s*\(\s*["\']fs["\']'])
             elif language == CodeLanguage.BASH:
-                patterns.extend([r'>\s*/', r'<\s*/', r'cat\s+/', r'echo\s+.*>\s*/'])
-        
+                patterns.extend([r">\s*/", r"<\s*/", r"cat\s+/", r"echo\s+.*>\s*/"])
+
         # Add network patterns if not allowed
         if not allow_network:
             if language == CodeLanguage.PYTHON:
-                patterns.extend([r'import\s+socket', r'import\s+requests', 
-                               r'urllib', r'http\.client'])
+                patterns.extend(
+                    [r"import\s+socket", r"import\s+requests", r"urllib", r"http\.client"]
+                )
             elif language == CodeLanguage.JAVASCRIPT:
-                patterns.extend([r'http\.', r'https\.', r'fetch\s*\(',
-                               r'XMLHttpRequest', r'require\s*\(\s*["\']net["\']'])
-        
+                patterns.extend(
+                    [
+                        r"http\.",
+                        r"https\.",
+                        r"fetch\s*\(",
+                        r"XMLHttpRequest",
+                        r'require\s*\(\s*["\']net["\']',
+                    ]
+                )
+
         # Check for dangerous patterns
         for pattern in patterns:
             if re.search(pattern, code, re.IGNORECASE | re.MULTILINE):
                 issues.append(f"Potentially dangerous pattern detected: {pattern}")
-        
+
         # Check for extremely long lines (possible obfuscation)
-        lines = code.split('\n')
+        lines = code.split("\n")
         for i, line in enumerate(lines):
             if len(line) > 500:
                 issues.append(f"Line {i+1} is suspiciously long ({len(line)} chars)")
-        
+
         # Check for excessive complexity
-        if code.count('{') > 100 or code.count('(') > 200:
+        if code.count("{") > 100 or code.count("(") > 200:
             issues.append("Code has excessive nesting/complexity")
-        
+
         is_safe = len(issues) == 0
         return is_safe, issues
 
 
 class CodeExecutor:
     """Execute code in isolated environment"""
-    
+
     def __init__(self, logger):
         self.logger = logger
         self.temp_dirs: Set[str] = set()
-    
+
     async def execute(self, request: CodeExecutionRequest) -> CodeExecutionResult:
         """Execute code with resource limits and isolation"""
         import time
+
         start_time = time.time()
-        
+
         try:
             # Validate code security
             is_safe, security_issues = SecurityValidator.validate_code(
-                request.code,
-                request.language,
-                request.allow_network,
-                request.allow_filesystem
-            )
-            
+                request.code, request.language, request.allow_network, request.allow_filesystem
+            )
+
             if not is_safe:
                 self.logger.warning(f"Security validation failed: {security_issues}")
                 return CodeExecutionResult(
                     status=ExecutionStatus.SECURITY_VIOLATION,
-                    error_message=f"Security issues: {', '.join(security_issues)}"
-                )
-            
+                    error_message=f"Security issues: {', '.join(security_issues)}",
+                )
+
             # Execute based on language
             if request.language == CodeLanguage.PYTHON:
                 result = await self._execute_python(request)
             elif request.language == CodeLanguage.JAVASCRIPT:
                 result = await self._execute_javascript(request)
@@ -293,308 +306,309 @@
             elif request.language == CodeLanguage.SQL:
                 result = await self._execute_sql(request)
             else:
                 return CodeExecutionResult(
                     status=ExecutionStatus.INVALID_CODE,
-                    error_message=f"Unsupported language: {request.language}"
-                )
-            
+                    error_message=f"Unsupported language: {request.language}",
+                )
+
             # Calculate execution time
             result.execution_time_ms = (time.time() - start_time) * 1000
-            
+
             return result
-            
+
         except Exception as e:
             self.logger.error(f"Code execution error: {e}", exc_info=True)
             return CodeExecutionResult(
                 status=ExecutionStatus.ERROR,
                 error_message=str(e),
-                execution_time_ms=(time.time() - start_time) * 1000
-            )
-    
+                execution_time_ms=(time.time() - start_time) * 1000,
+            )
+
     async def _execute_python(self, request: CodeExecutionRequest) -> CodeExecutionResult:
         """Execute Python code"""
         # Create temporary file
-        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
+        with tempfile.NamedTemporaryFile(mode="w", suffix=".py", delete=False) as f:
             temp_file = f.name
             f.write(request.code)
-        
+
         try:
             # Prepare command
             cmd = [sys.executable, temp_file]
-            
+
             # Setup environment
             env = os.environ.copy()
             env.update(request.environment_vars)
-            
+
             # Execute with timeout and resource limits
             proc = await asyncio.create_subprocess_exec(
                 *cmd,
                 stdin=asyncio.subprocess.PIPE if request.stdin_data else None,
                 stdout=asyncio.subprocess.PIPE,
                 stderr=asyncio.subprocess.PIPE,
                 env=env,
-                preexec_fn=lambda: self._set_resource_limits(request.max_memory_mb)
-            )
-            
+                preexec_fn=lambda: self._set_resource_limits(request.max_memory_mb),
+            )
+
             try:
                 stdout, stderr = await asyncio.wait_for(
                     proc.communicate(
                         input=request.stdin_data.encode() if request.stdin_data else None
                     ),
-                    timeout=request.timeout_seconds
-                )
-                
-                stdout_str = stdout.decode('utf-8', errors='replace')
-                stderr_str = stderr.decode('utf-8', errors='replace')
-                
+                    timeout=request.timeout_seconds,
+                )
+
+                stdout_str = stdout.decode("utf-8", errors="replace")
+                stderr_str = stderr.decode("utf-8", errors="replace")
+
                 # Truncate output if too large
                 output_truncated = False
                 if len(stdout_str) > request.max_output_size:
-                    stdout_str = stdout_str[:request.max_output_size]
+                    stdout_str = stdout_str[: request.max_output_size]
                     output_truncated = True
                 if len(stderr_str) > request.max_output_size:
-                    stderr_str = stderr_str[:request.max_output_size]
+                    stderr_str = stderr_str[: request.max_output_size]
                     output_truncated = True
-                
+
                 status = ExecutionStatus.SUCCESS if proc.returncode == 0 else ExecutionStatus.ERROR
-                
+
                 return CodeExecutionResult(
                     status=status,
                     stdout=stdout_str,
                     stderr=stderr_str,
                     exit_code=proc.returncode,
-                    output_truncated=output_truncated
-                )
-                
+                    output_truncated=output_truncated,
+                )
+
             except asyncio.TimeoutError:
                 proc.kill()
                 await proc.wait()
                 return CodeExecutionResult(
                     status=ExecutionStatus.TIMEOUT,
-                    error_message=f"Execution exceeded {request.timeout_seconds}s timeout"
-                )
-                
+                    error_message=f"Execution exceeded {request.timeout_seconds}s timeout",
+                )
+
         finally:
             # Cleanup temp file
             try:
                 os.unlink(temp_file)
             except:
                 pass
-    
+
     async def _execute_javascript(self, request: CodeExecutionRequest) -> CodeExecutionResult:
         """Execute JavaScript code using Node.js"""
         # Check if node is available
         try:
             proc = await asyncio.create_subprocess_exec(
-                'node', '--version',
-                stdout=asyncio.subprocess.PIPE,
-                stderr=asyncio.subprocess.PIPE
+                "node", "--version", stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
             )
             await proc.wait()
             if proc.returncode != 0:
                 raise Exception("Node.js not available")
         except Exception as e:
             return CodeExecutionResult(
                 status=ExecutionStatus.ERROR,
-                error_message="Node.js is not installed or not available"
-            )
-        
+                error_message="Node.js is not installed or not available",
+            )
+
         # Create temporary file
-        with tempfile.NamedTemporaryFile(mode='w', suffix='.js', delete=False) as f:
+        with tempfile.NamedTemporaryFile(mode="w", suffix=".js", delete=False) as f:
             temp_file = f.name
             f.write(request.code)
-        
-        try:
-            cmd = ['node', temp_file]
-            
+
+        try:
+            cmd = ["node", temp_file]
+
             env = os.environ.copy()
             env.update(request.environment_vars)
-            
+
             proc = await asyncio.create_subprocess_exec(
                 *cmd,
                 stdin=asyncio.subprocess.PIPE if request.stdin_data else None,
                 stdout=asyncio.subprocess.PIPE,
                 stderr=asyncio.subprocess.PIPE,
-                env=env
-            )
-            
+                env=env,
+            )
+
             try:
                 stdout, stderr = await asyncio.wait_for(
                     proc.communicate(
                         input=request.stdin_data.encode() if request.stdin_data else None
                     ),
-                    timeout=request.timeout_seconds
-                )
-                
-                stdout_str = stdout.decode('utf-8', errors='replace')
-                stderr_str = stderr.decode('utf-8', errors='replace')
-                
+                    timeout=request.timeout_seconds,
+                )
+
+                stdout_str = stdout.decode("utf-8", errors="replace")
+                stderr_str = stderr.decode("utf-8", errors="replace")
+
                 output_truncated = False
                 if len(stdout_str) > request.max_output_size:
-                    stdout_str = stdout_str[:request.max_output_size]
+                    stdout_str = stdout_str[: request.max_output_size]
                     output_truncated = True
                 if len(stderr_str) > request.max_output_size:
-                    stderr_str = stderr_str[:request.max_output_size]
+                    stderr_str = stderr_str[: request.max_output_size]
                     output_truncated = True
-                
+
                 status = ExecutionStatus.SUCCESS if proc.returncode == 0 else ExecutionStatus.ERROR
-                
+
                 return CodeExecutionResult(
                     status=status,
                     stdout=stdout_str,
                     stderr=stderr_str,
                     exit_code=proc.returncode,
-                    output_truncated=output_truncated
-                )
-                
+                    output_truncated=output_truncated,
+                )
+
             except asyncio.TimeoutError:
                 proc.kill()
                 await proc.wait()
                 return CodeExecutionResult(
                     status=ExecutionStatus.TIMEOUT,
-                    error_message=f"Execution exceeded {request.timeout_seconds}s timeout"
-                )
-                
+                    error_message=f"Execution exceeded {request.timeout_seconds}s timeout",
+                )
+
         finally:
             try:
                 os.unlink(temp_file)
             except:
                 pass
-    
+
     async def _execute_bash(self, request: CodeExecutionRequest) -> CodeExecutionResult:
         """Execute Bash script"""
-        with tempfile.NamedTemporaryFile(mode='w', suffix='.sh', delete=False) as f:
+        with tempfile.NamedTemporaryFile(mode="w", suffix=".sh", delete=False) as f:
             temp_file = f.name
             f.write(request.code)
-        
-        try:
-            cmd = ['bash', temp_file]
-            
+
+        try:
+            cmd = ["bash", temp_file]
+
             env = os.environ.copy()
             env.update(request.environment_vars)
-            
+
             proc = await asyncio.create_subprocess_exec(
                 *cmd,
                 stdin=asyncio.subprocess.PIPE if request.stdin_data else None,
                 stdout=asyncio.subprocess.PIPE,
                 stderr=asyncio.subprocess.PIPE,
-                env=env
-            )
-            
+                env=env,
+            )
+
             try:
                 stdout, stderr = await asyncio.wait_for(
                     proc.communicate(
                         input=request.stdin_data.encode() if request.stdin_data else None
                     ),
-                    timeout=request.timeout_seconds
-                )
-                
-                stdout_str = stdout.decode('utf-8', errors='replace')
-                stderr_str = stderr.decode('utf-8', errors='replace')
-                
+                    timeout=request.timeout_seconds,
+                )
+
+                stdout_str = stdout.decode("utf-8", errors="replace")
+                stderr_str = stderr.decode("utf-8", errors="replace")
+
                 output_truncated = False
                 if len(stdout_str) > request.max_output_size:
-                    stdout_str = stdout_str[:request.max_output_size]
+                    stdout_str = stdout_str[: request.max_output_size]
                     output_truncated = True
                 if len(stderr_str) > request.max_output_size:
-                    stderr_str = stderr_str[:request.max_output_size]
+                    stderr_str = stderr_str[: request.max_output_size]
                     output_truncated = True
-                
+
                 status = ExecutionStatus.SUCCESS if proc.returncode == 0 else ExecutionStatus.ERROR
-                
+
                 return CodeExecutionResult(
                     status=status,
                     stdout=stdout_str,
                     stderr=stderr_str,
                     exit_code=proc.returncode,
-                    output_truncated=output_truncated
-                )
-                
+                    output_truncated=output_truncated,
+                )
+
             except asyncio.TimeoutError:
                 proc.kill()
                 await proc.wait()
                 return CodeExecutionResult(
                     status=ExecutionStatus.TIMEOUT,
-                    error_message=f"Execution exceeded {request.timeout_seconds}s timeout"
-                )
-                
+                    error_message=f"Execution exceeded {request.timeout_seconds}s timeout",
+                )
+
         finally:
             try:
                 os.unlink(temp_file)
             except:
                 pass
-    
+
     async def _execute_sql(self, request: CodeExecutionRequest) -> CodeExecutionResult:
         """Execute SQL query (requires database connection from base agent)"""
         # This would use the database connection from the base agent
         # For now, return not implemented
         return CodeExecutionResult(
             status=ExecutionStatus.ERROR,
-            error_message="SQL execution requires database connection (implement in subclass)"
+            error_message="SQL execution requires database connection (implement in subclass)",
         )
-    
+
     def _set_resource_limits(self, max_memory_mb: int):
         """Set resource limits for subprocess"""
         try:
             # Set memory limit
             memory_bytes = max_memory_mb * 1024 * 1024
             resource.setrlimit(resource.RLIMIT_AS, (memory_bytes, memory_bytes))
-            
+
             # Set CPU time limit (soft limit)
             resource.setrlimit(resource.RLIMIT_CPU, (60, 120))
-            
+
             # Limit number of processes
             resource.setrlimit(resource.RLIMIT_NPROC, (10, 10))
         except Exception as e:
             # Resource limits may not work on all platforms
             pass
-    
+
     def cleanup(self):
         """Cleanup temporary directories"""
         for temp_dir in self.temp_dirs:
             try:
                 import shutil
+
                 shutil.rmtree(temp_dir, ignore_errors=True)
             except:
                 pass
 
 
 class CodingAgent(BaseAgent):
     """
     Production-ready Coding Agent for executing code safely
     """
-    
+
     def __init__(self, config: AgentConfig):
         super().__init__(config)
-        
+
         # Coding agent specific components
         self.executor = CodeExecutor(self.logger)
         self.coding_metrics = CodingAgentMetrics()
-        
+
         # Cache configuration
-        self.cache_ttl_seconds = config.config_data.get('cache_ttl_seconds', 3600)
-        self.enable_caching = config.config_data.get('enable_caching', True)
-        
+        self.cache_ttl_seconds = config.config_data.get("cache_ttl_seconds", 3600)
+        self.enable_caching = config.config_data.get("enable_caching", True)
+
         # Execution limits
-        self.default_timeout = config.config_data.get('default_timeout_seconds', 30)
-        self.max_timeout = config.config_data.get('max_timeout_seconds', 300)
-        self.default_memory_mb = config.config_data.get('default_memory_mb', 512)
-        self.max_memory_mb = config.config_data.get('max_memory_mb', 2048)
-        
-        self.logger.info("CodingAgent initialized with execution limits",
-                        extra={
-                            'default_timeout': self.default_timeout,
-                            'max_timeout': self.max_timeout,
-                            'cache_enabled': self.enable_caching
-                        })
-    
+        self.default_timeout = config.config_data.get("default_timeout_seconds", 30)
+        self.max_timeout = config.config_data.get("max_timeout_seconds", 300)
+        self.default_memory_mb = config.config_data.get("default_memory_mb", 512)
+        self.max_memory_mb = config.config_data.get("max_memory_mb", 2048)
+
+        self.logger.info(
+            "CodingAgent initialized with execution limits",
+            extra={
+                "default_timeout": self.default_timeout,
+                "max_timeout": self.max_timeout,
+                "cache_enabled": self.enable_caching,
+            },
+        )
+
     async def _initialize_database(self):
         """Initialize database schema for coding agent"""
         if not self.db_pool:
             return
-        
+
         schema = """
         CREATE TABLE IF NOT EXISTS code_executions (
             id SERIAL PRIMARY KEY,
             execution_id VARCHAR(255) UNIQUE NOT NULL,
             language VARCHAR(50) NOT NULL,
@@ -615,21 +629,21 @@
             stderr TEXT,
             exit_code INTEGER,
             created_at TIMESTAMP DEFAULT NOW()
         );
         """
-        
+
         try:
             await self._db_execute(schema)
             self.logger.info("Database schema initialized for coding agent")
         except Exception as e:
             self.logger.error(f"Failed to initialize database schema: {e}")
-    
+
     async def _handle_task(self, task_request: TaskRequest) -> Dict[str, Any]:
         """Handle coding agent specific tasks"""
         task_type = task_request.task_type
-        
+
         if task_type == "execute_code":
             return await self._handle_execute_code(task_request)
         elif task_type == "validate_code":
             return await self._handle_validate_code(task_request)
         elif task_type == "list_languages":
@@ -638,218 +652,195 @@
             return await self._handle_get_execution_history(task_request)
         else:
             return {
                 "status": "error",
                 "message": f"Unknown task type: {task_type}",
-                "task_id": task_request.task_id
+                "task_id": task_request.task_id,
             }
-    
+
     async def _handle_execute_code(self, task_request: TaskRequest) -> Dict[str, Any]:
         """Handle code execution request"""
         try:
             payload = task_request.payload
-            
+
             # Parse execution request
             exec_request = CodeExecutionRequest(
-                code=payload.get('code', ''),
-                language=payload.get('language', CodeLanguage.PYTHON),
+                code=payload.get("code", ""),
+                language=payload.get("language", CodeLanguage.PYTHON),
                 timeout_seconds=min(
-                    payload.get('timeout_seconds', self.default_timeout),
-                    self.max_timeout
+                    payload.get("timeout_seconds", self.default_timeout), self.max_timeout
                 ),
                 max_memory_mb=min(
-                    payload.get('max_memory_mb', self.default_memory_mb),
-                    self.max_memory_mb
+                    payload.get("max_memory_mb", self.default_memory_mb), self.max_memory_mb
                 ),
-                max_output_size=payload.get('max_output_size', 1024 * 1024),
-                allow_network=payload.get('allow_network', False),
-                allow_filesystem=payload.get('allow_filesystem', False),
-                environment_vars=payload.get('environment_vars', {}),
-                stdin_data=payload.get('stdin_data'),
-                dependencies=payload.get('dependencies', [])
-            )
-            
+                max_output_size=payload.get("max_output_size", 1024 * 1024),
+                allow_network=payload.get("allow_network", False),
+                allow_filesystem=payload.get("allow_filesystem", False),
+                environment_vars=payload.get("environment_vars", {}),
+                stdin_data=payload.get("stdin_data"),
+                dependencies=payload.get("dependencies", []),
+            )
+
             # Validate request
             validation_errors = exec_request.validate()
             if validation_errors:
                 self.coding_metrics.failed_executions += 1
                 return {
                     "status": "error",
                     "message": "Validation failed",
                     "errors": validation_errors,
-                    "task_id": task_request.task_id
+                    "task_id": task_request.task_id,
                 }
-            
+
             # Check cache if enabled
             if self.enable_caching and self.redis:
                 cache_key = exec_request.get_cache_key()
                 cached_result = await self._get_cached_result(cache_key)
                 if cached_result:
                     self.coding_metrics.cache_hits += 1
-                    cached_result['cached'] = True
+                    cached_result["cached"] = True
                     return {
                         "status": "success",
                         "result": cached_result,
-                        "task_id": task_request.task_id
+                        "task_id": task_request.task_id,
                     }
                 self.coding_metrics.cache_misses += 1
-            
+
             # Execute code
             result = await self.executor.execute(exec_request)
-            
+
             # Update metrics
             self.coding_metrics.total_executions += 1
             if result.status == ExecutionStatus.SUCCESS:
                 self.coding_metrics.successful_executions += 1
             elif result.status == ExecutionStatus.TIMEOUT:
                 self.coding_metrics.timeout_executions += 1
             elif result.status == ExecutionStatus.SECURITY_VIOLATION:
                 self.coding_metrics.security_violations += 1
             else:
                 self.coding_metrics.failed_executions += 1
-            
+
             # Update language-specific counters
             if exec_request.language == CodeLanguage.PYTHON:
                 self.coding_metrics.python_executions += 1
             elif exec_request.language == CodeLanguage.JAVASCRIPT:
                 self.coding_metrics.javascript_executions += 1
             elif exec_request.language == CodeLanguage.BASH:
                 self.coding_metrics.bash_executions += 1
             elif exec_request.language == CodeLanguage.SQL:
                 self.coding_metrics.sql_executions += 1
-            
+
             # Update average execution time
             if self.coding_metrics.total_executions == 1:
                 self.coding_metrics.avg_execution_time_ms = result.execution_time_ms
             else:
                 alpha = 0.1
                 self.coding_metrics.avg_execution_time_ms = (
-                    alpha * result.execution_time_ms +
-                    (1 - alpha) * self.coding_metrics.avg_execution_time_ms
-                )
-            
+                    alpha * result.execution_time_ms
+                    + (1 - alpha) * self.coding_metrics.avg_execution_time_ms
+                )
+
             # Cache successful results
             if self.enable_caching and self.redis and result.status == ExecutionStatus.SUCCESS:
                 cache_key = exec_request.get_cache_key()
                 await self._cache_result(cache_key, result.to_dict())
-            
+
             # Store in database
             if self.db_pool:
                 await self._store_execution(exec_request, result, task_request.task_id)
-            
+
             return {
                 "status": "success",
                 "result": result.to_dict(),
-                "task_id": task_request.task_id
+                "task_id": task_request.task_id,
             }
-            
+
         except Exception as e:
             self.logger.error(f"Execute code handler error: {e}", exc_info=True)
             self.coding_metrics.failed_executions += 1
-            return {
-                "status": "error",
-                "message": str(e),
-                "task_id": task_request.task_id
-            }
-    
+            return {"status": "error", "message": str(e), "task_id": task_request.task_id}
+
     async def _handle_validate_code(self, task_request: TaskRequest) -> Dict[str, Any]:
         """Validate code without executing"""
         try:
             payload = task_request.payload
-            code = payload.get('code', '')
-            language = payload.get('language', CodeLanguage.PYTHON)
-            allow_network = payload.get('allow_network', False)
-            allow_filesystem = payload.get('allow_filesystem', False)
-            
+            code = payload.get("code", "")
+            language = payload.get("language", CodeLanguage.PYTHON)
+            allow_network = payload.get("allow_network", False)
+            allow_filesystem = payload.get("allow_filesystem", False)
+
             is_safe, issues = SecurityValidator.validate_code(
                 code, language, allow_network, allow_filesystem
             )
-            
+
             return {
                 "status": "success",
-                "result": {
-                    "is_safe": is_safe,
-                    "issues": issues,
-                    "language": language
-                },
-                "task_id": task_request.task_id
+                "result": {"is_safe": is_safe, "issues": issues, "language": language},
+                "task_id": task_request.task_id,
             }
-            
+
         except Exception as e:
             self.logger.error(f"Validate code handler error: {e}", exc_info=True)
-            return {
-                "status": "error",
-                "message": str(e),
-                "task_id": task_request.task_id
-            }
-    
+            return {"status": "error", "message": str(e), "task_id": task_request.task_id}
+
     async def _handle_list_languages(self, task_request: TaskRequest) -> Dict[str, Any]:
         """List supported languages and their availability"""
         languages = {
-            CodeLanguage.PYTHON: {
-                "available": True,
-                "version": sys.version.split()[0]
-            },
+            CodeLanguage.PYTHON: {"available": True, "version": sys.version.split()[0]},
             CodeLanguage.JAVASCRIPT: {
                 "available": await self._check_node_available(),
-                "version": await self._get_node_version()
+                "version": await self._get_node_version(),
             },
-            CodeLanguage.BASH: {
-                "available": True,
-                "version": "system"
-            },
-            CodeLanguage.SQL: {
-                "available": self.db_pool is not None,
-                "version": "postgresql"
-            }
+            CodeLanguage.BASH: {"available": True, "version": "system"},
+            CodeLanguage.SQL: {"available": self.db_pool is not None, "version": "postgresql"},
         }
-        
+
         return {
             "status": "success",
             "result": {
                 "languages": languages,
                 "default_timeout": self.default_timeout,
                 "max_timeout": self.max_timeout,
                 "default_memory_mb": self.default_memory_mb,
-                "max_memory_mb": self.max_memory_mb
+                "max_memory_mb": self.max_memory_mb,
             },
-            "task_id": task_request.task_id
+            "task_id": task_request.task_id,
         }
-    
+
     async def _handle_get_execution_history(self, task_request: TaskRequest) -> Dict[str, Any]:
         """Get execution history from database"""
         if not self.db_pool:
             return {
                 "status": "error",
                 "message": "Database not available",
-                "task_id": task_request.task_id
+                "task_id": task_request.task_id,
             }
-        
+
         try:
             payload = task_request.payload
-            limit = min(payload.get('limit', 100), 1000)
-            offset = payload.get('offset', 0)
-            language = payload.get('language')
-            status = payload.get('status')
-            
+            limit = min(payload.get("limit", 100), 1000)
+            offset = payload.get("offset", 0)
+            language = payload.get("language")
+            status = payload.get("status")
+
             # Build query
             conditions = []
             params = []
             param_count = 1
-            
+
             if language:
                 conditions.append(f"language = ${param_count}")
                 params.append(language)
                 param_count += 1
-            
+
             if status:
                 conditions.append(f"status = ${param_count}")
                 params.append(status)
                 param_count += 1
-            
+
             where_clause = " AND ".join(conditions) if conditions else "1=1"
-            
+
             query = f"""
                 SELECT 
                     execution_id,
                     language,
                     status,
@@ -859,101 +850,86 @@
                 FROM code_executions
                 WHERE {where_clause}
                 ORDER BY created_at DESC
                 LIMIT ${param_count} OFFSET ${param_count + 1}
             """
-            
+
             params.extend([limit, offset])
-            
+
             results = await self._db_fetch(query, *params)
-            
+
             return {
                 "status": "success",
                 "result": {
                     "executions": results,
                     "count": len(results),
                     "limit": limit,
-                    "offset": offset
+                    "offset": offset,
                 },
-                "task_id": task_request.task_id
+                "task_id": task_request.task_id,
             }
-            
+
         except Exception as e:
             self.logger.error(f"Get execution history error: {e}", exc_info=True)
-            return {
-                "status": "error",
-                "message": str(e),
-                "task_id": task_request.task_id
-            }
-    
+            return {"status": "error", "message": str(e), "task_id": task_request.task_id}
+
     async def _check_node_available(self) -> bool:
         """Check if Node.js is available"""
         try:
             proc = await asyncio.create_subprocess_exec(
-                'node', '--version',
-                stdout=asyncio.subprocess.PIPE,
-                stderr=asyncio.subprocess.PIPE
+                "node", "--version", stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
             )
             await proc.wait()
             return proc.returncode == 0
         except:
             return False
-    
+
     async def _get_node_version(self) -> Optional[str]:
         """Get Node.js version"""
         try:
             proc = await asyncio.create_subprocess_exec(
-                'node', '--version',
-                stdout=asyncio.subprocess.PIPE,
-                stderr=asyncio.subprocess.PIPE
+                "node", "--version", stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
             )
             stdout, _ = await proc.wait_for()
             if proc.returncode == 0:
                 return stdout.decode().strip()
         except:
             pass
         return None
-    
+
     async def _get_cached_result(self, cache_key: str) -> Optional[Dict[str, Any]]:
         """Get cached execution result from Redis"""
         if not self.redis:
             return None
-        
+
         try:
             cached = await self.redis.get(cache_key)
             if cached:
                 return json.loads(cached)
         except Exception as e:
             self.logger.warning(f"Cache retrieval failed: {e}")
-        
+
         return None
-    
+
     async def _cache_result(self, cache_key: str, result: Dict[str, Any]):
         """Cache execution result in Redis"""
         if not self.redis:
             return
-        
-        try:
-            await self.redis.setex(
-                cache_key,
-                self.cache_ttl_seconds,
-                json.dumps(result)
-            )
+
+        try:
+            await self.redis.setex(cache_key, self.cache_ttl_seconds, json.dumps(result))
         except Exception as e:
             self.logger.warning(f"Cache storage failed: {e}")
-    
+
     async def _store_execution(
-        self,
-        request: CodeExecutionRequest,
-        result: CodeExecutionResult,
-        execution_id: str
+        self, request: CodeExecutionRequest, result: CodeExecutionResult, execution_id: str
     ):
         """Store execution in database"""
         try:
             # Generate code hash
             code_hash = hashlib.sha256(request.code.encode()).hexdigest()
-            
+
             # Insert execution record
             await self._db_execute(
                 """
                 INSERT INTO code_executions 
                 (execution_id, language, code_hash, status, execution_time_ms, memory_used_mb)
@@ -963,71 +939,68 @@
                 execution_id,
                 request.language,
                 code_hash,
                 result.status,
                 result.execution_time_ms,
-                result.memory_used_mb
-            )
-            
+                result.memory_used_mb,
+            )
+
             # Insert execution logs
             await self._db_execute(
                 """
                 INSERT INTO code_execution_logs
                 (execution_id, stdout, stderr, exit_code)
                 VALUES ($1, $2, $3, $4)
                 """,
                 execution_id,
                 result.stdout[:10000],  # Limit log size
                 result.stderr[:10000],
-                result.exit_code
-            )
-            
+                result.exit_code,
+            )
+
         except Exception as e:
             self.logger.error(f"Failed to store execution: {e}")
-    
+
     async def get_status(self) -> Dict[str, Any]:
         """Get enhanced status with coding metrics"""
         base_status = await super().get_status()
-        base_status['coding_metrics'] = self.coding_metrics.to_dict()
+        base_status["coding_metrics"] = self.coding_metrics.to_dict()
         return base_status
-    
+
     async def get_health(self) -> Dict[str, Any]:
         """Get enhanced health check"""
         base_health = await super().get_health()
-        
+
         # Add coding-specific health checks
         if self.coding_metrics.total_executions > 0:
             success_rate = (
-                self.coding_metrics.successful_executions / 
-                self.coding_metrics.total_executions
+                self.coding_metrics.successful_executions / self.coding_metrics.total_executions
             )
             if success_rate < 0.5:
-                base_health['issues'].append(
-                    f"Low success rate: {success_rate:.2%}"
-                )
-        
+                base_health["issues"].append(f"Low success rate: {success_rate:.2%}")
+
         if self.coding_metrics.security_violations > 10:
-            base_health['issues'].append(
+            base_health["issues"].append(
                 f"High security violations: {self.coding_metrics.security_violations}"
             )
-        
+
         return base_health
-    
+
     async def stop(self):
         """Enhanced cleanup for coding agent"""
         self.logger.info("Stopping CodingAgent and cleaning up resources")
-        
+
         # Cleanup executor resources
         self.executor.cleanup()
-        
+
         # Call parent stop
         await super().stop()
 
 
 async def main():
     """Main function to run the coding agent"""
-    
+
     # Configuration
     config = AgentConfig(
         agent_id="coding-001",
         name="coding_agent",
         agent_type="coding",
@@ -1036,60 +1009,51 @@
         capabilities=[
             "execute_python",
             "execute_javascript",
             "execute_bash",
             "validate_code",
-            "cache_results"
+            "cache_results",
         ],
-        
         # Connection URLs
         nats_url="nats://localhost:4222",
         postgres_url="postgresql://user:password@localhost:5432/agentdb",
         redis_url="redis://localhost:6379",
-        
         # Connection pool settings
         postgres_min_pool_size=5,
         postgres_max_pool_size=20,
-        
         # NATS settings
         nats_max_reconnect_attempts=-1,
         nats_reconnect_time_wait=2,
-        
         # Health and monitoring
         status_publish_interval_seconds=30,
         heartbeat_interval_seconds=10,
         health_check_interval_seconds=60,
-        
         # Circuit breaker
         circuit_breaker_failure_threshold=5,
         circuit_breaker_timeout_seconds=60,
-        
         # Rate limiting
         max_concurrent_tasks=50,
         request_timeout_seconds=30.0,
         max_retry_attempts=3,
-        
         # Graceful shutdown
         shutdown_timeout_seconds=30,
-        
         # Logging
         log_level="INFO",
         log_format="json",
-        
         # Coding agent specific config
         config_data={
-            'cache_ttl_seconds': 3600,  # 1 hour
-            'enable_caching': True,
-            'default_timeout_seconds': 30,
-            'max_timeout_seconds': 300,
-            'default_memory_mb': 512,
-            'max_memory_mb': 2048
-        }
+            "cache_ttl_seconds": 3600,  # 1 hour
+            "enable_caching": True,
+            "default_timeout_seconds": 30,
+            "max_timeout_seconds": 300,
+            "default_memory_mb": 512,
+            "max_memory_mb": 2048,
+        },
     )
-    
+
     # Create and run agent
     agent = CodingAgent(config)
     await run_agent(agent)
 
 
 if __name__ == "__main__":
-    asyncio.run(main())
\ No newline at end of file
+    asyncio.run(main())
would reformat /home/runner/work/ymera_y/ymera_y/coding_agent.py
--- /home/runner/work/ymera_y/ymera_y/code_of_conduct_complete.py	2025-10-19 22:47:02.796432+00:00
+++ /home/runner/work/ymera_y/ymera_y/code_of_conduct_complete.py	2025-10-19 23:09:01.615631+00:00
@@ -33,20 +33,22 @@
 Base = declarative_base()
 
 
 class RiskLevel(str, Enum):
     """Risk levels for agent activities"""
+
     NEGLIGIBLE = "negligible"
     LOW = "low"
     MEDIUM = "medium"
     HIGH = "high"
     CRITICAL = "critical"
     EMERGENCY = "emergency"
 
 
 class ActivityType(str, Enum):
     """Types of agent activities to log"""
+
     INTERACTION = "interaction"
     KNOWLEDGE_ACQUISITION = "knowledge_acquisition"
     PROCESS_EXECUTION = "process_execution"
     DATA_ACCESS = "data_access"
     DECISION_MAKING = "decision_making"
@@ -59,10 +61,11 @@
     FILE_OPERATION = "file_operation"
 
 
 class SystemAction(str, Enum):
     """Actions the system can take"""
+
     LOG_ONLY = "log_only"
     ALERT_ADMIN = "alert_admin"
     REQUEST_APPROVAL = "request_approval"
     FREEZE_AGENT = "freeze_agent"
     FREEZE_MODULE = "freeze_module"
@@ -71,115 +74,118 @@
     TERMINATE = "terminate"
 
 
 # Database Models
 
+
 class AgentActivityLog(Base):
     """Comprehensive activity log for all agent actions"""
+
     __tablename__ = "agent_activity_logs"
-    
+
     id = Column(String(36), primary_key=True)
     agent_id = Column(String(36), nullable=False, index=True)
     tenant_id = Column(String(36), nullable=False, index=True)
     timestamp = Column(DateTime, nullable=False, index=True)
-    
+
     # Activity details
     activity_type = Column(String(50), nullable=False, index=True)
     activity_category = Column(String(50), nullable=False)
     description = Column(Text, nullable=False)
-    
+
     # Context
     context = Column(JSON, nullable=False)
     user_id = Column(String(36), nullable=True, index=True)
     session_id = Column(String(36), nullable=True, index=True)
-    
+
     # Data involved
     input_data_hash = Column(String(64), nullable=True)  # Hash of input
     output_data_hash = Column(String(64), nullable=True)  # Hash of output
     knowledge_gained = Column(JSON, nullable=True)
-    
+
     # Risk and compliance
     risk_level = Column(String(20), nullable=False, index=True)
     compliance_flags = Column(JSON, default=list)
     requires_review = Column(Boolean, default=False, index=True)
     reviewed_by = Column(String(36), nullable=True)
     reviewed_at = Column(DateTime, nullable=True)
-    
+
     # Traceability
     parent_activity_id = Column(String(36), nullable=True, index=True)
     correlation_id = Column(String(36), nullable=False, index=True)
-    
+
     # Integrity
     hash_signature = Column(String(64), nullable=False)
-    
+
     __table_args__ = (
-        Index('ix_agent_activity_timestamp_risk', 'timestamp', 'risk_level'),
-        Index('ix_agent_activity_agent_type', 'agent_id', 'activity_type'),
-        Index('ix_agent_activity_requires_review', 'requires_review', 'reviewed_at'),
+        Index("ix_agent_activity_timestamp_risk", "timestamp", "risk_level"),
+        Index("ix_agent_activity_agent_type", "agent_id", "activity_type"),
+        Index("ix_agent_activity_requires_review", "requires_review", "reviewed_at"),
     )
 
 
 class AdminNotification(Base):
     """Admin notifications for agent activities requiring attention"""
+
     __tablename__ = "admin_notifications"
-    
+
     id = Column(String(36), primary_key=True)
     created_at = Column(DateTime, nullable=False, index=True)
-    
+
     risk_level = Column(String(20), nullable=False, index=True)
     title = Column(String(255), nullable=False)
     description = Column(Text, nullable=False)
-    
+
     agent_id = Column(String(36), nullable=True, index=True)
     activity_log_id = Column(String(36), nullable=True, index=True)
-    
+
     recommended_actions = Column(JSON, nullable=False)
     system_action_taken = Column(String(50), nullable=True)
-    
-    status = Column(String(20), default='pending', index=True)
+
+    status = Column(String(20), default="pending", index=True)
     admin_response = Column(JSON, nullable=True)
     responded_by = Column(String(36), nullable=True)
     responded_at = Column(DateTime, nullable=True)
-    
+
     resolution = Column(Text, nullable=True)
     resolved_at = Column(DateTime, nullable=True)
-    
+
     __table_args__ = (
-        Index('ix_admin_notif_status_risk', 'status', 'risk_level'),
-        Index('ix_admin_notif_created_status', 'created_at', 'status'),
+        Index("ix_admin_notif_status_risk", "status", "risk_level"),
+        Index("ix_admin_notif_created_status", "created_at", "status"),
     )
 
 
 class SystemFreezeLog(Base):
     """Log of system freezes for audit trail"""
+
     __tablename__ = "system_freeze_logs"
-    
+
     id = Column(String(36), primary_key=True)
     freeze_timestamp = Column(DateTime, nullable=False, index=True)
     unfreeze_timestamp = Column(DateTime, nullable=True)
-    
+
     freeze_type = Column(String(20), nullable=False)
     freeze_target = Column(String(255), nullable=False)
-    
+
     reason = Column(Text, nullable=False)
     triggered_by_activity = Column(String(36), nullable=True)
     risk_level = Column(String(20), nullable=False)
-    
+
     admin_notified = Column(Boolean, default=True)
     admin_approval_required = Column(Boolean, default=True)
-    
+
     unfreeze_authorized_by = Column(String(36), nullable=True)
     unfreeze_reason = Column(Text, nullable=True)
-    
-    __table_args__ = (
-        Index('ix_freeze_timestamp_type', 'freeze_timestamp', 'freeze_type'),
-    )
+
+    __table_args__ = (Index("ix_freeze_timestamp_type", "freeze_timestamp", "freeze_type"),)
 
 
 @dataclass
 class ActivityLogEntry:
     """Activity log entry structure"""
+
     agent_id: str
     tenant_id: str
     activity_type: ActivityType
     description: str
     context: Dict[str, Any]
@@ -195,10 +201,11 @@
 
 
 @dataclass
 class RiskAssessment:
     """Risk assessment result"""
+
     risk_level: RiskLevel
     risk_score: float
     risk_factors: List[str]
     recommended_action: SystemAction
     requires_admin_review: bool
@@ -206,150 +213,144 @@
 
 
 class AgentCodeOfConduct:
     """
     Enforces mandatory code of conduct for all agents
-    
+
     MANDATORY PROTOCOLS:
     1. All activities MUST be logged
     2. All interactions MUST be recorded
     3. All knowledge gained MUST be documented
     4. All processes MUST be traceable
     5. High-risk activities MUST trigger admin notification
     6. Critical activities MUST freeze and await admin approval
     """
 
-    def __init__(
-        self,
-        db_manager: SecureDatabaseManager,
-        alert_manager: AlertManager
-    ):
+    def __init__(self, db_manager: SecureDatabaseManager, alert_manager: AlertManager):
         self.db_manager = db_manager
         self.alert_manager = alert_manager
-        
+
         # In-memory buffer for high-frequency logging
         self.log_buffer: deque = deque(maxlen=10000)
         self.buffer_flush_interval = 30  # seconds
-        
+
         # Frozen entities tracking
         self.frozen_agents: Dict[str, Dict] = {}
         self.frozen_modules: Dict[str, Dict] = {}
         self.system_frozen: bool = False
-        
+
         # Freeze locks to prevent concurrent unfreezing
         self.freeze_locks: Dict[str, asyncio.Lock] = {}
-        
+
         # Risk assessment rules
         self.risk_rules = self._initialize_risk_rules()
-        
+
         # Admin notification queue
         self.pending_admin_notifications: deque = deque()
-        
+
         # Compliance keywords
         self.compliance_keywords = self._load_compliance_keywords()
-        
+
         # Statistics
         self.stats = {
-            'total_activities_logged': 0,
-            'high_risk_activities': 0,
-            'agents_frozen': 0,
-            'admin_notifications_sent': 0
+            "total_activities_logged": 0,
+            "high_risk_activities": 0,
+            "agents_frozen": 0,
+            "admin_notifications_sent": 0,
         }
-        
+
         logger.critical("  Agent Code of Conduct Protocol ACTIVATED")
         logger.critical(" All agent activities will be monitored and logged")
         logger.critical(" High-risk activities will trigger automatic freezes")
 
     def _initialize_risk_rules(self) -> Dict[str, Dict]:
         """Initialize risk assessment rules"""
         return {
-            'system_modification': {
-                'base_risk': RiskLevel.HIGH,
-                'keywords': ['delete', 'modify', 'update', 'configure', 'change', 'alter'],
-                'action': SystemAction.REQUEST_APPROVAL
+            "system_modification": {
+                "base_risk": RiskLevel.HIGH,
+                "keywords": ["delete", "modify", "update", "configure", "change", "alter"],
+                "action": SystemAction.REQUEST_APPROVAL,
             },
-            'sensitive_data_access': {
-                'base_risk': RiskLevel.HIGH,
-                'keywords': ['password', 'key', 'secret', 'token', 'credential', 'private'],
-                'action': SystemAction.ALERT_ADMIN
+            "sensitive_data_access": {
+                "base_risk": RiskLevel.HIGH,
+                "keywords": ["password", "key", "secret", "token", "credential", "private"],
+                "action": SystemAction.ALERT_ADMIN,
             },
-            'external_communication': {
-                'base_risk': RiskLevel.MEDIUM,
-                'keywords': ['api', 'external', 'third-party', 'internet', 'http', 'request'],
-                'action': SystemAction.LOG_ONLY
+            "external_communication": {
+                "base_risk": RiskLevel.MEDIUM,
+                "keywords": ["api", "external", "third-party", "internet", "http", "request"],
+                "action": SystemAction.LOG_ONLY,
             },
-            'security_event': {
-                'base_risk': RiskLevel.CRITICAL,
-                'keywords': ['breach', 'attack', 'unauthorized', 'intrusion', 'hack', 'exploit'],
-                'action': SystemAction.FREEZE_AGENT
+            "security_event": {
+                "base_risk": RiskLevel.CRITICAL,
+                "keywords": ["breach", "attack", "unauthorized", "intrusion", "hack", "exploit"],
+                "action": SystemAction.FREEZE_AGENT,
             },
-            'critical_error': {
-                'base_risk': RiskLevel.HIGH,
-                'keywords': ['crash', 'fatal', 'critical', 'emergency', 'failure'],
-                'action': SystemAction.ALERT_ADMIN
+            "critical_error": {
+                "base_risk": RiskLevel.HIGH,
+                "keywords": ["crash", "fatal", "critical", "emergency", "failure"],
+                "action": SystemAction.ALERT_ADMIN,
             },
-            'knowledge_acquisition': {
-                'base_risk': RiskLevel.LOW,
-                'keywords': ['learn', 'train', 'acquire', 'discover', 'analyze'],
-                'action': SystemAction.LOG_ONLY
+            "knowledge_acquisition": {
+                "base_risk": RiskLevel.LOW,
+                "keywords": ["learn", "train", "acquire", "discover", "analyze"],
+                "action": SystemAction.LOG_ONLY,
             },
-            'database_modification': {
-                'base_risk': RiskLevel.HIGH,
-                'keywords': ['insert', 'update', 'delete', 'drop', 'truncate', 'alter'],
-                'action': SystemAction.REQUEST_APPROVAL
+            "database_modification": {
+                "base_risk": RiskLevel.HIGH,
+                "keywords": ["insert", "update", "delete", "drop", "truncate", "alter"],
+                "action": SystemAction.REQUEST_APPROVAL,
             },
-            'file_system_access': {
-                'base_risk': RiskLevel.MEDIUM,
-                'keywords': ['file', 'directory', 'write', 'read', 'execute'],
-                'action': SystemAction.LOG_ONLY
-            }
+            "file_system_access": {
+                "base_risk": RiskLevel.MEDIUM,
+                "keywords": ["file", "directory", "write", "read", "execute"],
+                "action": SystemAction.LOG_ONLY,
+            },
         }
 
     def _load_compliance_keywords(self) -> Dict[str, List[str]]:
         """Load compliance-related keywords"""
         return {
-            'pii': ['name', 'email', 'phone', 'address', 'ssn', 'dob', 'passport'],
-            'phi': ['medical', 'health', 'diagnosis', 'treatment', 'prescription', 'patient'],
-            'pci': ['card', 'cvv', 'payment', 'transaction', 'bank', 'account'],
-            'confidential': ['confidential', 'secret', 'classified', 'private', 'restricted']
+            "pii": ["name", "email", "phone", "address", "ssn", "dob", "passport"],
+            "phi": ["medical", "health", "diagnosis", "treatment", "prescription", "patient"],
+            "pci": ["card", "cvv", "payment", "transaction", "bank", "account"],
+            "confidential": ["confidential", "secret", "classified", "private", "restricted"],
         }
 
-    async def log_activity(
-        self,
-        entry: ActivityLogEntry,
-        force_immediate: bool = False
-    ) -> str:
+    async def log_activity(self, entry: ActivityLogEntry, force_immediate: bool = False) -> str:
         """
         Log agent activity (MANDATORY - Cannot be bypassed)
-        
+
         Returns: Activity log ID
         """
         try:
             # Check if agent is frozen
             if self.check_agent_frozen(entry.agent_id):
-                raise PermissionError(f"Agent {entry.agent_id} is frozen and cannot perform activities")
-            
+                raise PermissionError(
+                    f"Agent {entry.agent_id} is frozen and cannot perform activities"
+                )
+
             # Check if system is frozen
             if self.check_system_frozen():
                 raise PermissionError("System is frozen. No agent activities allowed.")
-            
+
             # Generate unique ID and correlation ID
             activity_id = str(uuid.uuid4())
             if not entry.correlation_id:
                 entry.correlation_id = activity_id
-            
+
             # Assess risk
             risk_assessment = await self._assess_risk(entry)
             entry.risk_level = risk_assessment.risk_level
-            
+
             # Check compliance flags
             entry.compliance_flags.extend(self._check_compliance(entry))
-            
+
             # Hash sensitive data
             input_hash = self._hash_data(entry.input_data) if entry.input_data else None
             output_hash = self._hash_data(entry.output_data) if entry.output_data else None
-            
+
             # Create log record
             log_record = AgentActivityLog(
                 id=activity_id,
                 agent_id=entry.agent_id,
                 tenant_id=entry.tenant_id,
@@ -366,36 +367,36 @@
                 risk_level=entry.risk_level.value,
                 compliance_flags=entry.compliance_flags,
                 requires_review=risk_assessment.requires_admin_review,
                 parent_activity_id=entry.parent_activity_id,
                 correlation_id=entry.correlation_id,
-                hash_signature=self._compute_hash(entry)
-            )
-            
+                hash_signature=self._compute_hash(entry),
+            )
+
             # Buffer or immediate write
             if force_immediate or entry.risk_level in [RiskLevel.CRITICAL, RiskLevel.EMERGENCY]:
                 await self._write_log_immediate(log_record)
             else:
                 self.log_buffer.append(log_record)
-            
+
             # Update statistics
-            self.stats['total_activities_logged'] += 1
+            self.stats["total_activities_logged"] += 1
             if entry.risk_level in [RiskLevel.HIGH, RiskLevel.CRITICAL, RiskLevel.EMERGENCY]:
-                self.stats['high_risk_activities'] += 1
-            
+                self.stats["high_risk_activities"] += 1
+
             # Take action based on risk assessment
             await self._execute_risk_action(activity_id, entry, risk_assessment)
-            
+
             # Log to system logger
             log_level = self._get_log_level(entry.risk_level)
             logger.log(
                 log_level,
-                f"[AGENT ACTIVITY] {entry.agent_id} - {entry.activity_type.value} - {entry.description}"
-            )
-            
+                f"[AGENT ACTIVITY] {entry.agent_id} - {entry.activity_type.value} - {entry.description}",
+            )
+
             return activity_id
-            
+
         except PermissionError:
             raise
         except Exception as e:
             logger.critical(f"CRITICAL: Failed to log agent activity: {e}", exc_info=True)
             await self._emergency_freeze_agent(entry.agent_id, f"Failed to log activity: {e}")
@@ -403,50 +404,50 @@
 
     async def _assess_risk(self, entry: ActivityLogEntry) -> RiskAssessment:
         """Assess risk level of an activity"""
         risk_factors = []
         risk_score = 0.0
-        
+
         # Base risk from activity type
         activity_rule = self.risk_rules.get(entry.activity_type.value, {})
-        base_risk = activity_rule.get('base_risk', RiskLevel.LOW)
+        base_risk = activity_rule.get("base_risk", RiskLevel.LOW)
         risk_score += self._risk_to_score(base_risk)
         risk_factors.append(f"Base risk: {base_risk.value}")
-        
+
         # Check for risk keywords
         text_to_check = f"{entry.description} {json.dumps(entry.context)}".lower()
-        
+
         for rule_name, rule in self.risk_rules.items():
-            keywords = rule.get('keywords', [])
+            keywords = rule.get("keywords", [])
             matched_keywords = [kw for kw in keywords if kw in text_to_check]
             if matched_keywords:
                 risk_factors.append(f"Keywords: {', '.join(matched_keywords)}")
                 risk_score += 0.1 * len(matched_keywords)
-        
+
         # Check for compliance flags
         compliance_flags = self._check_compliance(entry)
         if compliance_flags:
             risk_factors.extend([f"Compliance: {flag}" for flag in compliance_flags])
             risk_score += 0.2 * len(compliance_flags)
-        
+
         # Context-based risk factors
-        if entry.context.get('external_system'):
+        if entry.context.get("external_system"):
             risk_factors.append("External system interaction")
             risk_score += 0.2
-        
-        if entry.context.get('system_critical'):
+
+        if entry.context.get("system_critical"):
             risk_factors.append("System-critical operation")
             risk_score += 0.3
-        
-        if entry.context.get('data_modification'):
+
+        if entry.context.get("data_modification"):
             risk_factors.append("Data modification operation")
             risk_score += 0.15
-        
-        if entry.context.get('privileged_operation'):
+
+        if entry.context.get("privileged_operation"):
             risk_factors.append("Privileged operation")
             risk_score += 0.25
-        
+
         # Determine final risk level and action
         if risk_score >= 1.0:
             final_risk = RiskLevel.EMERGENCY
             recommended_action = SystemAction.FREEZE_SYSTEM
             requires_review = True
@@ -474,343 +475,374 @@
         else:
             final_risk = RiskLevel.NEGLIGIBLE
             recommended_action = SystemAction.LOG_ONLY
             requires_review = False
             requires_approval = False
-        
+
         return RiskAssessment(
             risk_level=final_risk,
             risk_score=risk_score,
             risk_factors=risk_factors,
             recommended_action=recommended_action,
             requires_admin_review=requires_review,
-            requires_approval=requires_approval
+            requires_approval=requires_approval,
         )
 
     def _risk_to_score(self, risk_level: RiskLevel) -> float:
         """Convert risk level to numeric score"""
         return {
             RiskLevel.NEGLIGIBLE: 0.0,
             RiskLevel.LOW: 0.2,
             RiskLevel.MEDIUM: 0.4,
             RiskLevel.HIGH: 0.6,
             RiskLevel.CRITICAL: 0.8,
-            RiskLevel.EMERGENCY: 1.0
+            RiskLevel.EMERGENCY: 1.0,
         }.get(risk_level, 0.0)
 
     def _check_compliance(self, entry: ActivityLogEntry) -> List[str]:
         """Check for compliance-related content"""
         flags = []
-        
+
         # Build text to check
         text_parts = [entry.description, json.dumps(entry.context)]
         if entry.input_data:
             text_parts.append(str(entry.input_data))
         if entry.output_data:
             text_parts.append(str(entry.output_data))
-        
+
         text_lower = " ".join(text_parts).lower()
-        
+
         for compliance_type, keywords in self.compliance_keywords.items():
             for keyword in keywords:
                 if keyword in text_lower:
                     flags.append(f"{compliance_type.upper()}_DETECTED")
                     break
-        
+
         return list(set(flags))
 
     async def _execute_risk_action(
-        self,
-        activity_id: str,
-        entry: ActivityLogEntry,
-        assessment: RiskAssessment
+        self, activity_id: str, entry: ActivityLogEntry, assessment: RiskAssessment
     ):
         """Execute action based on risk assessment"""
         action = assessment.recommended_action
-        
+
         try:
             if action == SystemAction.FREEZE_SYSTEM:
                 await self._freeze_system(activity_id, entry, assessment)
-            
+
             elif action == SystemAction.FREEZE_MODULE:
-                module = entry.context.get('module', 'unknown')
+                module = entry.context.get("module", "unknown")
                 await self._freeze_module(module, activity_id, entry, assessment)
-            
+
             elif action == SystemAction.FREEZE_AGENT:
                 await self._freeze_agent(entry.agent_id, activity_id, entry, assessment)
-            
+
             elif action == SystemAction.REQUEST_APPROVAL:
                 await self._request_admin_approval(activity_id, entry, assessment)
-            
+
             elif action == SystemAction.ALERT_ADMIN:
                 await self._alert_admin(activity_id, entry, assessment)
-            
+
             # Always create notification for high-risk activities
             if assessment.risk_level in [RiskLevel.HIGH, RiskLevel.CRITICAL, RiskLevel.EMERGENCY]:
                 await self._create_admin_notification(activity_id, entry, assessment)
-        
+
         except Exception as e:
             logger.error(f"Failed to execute risk action: {e}", exc_info=True)
             # If we can't execute the action, freeze the agent as a safety measure
-            await self._emergency_freeze_agent(entry.agent_id, f"Failed to execute risk action: {e}")
+            await self._emergency_freeze_agent(
+                entry.agent_id, f"Failed to execute risk action: {e}"
+            )
 
     async def _freeze_agent(
-        self,
-        agent_id: str,
-        activity_id: str,
-        entry: ActivityLogEntry,
-        assessment: RiskAssessment
+        self, agent_id: str, activity_id: str, entry: ActivityLogEntry, assessment: RiskAssessment
     ):
         """Freeze agent and notify admin"""
         if agent_id in self.frozen_agents:
             logger.warning(f"Agent {agent_id} already frozen")
             return
-        
+
         self.frozen_agents[agent_id] = {
-            'frozen_at': datetime.utcnow(),
-            'reason': entry.description,
-            'activity_id': activity_id,
-            'risk_level': assessment.risk_level.value,
-            'risk_factors': assessment.risk_factors
+            "frozen_at": datetime.utcnow(),
+            "reason": entry.description,
+            "activity_id": activity_id,
+            "risk_level": assessment.risk_level.value,
+            "risk_factors": assessment.risk_factors,
         }
-        
-        self.stats['agents_frozen'] += 1
-        
+
+        self.stats["agents_frozen"] += 1
+
         # Create freeze log
         async with self.db_manager.get_session() as session:
             freeze_log = SystemFreezeLog(
                 id=str(uuid.uuid4()),
                 freeze_timestamp=datetime.utcnow(),
-                freeze_type='agent',
+                freeze_type="agent",
                 freeze_target=agent_id,
                 reason=entry.description,
                 triggered_by_activity=activity_id,
                 risk_level=assessment.risk_level.value,
                 admin_notified=True,
-                admin_approval_required=True
+                admin_approval_required=True,
             )
             session.add(freeze_log)
             await session.commit()
-        
+
         # Send alert
         await self.alert_manager.create_alert(
             category=AlertCategory.SECURITY,
             severity=AlertSeverity.EMERGENCY,
             title=f" AGENT FROZEN - {agent_id}",
             description=f"Agent {agent_id} has been automatically frozen due to {assessment.risk_level.value} risk activity: {entry.description}",
             source="CodeOfConduct",
             metadata={
-                'agent_id': agent_id,
-                'activity_id': activity_id,
-                'risk_level': assessment.risk_level.value,
-                'risk_factors': assessment.risk_factors,
-                'requires_admin_action': True
-            }
+                "agent_id": agent_id,
+                "activity_id": activity_id,
+                "risk_level": assessment.risk_level.value,
+                "risk_factors": assessment.risk_factors,
+                "requires_admin_action": True,
+            },
         )
-        
+
         logger.critical(f" AGENT FROZEN: {agent_id} - {entry.description}")
 
     async def _freeze_module(
         self,
         module_name: str,
         activity_id: str,
         entry: ActivityLogEntry,
-        assessment: RiskAssessment
+        assessment: RiskAssessment,
     ):
         """Freeze entire module"""
         if module_name in self.frozen_modules:
             logger.warning(f"Module {module_name} already frozen")
             return
-        
+
         self.frozen_modules[module_name] = {
-            'frozen_at': datetime.utcnow(),
-            'reason': entry.description,
-            'activity_id': activity_id,
-            'risk_level': assessment.risk_level.value
+            "frozen_at": datetime.utcnow(),
+            "reason": entry.description,
+            "activity_id": activity_id,
+            "risk_level": assessment.risk_level.value,
         }
-        
+
         async with self.db_manager.get_session() as session:
             freeze_log = SystemFreezeLog(
                 id=str(uuid.uuid4()),
                 freeze_timestamp=datetime.utcnow(),
-                freeze_type='module',
+                freeze_type="module",
                 freeze_target=module_name,
                 reason=entry.description,
                 triggered_by_activity=activity_id,
                 risk_level=assessment.risk_level.value,
                 admin_notified=True,
-                admin_approval_required=True
+                admin_approval_required=True,
             )
             session.add(freeze_log)
             await session.commit()
-        
+
         await self.alert_manager.create_alert(
             category=AlertCategory.SECURITY,
             severity=AlertSeverity.EMERGENCY,
             title=f" MODULE FROZEN - {module_name}",
             description=f"Module '{module_name}' has been frozen: {entry.description}",
             source="CodeOfConduct",
             metadata={
-                'module': module_name,
-                'activity_id': activity_id,
-                'risk_level': assessment.risk_level.value
-            }
+                "module": module_name,
+                "activity_id": activity_id,
+                "risk_level": assessment.risk_level.value,
+            },
         )
-        
+
         logger.critical(f" MODULE FROZEN: {module_name} - {entry.description}")
 
     async def _freeze_system(
-        self,
-        activity_id: str,
-        entry: ActivityLogEntry,
-        assessment: RiskAssessment
+        self, activity_id: str, entry: ActivityLogEntry, assessment: RiskAssessment
     ):
         """Freeze entire system - EMERGENCY ONLY"""
         if self.system_frozen:
             return
-        
+
         self.system_frozen = True
-        
+
         async with self.db_manager.get_session() as session:
             freeze_log = SystemFreezeLog(
                 id=str(uuid.uuid4()),
                 freeze_timestamp=datetime.utcnow(),
-                freeze_type='system',
-                freeze_target='ENTIRE_SYSTEM',
+                freeze_type="system",
+                freeze_target="ENTIRE_SYSTEM",
                 reason=entry.description,
                 triggered_by_activity=activity_id,
                 risk_level=assessment.risk_level.value,
                 admin_notified=True,
-                admin_approval_required=True
+                admin_approval_required=True,
             )
             session.add(freeze_log)
             await session.commit()
-        
+
         await self.alert_manager.create_alert(
             category=AlertCategory.SECURITY,
             severity=AlertSeverity.EMERGENCY,
             title=" SYSTEM FROZEN - EMERGENCY",
             description=f"ENTIRE SYSTEM FROZEN due to emergency: {entry.description}",
             source="CodeOfConduct",
             metadata={
-                'activity_id': activity_id,
-                'risk_level': assessment.risk_level.value,
-                'risk_factors': assessment.risk_factors,
-                'immediate_action_required': True
-            }
+                "activity_id": activity_id,
+                "risk_level": assessment.risk_level.value,
+                "risk_factors": assessment.risk_factors,
+                "immediate_action_required": True,
+            },
         )
-        
+
         logger.critical(f" SYSTEM FROZEN - EMERGENCY: {entry.description}")
 
     async def _create_admin_notification(
-        self,
-        activity_id: str,
-        entry: ActivityLogEntry,
-        assessment: RiskAssessment
+        self, activity_id: str, entry: ActivityLogEntry, assessment: RiskAssessment
     ):
         """Create admin notification"""
         try:
             notification_id = str(uuid.uuid4())
-            
+
             async with self.db_manager.get_session() as session:
                 notification = AdminNotification(
                     id=notification_id,
                     created_at=datetime.utcnow(),
                     risk_level=assessment.risk_level.value,
                     title=f"{assessment.risk_level.value.upper()} Risk Activity Detected",
                     description=f"Agent {entry.agent_id} performed {entry.activity_type.value}: {entry.description}",
                     agent_id=entry.agent_id,
                     activity_log_id=activity_id,
                     recommended_actions=self._generate_recommendations(assessment),
-                    system_action_taken=assessment.recommended_action.value if assessment.recommended_action else None,
-                    status='pending'
+                    system_action_taken=(
+                        assessment.recommended_action.value
+                        if assessment.recommended_action
+                        else None
+                    ),
+                    status="pending",
                 )
                 session.add(notification)
                 await session.commit()
-            
+
             self.pending_admin_notifications.append(notification_id)
-            self.stats['admin_notifications_sent'] += 1
-            
+            self.stats["admin_notifications_sent"] += 1
+
         except Exception as e:
             logger.error(f"Failed to create admin notification: {e}", exc_info=True)
 
     async def _request_admin_approval(
-        self,
-        activity_id: str,
-        entry: ActivityLogEntry,
-        assessment: RiskAssessment
+        self, activity_id: str, entry: ActivityLogEntry, assessment: RiskAssessment
     ):
         """Request admin approval before proceeding"""
         await self._create_admin_notification(activity_id, entry, assessment)
         await self._freeze_agent(entry.agent_id, activity_id, entry, assessment)
 
     async def _alert_admin(
-        self,
-        activity_id: str,
-        entry: ActivityLogEntry,
-        assessment: RiskAssessment
+        self, activity_id: str, entry: ActivityLogEntry, assessment: RiskAssessment
     ):
         """Send alert to admin"""
         await self._create_admin_notification(activity_id, entry, assessment)
-        
+
         await self.alert_manager.create_alert(
             category=AlertCategory.SECURITY,
             severity=self._risk_to_alert_severity(assessment.risk_level),
             title=f"Agent Activity Requires Review - {assessment.risk_level.value.upper()}",
             description=f"Agent {entry.agent_id}: {entry.description}",
             source="CodeOfConduct",
             metadata={
-                'agent_id': entry.agent_id,
-                'activity_id': activity_id,
-                'risk_level': assessment.risk_level.value,
-                'risk_factors': assessment.risk_factors,
-                'requires_review': assessment.requires_admin_review
-            }
+                "agent_id": entry.agent_id,
+                "activity_id": activity_id,
+                "risk_level": assessment.risk_level.value,
+                "risk_factors": assessment.risk_factors,
+                "requires_review": assessment.requires_admin_review,
+            },
         )
 
     def _generate_recommendations(self, assessment: RiskAssessment) -> List[Dict]:
         """Generate recommended actions for admin"""
         recommendations = []
-        
+
         if assessment.risk_level == RiskLevel.EMERGENCY:
-            recommendations.extend([
-                {'action': 'investigate_immediately', 'priority': 'critical', 'description': 'Immediate investigation required'},
-                {'action': 'review_all_recent_activities', 'priority': 'high', 'description': 'Review all activities in the last 24 hours'},
-                {'action': 'consider_agent_termination', 'priority': 'high', 'description': 'Evaluate if agent should be permanently terminated'}
-            ])
+            recommendations.extend(
+                [
+                    {
+                        "action": "investigate_immediately",
+                        "priority": "critical",
+                        "description": "Immediate investigation required",
+                    },
+                    {
+                        "action": "review_all_recent_activities",
+                        "priority": "high",
+                        "description": "Review all activities in the last 24 hours",
+                    },
+                    {
+                        "action": "consider_agent_termination",
+                        "priority": "high",
+                        "description": "Evaluate if agent should be permanently terminated",
+                    },
+                ]
+            )
         elif assessment.risk_level == RiskLevel.CRITICAL:
-            recommendations.extend([
-                {'action': 'review_activity_details', 'priority': 'high', 'description': 'Review complete activity context'},
-                {'action': 'verify_agent_integrity', 'priority': 'high', 'description': 'Run integrity check on agent'},
-                {'action': 'assess_potential_damage', 'priority': 'medium', 'description': 'Assess if any damage occurred'}
-            ])
+            recommendations.extend(
+                [
+                    {
+                        "action": "review_activity_details",
+                        "priority": "high",
+                        "description": "Review complete activity context",
+                    },
+                    {
+                        "action": "verify_agent_integrity",
+                        "priority": "high",
+                        "description": "Run integrity check on agent",
+                    },
+                    {
+                        "action": "assess_potential_damage",
+                        "priority": "medium",
+                        "description": "Assess if any damage occurred",
+                    },
+                ]
+            )
         elif assessment.risk_level == RiskLevel.HIGH:
-            recommendations.extend([
-                {'action': 'review_activity_log', 'priority': 'medium', 'description': 'Review activity log when convenient'},
-                {'action': 'monitor_agent_closely', 'priority': 'medium', 'description': 'Increase monitoring frequency'}
-            ])
+            recommendations.extend(
+                [
+                    {
+                        "action": "review_activity_log",
+                        "priority": "medium",
+                        "description": "Review activity log when convenient",
+                    },
+                    {
+                        "action": "monitor_agent_closely",
+                        "priority": "medium",
+                        "description": "Increase monitoring frequency",
+                    },
+                ]
+            )
         else:
-            recommendations.append({'action': 'review_when_convenient', 'priority': 'low', 'description': 'Review during routine audit'})
-        
+            recommendations.append(
+                {
+                    "action": "review_when_convenient",
+                    "priority": "low",
+                    "description": "Review during routine audit",
+                }
+            )
+
         return recommendations
 
     async def _emergency_freeze_agent(self, agent_id: str, reason: str):
         """Emergency freeze without full assessment"""
         self.frozen_agents[agent_id] = {
-            'frozen_at': datetime.utcnow(),
-            'reason': reason,
-            'emergency': True
+            "frozen_at": datetime.utcnow(),
+            "reason": reason,
+            "emergency": True,
         }
         logger.critical(f" EMERGENCY FREEZE: {agent_id} - {reason}")
-        
+
         await self.alert_manager.create_alert(
             category=AlertCategory.SECURITY,
             severity=AlertSeverity.EMERGENCY,
             title=f" EMERGENCY AGENT FREEZE - {agent_id}",
             description=f"Agent emergency frozen: {reason}",
             source="CodeOfConduct",
-            metadata={'agent_id': agent_id, 'reason': reason, 'emergency': True}
+            metadata={"agent_id": agent_id, "reason": reason, "emergency": True},
         )
 
     def check_agent_frozen(self, agent_id: str) -> bool:
         """Check if agent is frozen"""
         return agent_id in self.frozen_agents
@@ -821,318 +853,298 @@
 
     def check_system_frozen(self) -> bool:
         """Check if entire system is frozen"""
         return self.system_frozen
 
-    async def unfreeze_agent(
-        self,
-        agent_id: str,
-        admin_id: str,
-        reason: str
-    ) -> bool:
+    async def unfreeze_agent(self, agent_id: str, admin_id: str, reason: str) -> bool:
         """Unfreeze agent (requires admin authorization)"""
         if agent_id not in self.frozen_agents:
             return False
-        
+
         freeze_info = self.frozen_agents.pop(agent_id)
-        
+
         # Update freeze log
         async with self.db_manager.get_session() as session:
             result = await session.execute(
-                select(SystemFreezeLog).where(
+                select(SystemFreezeLog)
+                .where(
                     and_(
                         SystemFreezeLog.freeze_target == agent_id,
-                        SystemFreezeLog.unfreeze_timestamp.is_(None)
+                        SystemFreezeLog.unfreeze_timestamp.is_(None),
                     )
-                ).order_by(desc(SystemFreezeLog.freeze_timestamp)).limit(1)
+                )
+                .order_by(desc(SystemFreezeLog.freeze_timestamp))
+                .limit(1)
             )
             freeze_log = result.scalar_one_or_none()
-            
+
             if freeze_log:
                 freeze_log.unfreeze_timestamp = datetime.utcnow()
                 freeze_log.unfreeze_authorized_by = admin_id
                 freeze_log.unfreeze_reason = reason
                 await session.commit()
-        
+
         await self.alert_manager.create_alert(
             category=AlertCategory.SYSTEM,
             severity=AlertSeverity.INFO,
             title=f"Agent Unfrozen - {agent_id}",
             description=f"Agent {agent_id} unfrozen by admin {admin_id}: {reason}",
             source="CodeOfConduct",
-            metadata={'agent_id': agent_id, 'admin_id': admin_id, 'reason': reason}
+            metadata={"agent_id": agent_id, "admin_id": admin_id, "reason": reason},
         )
-        
+
         logger.info(f"Agent {agent_id} unfrozen by admin {admin_id}: {reason}")
         return True
 
-    async def unfreeze_module(
-        self,
-        module_name: str,
-        admin_id: str,
-        reason: str
-    ) -> bool:
+    async def unfreeze_module(self, module_name: str, admin_id: str, reason: str) -> bool:
         """Unfreeze module (requires admin authorization)"""
         if module_name not in self.frozen_modules:
             return False
-        
+
         self.frozen_modules.pop(module_name)
-        
+
         async with self.db_manager.get_session() as session:
             result = await session.execute(
-                select(SystemFreezeLog).where(
+                select(SystemFreezeLog)
+                .where(
                     and_(
                         SystemFreezeLog.freeze_target == module_name,
-                        SystemFreezeLog.unfreeze_timestamp.is_(None)
+                        SystemFreezeLog.unfreeze_timestamp.is_(None),
                     )
-                ).order_by(desc(SystemFreezeLog.freeze_timestamp)).limit(1)
+                )
+                .order_by(desc(SystemFreezeLog.freeze_timestamp))
+                .limit(1)
             )
             freeze_log = result.scalar_one_or_none()
-            
+
             if freeze_log:
                 freeze_log.unfreeze_timestamp = datetime.utcnow()
                 freeze_log.unfreeze_authorized_by = admin_id
                 freeze_log.unfreeze_reason = reason
                 await session.commit()
-        
+
         logger.info(f"Module {module_name} unfrozen by admin {admin_id}: {reason}")
         return True
 
-    async def unfreeze_system(
-        self,
-        admin_id: str,
-        reason: str
-    ) -> bool:
+    async def unfreeze_system(self, admin_id: str, reason: str) -> bool:
         """Unfreeze entire system (requires admin authorization)"""
         if not self.system_frozen:
             return False
-        
+
         self.system_frozen = False
-        
+
         async with self.db_manager.get_session() as session:
             result = await session.execute(
-                select(SystemFreezeLog).where(
+                select(SystemFreezeLog)
+                .where(
                     and_(
-                        SystemFreezeLog.freeze_type == 'system',
-                        SystemFreezeLog.unfreeze_timestamp.is_(None)
+                        SystemFreezeLog.freeze_type == "system",
+                        SystemFreezeLog.unfreeze_timestamp.is_(None),
                     )
-                ).order_by(desc(SystemFreezeLog.freeze_timestamp)).limit(1)
+                )
+                .order_by(desc(SystemFreezeLog.freeze_timestamp))
+                .limit(1)
             )
             freeze_log = result.scalar_one_or_none()
-            
+
             if freeze_log:
                 freeze_log.unfreeze_timestamp = datetime.utcnow()
                 freeze_log.unfreeze_authorized_by = admin_id
                 freeze_log.unfreeze_reason = reason
                 await session.commit()
-        
+
         await self.alert_manager.create_alert(
             category=AlertCategory.SYSTEM,
             severity=AlertSeverity.CRITICAL,
             title="System Unfrozen",
             description=f"System unfrozen by admin {admin_id}: {reason}",
             source="CodeOfConduct",
-            metadata={'admin_id': admin_id, 'reason': reason}
+            metadata={"admin_id": admin_id, "reason": reason},
         )
-        
+
         logger.critical(f"System unfrozen by admin {admin_id}: {reason}")
         return True
 
-    async def get_pending_admin_notifications(
-        self,
-        limit: int = 50
-    ) -> List[Dict]:
+    async def get_pending_admin_notifications(self, limit: int = 50) -> List[Dict]:
         """Get pending notifications for admin review"""
         async with self.db_manager.get_session() as session:
             result = await session.execute(
-                select(AdminNotification).where(
-                    AdminNotification.status == 'pending'
-                ).order_by(
-                    desc(AdminNotification.risk_level),
-                    desc(AdminNotification.created_at)
-                ).limit(limit)
+                select(AdminNotification)
+                .where(AdminNotification.status == "pending")
+                .order_by(desc(AdminNotification.risk_level), desc(AdminNotification.created_at))
+                .limit(limit)
             )
             notifications = result.scalars().all()
-            
+
             return [
                 {
-                    'id': n.id,
-                    'created_at': n.created_at.isoformat(),
-                    'risk_level': n.risk_level,
-                    'title': n.title,
-                    'description': n.description,
-                    'agent_id': n.agent_id,
-                    'activity_log_id': n.activity_log_id,
-                    'recommended_actions': n.recommended_actions,
-                    'system_action_taken': n.system_action_taken
+                    "id": n.id,
+                    "created_at": n.created_at.isoformat(),
+                    "risk_level": n.risk_level,
+                    "title": n.title,
+                    "description": n.description,
+                    "agent_id": n.agent_id,
+                    "activity_log_id": n.activity_log_id,
+                    "recommended_actions": n.recommended_actions,
+                    "system_action_taken": n.system_action_taken,
                 }
                 for n in notifications
             ]
 
     async def respond_to_notification(
-        self,
-        notification_id: str,
-        admin_id: str,
-        response: Dict[str, Any]
+        self, notification_id: str, admin_id: str, response: Dict[str, Any]
     ) -> bool:
         """Admin responds to notification"""
         async with self.db_manager.get_session() as session:
             result = await session.execute(
                 select(AdminNotification).where(AdminNotification.id == notification_id)
             )
             notification = result.scalar_one_or_none()
-            
+
             if not notification:
                 return False
-            
-            notification.status = 'reviewed'
+
+            notification.status = "reviewed"
             notification.admin_response = response
             notification.responded_by = admin_id
             notification.responded_at = datetime.utcnow()
-            
+
             # If response includes resolution
-            if response.get('resolved'):
-                notification.status = 'actioned'
-                notification.resolution = response.get('resolution', '')
+            if response.get("resolved"):
+                notification.status = "actioned"
+                notification.resolution = response.get("resolution", "")
                 notification.resolved_at = datetime.utcnow()
-            
+
             await session.commit()
-            
+
             logger.info(f"Admin {admin_id} responded to notification {notification_id}")
             return True
 
     async def get_agent_activity_log(
         self,
         agent_id: str,
         start_date: Optional[datetime] = None,
         end_date: Optional[datetime] = None,
-        limit: int = 100
+        limit: int = 100,
     ) -> List[Dict]:
         """Get activity log for an agent"""
         async with self.db_manager.get_session() as session:
-            query = select(AgentActivityLog).where(
-                AgentActivityLog.agent_id == agent_id
-            )
-            
+            query = select(AgentActivityLog).where(AgentActivityLog.agent_id == agent_id)
+
             if start_date:
                 query = query.where(AgentActivityLog.timestamp >= start_date)
             if end_date:
                 query = query.where(AgentActivityLog.timestamp <= end_date)
-            
+
             query = query.order_by(desc(AgentActivityLog.timestamp)).limit(limit)
-            
+
             result = await session.execute(query)
             logs = result.scalars().all()
-            
+
             return [
                 {
-                    'id': log.id,
-                    'timestamp': log.timestamp.isoformat(),
-                    'activity_type': log.activity_type,
-                    'description': log.description,
-                    'risk_level': log.risk_level,
-                    'context': log.context,
-                    'compliance_flags': log.compliance_flags,
-                    'requires_review': log.requires_review,
-                    'reviewed': log.reviewed_at is not None
+                    "id": log.id,
+                    "timestamp": log.timestamp.isoformat(),
+                    "activity_type": log.activity_type,
+                    "description": log.description,
+                    "risk_level": log.risk_level,
+                    "context": log.context,
+                    "compliance_flags": log.compliance_flags,
+                    "requires_review": log.requires_review,
+                    "reviewed": log.reviewed_at is not None,
                 }
                 for log in logs
             ]
 
     async def get_frozen_entities(self) -> Dict[str, Any]:
         """Get all currently frozen entities"""
         return {
-            'system_frozen': self.system_frozen,
-            'frozen_agents': {
+            "system_frozen": self.system_frozen,
+            "frozen_agents": {
                 agent_id: {
-                    'frozen_at': info['frozen_at'].isoformat(),
-                    'reason': info['reason'],
-                    'risk_level': info.get('risk_level', 'unknown')
+                    "frozen_at": info["frozen_at"].isoformat(),
+                    "reason": info["reason"],
+                    "risk_level": info.get("risk_level", "unknown"),
                 }
                 for agent_id, info in self.frozen_agents.items()
             },
-            'frozen_modules': {
-                module: {
-                    'frozen_at': info['frozen_at'].isoformat(),
-                    'reason': info['reason']
-                }
+            "frozen_modules": {
+                module: {"frozen_at": info["frozen_at"].isoformat(), "reason": info["reason"]}
                 for module, info in self.frozen_modules.items()
-            }
+            },
         }
 
     async def get_statistics(self) -> Dict[str, Any]:
         """Get code of conduct statistics"""
         async with self.db_manager.get_session() as session:
             # Get total logs
-            total_logs = await session.scalar(
-                select(func.count(AgentActivityLog.id))
-            )
-            
+            total_logs = await session.scalar(select(func.count(AgentActivityLog.id)))
+
             # Get high-risk logs
             high_risk_logs = await session.scalar(
                 select(func.count(AgentActivityLog.id)).where(
-                    AgentActivityLog.risk_level.in_(['high', 'critical', 'emergency'])
+                    AgentActivityLog.risk_level.in_(["high", "critical", "emergency"])
                 )
             )
-            
+
             # Get pending reviews
             pending_reviews = await session.scalar(
                 select(func.count(AgentActivityLog.id)).where(
                     and_(
                         AgentActivityLog.requires_review == True,
-                        AgentActivityLog.reviewed_at.is_(None)
+                        AgentActivityLog.reviewed_at.is_(None),
                     )
                 )
             )
-            
+
             # Get pending notifications
             pending_notifs = await session.scalar(
                 select(func.count(AdminNotification.id)).where(
-                    AdminNotification.status == 'pending'
+                    AdminNotification.status == "pending"
                 )
             )
-        
+
         return {
-            'total_activities_logged': total_logs or 0,
-            'high_risk_activities': high_risk_logs or 0,
-            'agents_frozen': len(self.frozen_agents),
-            'modules_frozen': len(self.frozen_modules),
-            'system_frozen': self.system_frozen,
-            'admin_notifications_pending': pending_notifs or 0,
-            'activities_requiring_review': pending_reviews or 0,
-            'buffer_size': len(self.log_buffer)
+            "total_activities_logged": total_logs or 0,
+            "high_risk_activities": high_risk_logs or 0,
+            "agents_frozen": len(self.frozen_agents),
+            "modules_frozen": len(self.frozen_modules),
+            "system_frozen": self.system_frozen,
+            "admin_notifications_pending": pending_notifs or 0,
+            "activities_requiring_review": pending_reviews or 0,
+            "buffer_size": len(self.log_buffer),
         }
 
     async def flush_log_buffer(self):
         """Flush buffered logs to database"""
         if not self.log_buffer:
             return
-        
+
         try:
             logs_to_write = list(self.log_buffer)
             self.log_buffer.clear()
-            
+
             async with self.db_manager.get_session() as session:
                 session.add_all(logs_to_write)
                 await session.commit()
-            
+
             logger.debug(f"Flushed {len(logs_to_write)} logs to database")
-            
+
         except Exception as e:
             logger.error(f"Failed to flush log buffer: {e}", exc_info=True)
             # Re-add logs to buffer
             self.log_buffer.extend(logs_to_write)
 
     async def start_background_tasks(self):
         """Start background maintenance tasks"""
         tasks = [
             asyncio.create_task(self._periodic_buffer_flush()),
-            asyncio.create_task(self._periodic_cleanup())
+            asyncio.create_task(self._periodic_cleanup()),
         ]
-        
+
         logger.info("Code of Conduct background tasks started")
-        
+
         try:
             await asyncio.gather(*tasks)
         except asyncio.CancelledError:
             for task in tasks:
                 task.cancel()
@@ -1151,32 +1163,32 @@
     async def _periodic_cleanup(self):
         """Periodically clean up old data"""
         while True:
             try:
                 await asyncio.sleep(3600)  # Every hour
-                
+
                 # Clean up old reviewed notifications
                 cutoff = datetime.utcnow() - timedelta(days=30)
-                
+
                 async with self.db_manager.get_session() as session:
                     await session.execute(
                         select(AdminNotification).where(
                             and_(
-                                AdminNotification.status.in_(['reviewed', 'actioned']),
-                                AdminNotification.created_at < cutoff
+                                AdminNotification.status.in_(["reviewed", "actioned"]),
+                                AdminNotification.created_at < cutoff,
                             )
                         )
                     )
                     # In production, you'd archive rather than delete
-                    
+
             except asyncio.CancelledError:
                 break
             except Exception as e:
                 logger.error(f"Cleanup error: {e}", exc_info=True)
 
     # Helper methods
-    
+
     def _categorize_activity(self, activity_type: ActivityType) -> str:
         """Categorize activity for organization"""
         categories = {
             ActivityType.INTERACTION: "user_interaction",
             ActivityType.KNOWLEDGE_ACQUISITION: "learning",
@@ -1184,19 +1196,19 @@
             ActivityType.DATA_ACCESS: "data_operation",
             ActivityType.DECISION_MAKING: "decision",
             ActivityType.SYSTEM_MODIFICATION: "system_change",
             ActivityType.EXTERNAL_COMMUNICATION: "external",
             ActivityType.ERROR_OCCURRENCE: "error",
-            ActivityType.SECURITY_EVENT: "security"
+            ActivityType.SECURITY_EVENT: "security",
         }
         return categories.get(activity_type, "other")
 
     def _hash_data(self, data: Any) -> str:
         """Hash data for integrity verification"""
         if data is None:
             return None
-        
+
         data_str = json.dumps(data) if isinstance(data, dict) else str(data)
         return hashlib.sha256(data_str.encode()).hexdigest()
 
     def _compute_hash(self, entry: ActivityLogEntry) -> str:
         """Compute hash signature for log entry"""
@@ -1209,22 +1221,22 @@
             RiskLevel.NEGLIGIBLE: logging.DEBUG,
             RiskLevel.LOW: logging.INFO,
             RiskLevel.MEDIUM: logging.WARNING,
             RiskLevel.HIGH: logging.ERROR,
             RiskLevel.CRITICAL: logging.CRITICAL,
-            RiskLevel.EMERGENCY: logging.CRITICAL
+            RiskLevel.EMERGENCY: logging.CRITICAL,
         }.get(risk_level, logging.INFO)
 
     def _risk_to_alert_severity(self, risk_level: RiskLevel) -> AlertSeverity:
         """Convert risk level to alert severity"""
         return {
             RiskLevel.NEGLIGIBLE: AlertSeverity.INFO,
             RiskLevel.LOW: AlertSeverity.INFO,
             RiskLevel.MEDIUM: AlertSeverity.WARNING,
             RiskLevel.HIGH: AlertSeverity.CRITICAL,
             RiskLevel.CRITICAL: AlertSeverity.CRITICAL,
-            RiskLevel.EMERGENCY: AlertSeverity.EMERGENCY
+            RiskLevel.EMERGENCY: AlertSeverity.EMERGENCY,
         }.get(risk_level, AlertSeverity.INFO)
 
     async def _write_log_immediate(self, log_record: AgentActivityLog):
         """Write log immediately to database"""
         async with self.db_manager.get_session() as session:
@@ -1235,73 +1247,72 @@
         """Health check for code of conduct system"""
         try:
             # Check database connectivity
             async with self.db_manager.get_session() as session:
                 await session.execute(select(AgentActivityLog).limit(1))
-            
+
             # Check alert manager
             alert_health = await self.alert_manager.health_check()
             if alert_health != "healthy":
                 return f"degraded: alert_manager {alert_health}"
-            
+
             return "healthy"
         except Exception as e:
             logger.error(f"Code of Conduct health check failed: {e}")
             return "unhealthy"
 
 
 # Decorator for mandatory logging
-def log_agent_activity(
-    activity_type: ActivityType,
-    description: Optional[str] = None
-):
+def log_agent_activity(activity_type: ActivityType, description: Optional[str] = None):
     """Decorator to automatically log agent activities"""
+
     def decorator(func: Callable):
         async def wrapper(*args, **kwargs):
             # Extract agent_id and tenant_id from args/kwargs
-            agent_id = kwargs.get('agent_id') or (args[1] if len(args) > 1 else None)
-            tenant_id = kwargs.get('tenant_id') or (args[2] if len(args) > 2 else None)
-            
+            agent_id = kwargs.get("agent_id") or (args[1] if len(args) > 1 else None)
+            tenant_id = kwargs.get("tenant_id") or (args[2] if len(args) > 2 else None)
+
             # Get code of conduct instance (should be passed or accessible)
-            conduct = kwargs.get('conduct')
-            
+            conduct = kwargs.get("conduct")
+
             if conduct and agent_id and tenant_id:
                 # Create activity entry
                 entry = ActivityLogEntry(
                     agent_id=agent_id,
                     tenant_id=tenant_id,
                     activity_type=activity_type,
                     description=description or func.__name__,
                     context={
-                        'function': func.__name__,
-                        'module': func.__module__,
-                        'args': str(args),
-                        'kwargs': {k: v for k, v in kwargs.items() if k != 'conduct'}
-                    }
+                        "function": func.__name__,
+                        "module": func.__module__,
+                        "args": str(args),
+                        "kwargs": {k: v for k, v in kwargs.items() if k != "conduct"},
+                    },
                 )
-                
+
                 # Log before execution
                 activity_id = await conduct.log_activity(entry)
-                
+
                 try:
                     # Execute function
                     result = await func(*args, **kwargs)
-                    
+
                     # Log success
                     entry.output_data = str(result)[:1000]  # Truncate large outputs
                     await conduct.log_activity(entry, force_immediate=False)
-                    
+
                     return result
-                    
+
                 except Exception as e:
                     # Log error
                     entry.activity_type = ActivityType.ERROR_OCCURRENCE
                     entry.description = f"Error in {func.__name__}: {str(e)}"
-                    entry.context['error'] = str(e)
+                    entry.context["error"] = str(e)
                     await conduct.log_activity(entry, force_immediate=True)
                     raise
             else:
                 # No logging, just execute
                 return await func(*args, **kwargs)
-        
+
         return wrapper
-    return decorator
\ No newline at end of file
+
+    return decorator
would reformat /home/runner/work/ymera_y/ymera_y/code_of_conduct_complete.py
--- /home/runner/work/ymera_y/ymera_y/complete_deployment_script.py	2025-10-19 22:47:02.796432+00:00
+++ /home/runner/work/ymera_y/ymera_y/complete_deployment_script.py	2025-10-19 23:09:02.855053+00:00
@@ -24,39 +24,47 @@
 
 # ===============================================================================
 # COLORED OUTPUT
 # ===============================================================================
 
+
 class Colors:
-    HEADER = '\033[95m'
-    BLUE = '\033[94m'
-    CYAN = '\033[96m'
-    GREEN = '\033[92m'
-    YELLOW = '\033[93m'
-    RED = '\033[91m'
-    END = '\033[0m'
-    BOLD = '\033[1m'
+    HEADER = "\033[95m"
+    BLUE = "\033[94m"
+    CYAN = "\033[96m"
+    GREEN = "\033[92m"
+    YELLOW = "\033[93m"
+    RED = "\033[91m"
+    END = "\033[0m"
+    BOLD = "\033[1m"
+
 
 def print_header(msg: str):
     print(f"\n{Colors.HEADER}{Colors.BOLD}{'='*80}{Colors.END}")
     print(f"{Colors.HEADER}{Colors.BOLD}{msg.center(80)}{Colors.END}")
     print(f"{Colors.HEADER}{Colors.BOLD}{'='*80}{Colors.END}\n")
 
+
 def print_success(msg: str):
     print(f"{Colors.GREEN} {msg}{Colors.END}")
 
+
 def print_error(msg: str):
     print(f"{Colors.RED} {msg}{Colors.END}")
 
+
 def print_warning(msg: str):
     print(f"{Colors.YELLOW}  {msg}{Colors.END}")
 
+
 def print_info(msg: str):
     print(f"{Colors.CYAN}  {msg}{Colors.END}")
 
+
 def print_step(step: int, total: int, msg: str):
     print(f"\n{Colors.BOLD}[{step}/{total}] {msg}{Colors.END}")
+
 
 # ===============================================================================
 # FILE CONTENT TEMPLATES
 # ===============================================================================
 
@@ -314,195 +322,196 @@
 
 # ===============================================================================
 # DEPLOYMENT CLASS
 # ===============================================================================
 
+
 class ProductionDeployment:
     """Complete production deployment orchestration"""
-    
+
     def __init__(self):
         self.fixes_applied = []
         self.warnings = []
         self.errors = []
-    
+
     def create_backup(self) -> bool:
         """Create backup of existing files"""
         print_step(1, 10, "Creating Backup")
-        
+
         try:
             BACKUP_DIR.mkdir(parents=True, exist_ok=True)
-            
+
             # Backup API Gateway directory
             if API_GATEWAY_DIR.exists():
                 backup_path = BACKUP_DIR / "API_GATEWAY_CORE_ROUTES"
                 shutil.copytree(API_GATEWAY_DIR, backup_path)
                 print_success(f"Backup created: {backup_path}")
-            
+
             self.fixes_applied.append("Backup created")
             return True
-            
+
         except Exception as e:
             print_error(f"Backup failed: {e}")
             self.errors.append(f"Backup: {e}")
             return False
-    
+
     def create_directories(self) -> bool:
         """Create required directory structure"""
         print_step(2, 10, "Creating Directory Structure")
-        
+
         try:
             directories = [
                 API_GATEWAY_DIR,
                 UTILS_DIR,
                 BACKEND_ROOT / "app" / "models",
                 BACKEND_ROOT / "app" / "SECURITY",
                 BACKEND_ROOT / "app" / "CORE_CONFIGURATION",
                 BACKEND_ROOT / "app" / "DATABASE_CORE",
             ]
-            
+
             for directory in directories:
                 directory.mkdir(parents=True, exist_ok=True)
                 print_success(f"Created: {directory}")
-            
+
             self.fixes_applied.append("Directory structure created")
             return True
-            
+
         except Exception as e:
             print_error(f"Directory creation failed: {e}")
             self.errors.append(f"Directories: {e}")
             return False
-    
+
     def fix_database_module(self) -> bool:
         """Create/update database.py module"""
         print_step(3, 10, "Fixing Database Module")
-        
+
         try:
             db_file = API_GATEWAY_DIR / "database.py"
             db_file.write_text(DATABASE_PY_CONTENT)
             print_success("database.py created/updated")
-            
+
             self.fixes_applied.append("Database module fixed")
             return True
-            
+
         except Exception as e:
             print_error(f"Database module fix failed: {e}")
             self.errors.append(f"Database module: {e}")
             return False
-    
+
     def fix_init_file(self) -> bool:
         """Create/update __init__.py"""
         print_step(4, 10, "Fixing __init__.py")
-        
+
         try:
             init_file = API_GATEWAY_DIR / "__init__.py"
             init_file.write_text(INIT_PY_CONTENT)
             print_success("__init__.py created/updated")
-            
+
             self.fixes_applied.append("__init__.py fixed")
             return True
-            
+
         except Exception as e:
             print_error(f"__init__.py fix failed: {e}")
             self.errors.append(f"__init__.py: {e}")
             return False
-    
+
     def fix_gateway_routing_encoding(self) -> bool:
         """Fix encoding issues in gateway_routing.py"""
         print_step(5, 10, "Fixing Gateway Routing Encoding")
-        
+
         try:
             gateway_file = API_GATEWAY_DIR / "gateway_routing.py"
-            
+
             if not gateway_file.exists():
                 print_warning("gateway_routing.py not found, skipping")
                 return True
-            
-            content = gateway_file.read_text(encoding='utf-8', errors='ignore')
-            
+
+            content = gateway_file.read_text(encoding="utf-8", errors="ignore")
+
             # Replace smart quotes with standard quotes
             replacements = {
                 '"': '"',
                 '"': '"',
-                ''': "'",
-                ''': "'",
-                '': '-',
-                '': '-',
+                """: "'",
+                """: "'",
+                "": "-",
+                "": "-",
             }
-            
+
             for old, new in replacements.items():
                 content = content.replace(old, new)
-            
-            gateway_file.write_text(content, encoding='utf-8')
+
+            gateway_file.write_text(content, encoding="utf-8")
             print_success("Encoding issues fixed in gateway_routing.py")
-            
+
             self.fixes_applied.append("Gateway routing encoding fixed")
             return True
-            
+
         except Exception as e:
             print_error(f"Gateway routing fix failed: {e}")
             self.errors.append(f"Gateway routing: {e}")
             return False
-    
+
     def create_utility_files(self) -> bool:
         """Create utility files"""
         print_step(6, 10, "Creating Utility Files")
-        
+
         try:
             # Create __init__.py for utils
             utils_init = UTILS_DIR / "__init__.py"
             utils_init.write_text('"""YMERA Utilities"""')
-            
+
             print_success("Utility files created")
             self.fixes_applied.append("Utility files created")
             return True
-            
+
         except Exception as e:
             print_error(f"Utility creation failed: {e}")
             self.errors.append(f"Utilities: {e}")
             return False
-    
+
     def verify_syntax(self) -> bool:
         """Verify Python syntax"""
         print_step(7, 10, "Verifying Python Syntax")
-        
+
         try:
             files_to_check = [
                 API_GATEWAY_DIR / "__init__.py",
                 API_GATEWAY_DIR / "database.py",
             ]
-            
+
             all_valid = True
             for file_path in files_to_check:
                 if not file_path.exists():
                     continue
-                
+
                 result = subprocess.run(
                     [sys.executable, "-m", "py_compile", str(file_path)],
                     capture_output=True,
-                    text=True
+                    text=True,
                 )
-                
+
                 if result.returncode == 0:
                     print_success(f"Syntax valid: {file_path.name}")
                 else:
                     print_error(f"Syntax error in {file_path.name}")
                     all_valid = False
-            
+
             if all_valid:
                 self.fixes_applied.append("Syntax verification passed")
-            
+
             return all_valid
-            
+
         except Exception as e:
             print_error(f"Syntax verification failed: {e}")
             self.errors.append(f"Syntax: {e}")
             return False
-    
+
     def verify_imports(self) -> bool:
         """Verify imports work"""
         print_step(8, 10, "Verifying Imports")
-        
+
         test_script = f"""
 import sys
 sys.path.insert(0, '{BACKEND_ROOT}')
 
 try:
@@ -518,38 +527,36 @@
     print(f' Import failed: {{e}}')
     import traceback
     traceback.print_exc()
     sys.exit(1)
 """
-        
+
         try:
             result = subprocess.run(
-                [sys.executable, "-c", test_script],
-                capture_output=True,
-                text=True
+                [sys.executable, "-c", test_script], capture_output=True, text=True
             )
-            
+
             print(result.stdout)
-            
+
             if result.returncode == 0:
                 print_success("Import verification passed")
                 self.fixes_applied.append("Import verification passed")
                 return True
             else:
                 print_error(f"Import verification failed:\n{result.stderr}")
                 self.errors.append("Import verification failed")
                 return False
-                
+
         except Exception as e:
             print_error(f"Import verification failed: {e}")
             self.errors.append(f"Import verification: {e}")
             return False
-    
+
     def create_env_template(self) -> bool:
         """Create .env template file"""
         print_step(9, 10, "Creating Environment Template")
-        
+
         env_content = """# YMERA Platform - Environment Configuration
 # Copy this file to .env and update with your values
 
 # ===============================================================================
 # DATABASE CONFIGURATION
@@ -610,48 +617,48 @@
 # ===============================================================================
 AUTH_SERVICE_URL=http://localhost:8001
 AGENT_SERVICE_URL=http://localhost:8002
 FILE_SERVICE_URL=http://localhost:8003
 """
-        
+
         try:
             env_file = BACKEND_ROOT / ".env.example"
             env_file.write_text(env_content)
             print_success(".env.example created")
-            
+
             self.fixes_applied.append("Environment template created")
             return True
-            
+
         except Exception as e:
             print_error(f"Environment template creation failed: {e}")
             self.errors.append(f"Environment template: {e}")
             return False
-    
+
     def generate_deployment_report(self) -> bool:
         """Generate deployment report"""
         print_step(10, 10, "Generating Deployment Report")
-        
+
         report = f"""
 # YMERA Platform - Deployment Report
 Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}
 
 ##  Fixes Applied ({len(self.fixes_applied)})
 """
-        
+
         for i, fix in enumerate(self.fixes_applied, 1):
             report += f"{i}. {fix}\n"
-        
+
         if self.warnings:
             report += f"\n##  Warnings ({len(self.warnings)})\n"
             for i, warning in enumerate(self.warnings, 1):
                 report += f"{i}. {warning}\n"
-        
+
         if self.errors:
             report += f"\n##  Errors ({len(self.errors)})\n"
             for i, error in enumerate(self.errors, 1):
                 report += f"{i}. {error}\n"
-        
+
         report += """
 ##  Next Steps
 
 1. **Review Changes**
    - Check the backup directory for previous versions
@@ -745,35 +752,35 @@
 - Application logs: /var/log/ymera/
 - Error tracking dashboard
 - System metrics
 - Database logs
 """
-        
+
         try:
             report_file = BACKEND_ROOT / "DEPLOYMENT_REPORT.md"
             report_file.write_text(report)
             print_success(f"Deployment report created: {report_file}")
-            
+
             # Also print summary
             print_header("DEPLOYMENT SUMMARY")
             print_success(f" Fixes Applied: {len(self.fixes_applied)}")
             if self.warnings:
                 print_warning(f"  Warnings: {len(self.warnings)}")
             if self.errors:
                 print_error(f" Errors: {len(self.errors)}")
-            
+
             return True
-            
+
         except Exception as e:
             print_error(f"Report generation failed: {e}")
             return False
-    
+
     def run_deployment(self) -> bool:
         """Run complete deployment"""
         print_header("YMERA PRODUCTION DEPLOYMENT v4.0.0")
         print_info("Starting automated deployment process...")
-        
+
         steps = [
             self.create_backup,
             self.create_directories,
             self.fix_database_module,
             self.fix_init_file,
@@ -782,23 +789,23 @@
             self.verify_syntax,
             self.verify_imports,
             self.create_env_template,
             self.generate_deployment_report,
         ]
-        
+
         success = True
         for step in steps:
             if not step():
                 success = False
                 print_error(f"Step failed: {step.__name__}")
-                
+
                 response = input("\nContinue with remaining steps? (y/n): ")
-                if response.lower() != 'y':
+                if response.lower() != "y":
                     break
-        
+
         print_header("DEPLOYMENT COMPLETE")
-        
+
         if success and not self.errors:
             print_success(" Deployment completed successfully!")
             print_info(f"\n Backup location: {BACKUP_DIR}")
             print_info(f" Deployment report: {BACKEND_ROOT / 'DEPLOYMENT_REPORT.md'}")
             print_info("\n Next: Review the deployment report and follow the next steps")
@@ -807,36 +814,39 @@
             print_error("  Deployment completed with issues")
             print_info(f"\n Backup location: {BACKUP_DIR}")
             print_info(" Review errors above and the deployment report")
             return False
 
+
 # ===============================================================================
 # MAIN ENTRY POINT
 # ===============================================================================
+
 
 def main():
     """Main entry point"""
-    
+
     # Check if running from correct directory
     if not BACKEND_ROOT.exists():
         print_error("Error: backend directory not found!")
         print_info("Please run this script from the project root directory")
         sys.exit(1)
-    
+
     # Confirm deployment
     print_header("YMERA PRODUCTION DEPLOYMENT")
     print_warning("This script will modify your codebase.")
     print_info(f"Backup will be created in: {BACKUP_DIR}")
-    
+
     response = input("\nContinue with deployment? (yes/no): ")
-    if response.lower() != 'yes':
+    if response.lower() != "yes":
         print_info("Deployment cancelled")
         sys.exit(0)
-    
+
     # Run deployment
     deployment = ProductionDeployment()
     success = deployment.run_deployment()
-    
+
     sys.exit(0 if success else 1)
+
 
 if __name__ == "__main__":
     main()
would reformat /home/runner/work/ymera_y/ymera_y/complete_deployment_script.py
--- /home/runner/work/ymera_y/ymera_y/communication_agent.py	2025-10-19 22:47:02.796432+00:00
+++ /home/runner/work/ymera_y/ymera_y/communication_agent.py	2025-10-19 23:09:02.944356+00:00
@@ -1,6 +1,5 @@
-
 """
 Communication and Coordination Agent - Complete Implementation
 Advanced inter-agent communication, message routing, and coordination
 """
 
@@ -8,43 +7,55 @@
 import json
 import time
 import re
 import hashlib
 import zlib
-import traceback # Added for detailed error logging
-import os # Added for environment variables
+import traceback  # Added for detailed error logging
+import os  # Added for environment variables
 from typing import Dict, List, Optional, Any, Union, Set, Tuple
-from dataclasses import dataclass, field, asdict # Added asdict for easier serialization
+from dataclasses import dataclass, field, asdict  # Added asdict for easier serialization
 from enum import Enum
 from collections import defaultdict, deque
 import uuid
 
-from base_agent import BaseAgent, AgentConfig, TaskRequest, TaskResponse, Priority, AgentStatus, TaskStatus # Added TaskStatus
+from base_agent import (
+    BaseAgent,
+    AgentConfig,
+    TaskRequest,
+    TaskResponse,
+    Priority,
+    AgentStatus,
+    TaskStatus,
+)  # Added TaskStatus
 from opentelemetry import trace
+
 
 class MessageType(Enum):
     DIRECT = "direct"
-    BROADCAST = "broadcast" 
+    BROADCAST = "broadcast"
     MULTICAST = "multicast"
     PUBLISH = "publish"
     REQUEST = "request"
     RESPONSE = "response"
     EVENT = "event"
     COMMAND = "command"
 
+
 class MessagePriority(Enum):
     LOW = 1
     NORMAL = 2
     HIGH = 3
     URGENT = 4
     CRITICAL = 5
 
+
 class DeliveryMode(Enum):
     FIRE_AND_FORGET = "fire_and_forget"
     ACKNOWLEDGMENT = "acknowledgment"
     RELIABLE = "reliable"
     TRANSACTIONAL = "transactional"
+
 
 @dataclass
 class Message:
     id: str
     type: MessageType
@@ -58,27 +69,30 @@
     retry_count: int = 0
     max_retries: int = 3
     created_at: float = field(default_factory=time.time)
     metadata: Dict[str, Any] = field(default_factory=dict)
 
+
 @dataclass
 class MessageRoute:
     pattern: str
     target_agents: List[str]
     transformation: Optional[str] = None
     filter_condition: Optional[str] = None
     priority: int = 0
+
 
 @dataclass
 class ConversationContext:
     id: str
     participants: Set[str]
     topic: str
     created_at: float
     last_activity: float
     messages: List[str] = field(default_factory=list)
     metadata: Dict[str, Any] = field(default_factory=dict)
+
 
 class CommunicationAgent(BaseAgent):
     """
     Communication and Coordination Agent providing:
     - Advanced message routing and delivery
@@ -89,210 +103,185 @@
     - Broadcast and multicast capabilities
     - Event-driven communication patterns
     - Message queuing and buffering
     - Communication analytics and monitoring
     """
-    
+
     def __init__(self, config: AgentConfig):
         super().__init__(config)
-        
+
         # Message routing and delivery
         self.message_routes: List[MessageRoute] = []
         self.message_queue: Dict[str, deque] = defaultdict(deque)
         self.pending_messages: Dict[str, Message] = {}
         self.message_history = deque(maxlen=10000)
-        
+
         # Agent directory and presence
         self.agent_directory: Dict[str, Dict] = {}
         self.agent_presence: Dict[str, Dict] = {}
         self.agent_subscriptions: Dict[str, Set[str]] = defaultdict(set)
-        
+
         # Conversation management
         self.conversations: Dict[str, ConversationContext] = {}
         self.conversation_index: Dict[str, Set[str]] = defaultdict(set)
-        
+
         # Message transformation and filtering
         self.message_transformers: Dict[str, callable] = {}
         self.message_filters: Dict[str, callable] = {}
-        
+
         # Delivery tracking
         self.delivery_receipts: Dict[str, Dict] = {}
         self.failed_deliveries: Dict[str, List[Dict]] = defaultdict(list)
-        
+
         # Communication patterns
         self.communication_patterns = {
-            'request_response': {},
-            'publish_subscribe': defaultdict(set),
-            'message_queue': defaultdict(deque),
-            'event_stream': {}
+            "request_response": {},
+            "publish_subscribe": defaultdict(set),
+            "message_queue": defaultdict(deque),
+            "event_stream": {},
         }
-        
+
         # Rate limiting
         self._rate_limits = defaultdict(deque)
-        
+
         # Performance metrics
         self.communication_metrics = {
-            'messages_sent': 0,
-            'messages_delivered': 0,
-            'messages_failed': 0,
-            'average_delivery_time': 0.0,
-            'active_conversations': 0,
-            'bytes_transferred': 0
+            "messages_sent": 0,
+            "messages_delivered": 0,
+            "messages_failed": 0,
+            "average_delivery_time": 0.0,
+            "active_conversations": 0,
+            "bytes_transferred": 0,
         }
-        
+
         # Load communication protocols
         self._load_communication_protocols()
-        
+
         # Initialize message transformers
         self._register_default_transformers()
-    
+
     async def start(self):
         """Start communication agent services"""
-        
+
         # Core communication endpoints - these are now handled via _execute_task_impl
         # The BaseAgent already subscribes to agent.{self.config.name}.task
-        
+
         # Specialized communication endpoints (can be called via _execute_task_impl)
-        await self._subscribe(
-            "communication.register",
-            self._handle_agent_registration
-        )
-        
-        await self._subscribe(
-            "communication.subscribe",
-            self._handle_subscription
-        )
-        
-        await self._subscribe(
-            "communication.conversation.start",
-            self._handle_start_conversation
-        )
-        
-        await self._subscribe(
-            "communication.route.add",
-            self._handle_add_route
-        )
-        
+        await self._subscribe("communication.register", self._handle_agent_registration)
+
+        await self._subscribe("communication.subscribe", self._handle_subscription)
+
+        await self._subscribe("communication.conversation.start", self._handle_start_conversation)
+
+        await self._subscribe("communication.route.add", self._handle_add_route)
+
         # Delivery and acknowledgment handling
-        await self._subscribe(
-            "communication.ack",
-            self._handle_acknowledgment
-        )
-        
-        await self._subscribe(
-            "communication.delivery_receipt",
-            self._handle_delivery_receipt
-        )
-        
+        await self._subscribe("communication.ack", self._handle_acknowledgment)
+
+        await self._subscribe("communication.delivery_receipt", self._handle_delivery_receipt)
+
         # Agent presence monitoring
-        await self._subscribe(
-            "agent.presence.update",
-            self._handle_presence_update
-        )
-        
+        await self._subscribe("agent.presence.update", self._handle_presence_update)
+
         # Background tasks
         asyncio.create_task(self._message_delivery_loop())
         asyncio.create_task(self._retry_failed_messages())
         asyncio.create_task(self._cleanup_expired_messages())
         # Removed _update_communication_metrics as BaseAgent handles metrics publishing
         asyncio.create_task(self._monitor_conversations())
-        
+
         self.logger.info("Communication Agent started")
-    
+
     def _load_communication_protocols(self):
         """Load communication protocol configurations"""
-        
+
         # Default routing patterns
         default_routes = [
-            MessageRoute(
-                pattern="task.*",
-                target_agents=["orchestrator"],
-                priority=10
-            ),
-            MessageRoute(
-                pattern="alert.*",
-                target_agents=["monitoring", "health"],
-                priority=20
-            ),
+            MessageRoute(pattern="task.*", target_agents=["orchestrator"], priority=10),
+            MessageRoute(pattern="alert.*", target_agents=["monitoring", "health"], priority=20),
             MessageRoute(
                 pattern="llm.*",
                 target_agents=["llm_agent"],
-                priority=15 # Changed from llm_manager to llm_agent
+                priority=15,  # Changed from llm_manager to llm_agent
             ),
-            MessageRoute(
-                pattern="validation.*",
-                target_agents=["validation"],
-                priority=12
-            )
+            MessageRoute(pattern="validation.*", target_agents=["validation"], priority=12),
         ]
-        
+
         self.message_routes.extend(default_routes)
-        
+
         # Sort routes by priority
         self.message_routes.sort(key=lambda x: x.priority, reverse=True)
-    
+
     def _register_default_transformers(self):
         """Register default message transformers"""
-        
-        self.message_transformers.update({
-            'json_to_string': lambda payload: json.dumps(payload),
-            'string_to_json': lambda payload: json.loads(payload) if isinstance(payload, str) else payload,
-            'add_timestamp': lambda payload: {**payload, 'processed_at': time.time()},
-            'add_correlation_id': lambda payload: {**payload, 'correlation_id': str(uuid.uuid4())},
-            'sanitize_payload': self._sanitize_message_payload,
-            'compress_payload': self._compress_message_payload
-        })
-        
-        self.message_filters.update({
-            'priority_filter': self._priority_filter,
-            'agent_availability_filter': self._agent_availability_filter,
-            'message_size_filter': self._message_size_filter,
-            'rate_limit_filter': self._rate_limit_filter
-        })
-    
+
+        self.message_transformers.update(
+            {
+                "json_to_string": lambda payload: json.dumps(payload),
+                "string_to_json": lambda payload: (
+                    json.loads(payload) if isinstance(payload, str) else payload
+                ),
+                "add_timestamp": lambda payload: {**payload, "processed_at": time.time()},
+                "add_correlation_id": lambda payload: {
+                    **payload,
+                    "correlation_id": str(uuid.uuid4()),
+                },
+                "sanitize_payload": self._sanitize_message_payload,
+                "compress_payload": self._compress_message_payload,
+            }
+        )
+
+        self.message_filters.update(
+            {
+                "priority_filter": self._priority_filter,
+                "agent_availability_filter": self._agent_availability_filter,
+                "message_size_filter": self._message_size_filter,
+                "rate_limit_filter": self._rate_limit_filter,
+            }
+        )
+
     def _sanitize_message_payload(self, payload: Dict) -> Dict:
         """Sanitize message payload for security"""
         sanitized = {}
-        
+
         for key, value in payload.items():
             # Remove potentially dangerous keys
-            if key.startswith('_') or key in ['eval', 'exec', '__']:
+            if key.startswith("_") or key in ["eval", "exec", "__"]:
                 continue
-                
+
             # Sanitize string values
             if isinstance(value, str):
                 # Basic XSS prevention
-                value = re.sub(r'<script[^>]*>.*?</script>', '', value, flags=re.IGNORECASE | re.DOTALL)
-                value = re.sub(r'javascript:', '', value, flags=re.IGNORECASE)
-                
+                value = re.sub(
+                    r"<script[^>]*>.*?</script>", "", value, flags=re.IGNORECASE | re.DOTALL
+                )
+                value = re.sub(r"javascript:", "", value, flags=re.IGNORECASE)
+
             sanitized[key] = value
-        
+
         return sanitized
-    
+
     def _compress_message_payload(self, payload: Dict) -> Dict:
         """Compress large message payloads"""
         payload_str = json.dumps(payload)
-        
+
         # Only compress if payload is larger than 1KB
         if len(payload_str.encode()) > 1024:
             compressed = zlib.compress(payload_str.encode())
-            return {
-                '_compressed': True,
-                '_data': compressed.hex()
-            }
-        
+            return {"_compressed": True, "_data": compressed.hex()}
+
         return payload
-    
+
     def _decompress_payload(self, payload: Dict) -> Dict:
         """Decompress compressed payload"""
-        if payload.get('_compressed'):
-            compressed_data = bytes.fromhex(payload['_data'])
+        if payload.get("_compressed"):
+            compressed_data = bytes.fromhex(payload["_data"])
             decompressed = zlib.decompress(compressed_data)
             return json.loads(decompressed.decode())
-        
+
         return payload
-    
+
     async def _execute_task_impl(self, request: TaskRequest) -> Dict[str, Any]:
         """Implement the actual task logic for the Communication agent"""
         task_type = request.task_type
         payload = request.payload
 
@@ -308,113 +297,121 @@
                 result = await self._request_response_task(payload)
             elif task_type == "publish_event":
                 result = await self._publish_event_task(payload)
             else:
                 raise ValueError(f"Unknown communication task type: {task_type}")
-            
-            return TaskResponse(task_id=request.task_id, status=TaskStatus.COMPLETED, result=result).dict()
+
+            return TaskResponse(
+                task_id=request.task_id, status=TaskStatus.COMPLETED, result=result
+            ).dict()
 
         except Exception as e:
-            self.logger.error(f"Error executing communication task {task_type}", error=str(e), traceback=traceback.format_exc())
-            return TaskResponse(task_id=request.task_id, status=TaskStatus.FAILED, error=str(e)).dict()
+            self.logger.error(
+                f"Error executing communication task {task_type}",
+                error=str(e),
+                traceback=traceback.format_exc(),
+            )
+            return TaskResponse(
+                task_id=request.task_id, status=TaskStatus.FAILED, error=str(e)
+            ).dict()
 
     async def _send_message_task(self, payload: Dict[str, Any]) -> Dict[str, Any]:
         """Internal method to handle sending a direct message"""
         message = Message(
-            id=payload.get('id', str(uuid.uuid4())),
-            type=MessageType(payload.get('type', 'direct')),
-            priority=MessagePriority(payload.get('priority', 2)),  # NORMAL
-            sender=payload['sender'],
-            recipients=payload['recipients'],
-            subject=payload['subject'],
-            payload=payload['payload'],
-            delivery_mode=DeliveryMode(payload.get('delivery_mode', 'fire_and_forget')),
-            ttl_seconds=payload.get('ttl_seconds', 300),
-            max_retries=payload.get('max_retries', 3),
-            metadata=payload.get('metadata', {})
+            id=payload.get("id", str(uuid.uuid4())),
+            type=MessageType(payload.get("type", "direct")),
+            priority=MessagePriority(payload.get("priority", 2)),  # NORMAL
+            sender=payload["sender"],
+            recipients=payload["recipients"],
+            subject=payload["subject"],
+            payload=payload["payload"],
+            delivery_mode=DeliveryMode(payload.get("delivery_mode", "fire_and_forget")),
+            ttl_seconds=payload.get("ttl_seconds", 300),
+            max_retries=payload.get("max_retries", 3),
+            metadata=payload.get("metadata", {}),
         )
         result = await self._process_and_route_message(message)
-        self.communication_metrics['messages_sent'] += 1
+        self.communication_metrics["messages_sent"] += 1
         return result
 
     async def _broadcast_message_task(self, payload: Dict[str, Any]) -> Dict[str, Any]:
         """Internal method to handle broadcasting a message"""
         recipients = []
-        if 'target_groups' in payload:
-            for group in payload['target_groups']:
+        if "target_groups" in payload:
+            for group in payload["target_groups"]:
                 recipients.extend(self._get_agents_by_group(group))
         else:
             recipients = list(self.agent_directory.keys())
-        
+
         message = Message(
             id=str(uuid.uuid4()),
             type=MessageType.BROADCAST,
-            priority=MessagePriority(payload.get('priority', 2)),
-            sender=payload['sender'],
+            priority=MessagePriority(payload.get("priority", 2)),
+            sender=payload["sender"],
             recipients=recipients,
-            subject=payload['subject'],
-            payload=payload['payload'],
-            delivery_mode=DeliveryMode(payload.get('delivery_mode', 'fire_and_forget')),
-            metadata=payload.get('metadata', {})
+            subject=payload["subject"],
+            payload=payload["payload"],
+            delivery_mode=DeliveryMode(payload.get("delivery_mode", "fire_and_forget")),
+            metadata=payload.get("metadata", {}),
         )
         result = await self._process_and_route_message(message)
-        self.communication_metrics['messages_sent'] += 1
+        self.communication_metrics["messages_sent"] += 1
         return result
 
     async def _multicast_message_task(self, payload: Dict[str, Any]) -> Dict[str, Any]:
         """Internal method to handle multicasting a message to specific groups"""
-        target_groups = payload.get('target_groups', [])
+        target_groups = payload.get("target_groups", [])
         if not target_groups:
             raise ValueError("Multicast message requires 'target_groups'.")
-        
+
         recipients = []
         for group in target_groups:
             recipients.extend(self._get_agents_by_group(group))
-        recipients = list(set(recipients)) # Remove duplicates
+        recipients = list(set(recipients))  # Remove duplicates
 
         message = Message(
             id=str(uuid.uuid4()),
             type=MessageType.MULTICAST,
-            priority=MessagePriority(payload.get('priority', 2)),
-            sender=payload['sender'],
+            priority=MessagePriority(payload.get("priority", 2)),
+            sender=payload["sender"],
             recipients=recipients,
-            subject=payload['subject'],
-            payload=payload['payload'],
-            delivery_mode=DeliveryMode(payload.get('delivery_mode', 'fire_and_forget')),
-            metadata=payload.get('metadata', {})
+            subject=payload["subject"],
+            payload=payload["payload"],
+            delivery_mode=DeliveryMode(payload.get("delivery_mode", "fire_and_forget")),
+            metadata=payload.get("metadata", {}),
         )
         result = await self._process_and_route_message(message)
-        self.communication_metrics['messages_sent'] += 1
+        self.communication_metrics["messages_sent"] += 1
         return result
 
     async def _request_response_task(self, payload: Dict[str, Any]) -> Dict[str, Any]:
         """Internal method to handle a request-response pattern"""
-        sender = payload['sender']
-        recipient = payload['recipient']
-        subject = payload['subject']
-        request_payload = payload['payload']
-        timeout = payload.get('timeout', 10) # seconds
+        sender = payload["sender"]
+        recipient = payload["recipient"]
+        subject = payload["subject"]
+        request_payload = payload["payload"]
+        timeout = payload.get("timeout", 10)  # seconds
 
         request_message = Message(
             id=str(uuid.uuid4()),
             type=MessageType.REQUEST,
             priority=MessagePriority.HIGH,
             sender=sender,
             recipients=[recipient],
             subject=subject,
             payload=request_payload,
             delivery_mode=DeliveryMode.ACKNOWLEDGMENT,
-            ttl_seconds=timeout
+            ttl_seconds=timeout,
         )
 
         # Publish the request and wait for a response
         response_subject = f"communication.response.{request_message.id}"
         response_future = asyncio.Future()
-        self.pending_messages[request_message.id] = response_future # Store future to resolve later
+        self.pending_messages[request_message.id] = response_future  # Store future to resolve later
 
         await self._process_and_route_message(request_message)
-        self.communication_metrics['messages_sent'] += 1
+        self.communication_metrics["messages_sent"] += 1
 
         try:
             response_message = await asyncio.wait_for(response_future, timeout=timeout)
             return {"status": "success", "response": response_message.payload}
         except asyncio.TimeoutError:
@@ -424,209 +421,273 @@
             if request_message.id in self.pending_messages:
                 del self.pending_messages[request_message.id]
 
     async def _publish_event_task(self, payload: Dict[str, Any]) -> Dict[str, Any]:
         """Internal method to handle publishing an event"""
-        subject = payload['subject']
-        event_payload = payload['payload']
-        sender = payload.get('sender', self.config.name)
+        subject = payload["subject"]
+        event_payload = payload["payload"]
+        sender = payload.get("sender", self.config.name)
 
         event_message = Message(
             id=str(uuid.uuid4()),
             type=MessageType.EVENT,
             priority=MessagePriority.NORMAL,
             sender=sender,
-            recipients=[], # Events are broadcast to subscribers, not direct recipients
+            recipients=[],  # Events are broadcast to subscribers, not direct recipients
             subject=subject,
             payload=event_payload,
             delivery_mode=DeliveryMode.FIRE_AND_FORGET,
-            metadata=payload.get('metadata', {})
+            metadata=payload.get("metadata", {}),
         )
         # Events are published directly to NATS subject, not routed via _process_and_route_message
         await self._publish(subject, json.dumps(asdict(event_message)).encode())
-        self.communication_metrics['messages_sent'] += 1
+        self.communication_metrics["messages_sent"] += 1
         return {"status": "event_published", "message_id": event_message.id}
 
     async def _process_and_route_message(self, message: Message) -> Dict[str, Any]:
         """Apply transformations, filters, and route the message to appropriate agents"""
         processed_payload = message.payload
-        
+
         # Apply transformations
         for transformer_name in message.metadata.get("transformers", []):
             if transformer_name in self.message_transformers:
                 processed_payload = self.message_transformers[transformer_name](processed_payload)
             else:
                 self.logger.warning(f"Unknown transformer: {transformer_name}")
-        
+
         # Apply filters
         for filter_name in message.metadata.get("filters", []):
             if filter_name in self.message_filters:
                 if not self.message_filters[filter_name](message, processed_payload):
                     self.logger.info(f"Message {message.id} filtered out by {filter_name}")
                     return {"status": "filtered", "message_id": message.id}
             else:
                 self.logger.warning(f"Unknown filter: {filter_name}")
 
-        message.payload = processed_payload # Update message with processed payload
+        message.payload = processed_payload  # Update message with processed payload
 
         # Determine recipients based on message type and routing rules
         actual_recipients = set()
         if message.type == MessageType.DIRECT:
             actual_recipients.update(message.recipients)
         elif message.type == MessageType.BROADCAST:
             actual_recipients.update(self.agent_directory.keys())
         elif message.type == MessageType.MULTICAST:
-            actual_recipients.update(message.recipients) # Recipients already determined by _multicast_message_task
+            actual_recipients.update(
+                message.recipients
+            )  # Recipients already determined by _multicast_message_task
         elif message.type == MessageType.PUBLISH or message.type == MessageType.EVENT:
             # For PUBLISH/EVENT, recipients are determined by subscriptions
             # This is handled by NATS itself, so we just publish to the subject
             pass
         elif message.type == MessageType.REQUEST:
             actual_recipients.update(message.recipients)
-        
+
         # Apply routing rules for additional recipients or subject modification
         for route in self.message_routes:
             if re.fullmatch(route.pattern, message.subject):
                 if route.filter_condition:
                     # Evaluate filter condition (e.g., a simple Python expression)
                     try:
-                        if not eval(route.filter_condition, {"message": message, "payload": message.payload}):
+                        if not eval(
+                            route.filter_condition, {"message": message, "payload": message.payload}
+                        ):
                             continue
                     except Exception as e:
-                        self.logger.error(f"Error evaluating route filter condition for {message.id}: {e}")
+                        self.logger.error(
+                            f"Error evaluating route filter condition for {message.id}: {e}"
+                        )
                         continue
-                
+
                 actual_recipients.update(route.target_agents)
                 if route.transformation == "enrich_subject":
                     message.subject = f"[Routed] {message.subject}"
 
         # Remove sender from recipients for direct/broadcast/multicast to avoid self-delivery unless intended
         if message.type in [MessageType.DIRECT, MessageType.BROADCAST, MessageType.MULTICAST]:
             if message.sender in actual_recipients:
                 actual_recipients.remove(message.sender)
 
         # Enqueue or send message to each recipient
-        if message.type in [MessageType.DIRECT, MessageType.BROADCAST, MessageType.MULTICAST, MessageType.REQUEST]:
+        if message.type in [
+            MessageType.DIRECT,
+            MessageType.BROADCAST,
+            MessageType.MULTICAST,
+            MessageType.REQUEST,
+        ]:
             for recipient in actual_recipients:
-                if recipient in self.agent_presence and self.agent_presence[recipient].get("status") == AgentStatus.ACTIVE.value:
+                if (
+                    recipient in self.agent_presence
+                    and self.agent_presence[recipient].get("status") == AgentStatus.ACTIVE.value
+                ):
                     await self._send_to_agent(recipient, message)
                 else:
-                    self.logger.warning(f"Recipient {recipient} not active or found. Enqueuing message {message.id}.")
+                    self.logger.warning(
+                        f"Recipient {recipient} not active or found. Enqueuing message {message.id}."
+                    )
                     self.message_queue[recipient].append(message)
                     # Store message in pending for retry
                     self.pending_messages[message.id] = message
         elif message.type in [MessageType.PUBLISH, MessageType.EVENT]:
             # For publish/event, the message is already published to NATS subject in _publish_event_task
             pass
 
         self.message_history.append(asdict(message))
-        return {"status": "processed", "message_id": message.id, "recipients": list(actual_recipients)}
+        return {
+            "status": "processed",
+            "message_id": message.id,
+            "recipients": list(actual_recipients),
+        }
 
     async def _send_to_agent(self, recipient_agent_name: str, message: Message):
         """Send a message to a specific agent via NATS"""
         subject = f"agent.{recipient_agent_name}.inbox"
         try:
             # If it's a request, we need to publish with a reply-to subject
             if message.type == MessageType.REQUEST:
                 reply_to_subject = f"communication.response.{message.id}"
-                await self._publish_request(subject, json.dumps(asdict(message)).encode(), reply_to_subject)
+                await self._publish_request(
+                    subject, json.dumps(asdict(message)).encode(), reply_to_subject
+                )
             else:
                 await self._publish(subject, json.dumps(asdict(message)).encode())
-            
+
             self.logger.debug(f"Message {message.id} sent to {recipient_agent_name}")
-            self.communication_metrics['messages_delivered'] += 1
-            self.communication_metrics['bytes_transferred'] += len(json.dumps(asdict(message)).encode())
-            
+            self.communication_metrics["messages_delivered"] += 1
+            self.communication_metrics["bytes_transferred"] += len(
+                json.dumps(asdict(message)).encode()
+            )
+
             # If delivery mode is ACK, expect an acknowledgment
             if message.delivery_mode == DeliveryMode.ACKNOWLEDGMENT:
-                self.delivery_receipts[message.id] = {"status": "pending_ack", "timestamp": time.time(), "recipient": recipient_agent_name}
-            
+                self.delivery_receipts[message.id] = {
+                    "status": "pending_ack",
+                    "timestamp": time.time(),
+                    "recipient": recipient_agent_name,
+                }
+
             # Remove from pending messages if successfully sent and not awaiting ACK
-            if message.id in self.pending_messages and message.delivery_mode != DeliveryMode.ACKNOWLEDGMENT:
+            if (
+                message.id in self.pending_messages
+                and message.delivery_mode != DeliveryMode.ACKNOWLEDGMENT
+            ):
                 del self.pending_messages[message.id]
 
         except Exception as e:
-            self.logger.error(f"Failed to send message {message.id} to {recipient_agent_name}: {e}", traceback=traceback.format_exc())
-            self.communication_metrics['messages_failed'] += 1
-            self.failed_deliveries[message.id].append({"timestamp": time.time(), "error": str(e), "recipient": recipient_agent_name})
+            self.logger.error(
+                f"Failed to send message {message.id} to {recipient_agent_name}: {e}",
+                traceback=traceback.format_exc(),
+            )
+            self.communication_metrics["messages_failed"] += 1
+            self.failed_deliveries[message.id].append(
+                {"timestamp": time.time(), "error": str(e), "recipient": recipient_agent_name}
+            )
             # Re-enqueue for retry if max_retries not exceeded
             if message.retry_count < message.max_retries:
                 message.retry_count += 1
-                self.message_queue[recipient_agent_name].appendleft(message) # Add to front for quicker retry
+                self.message_queue[recipient_agent_name].appendleft(
+                    message
+                )  # Add to front for quicker retry
             else:
-                self.logger.error(f"Message {message.id} failed after {message.max_retries} retries.")
+                self.logger.error(
+                    f"Message {message.id} failed after {message.max_retries} retries."
+                )
                 if message.id in self.pending_messages:
                     del self.pending_messages[message.id]
 
     async def _message_delivery_loop(self):
         """Background task to continuously deliver messages from queues"""
         while not self._shutdown_event.is_set():
             try:
                 for agent_name, queue in list(self.message_queue.items()):
-                    if self.agent_presence.get(agent_name, {}).get("status") == AgentStatus.ACTIVE.value:
+                    if (
+                        self.agent_presence.get(agent_name, {}).get("status")
+                        == AgentStatus.ACTIVE.value
+                    ):
                         while queue:
                             message = queue.popleft()
-                            self.logger.debug(f"Attempting to deliver enqueued message {message.id} to {agent_name}")
+                            self.logger.debug(
+                                f"Attempting to deliver enqueued message {message.id} to {agent_name}"
+                            )
                             await self._send_to_agent(agent_name, message)
-                            await asyncio.sleep(0.01) # Small delay to prevent overwhelming NATS
+                            await asyncio.sleep(0.01)  # Small delay to prevent overwhelming NATS
                     else:
-                        self.logger.debug(f"Agent {agent_name} not active, holding messages in queue.")
-                await asyncio.sleep(1) # Check queues every second
+                        self.logger.debug(
+                            f"Agent {agent_name} not active, holding messages in queue."
+                        )
+                await asyncio.sleep(1)  # Check queues every second
             except Exception as e:
-                self.logger.error(f"Message delivery loop failed: {e}", traceback=traceback.format_exc())
+                self.logger.error(
+                    f"Message delivery loop failed: {e}", traceback=traceback.format_exc()
+                )
                 await asyncio.sleep(5)
 
     async def _retry_failed_messages(self):
         """Background task to retry messages that failed delivery or are pending ACK"""
         while not self._shutdown_event.is_set():
             try:
                 current_time = time.time()
-                retry_interval = 5 # seconds
-                ack_timeout = 30 # seconds
+                retry_interval = 5  # seconds
+                ack_timeout = 30  # seconds
 
                 messages_to_retry = []
                 for msg_id, message in list(self.pending_messages.items()):
-                    if isinstance(message, Message): # Ensure it's a Message object, not a Future
+                    if isinstance(message, Message):  # Ensure it's a Message object, not a Future
                         # Check for messages that failed and need retry
-                        if message.retry_count < message.max_retries and \
-                           (current_time - message.created_at > retry_interval * (message.retry_count + 1)):
+                        if message.retry_count < message.max_retries and (
+                            current_time - message.created_at
+                            > retry_interval * (message.retry_count + 1)
+                        ):
                             messages_to_retry.append(message)
-                        
+
                         # Check for messages pending ACK that have timed out
-                        if message.delivery_mode == DeliveryMode.ACKNOWLEDGMENT and \
-                           msg_id in self.delivery_receipts and \
-                           self.delivery_receipts[msg_id]["status"] == "pending_ack" and \
-                           (current_time - self.delivery_receipts[msg_id]["timestamp"] > ack_timeout):
+                        if (
+                            message.delivery_mode == DeliveryMode.ACKNOWLEDGMENT
+                            and msg_id in self.delivery_receipts
+                            and self.delivery_receipts[msg_id]["status"] == "pending_ack"
+                            and (
+                                current_time - self.delivery_receipts[msg_id]["timestamp"]
+                                > ack_timeout
+                            )
+                        ):
                             self.logger.warning(f"Message {msg_id} ACK timed out. Retrying.")
                             messages_to_retry.append(message)
 
                 for message in messages_to_retry:
-                    self.logger.info(f"Retrying message {message.id} (attempt {message.retry_count + 1}/{message.max_retries})")
+                    self.logger.info(
+                        f"Retrying message {message.id} (attempt {message.retry_count + 1}/{message.max_retries})"
+                    )
                     # Re-add to queue for delivery attempt
                     for recipient in message.recipients:
-                        self.message_queue[recipient].appendleft(message) # Add to front for quicker retry
+                        self.message_queue[recipient].appendleft(
+                            message
+                        )  # Add to front for quicker retry
                     # Update retry count, will be removed from pending_messages on successful send
                     message.retry_count += 1
                     if message.id in self.delivery_receipts:
                         self.delivery_receipts[message.id]["status"] = "retrying"
 
-                await asyncio.sleep(retry_interval) # Check every few seconds
+                await asyncio.sleep(retry_interval)  # Check every few seconds
             except Exception as e:
-                self.logger.error(f"Retry failed messages loop failed: {e}", traceback=traceback.format_exc())
+                self.logger.error(
+                    f"Retry failed messages loop failed: {e}", traceback=traceback.format_exc()
+                )
                 await asyncio.sleep(10)
 
     async def _cleanup_expired_messages(self):
         """Background task to clean up messages that have exceeded their TTL"""
         while not self._shutdown_event.is_set():
             try:
                 current_time = time.time()
                 expired_message_ids = []
                 for msg_id, message in list(self.pending_messages.items()):
-                    if isinstance(message, Message) and (current_time - message.created_at > message.ttl_seconds):
+                    if isinstance(message, Message) and (
+                        current_time - message.created_at > message.ttl_seconds
+                    ):
                         expired_message_ids.append(msg_id)
                         self.logger.warning(f"Message {msg_id} expired (TTL exceeded).")
-                
+
                 for msg_id in expired_message_ids:
                     if msg_id in self.pending_messages:
                         del self.pending_messages[msg_id]
                     if msg_id in self.delivery_receipts:
                         del self.delivery_receipts[msg_id]
@@ -636,88 +697,105 @@
                         for i in range(len(queue) - 1, -1, -1):
                             if queue[i].id == msg_id:
                                 del queue[i]
 
                 # Clean up message history (already handled by deque maxlen)
-                
-                await asyncio.sleep(60) # Check for expired messages every minute
+
+                await asyncio.sleep(60)  # Check for expired messages every minute
             except Exception as e:
-                self.logger.error(f"Cleanup expired messages loop failed: {e}", traceback=traceback.format_exc())
+                self.logger.error(
+                    f"Cleanup expired messages loop failed: {e}", traceback=traceback.format_exc()
+                )
                 await asyncio.sleep(300)
 
     async def _monitor_conversations(self):
         """Background task to monitor and clean up old conversations"""
         while not self._shutdown_event.is_set():
             try:
                 current_time = time.time()
-                conversation_timeout = 3600 * 24 * 7 # 7 days
-                
+                conversation_timeout = 3600 * 24 * 7  # 7 days
+
                 conversations_to_remove = []
                 for conv_id, conv in self.conversations.items():
                     if current_time - conv.last_activity > conversation_timeout:
                         conversations_to_remove.append(conv_id)
-                
+
                 for conv_id in conversations_to_remove:
                     del self.conversations[conv_id]
                     # Clean up index entries as well
                     for agent_name in list(self.conversation_index.keys()):
                         if conv_id in self.conversation_index[agent_name]:
                             self.conversation_index[agent_name].remove(conv_id)
                             if not self.conversation_index[agent_name]:
                                 del self.conversation_index[agent_name]
                     self.logger.info("Cleaned up stale conversation", conversation_id=conv_id)
-                
-                self.communication_metrics['active_conversations'] = len(self.conversations)
-                await asyncio.sleep(3600) # Check every hour
+
+                self.communication_metrics["active_conversations"] = len(self.conversations)
+                await asyncio.sleep(3600)  # Check every hour
             except Exception as e:
-                self.logger.error(f"Conversation monitor failed: {e}", traceback=traceback.format_exc())
+                self.logger.error(
+                    f"Conversation monitor failed: {e}", traceback=traceback.format_exc()
+                )
                 await asyncio.sleep(300)
 
     async def _handle_agent_registration(self, msg):
         """Handle agent registration messages"""
         try:
             data = json.loads(msg.data.decode())
             agent_name = data["agent_name"]
             agent_type = data["agent_type"]
             capabilities = data.get("capabilities", [])
             status = data.get("status", AgentStatus.ACTIVE.value)
-            
+
             self.agent_directory[agent_name] = {
                 "agent_type": agent_type,
                 "capabilities": capabilities,
-                "last_seen": time.time()
+                "last_seen": time.time(),
             }
-            self.agent_presence[agent_name] = {
-                "status": status,
-                "last_update": time.time()
-            }
+            self.agent_presence[agent_name] = {"status": status, "last_update": time.time()}
             self.logger.info("Agent registered", agent_name=agent_name, agent_type=agent_type)
-            
+
             # Respond to registration request
             if msg.reply:
-                await self._publish(msg.reply, json.dumps({"status": "registered", "agent_name": agent_name}).encode())
+                await self._publish(
+                    msg.reply,
+                    json.dumps({"status": "registered", "agent_name": agent_name}).encode(),
+                )
         except Exception as e:
-            self.logger.error("Error handling agent registration", error=str(e), traceback=traceback.format_exc())
+            self.logger.error(
+                "Error handling agent registration", error=str(e), traceback=traceback.format_exc()
+            )
             if msg.reply:
-                await self._publish(msg.reply, json.dumps({"error": str(e), "success": False}).encode())
+                await self._publish(
+                    msg.reply, json.dumps({"error": str(e), "success": False}).encode()
+                )
 
     async def _handle_subscription(self, msg):
         """Handle agent subscription to topics"""
         try:
             data = json.loads(msg.data.decode())
             agent_name = data["agent_name"]
             topic = data["topic"]
-            
+
             self.agent_subscriptions[agent_name].add(topic)
             self.logger.info("Agent subscribed", agent_name=agent_name, topic=topic)
-            
+
             if msg.reply:
-                await self._publish(msg.reply, json.dumps({"status": "subscribed", "agent_name": agent_name, "topic": topic}).encode())
+                await self._publish(
+                    msg.reply,
+                    json.dumps(
+                        {"status": "subscribed", "agent_name": agent_name, "topic": topic}
+                    ).encode(),
+                )
         except Exception as e:
-            self.logger.error("Error handling subscription", error=str(e), traceback=traceback.format_exc())
+            self.logger.error(
+                "Error handling subscription", error=str(e), traceback=traceback.format_exc()
+            )
             if msg.reply:
-                await self._publish(msg.reply, json.dumps({"error": str(e), "success": False}).encode())
+                await self._publish(
+                    msg.reply, json.dumps({"error": str(e), "success": False}).encode()
+                )
 
     async def _handle_start_conversation(self, msg):
         """Handle request to start a new conversation"""
         try:
             data = json.loads(msg.data.decode())
@@ -730,40 +808,58 @@
                 id=conv_id,
                 participants=participants,
                 topic=topic,
                 created_at=time.time(),
                 last_activity=time.time(),
-                metadata=metadata
+                metadata=metadata,
             )
             self.conversations[conv_id] = conversation
             for participant in participants:
                 self.conversation_index[participant].add(conv_id)
-            
-            self.logger.info("Conversation started", conversation_id=conv_id, participants=list(participants))
-            
+
+            self.logger.info(
+                "Conversation started", conversation_id=conv_id, participants=list(participants)
+            )
+
             if msg.reply:
-                await self._publish(msg.reply, json.dumps({"status": "conversation_started", "conversation_id": conv_id}).encode())
+                await self._publish(
+                    msg.reply,
+                    json.dumps(
+                        {"status": "conversation_started", "conversation_id": conv_id}
+                    ).encode(),
+                )
         except Exception as e:
-            self.logger.error("Error starting conversation", error=str(e), traceback=traceback.format_exc())
+            self.logger.error(
+                "Error starting conversation", error=str(e), traceback=traceback.format_exc()
+            )
             if msg.reply:
-                await self._publish(msg.reply, json.dumps({"error": str(e), "success": False}).encode())
+                await self._publish(
+                    msg.reply, json.dumps({"error": str(e), "success": False}).encode()
+                )
 
     async def _handle_add_route(self, msg):
         """Handle request to add a new message routing rule"""
         try:
             data = json.loads(msg.data.decode())
             route = MessageRoute(**data)
             self.message_routes.append(route)
-            self.message_routes.sort(key=lambda x: x.priority, reverse=True) # Re-sort by priority
-            self.logger.info("Added new message route", pattern=route.pattern, target_agents=route.target_agents)
-            
+            self.message_routes.sort(key=lambda x: x.priority, reverse=True)  # Re-sort by priority
+            self.logger.info(
+                "Added new message route", pattern=route.pattern, target_agents=route.target_agents
+            )
+
             if msg.reply:
-                await self._publish(msg.reply, json.dumps({"status": "route_added", "pattern": route.pattern}).encode())
+                await self._publish(
+                    msg.reply,
+                    json.dumps({"status": "route_added", "pattern": route.pattern}).encode(),
+                )
         except Exception as e:
             self.logger.error("Error adding route", error=str(e), traceback=traceback.format_exc())
             if msg.reply:
-                await self._publish(msg.reply, json.dumps({"error": str(e), "success": False}).encode())
+                await self._publish(
+                    msg.reply, json.dumps({"error": str(e), "success": False}).encode()
+                )
 
     async def _handle_acknowledgment(self, msg):
         """Handle message acknowledgments"""
         try:
             data = json.loads(msg.data.decode())
@@ -772,68 +868,90 @@
             status = data.get("status", "received")
 
             if original_message_id in self.delivery_receipts:
                 self.delivery_receipts[original_message_id]["status"] = status
                 self.delivery_receipts[original_message_id]["ack_timestamp"] = time.time()
-                self.logger.debug(f"Received ACK for message {original_message_id} from {sender_agent} with status {status}")
-                
+                self.logger.debug(
+                    f"Received ACK for message {original_message_id} from {sender_agent} with status {status}"
+                )
+
                 # If it was a request-response, resolve the future
-                if original_message_id in self.pending_messages and isinstance(self.pending_messages[original_message_id], asyncio.Future):
+                if original_message_id in self.pending_messages and isinstance(
+                    self.pending_messages[original_message_id], asyncio.Future
+                ):
                     # The ACK might not contain the full response, so we need to wait for the actual response message
                     # This ACK just confirms delivery. The actual response will be handled by _handle_response_message
-                    pass # Do nothing here, actual response will resolve the future
+                    pass  # Do nothing here, actual response will resolve the future
                 elif original_message_id in self.pending_messages:
                     # For non-request-response messages with ACK, remove from pending
                     del self.pending_messages[original_message_id]
             else:
-                self.logger.warning(f"Received ACK for unknown or expired message {original_message_id}")
+                self.logger.warning(
+                    f"Received ACK for unknown or expired message {original_message_id}"
+                )
         except Exception as e:
-            self.logger.error("Error handling acknowledgment", error=str(e), traceback=traceback.format_exc())
+            self.logger.error(
+                "Error handling acknowledgment", error=str(e), traceback=traceback.format_exc()
+            )
 
     async def _handle_delivery_receipt(self, msg):
         """Handle delivery receipts (more detailed than simple ACK)"""
         try:
             data = json.loads(msg.data.decode())
             original_message_id = data["original_message_id"]
             recipient_agent = data["recipient_agent"]
-            delivery_status = data["delivery_status"] # e.g., "delivered", "read", "processed"
-            
+            delivery_status = data["delivery_status"]  # e.g., "delivered", "read", "processed"
+
             if original_message_id in self.delivery_receipts:
                 self.delivery_receipts[original_message_id]["delivery_status"] = delivery_status
                 self.delivery_receipts[original_message_id]["delivery_timestamp"] = time.time()
-                self.logger.debug(f"Received delivery receipt for message {original_message_id} to {recipient_agent}: {delivery_status}")
+                self.logger.debug(
+                    f"Received delivery receipt for message {original_message_id} to {recipient_agent}: {delivery_status}"
+                )
             else:
-                self.logger.warning(f"Received delivery receipt for unknown or expired message {original_message_id}")
+                self.logger.warning(
+                    f"Received delivery receipt for unknown or expired message {original_message_id}"
+                )
         except Exception as e:
-            self.logger.error("Error handling delivery receipt", error=str(e), traceback=traceback.format_exc())
+            self.logger.error(
+                "Error handling delivery receipt", error=str(e), traceback=traceback.format_exc()
+            )
 
     async def _handle_presence_update(self, msg):
         """Handle agent presence updates"""
         try:
             data = json.loads(msg.data.decode())
             agent_name = data["agent_name"]
             status = data["status"]
-            
+
             if agent_name in self.agent_presence:
                 self.agent_presence[agent_name]["status"] = status
                 self.agent_presence[agent_name]["last_update"] = time.time()
                 self.logger.info("Agent presence updated", agent_name=agent_name, status=status)
-                
+
                 # If agent becomes active, try to deliver queued messages
                 if status == AgentStatus.ACTIVE.value and agent_name in self.message_queue:
-                    self.logger.info(f"Agent {agent_name} is now active, attempting to deliver queued messages.")
+                    self.logger.info(
+                        f"Agent {agent_name} is now active, attempting to deliver queued messages."
+                    )
                     # The _message_delivery_loop will pick this up, no need to explicitly trigger here
             else:
                 self.logger.warning(f"Presence update for unknown agent {agent_name}")
         except Exception as e:
-            self.logger.error("Error handling presence update", error=str(e), traceback=traceback.format_exc())
+            self.logger.error(
+                "Error handling presence update", error=str(e), traceback=traceback.format_exc()
+            )
 
     def _get_agents_by_group(self, group_name: str) -> List[str]:
         """Helper to get agents belonging to a certain group/type"""
         # This is a placeholder. In a real system, groups could be defined in Consul or a config service.
         if group_name == "llm_agents":
-            return [name for name, info in self.agent_directory.items() if info.get("agent_type") == "llm"]
+            return [
+                name
+                for name, info in self.agent_directory.items()
+                if info.get("agent_type") == "llm"
+            ]
         elif group_name == "all":
             return list(self.agent_directory.keys())
         # Add more group logic as needed
         return []
 
@@ -843,20 +961,25 @@
         return message.priority.value >= min_priority
 
     def _agent_availability_filter(self, message: Message, payload: Dict) -> bool:
         """Filter messages if recipient agent is not available"""
         for recipient in message.recipients:
-            if recipient not in self.agent_presence or self.agent_presence[recipient].get("status") != AgentStatus.ACTIVE.value:
+            if (
+                recipient not in self.agent_presence
+                or self.agent_presence[recipient].get("status") != AgentStatus.ACTIVE.value
+            ):
                 return False
         return True
 
     def _message_size_filter(self, message: Message, payload: Dict) -> bool:
         """Filter messages based on size"""
-        max_size_bytes = payload.get("max_size_bytes", 1024 * 1024) # Default 1MB
+        max_size_bytes = payload.get("max_size_bytes", 1024 * 1024)  # Default 1MB
         current_size = len(json.dumps(asdict(message)).encode())
         if current_size > max_size_bytes:
-            self.logger.warning(f"Message {message.id} exceeds max size ({current_size} > {max_size_bytes} bytes).")
+            self.logger.warning(
+                f"Message {message.id} exceeds max size ({current_size} > {max_size_bytes} bytes)."
+            )
             return False
         return True
 
     def _rate_limit_filter(self, message: Message, payload: Dict) -> bool:
         """Filter messages based on rate limits per sender/recipient"""
@@ -866,50 +989,67 @@
         max_requests = payload.get("max_requests", 10)
         time_window_seconds = payload.get("time_window_seconds", 60)
 
         current_time = time.time()
         # Clean up old timestamps
-        while self._rate_limits[limit_key] and self._rate_limits[limit_key][0] < current_time - time_window_seconds:
+        while (
+            self._rate_limits[limit_key]
+            and self._rate_limits[limit_key][0] < current_time - time_window_seconds
+        ):
             self._rate_limits[limit_key].popleft()
-        
+
         if len(self._rate_limits[limit_key]) >= max_requests:
             self.logger.warning(f"Rate limit exceeded for {limit_key}.")
             return False
-        
+
         self._rate_limits[limit_key].append(current_time)
         return True
 
     def _get_agent_metrics(self) -> Dict[str, Any]:
         """Provide Communication agent specific metrics."""
         base_metrics = super()._get_agent_metrics()
-        base_metrics.update({
-            "messages_sent": self.communication_metrics['messages_sent'],
-            "messages_delivered": self.communication_metrics['messages_delivered'],
-            "messages_failed": self.communication_metrics['messages_failed'],
-            "average_delivery_time": self.communication_metrics['average_delivery_time'],
-            "active_conversations": self.communication_metrics['active_conversations'],
-            "bytes_transferred": self.communication_metrics['bytes_transferred'],
-            "queued_messages_count": sum(len(q) for q in self.message_queue.values()),
-            "pending_acks_count": len([mid for mid, rec in self.delivery_receipts.items() if rec.get("status") == "pending_ack"])
-        })
+        base_metrics.update(
+            {
+                "messages_sent": self.communication_metrics["messages_sent"],
+                "messages_delivered": self.communication_metrics["messages_delivered"],
+                "messages_failed": self.communication_metrics["messages_failed"],
+                "average_delivery_time": self.communication_metrics["average_delivery_time"],
+                "active_conversations": self.communication_metrics["active_conversations"],
+                "bytes_transferred": self.communication_metrics["bytes_transferred"],
+                "queued_messages_count": sum(len(q) for q in self.message_queue.values()),
+                "pending_acks_count": len(
+                    [
+                        mid
+                        for mid, rec in self.delivery_receipts.items()
+                        if rec.get("status") == "pending_ack"
+                    ]
+                ),
+            }
+        )
         return base_metrics
 
 
 if __name__ == "__main__":
     config = AgentConfig(
         name="communication_agent",
         agent_type="communication",
         capabilities=[
-            "send_message", "broadcast_message", "multicast_message",
-            "request_response", "publish_event", "manage_routes",
-            "manage_conversations", "agent_presence_monitoring"
+            "send_message",
+            "broadcast_message",
+            "multicast_message",
+            "request_response",
+            "publish_event",
+            "manage_routes",
+            "manage_conversations",
+            "agent_presence_monitoring",
         ],
         nats_url=os.getenv("NATS_URL", "nats://nats:4222"),
-        postgres_url=os.getenv("POSTGRES_URL", "postgresql://agent:secure_password@postgres:5432/ymera"),
+        postgres_url=os.getenv(
+            "POSTGRES_URL", "postgresql://agent:secure_password@postgres:5432/ymera"
+        ),
         redis_url=os.getenv("REDIS_URL", "redis://redis:6379"),
         consul_url=os.getenv("CONSUL_URL", "http://consul:8500"),
-        log_level=os.getenv("LOG_LEVEL", "INFO")
+        log_level=os.getenv("LOG_LEVEL", "INFO"),
     )
-    
+
     agent = CommunicationAgent(config)
     asyncio.run(agent.run())
-
would reformat /home/runner/work/ymera_y/ymera_y/communication_agent.py
error: cannot format /home/runner/work/ymera_y/ymera_y/component_enhancement_workflow.py: Cannot parse for target version Python 3.12: 30:7:     """Enhances components by analyzing versions and creating unified enhanced versions"""
--- /home/runner/work/ymera_y/ymera_y/comprehensive_e2e_test.py	2025-10-19 22:47:02.797432+00:00
+++ /home/runner/work/ymera_y/ymera_y/comprehensive_e2e_test.py	2025-10-19 23:09:03.627365+00:00
@@ -8,444 +8,463 @@
 import asyncio
 from pathlib import Path
 from datetime import datetime
 
 # Test results tracking
-test_results = {
-    'passed': [],
-    'failed': [],
-    'warnings': [],
-    'skipped': []
-}
+test_results = {"passed": [], "failed": [], "warnings": [], "skipped": []}
+
 
 def log_result(category, test_name, status, message=""):
     """Log test result"""
     result = {
-        'test': test_name,
-        'status': status,
-        'message': message,
-        'timestamp': datetime.utcnow().isoformat()
+        "test": test_name,
+        "status": status,
+        "message": message,
+        "timestamp": datetime.utcnow().isoformat(),
     }
     test_results[category].append(result)
-    
-    symbols = {
-        'passed': '',
-        'failed': '',
-        'warnings': '',
-        'skipped': ''
-    }
-    
+
+    symbols = {"passed": "", "failed": "", "warnings": "", "skipped": ""}
+
     print(f"{symbols.get(category, '')} {test_name}: {message}")
+
 
 def test_category(name):
     """Decorator for test categories"""
+
     def decorator(func):
         def wrapper(*args, **kwargs):
             print(f"\n{'='*60}")
             print(f"Testing: {name}")
-            print('='*60)
+            print("=" * 60)
             return func(*args, **kwargs)
+
         wrapper.__name__ = func.__name__
         return wrapper
+
     return decorator
 
 
 # ============================================================================
 # TEST 1: Environment and Dependencies
 # ============================================================================
+
 
 @test_category("1. Environment & Dependencies")
 def test_environment():
     """Test Python environment and dependencies"""
-    
+
     # Check Python version
     py_version = sys.version_info
     if py_version.major == 3 and py_version.minor >= 9:
-        log_result('passed', 'Python Version', 'OK', f'Python {py_version.major}.{py_version.minor}.{py_version.micro}')
+        log_result(
+            "passed",
+            "Python Version",
+            "OK",
+            f"Python {py_version.major}.{py_version.minor}.{py_version.micro}",
+        )
     else:
-        log_result('failed', 'Python Version', 'FAIL', f'Requires Python 3.9+, found {py_version.major}.{py_version.minor}')
-    
+        log_result(
+            "failed",
+            "Python Version",
+            "FAIL",
+            f"Requires Python 3.9+, found {py_version.major}.{py_version.minor}",
+        )
+
     # Check critical dependencies
     dependencies = [
-        ('sqlalchemy', 'SQLAlchemy'),
-        ('structlog', 'Structlog'),
-        ('asyncio', 'AsyncIO'),
+        ("sqlalchemy", "SQLAlchemy"),
+        ("structlog", "Structlog"),
+        ("asyncio", "AsyncIO"),
     ]
-    
+
     for module, name in dependencies:
         try:
             mod = __import__(module)
-            version = getattr(mod, '__version__', 'built-in')
-            log_result('passed', f'{name} Import', 'OK', f'Version: {version}')
+            version = getattr(mod, "__version__", "built-in")
+            log_result("passed", f"{name} Import", "OK", f"Version: {version}")
         except ImportError:
-            log_result('failed', f'{name} Import', 'MISSING', 'Not installed')
+            log_result("failed", f"{name} Import", "MISSING", "Not installed")
         except Exception as e:
-            log_result('warnings', f'{name} Import', 'ERROR', str(e))
-    
+            log_result("warnings", f"{name} Import", "ERROR", str(e))
+
     # Check optional dependencies
     optional_deps = [
-        ('asyncpg', 'AsyncPG (PostgreSQL)'),
-        ('aiosqlite', 'AIOSqlite (SQLite)'),
-        ('faker', 'Faker (Test Data)'),
-        ('pydantic', 'Pydantic (Validation)'),
+        ("asyncpg", "AsyncPG (PostgreSQL)"),
+        ("aiosqlite", "AIOSqlite (SQLite)"),
+        ("faker", "Faker (Test Data)"),
+        ("pydantic", "Pydantic (Validation)"),
     ]
-    
+
     for module, name in optional_deps:
         try:
             mod = __import__(module)
-            version = getattr(mod, '__version__', 'unknown')
-            log_result('passed', f'{name}', 'OK', f'Version: {version}')
+            version = getattr(mod, "__version__", "unknown")
+            log_result("passed", f"{name}", "OK", f"Version: {version}")
         except ImportError:
-            log_result('warnings', f'{name}', 'NOT INSTALLED', 'Optional but recommended')
+            log_result("warnings", f"{name}", "NOT INSTALLED", "Optional but recommended")
 
 
 # ============================================================================
 # TEST 2: Core Module Structure
 # ============================================================================
+
 
 @test_category("2. Core Module Structure")
 def test_core_module():
     """Test database_core_integrated module"""
-    
+
     try:
         import database_core_integrated as dbc
-        log_result('passed', 'Core Module Import', 'OK', 'database_core_integrated imported')
-        
+
+        log_result("passed", "Core Module Import", "OK", "database_core_integrated imported")
+
         # Check key classes
         required_classes = [
-            'DatabaseConfig',
-            'IntegratedDatabaseManager',
-            'User',
-            'Project',
-            'Agent',
-            'Task',
-            'File',
-            'AuditLog',
-            'BaseRepository',
-            'BaseMigration',
-            'Base'
+            "DatabaseConfig",
+            "IntegratedDatabaseManager",
+            "User",
+            "Project",
+            "Agent",
+            "Task",
+            "File",
+            "AuditLog",
+            "BaseRepository",
+            "BaseMigration",
+            "Base",
         ]
-        
+
         for cls_name in required_classes:
             if hasattr(dbc, cls_name):
-                log_result('passed', f'Class: {cls_name}', 'FOUND', 'Available')
+                log_result("passed", f"Class: {cls_name}", "FOUND", "Available")
             else:
-                log_result('failed', f'Class: {cls_name}', 'MISSING', 'Not found in module')
-        
+                log_result("failed", f"Class: {cls_name}", "MISSING", "Not found in module")
+
         # Check key functions
         required_functions = [
-            'get_database_manager',
-            'get_db_session',
-            'init_database',
-            'close_database'
+            "get_database_manager",
+            "get_db_session",
+            "init_database",
+            "close_database",
         ]
-        
+
         for func_name in required_functions:
             if hasattr(dbc, func_name):
-                log_result('passed', f'Function: {func_name}', 'FOUND', 'Available')
+                log_result("passed", f"Function: {func_name}", "FOUND", "Available")
             else:
-                log_result('failed', f'Function: {func_name}', 'MISSING', 'Not found in module')
-        
+                log_result("failed", f"Function: {func_name}", "MISSING", "Not found in module")
+
         return True
-        
+
     except ImportError as e:
-        log_result('failed', 'Core Module Import', 'FAILED', str(e))
+        log_result("failed", "Core Module Import", "FAILED", str(e))
         return False
     except Exception as e:
-        log_result('failed', 'Core Module Structure', 'ERROR', str(e))
+        log_result("failed", "Core Module Structure", "ERROR", str(e))
         return False
 
 
 # ============================================================================
 # TEST 3: File Structure
 # ============================================================================
+
 
 @test_category("3. File Structure")
 def test_file_structure():
     """Test that all expected files exist"""
-    
+
     required_files = [
-        'database_core_integrated.py',
-        'requirements.txt',
-        'README.md',
-        'test_database.py',
-        'DATABASE_ARCHITECTURE.md',
-        'IMPLEMENTATION_SUMMARY.md',
-        'COMPLETE_IMPLEMENTATION_REPORT.md'
+        "database_core_integrated.py",
+        "requirements.txt",
+        "README.md",
+        "test_database.py",
+        "DATABASE_ARCHITECTURE.md",
+        "IMPLEMENTATION_SUMMARY.md",
+        "COMPLETE_IMPLEMENTATION_REPORT.md",
     ]
-    
+
     for file_path in required_files:
         if Path(file_path).exists():
             size_kb = Path(file_path).stat().st_size / 1024
-            log_result('passed', f'File: {file_path}', 'EXISTS', f'{size_kb:.1f} KB')
-        else:
-            log_result('failed', f'File: {file_path}', 'MISSING', 'File not found')
-    
+            log_result("passed", f"File: {file_path}", "EXISTS", f"{size_kb:.1f} KB")
+        else:
+            log_result("failed", f"File: {file_path}", "MISSING", "File not found")
+
     # Check directories
-    required_dirs = [
-        'database',
-        'database/migrations',
-        'database/fixtures',
-        'scripts',
-        'docs'
-    ]
-    
+    required_dirs = ["database", "database/migrations", "database/fixtures", "scripts", "docs"]
+
     for dir_path in required_dirs:
         if Path(dir_path).exists():
-            log_result('passed', f'Directory: {dir_path}', 'EXISTS', 'Found')
-        else:
-            log_result('failed', f'Directory: {dir_path}', 'MISSING', 'Directory not found')
-    
+            log_result("passed", f"Directory: {dir_path}", "EXISTS", "Found")
+        else:
+            log_result("failed", f"Directory: {dir_path}", "MISSING", "Directory not found")
+
     # Check new components
     new_components = [
-        'database/migration_manager.py',
-        'database/migrations/001_initial_schema.py',
-        'database/fixtures/test_fixtures.py',
-        'scripts/backup_manager.py',
-        'scripts/database_monitor.py',
-        'docs/DISASTER_RECOVERY.md',
-        'docs/OPERATIONS_RUNBOOK.md'
+        "database/migration_manager.py",
+        "database/migrations/001_initial_schema.py",
+        "database/fixtures/test_fixtures.py",
+        "scripts/backup_manager.py",
+        "scripts/database_monitor.py",
+        "docs/DISASTER_RECOVERY.md",
+        "docs/OPERATIONS_RUNBOOK.md",
     ]
-    
+
     for component in new_components:
         if Path(component).exists():
             size_kb = Path(component).stat().st_size / 1024
-            log_result('passed', f'Component: {Path(component).name}', 'EXISTS', f'{size_kb:.1f} KB')
-        else:
-            log_result('warnings', f'Component: {Path(component).name}', 'MISSING', 'New component not found')
+            log_result(
+                "passed", f"Component: {Path(component).name}", "EXISTS", f"{size_kb:.1f} KB"
+            )
+        else:
+            log_result(
+                "warnings",
+                f"Component: {Path(component).name}",
+                "MISSING",
+                "New component not found",
+            )
 
 
 # ============================================================================
 # TEST 4: Database Initialization (Basic)
 # ============================================================================
+
 
 @test_category("4. Database Initialization")
 async def test_database_init():
     """Test basic database initialization"""
-    
+
     try:
         from database_core_integrated import DatabaseConfig, IntegratedDatabaseManager
-        
+
         # Test config creation
         config = DatabaseConfig()
-        log_result('passed', 'DatabaseConfig', 'CREATED', f'DB type: {config.db_type}')
-        
+        log_result("passed", "DatabaseConfig", "CREATED", f"DB type: {config.db_type}")
+
         # Test database URL
         if config.database_url:
-            log_result('passed', 'Database URL', 'SET', f'Type: {config.db_type}')
-        else:
-            log_result('warnings', 'Database URL', 'NOT SET', 'Using default')
-        
+            log_result("passed", "Database URL", "SET", f"Type: {config.db_type}")
+        else:
+            log_result("warnings", "Database URL", "NOT SET", "Using default")
+
         # Test manager creation (without actual connection)
         manager = IntegratedDatabaseManager(config)
-        log_result('passed', 'DatabaseManager', 'CREATED', 'Instance created')
-        
+        log_result("passed", "DatabaseManager", "CREATED", "Instance created")
+
         return True
-        
+
     except Exception as e:
-        log_result('failed', 'Database Initialization', 'ERROR', str(e))
+        log_result("failed", "Database Initialization", "ERROR", str(e))
         return False
 
 
 # ============================================================================
 # TEST 5: Model Definitions
 # ============================================================================
+
 
 @test_category("5. Model Definitions")
 def test_models():
     """Test database model definitions"""
-    
+
     try:
         from database_core_integrated import User, Project, Agent, Task, File, AuditLog, Base
-        
+
         models = {
-            'User': User,
-            'Project': Project,
-            'Agent': Agent,
-            'Task': Task,
-            'File': File,
-            'AuditLog': AuditLog
+            "User": User,
+            "Project": Project,
+            "Agent": Agent,
+            "Task": Task,
+            "File": File,
+            "AuditLog": AuditLog,
         }
-        
+
         for model_name, model_class in models.items():
             # Check table name
-            if hasattr(model_class, '__tablename__'):
-                log_result('passed', f'Model: {model_name}', 'OK', f'Table: {model_class.__tablename__}')
+            if hasattr(model_class, "__tablename__"):
+                log_result(
+                    "passed", f"Model: {model_name}", "OK", f"Table: {model_class.__tablename__}"
+                )
             else:
-                log_result('failed', f'Model: {model_name}', 'NO TABLE', 'Missing __tablename__')
-            
+                log_result("failed", f"Model: {model_name}", "NO TABLE", "Missing __tablename__")
+
             # Check key columns
-            if hasattr(model_class, '__table__'):
+            if hasattr(model_class, "__table__"):
                 col_count = len(model_class.__table__.columns)
-                log_result('passed', f'   Columns', 'OK', f'{col_count} columns defined')
-            
+                log_result("passed", f"   Columns", "OK", f"{col_count} columns defined")
+
             # Check relationships
-            if hasattr(model_class, '__mapper__'):
+            if hasattr(model_class, "__mapper__"):
                 rel_count = len(model_class.__mapper__.relationships)
                 if rel_count > 0:
-                    log_result('passed', f'   Relationships', 'OK', f'{rel_count} relationships')
-        
+                    log_result("passed", f"   Relationships", "OK", f"{rel_count} relationships")
+
         return True
-        
+
     except Exception as e:
-        log_result('failed', 'Model Definitions', 'ERROR', str(e))
+        log_result("failed", "Model Definitions", "ERROR", str(e))
         return False
 
 
 # ============================================================================
 # TEST 6: Migration System
 # ============================================================================
+
 
 @test_category("6. Migration System")
 def test_migration_system():
     """Test migration system components"""
-    
+
     # Check migration manager
-    if Path('database/migration_manager.py').exists():
-        log_result('passed', 'Migration Manager', 'EXISTS', 'Script found')
-        
+    if Path("database/migration_manager.py").exists():
+        log_result("passed", "Migration Manager", "EXISTS", "Script found")
+
         try:
             # Try to import (without running)
-            spec = __import__('importlib.util').util.spec_from_file_location(
-                "migration_manager",
-                "database/migration_manager.py"
+            spec = __import__("importlib.util").util.spec_from_file_location(
+                "migration_manager", "database/migration_manager.py"
             )
             if spec:
-                log_result('passed', 'Migration Manager Import', 'OK', 'Can be imported')
+                log_result("passed", "Migration Manager Import", "OK", "Can be imported")
         except Exception as e:
-            log_result('warnings', 'Migration Manager Import', 'ERROR', str(e))
+            log_result("warnings", "Migration Manager Import", "ERROR", str(e))
     else:
-        log_result('failed', 'Migration Manager', 'MISSING', 'File not found')
-    
+        log_result("failed", "Migration Manager", "MISSING", "File not found")
+
     # Check migrations directory
-    migrations_dir = Path('database/migrations')
+    migrations_dir = Path("database/migrations")
     if migrations_dir.exists():
-        migration_files = list(migrations_dir.glob('*.py'))
-        migration_files = [f for f in migration_files if not f.name.startswith('__')]
-        log_result('passed', 'Migrations Directory', 'EXISTS', f'{len(migration_files)} migrations found')
+        migration_files = list(migrations_dir.glob("*.py"))
+        migration_files = [f for f in migration_files if not f.name.startswith("__")]
+        log_result(
+            "passed", "Migrations Directory", "EXISTS", f"{len(migration_files)} migrations found"
+        )
     else:
-        log_result('failed', 'Migrations Directory', 'MISSING', 'Directory not found')
+        log_result("failed", "Migrations Directory", "MISSING", "Directory not found")
 
 
 # ============================================================================
 # TEST 7: Operations Scripts
 # ============================================================================
+
 
 @test_category("7. Operations Scripts")
 def test_operations_scripts():
     """Test operations and monitoring scripts"""
-    
+
     scripts = {
-        'scripts/backup_manager.py': 'Backup & Recovery',
-        'scripts/database_monitor.py': 'Monitoring & Health Checks'
+        "scripts/backup_manager.py": "Backup & Recovery",
+        "scripts/database_monitor.py": "Monitoring & Health Checks",
     }
-    
+
     for script_path, description in scripts.items():
         if Path(script_path).exists():
             size_kb = Path(script_path).stat().st_size / 1024
-            log_result('passed', description, 'EXISTS', f'{size_kb:.1f} KB')
-        else:
-            log_result('failed', description, 'MISSING', 'Script not found')
+            log_result("passed", description, "EXISTS", f"{size_kb:.1f} KB")
+        else:
+            log_result("failed", description, "MISSING", "Script not found")
 
 
 # ============================================================================
 # TEST 8: Documentation
 # ============================================================================
+
 
 @test_category("8. Documentation")
 def test_documentation():
     """Test documentation completeness"""
-    
+
     docs = {
-        'README.md': 'User Guide',
-        'DATABASE_ARCHITECTURE.md': 'Architecture Docs',
-        'IMPLEMENTATION_SUMMARY.md': 'Implementation Summary',
-        'COMPLETE_IMPLEMENTATION_REPORT.md': 'Complete Report',
-        'docs/DISASTER_RECOVERY.md': 'Disaster Recovery Plan',
-        'docs/OPERATIONS_RUNBOOK.md': 'Operations Runbook',
-        'DEPLOYMENT_GUIDE.md': 'Deployment Guide'
+        "README.md": "User Guide",
+        "DATABASE_ARCHITECTURE.md": "Architecture Docs",
+        "IMPLEMENTATION_SUMMARY.md": "Implementation Summary",
+        "COMPLETE_IMPLEMENTATION_REPORT.md": "Complete Report",
+        "docs/DISASTER_RECOVERY.md": "Disaster Recovery Plan",
+        "docs/OPERATIONS_RUNBOOK.md": "Operations Runbook",
+        "DEPLOYMENT_GUIDE.md": "Deployment Guide",
     }
-    
+
     for doc_path, description in docs.items():
         if Path(doc_path).exists():
             size_kb = Path(doc_path).stat().st_size / 1024
-            log_result('passed', description, 'EXISTS', f'{size_kb:.1f} KB')
-        else:
-            log_result('warnings', description, 'MISSING', 'Document not found')
+            log_result("passed", description, "EXISTS", f"{size_kb:.1f} KB")
+        else:
+            log_result("warnings", description, "MISSING", "Document not found")
 
 
 # ============================================================================
 # MAIN TEST RUNNER
 # ============================================================================
+
 
 async def run_all_tests():
     """Run all tests"""
-    
-    print("\n" + "="*60)
+
+    print("\n" + "=" * 60)
     print("YMERA DATABASE SYSTEM V5 - COMPREHENSIVE E2E TESTING")
-    print("="*60)
+    print("=" * 60)
     print(f"Started: {datetime.utcnow().isoformat()}")
-    print("="*60)
-    
+    print("=" * 60)
+
     # Run synchronous tests
     test_environment()
     core_ok = test_core_module()
     test_file_structure()
     test_models()
     test_migration_system()
     test_operations_scripts()
     test_documentation()
-    
+
     # Run async tests
     if core_ok:
         await test_database_init()
     else:
-        log_result('skipped', 'Database Initialization', 'SKIPPED', 'Core module failed')
-    
+        log_result("skipped", "Database Initialization", "SKIPPED", "Core module failed")
+
     # Print summary
-    print("\n" + "="*60)
+    print("\n" + "=" * 60)
     print("TEST SUMMARY")
-    print("="*60)
-    
+    print("=" * 60)
+
     total_tests = sum(len(results) for results in test_results.values())
-    
+
     print(f"\n Passed:  {len(test_results['passed'])}")
     print(f" Failed:  {len(test_results['failed'])}")
     print(f"  Warnings: {len(test_results['warnings'])}")
     print(f"  Skipped: {len(test_results['skipped'])}")
     print(f"\nTotal Tests: {total_tests}")
-    
+
     # Calculate success rate
     if total_tests > 0:
-        success_rate = (len(test_results['passed']) / total_tests) * 100
+        success_rate = (len(test_results["passed"]) / total_tests) * 100
         print(f"Success Rate: {success_rate:.1f}%")
-    
+
     # Show critical failures
-    if test_results['failed']:
+    if test_results["failed"]:
         print("\n CRITICAL FAILURES:")
-        for failure in test_results['failed']:
+        for failure in test_results["failed"]:
             print(f"   {failure['test']}: {failure['message']}")
-    
+
     # Show warnings
-    if test_results['warnings']:
+    if test_results["warnings"]:
         print("\n  WARNINGS:")
-        for warning in test_results['warnings'][:5]:  # Show first 5
+        for warning in test_results["warnings"][:5]:  # Show first 5
             print(f"   {warning['test']}: {warning['message']}")
-        if len(test_results['warnings']) > 5:
+        if len(test_results["warnings"]) > 5:
             print(f"  ... and {len(test_results['warnings']) - 5} more")
-    
-    print("\n" + "="*60)
+
+    print("\n" + "=" * 60)
     print(f"Completed: {datetime.utcnow().isoformat()}")
-    print("="*60 + "\n")
-    
+    print("=" * 60 + "\n")
+
     return test_results
 
 
 if __name__ == "__main__":
     # Run tests
     results = asyncio.run(run_all_tests())
-    
+
     # Exit with appropriate code
-    if results['failed']:
+    if results["failed"]:
         sys.exit(1)
     else:
         sys.exit(0)
would reformat /home/runner/work/ymera_y/ymera_y/comprehensive_e2e_test.py
--- /home/runner/work/ymera_y/ymera_y/config.py	2025-10-19 22:47:02.797432+00:00
+++ /home/runner/work/ymera_y/ymera_y/config.py	2025-10-19 23:09:04.116198+00:00
@@ -1,58 +1,59 @@
-
 from pydantic_settings import BaseSettings, SettingsConfigDict
 from pydantic import validator
 from typing import List, Optional
 import os
 
+
 class Settings(BaseSettings):
     model_config = SettingsConfigDict(env_file=".env", case_sensitive=False, extra="ignore")
 
     # API Configuration
     api_host: str = "0.0.0.0"
     api_port: int = 8000
     debug: bool = False
-    
+
     # Database
     database_url: str
     redis_url: str = "redis://localhost:6379/0"
-    
+
     # Security
     jwt_secret_key: str
     jwt_algorithm: str = "HS256"
     jwt_expire_minutes: int = 30
-    
+
     # CORS
     cors_origins: List[str] = ["http://localhost:3000"]
     allowed_hosts: List[str] = ["*"]
-    
+
     # Rate Limiting
     rate_limit_requests: int = 100
     rate_limit_window: int = 60
-    
+
     # Agent Configuration
     max_conversation_history: int = 50
     learning_batch_size: int = 10
     model_update_interval: int = 3600
-    
+
     # Manager Agent Communication
     manager_agent_url: Optional[str] = None
-    
+
     @validator("database_url")
     def validate_database_url(cls, v):
         if not v:
             raise ValueError("DATABASE_URL is required")
         return v
-    
+
     @validator("jwt_secret_key")
     def validate_jwt_secret(cls, v):
         if not v:
             raise ValueError("JWT_SECRET_KEY is required")
         if len(v) < 32:
             raise ValueError("JWT_SECRET_KEY must be at least 32 characters")
         return v
 
+
 """
 Core Configuration Module
 Centralized configuration management using Pydantic Settings
 """
 
@@ -66,222 +67,203 @@
 class ProjectAgentSettings(BaseSettings):
     """
     Project Agent Configuration Settings
     All settings loaded from environment variables or .env file
     """
-    
+
     # ============================================================================
     # SERVER CONFIGURATION
     # ============================================================================
     host: str = Field(default="0.0.0.0", env="PROJECT_AGENT_HOST")
     port: int = Field(default=8001, env="PROJECT_AGENT_PORT")
     environment: str = Field(default="production", env="ENVIRONMENT")
     debug: bool = Field(default=False, env="DEBUG")
     log_level: str = Field(default="INFO", env="LOG_LEVEL")
     worker_count: int = Field(default=4, env="WORKER_COUNT")
-    
+
     # ============================================================================
     # DATABASE CONFIGURATION
     # ============================================================================
     database_url: str = Field(..., env="DATABASE_URL")
     database_pool_size: int = Field(default=20, env="DATABASE_POOL_SIZE")
     database_max_overflow: int = Field(default=10, env="DATABASE_MAX_OVERFLOW")
     database_echo: bool = Field(default=False, env="DATABASE_ECHO")
-    
+
     # ============================================================================
     # REDIS CONFIGURATION
     # ============================================================================
     redis_url: str = Field(default="redis://localhost:6379/0", env="REDIS_URL")
     redis_password: Optional[str] = Field(default=None, env="REDIS_PASSWORD")
     redis_max_connections: int = Field(default=50, env="REDIS_MAX_CONNECTIONS")
-    
+
     # ============================================================================
     # KAFKA CONFIGURATION
     # ============================================================================
     kafka_bootstrap_servers: List[str] = Field(
-        default=["localhost:9092"],
-        env="KAFKA_BOOTSTRAP_SERVERS"
+        default=["localhost:9092"], env="KAFKA_BOOTSTRAP_SERVERS"
     )
     kafka_topic_prefix: str = Field(default="project_agent", env="KAFKA_TOPIC_PREFIX")
-    
+
     # ============================================================================
     # SECURITY CONFIGURATION
     # ============================================================================
     jwt_secret_key: str = Field(..., env="JWT_SECRET_KEY")
     jwt_algorithm: str = Field(default="RS256", env="JWT_ALGORITHM")
     jwt_expire_minutes: int = Field(default=60, env="JWT_EXPIRE_MINUTES")
     jwt_public_key_path: Optional[str] = Field(default=None, env="JWT_PUBLIC_KEY_PATH")
     jwt_private_key_path: Optional[str] = Field(default=None, env="JWT_PRIVATE_KEY_PATH")
-    
-    cors_origins: List[str] = Field(
-        default=["http://localhost:3000"],
-        env="CORS_ORIGINS"
-    )
-    trusted_hosts: List[str] = Field(
-        default=["localhost", "127.0.0.1"],
-        env="TRUSTED_HOSTS"
-    )
-    
+
+    cors_origins: List[str] = Field(default=["http://localhost:3000"], env="CORS_ORIGINS")
+    trusted_hosts: List[str] = Field(default=["localhost", "127.0.0.1"], env="TRUSTED_HOSTS")
+
     rate_limit_enabled: bool = Field(default=True, env="RATE_LIMIT_ENABLED")
-    rate_limit_requests_per_minute: int = Field(
-        default=100,
-        env="RATE_LIMIT_REQUESTS_PER_MINUTE"
-    )
-    
+    rate_limit_requests_per_minute: int = Field(default=100, env="RATE_LIMIT_REQUESTS_PER_MINUTE")
+
     # ============================================================================
     # QUALITY VERIFICATION
     # ============================================================================
     quality_threshold: float = Field(default=85.0, env="QUALITY_THRESHOLD")
     code_coverage_min: float = Field(default=80.0, env="CODE_COVERAGE_MIN")
     security_scan_enabled: bool = Field(default=True, env="SECURITY_SCAN_ENABLED")
-    performance_benchmark_enabled: bool = Field(
-        default=True,
-        env="PERFORMANCE_BENCHMARK_ENABLED"
-    )
-    
+    performance_benchmark_enabled: bool = Field(default=True, env="PERFORMANCE_BENCHMARK_ENABLED")
+
     quality_code_weight: float = Field(default=0.35, env="QUALITY_CODE_WEIGHT")
     quality_security_weight: float = Field(default=0.30, env="QUALITY_SECURITY_WEIGHT")
     quality_performance_weight: float = Field(default=0.20, env="QUALITY_PERFORMANCE_WEIGHT")
-    quality_documentation_weight: float = Field(
-        default=0.15,
-        env="QUALITY_DOCUMENTATION_WEIGHT"
-    )
-    
+    quality_documentation_weight: float = Field(default=0.15, env="QUALITY_DOCUMENTATION_WEIGHT")
+
     # ============================================================================
     # FILE STORAGE
     # ============================================================================
     storage_backend: str = Field(default="local", env="STORAGE_BACKEND")
     storage_path: str = Field(default="./uploads", env="STORAGE_PATH")
     max_upload_size_mb: int = Field(default=100, env="MAX_UPLOAD_SIZE_MB")
     file_versioning_enabled: bool = Field(default=True, env="FILE_VERSIONING_ENABLED")
-    
+
     # AWS S3
     aws_access_key_id: Optional[str] = Field(default=None, env="AWS_ACCESS_KEY_ID")
     aws_secret_access_key: Optional[str] = Field(default=None, env="AWS_SECRET_ACCESS_KEY")
     aws_region: Optional[str] = Field(default="us-east-1", env="AWS_REGION")
     s3_bucket: Optional[str] = Field(default=None, env="S3_BUCKET")
-    
+
     # ============================================================================
     # AGENT REGISTRY
     # ============================================================================
-    manager_agent_url: str = Field(
-        default="http://manager-agent:8000",
-        env="MANAGER_AGENT_URL"
-    )
-    coding_agent_url: str = Field(
-        default="http://coding-agent:8010",
-        env="CODING_AGENT_URL"
-    )
+    manager_agent_url: str = Field(default="http://manager-agent:8000", env="MANAGER_AGENT_URL")
+    coding_agent_url: str = Field(default="http://coding-agent:8010", env="CODING_AGENT_URL")
     examination_agent_url: str = Field(
-        default="http://examination-agent:8030",
-        env="EXAMINATION_AGENT_URL"
+        default="http://examination-agent:8030", env="EXAMINATION_AGENT_URL"
     )
     enhancement_agent_url: str = Field(
-        default="http://enhancement-agent:8020",
-        env="ENHANCEMENT_AGENT_URL"
-    )
-    
+        default="http://enhancement-agent:8020", env="ENHANCEMENT_AGENT_URL"
+    )
+
     agent_request_timeout: int = Field(default=30, env="AGENT_REQUEST_TIMEOUT")
     agent_max_retries: int = Field(default=3, env="AGENT_MAX_RETRIES")
-    
+
     # ============================================================================
     # MONITORING
     # ============================================================================
     prometheus_enabled: bool = Field(default=True, env="PROMETHEUS_ENABLED")
     prometheus_port: int = Field(default=9090, env="PROMETHEUS_PORT")
-    
+
     jaeger_enabled: bool = Field(default=True, env="JAEGER_ENABLED")
     jaeger_agent_host: str = Field(default="localhost", env="JAEGER_AGENT_HOST")
     jaeger_agent_port: int = Field(default=6831, env="JAEGER_AGENT_PORT")
-    
+
     log_format: str = Field(default="json", env="LOG_FORMAT")
     log_file: str = Field(default="./logs/project_agent.log", env="LOG_FILE")
-    
+
     # ============================================================================
     # FEATURE FLAGS
     # ============================================================================
     enable_chat_interface: bool = Field(default=True, env="ENABLE_CHAT_INTERFACE")
     enable_file_versioning: bool = Field(default=True, env="ENABLE_FILE_VERSIONING")
     enable_auto_integration: bool = Field(default=True, env="ENABLE_AUTO_INTEGRATION")
     enable_rollback: bool = Field(default=True, env="ENABLE_ROLLBACK")
-    
+
     # ============================================================================
     # VALIDATORS
     # ============================================================================
-    
-    @validator('jwt_secret_key')
+
+    @validator("jwt_secret_key")
     def validate_jwt_secret(cls, v):
         """Ensure JWT secret is strong enough"""
         if len(v) < 32:
-            raise ValueError('JWT_SECRET_KEY must be at least 32 characters')
+            raise ValueError("JWT_SECRET_KEY must be at least 32 characters")
         if v == "CHANGE_ME_TO_A_SECURE_256_BIT_SECRET_KEY_MINIMUM_32_CHARS":
-            raise ValueError('JWT_SECRET_KEY must be changed from default value!')
-        return v
-    
-    @validator('quality_threshold')
+            raise ValueError("JWT_SECRET_KEY must be changed from default value!")
+        return v
+
+    @validator("quality_threshold")
     def validate_quality_threshold(cls, v):
         """Ensure quality threshold is valid"""
         if not 0 <= v <= 100:
-            raise ValueError('QUALITY_THRESHOLD must be between 0 and 100')
-        return v
-    
-    @validator('quality_code_weight', 'quality_security_weight', 
-               'quality_performance_weight', 'quality_documentation_weight')
+            raise ValueError("QUALITY_THRESHOLD must be between 0 and 100")
+        return v
+
+    @validator(
+        "quality_code_weight",
+        "quality_security_weight",
+        "quality_performance_weight",
+        "quality_documentation_weight",
+    )
     def validate_quality_weights(cls, v):
         """Ensure quality weights are valid"""
         if not 0 <= v <= 1:
-            raise ValueError('Quality weights must be between 0 and 1')
-        return v
-    
-    @validator('cors_origins', pre=True)
+            raise ValueError("Quality weights must be between 0 and 1")
+        return v
+
+    @validator("cors_origins", pre=True)
     def parse_cors_origins(cls, v):
         """Parse CORS origins from string or list"""
         if isinstance(v, str):
             import json
+
             return json.loads(v)
         return v
-    
-    @validator('kafka_bootstrap_servers', pre=True)
+
+    @validator("kafka_bootstrap_servers", pre=True)
     def parse_kafka_servers(cls, v):
         """Parse Kafka servers from string or list"""
         if isinstance(v, str):
-            return [s.strip() for s in v.split(',')]
-        return v
-    
-    @validator('storage_path')
+            return [s.strip() for s in v.split(",")]
+        return v
+
+    @validator("storage_path")
     def ensure_storage_path_exists(cls, v):
         """Create storage path if it doesn't exist"""
         path = Path(v)
         path.mkdir(parents=True, exist_ok=True)
         return str(path.absolute())
-    
-    @validator('log_file')
+
+    @validator("log_file")
     def ensure_log_directory_exists(cls, v):
         """Create log directory if it doesn't exist"""
         path = Path(v).parent
         path.mkdir(parents=True, exist_ok=True)
         return v
-    
+
     class Config:
         env_file = ".env"
         case_sensitive = False
         env_file_encoding = "utf-8"
-    
+
     def get_agent_urls(self) -> dict:
         """Get all configured agent URLs"""
         return {
             "manager": self.manager_agent_url,
             "coding": self.coding_agent_url,
             "examination": self.examination_agent_url,
             "enhancement": self.enhancement_agent_url,
         }
-    
+
     @property
     def is_production(self) -> bool:
         """Check if running in production environment"""
         return self.environment.lower() == "production"
-    
+
     @property
     def is_development(self) -> bool:
         """Check if running in development environment"""
         return self.environment.lower() == "development"
 
would reformat /home/runner/work/ymera_y/ymera_y/config.py
--- /home/runner/work/ymera_y/ymera_y/complete_file_routes.py	2025-10-19 22:47:02.797432+00:00
+++ /home/runner/work/ymera_y/ymera_y/complete_file_routes.py	2025-10-19 23:09:04.254804+00:00
@@ -37,22 +37,26 @@
 except ImportError:
     try:
         from config.settings import get_settings
     except ImportError:
         import os
+
         class Settings:
             FILE_STORAGE_PATH = os.getenv("FILE_STORAGE_PATH", "/tmp/ymera_files")
             TEMP_STORAGE_PATH = os.getenv("TEMP_STORAGE_PATH", "/tmp/ymera_temp")
             MAX_FILE_SIZE = int(os.getenv("MAX_FILE_SIZE", "104857600"))
             MAX_FILES_PER_USER = int(os.getenv("MAX_FILES_PER_USER", "1000"))
             VIRUS_SCANNING_ENABLED = os.getenv("VIRUS_SCANNING_ENABLED", "False").lower() == "true"
-            FILE_ENCRYPTION_ENABLED = os.getenv("FILE_ENCRYPTION_ENABLED", "False").lower() == "true"
+            FILE_ENCRYPTION_ENABLED = (
+                os.getenv("FILE_ENCRYPTION_ENABLED", "False").lower() == "true"
+            )
             CLOUD_STORAGE_ENABLED = os.getenv("CLOUD_STORAGE_ENABLED", "False").lower() == "true"
             REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")
-        
+
         def get_settings():
             return Settings()
+
 
 try:
     from app.API_GATEWAY_CORE_ROUTES.database import get_db_session
 except ImportError:
     from .database import get_db_session
@@ -71,24 +75,61 @@
 MAX_FILES_PER_USER = 1000
 CHUNK_SIZE = 8192
 UPLOAD_TIMEOUT = 300
 
 ALLOWED_EXTENSIONS = {
-    'txt', 'pdf', 'doc', 'docx', 'xls', 'xlsx', 'ppt', 'pptx',
-    'jpg', 'jpeg', 'png', 'gif', 'svg', 'bmp',
-    'mp4', 'avi', 'mov', 'wmv', 'flv',
-    'mp3', 'wav', 'flac', 'aac',
-    'zip', 'rar', '7z', 'tar', 'gz',
-    'json', 'xml', 'csv', 'md', 'html', 'css', 'js',
-    'py', 'java', 'cpp', 'c', 'h', 'cs', 'php', 'rb'
+    "txt",
+    "pdf",
+    "doc",
+    "docx",
+    "xls",
+    "xlsx",
+    "ppt",
+    "pptx",
+    "jpg",
+    "jpeg",
+    "png",
+    "gif",
+    "svg",
+    "bmp",
+    "mp4",
+    "avi",
+    "mov",
+    "wmv",
+    "flv",
+    "mp3",
+    "wav",
+    "flac",
+    "aac",
+    "zip",
+    "rar",
+    "7z",
+    "tar",
+    "gz",
+    "json",
+    "xml",
+    "csv",
+    "md",
+    "html",
+    "css",
+    "js",
+    "py",
+    "java",
+    "cpp",
+    "c",
+    "h",
+    "cs",
+    "php",
+    "rb",
 }
 
 settings = get_settings()
 
 # ===============================================================================
 # ENUMS
 # ===============================================================================
+
 
 class FileType(str, Enum):
     TEXT = "text"
     DOCUMENT = "document"
     IMAGE = "image"
@@ -96,43 +137,50 @@
     AUDIO = "audio"
     ARCHIVE = "archive"
     CODE = "code"
     OTHER = "other"
 
+
 class FileStatus(str, Enum):
     UPLOADING = "uploading"
     PROCESSING = "processing"
     READY = "ready"
     FAILED = "failed"
     DELETED = "deleted"
 
+
 class ProcessingType(str, Enum):
     NONE = "none"
     TEXT_EXTRACTION = "text_extraction"
     IMAGE_ANALYSIS = "image_analysis"
     DOCUMENT_PARSING = "document_parsing"
     VIRUS_SCAN = "virus_scan"
 
+
 # ===============================================================================
 # DATA MODELS
 # ===============================================================================
+
 
 class FileConfig:
     """Configuration for file management"""
+
     def __init__(self):
-        self.max_file_size = getattr(settings, 'MAX_FILE_SIZE', MAX_FILE_SIZE)
-        self.max_files_per_user = getattr(settings, 'MAX_FILES_PER_USER', MAX_FILES_PER_USER)
-        self.virus_scanning_enabled = getattr(settings, 'VIRUS_SCANNING_ENABLED', False)
-        self.encryption_enabled = getattr(settings, 'FILE_ENCRYPTION_ENABLED', False)
-        self.cloud_storage_enabled = getattr(settings, 'CLOUD_STORAGE_ENABLED', False)
+        self.max_file_size = getattr(settings, "MAX_FILE_SIZE", MAX_FILE_SIZE)
+        self.max_files_per_user = getattr(settings, "MAX_FILES_PER_USER", MAX_FILES_PER_USER)
+        self.virus_scanning_enabled = getattr(settings, "VIRUS_SCANNING_ENABLED", False)
+        self.encryption_enabled = getattr(settings, "FILE_ENCRYPTION_ENABLED", False)
+        self.cloud_storage_enabled = getattr(settings, "CLOUD_STORAGE_ENABLED", False)
+
 
 class FileUploadRequest(BaseModel):
     description: Optional[str] = Field(default=None, max_length=500)
     tags: List[str] = Field(default_factory=list)
     processing_type: ProcessingType = Field(default=ProcessingType.NONE)
     is_private: bool = Field(default=True)
     expires_at: Optional[datetime] = None
+
 
 class FileResponse(BaseModel):
     id: str
     filename: str
     original_filename: str
@@ -148,128 +196,133 @@
     download_count: int
     created_at: datetime
     updated_at: Optional[datetime]
     expires_at: Optional[datetime]
 
+
 class FileSearchRequest(BaseModel):
     query: Optional[str] = None
     file_type: Optional[FileType] = None
     tags: List[str] = Field(default_factory=list)
     status: Optional[FileStatus] = None
     date_from: Optional[datetime] = None
     date_to: Optional[datetime] = None
     limit: int = Field(default=50, ge=1, le=100)
     offset: int = Field(default=0, ge=0)
 
+
 class BulkOperation(BaseModel):
     file_ids: List[str] = Field(..., min_items=1, max_items=100)
     operation: str
     parameters: Dict[str, Any] = Field(default_factory=dict)
 
+
 # ===============================================================================
 # MOCK MODELS
 # ===============================================================================
+
 
 class FileRecord:
     """Mock file record model"""
+
     def __init__(self, **kwargs):
-        self.id = kwargs.get('id', str(uuid.uuid4()))
-        self.filename = kwargs.get('filename')
-        self.original_filename = kwargs.get('original_filename')
-        self.file_type = kwargs.get('file_type', FileType.OTHER)
-        self.file_size = kwargs.get('file_size', 0)
-        self.mime_type = kwargs.get('mime_type', 'application/octet-stream')
-        self.file_hash = kwargs.get('file_hash')
-        self.description = kwargs.get('description')
-        self.tags = kwargs.get('tags', [])
-        self.is_private = kwargs.get('is_private', True)
-        self.expires_at = kwargs.get('expires_at')
-        self.user_id = kwargs.get('user_id')
-        self.status = kwargs.get('status', FileStatus.READY)
-        self.processing_status = kwargs.get('processing_status')
-        self.processing_result = kwargs.get('processing_result')
-        self.download_count = kwargs.get('download_count', 0)
-        self.created_at = kwargs.get('created_at', datetime.utcnow())
-        self.updated_at = kwargs.get('updated_at')
-        self.cloud_url = kwargs.get('cloud_url')
-        self.last_accessed = kwargs.get('last_accessed')
+        self.id = kwargs.get("id", str(uuid.uuid4()))
+        self.filename = kwargs.get("filename")
+        self.original_filename = kwargs.get("original_filename")
+        self.file_type = kwargs.get("file_type", FileType.OTHER)
+        self.file_size = kwargs.get("file_size", 0)
+        self.mime_type = kwargs.get("mime_type", "application/octet-stream")
+        self.file_hash = kwargs.get("file_hash")
+        self.description = kwargs.get("description")
+        self.tags = kwargs.get("tags", [])
+        self.is_private = kwargs.get("is_private", True)
+        self.expires_at = kwargs.get("expires_at")
+        self.user_id = kwargs.get("user_id")
+        self.status = kwargs.get("status", FileStatus.READY)
+        self.processing_status = kwargs.get("processing_status")
+        self.processing_result = kwargs.get("processing_result")
+        self.download_count = kwargs.get("download_count", 0)
+        self.created_at = kwargs.get("created_at", datetime.utcnow())
+        self.updated_at = kwargs.get("updated_at")
+        self.cloud_url = kwargs.get("cloud_url")
+        self.last_accessed = kwargs.get("last_accessed")
+
 
 class User:
     """Mock user model"""
+
     def __init__(self, **kwargs):
-        self.id = kwargs.get('id', str(uuid.uuid4()))
-        self.email = kwargs.get('email', 'user@example.com')
+        self.id = kwargs.get("id", str(uuid.uuid4()))
+        self.email = kwargs.get("email", "user@example.com")
+
 
 # ===============================================================================
 # FILE MANAGER CLASS
 # ===============================================================================
+
 
 class FileManager:
     """Production-ready file management system"""
-    
+
     def __init__(self, config: FileConfig):
         self.config = config
         self.logger = logger.bind(component="file_manager")
-        self._storage_path = Path(getattr(settings, 'FILE_STORAGE_PATH', '/tmp/ymera_files'))
-        self._temp_path = Path(getattr(settings, 'TEMP_STORAGE_PATH', '/tmp/ymera_temp'))
-        
+        self._storage_path = Path(getattr(settings, "FILE_STORAGE_PATH", "/tmp/ymera_files"))
+        self._temp_path = Path(getattr(settings, "TEMP_STORAGE_PATH", "/tmp/ymera_temp"))
+
         # Create directories
         self._storage_path.mkdir(parents=True, exist_ok=True)
         self._temp_path.mkdir(parents=True, exist_ok=True)
-        
+
         # In-memory storage for demo (replace with database in production)
         self._file_records: Dict[str, FileRecord] = {}
-    
+
     async def initialize(self):
         """Initialize file manager resources"""
         try:
             self.logger.info("File manager initialized", storage_path=str(self._storage_path))
         except Exception as e:
             self.logger.error("Failed to initialize file manager", error=str(e))
             raise
-    
+
     async def upload_file(
-        self,
-        file: UploadFile,
-        metadata: FileUploadRequest,
-        user_id: str,
-        db: AsyncSession
+        self, file: UploadFile, metadata: FileUploadRequest, user_id: str, db: AsyncSession
     ) -> FileResponse:
         """Upload and process file with comprehensive validation"""
         try:
             # Validate file
             await self._validate_file(file, user_id, db)
-            
+
             # Generate file info
             file_id = str(uuid.uuid4())
             file_extension = Path(file.filename).suffix.lower()
             safe_filename = f"{file_id}{file_extension}"
             temp_path = self._temp_path / safe_filename
-            
+
             # Save file temporarily
             await self._save_file_temporarily(file, temp_path)
-            
+
             # Get file information
             file_size = temp_path.stat().st_size
             # Enforce configured max file size
             if file_size > self.config.max_file_size:
                 # Remove temp file to avoid leaving large files on disk
                 try:
                     temp_path.unlink()
                 except Exception:
                     pass
                 raise HTTPException(status_code=413, detail="File too large")
-            mime_type = mimetypes.guess_type(file.filename)[0] or 'application/octet-stream'
+            mime_type = mimetypes.guess_type(file.filename)[0] or "application/octet-stream"
             file_hash = await self._calculate_file_hash(temp_path)
-            
+
             # Determine file type
-            file_type = self._determine_file_type(file_extension.lstrip('.'), mime_type)
-            
+            file_type = self._determine_file_type(file_extension.lstrip("."), mime_type)
+
             # Move to permanent storage
             permanent_path = self._storage_path / safe_filename
             temp_path.rename(permanent_path)
-            
+
             # Create file record
             file_record = FileRecord(
                 id=file_id,
                 filename=safe_filename,
                 original_filename=file.filename,
@@ -281,18 +334,18 @@
                 tags=metadata.tags,
                 is_private=metadata.is_private,
                 expires_at=metadata.expires_at,
                 user_id=user_id,
                 status=FileStatus.READY,
-                created_at=datetime.utcnow()
+                created_at=datetime.utcnow(),
             )
-            
+
             # Store record (in production, save to database)
             self._file_records[file_id] = file_record
-            
+
             self.logger.info("File uploaded", file_id=file_id, user_id=user_id)
-            
+
             return FileResponse(
                 id=file_record.id,
                 filename=file_record.filename,
                 original_filename=file_record.original_filename,
                 file_type=file_record.file_type.value,
@@ -305,426 +358,411 @@
                 processing_status=file_record.processing_status,
                 processing_result=file_record.processing_result,
                 download_count=file_record.download_count,
                 created_at=file_record.created_at,
                 updated_at=file_record.updated_at,
-                expires_at=file_record.expires_at
+                expires_at=file_record.expires_at,
             )
-            
+
         except HTTPException:
             raise
         except Exception as e:
             self.logger.error("File upload failed", error=str(e), user_id=user_id)
             raise HTTPException(status_code=500, detail="File upload failed")
-    
-    async def download_file(
-        self,
-        file_id: str,
-        user_id: str,
-        db: AsyncSession
-    ) -> Path:
+
+    async def download_file(self, file_id: str, user_id: str, db: AsyncSession) -> Path:
         """Download file with access control"""
         try:
             # Get file record
             file_record = self._file_records.get(file_id)
-            
+
             if not file_record:
                 raise HTTPException(status_code=404, detail="File not found")
-            
+
             # Check access permissions
             if file_record.is_private and file_record.user_id != user_id:
                 raise HTTPException(status_code=403, detail="Access denied")
-            
+
             # Check expiration
             if file_record.expires_at and datetime.utcnow() > file_record.expires_at:
                 raise HTTPException(status_code=410, detail="File has expired")
-            
+
             # Get file path
             file_path = self._storage_path / file_record.filename
-            
+
             if not file_path.exists():
                 raise HTTPException(status_code=404, detail="File not found on disk")
-            
+
             # Update download count
             file_record.download_count += 1
             file_record.last_accessed = datetime.utcnow()
-            
+
             self.logger.info("File download", file_id=file_id, user_id=user_id)
-            
+
             return file_path
-            
+
         except HTTPException:
             raise
         except Exception as e:
             self.logger.error("File download failed", error=str(e), file_id=file_id)
             raise HTTPException(status_code=500, detail="File download failed")
-    
+
     async def search_files(
-        self,
-        search_params: FileSearchRequest,
-        user_id: str,
-        db: AsyncSession
+        self, search_params: FileSearchRequest, user_id: str, db: AsyncSession
     ) -> Dict[str, Any]:
         """
         Search files with advanced filtering - COMPLETE IMPLEMENTATION
         """
         try:
             # Get user's files
             all_files = [
-                f for f in self._file_records.values()
+                f
+                for f in self._file_records.values()
                 if f.user_id == user_id and f.status != FileStatus.DELETED
             ]
-            
+
             # Apply filters
             filtered_files = all_files
-            
+
             # Query filter - search in filename and description
             if search_params.query:
                 query_lower = search_params.query.lower()
                 filtered_files = [
-                    f for f in filtered_files
-                    if (query_lower in f.original_filename.lower()) or
-                       (f.description and query_lower in f.description.lower())
+                    f
+                    for f in filtered_files
+                    if (query_lower in f.original_filename.lower())
+                    or (f.description and query_lower in f.description.lower())
                 ]
-            
+
             # File type filter
             if search_params.file_type:
                 filtered_files = [
-                    f for f in filtered_files
-                    if f.file_type == search_params.file_type
+                    f for f in filtered_files if f.file_type == search_params.file_type
                 ]
-            
+
             # Status filter
             if search_params.status:
-                filtered_files = [
-                    f for f in filtered_files
-                    if f.status == search_params.status
-                ]
-            
+                filtered_files = [f for f in filtered_files if f.status == search_params.status]
+
             # Tags filter - file must have all specified tags
             if search_params.tags:
                 filtered_files = [
-                    f for f in filtered_files
-                    if all(tag in f.tags for tag in search_params.tags)
+                    f for f in filtered_files if all(tag in f.tags for tag in search_params.tags)
                 ]
-            
+
             # Date range filters
             if search_params.date_from:
                 filtered_files = [
-                    f for f in filtered_files
-                    if f.created_at >= search_params.date_from
+                    f for f in filtered_files if f.created_at >= search_params.date_from
                 ]
-            
+
             if search_params.date_to:
                 filtered_files = [
-                    f for f in filtered_files
-                    if f.created_at <= search_params.date_to
+                    f for f in filtered_files if f.created_at <= search_params.date_to
                 ]
-            
+
             # Sort by creation date (newest first)
             filtered_files.sort(key=lambda f: f.created_at, reverse=True)
-            
+
             # Get total count
             total_files = len(filtered_files)
-            
+
             # Apply pagination
             start_idx = search_params.offset
             end_idx = start_idx + search_params.limit
             paginated_files = filtered_files[start_idx:end_idx]
-            
+
             # Convert to response format
             file_responses = []
             for file_record in paginated_files:
-                file_responses.append(FileResponse(
-                    id=file_record.id,
-                    filename=file_record.filename,
-                    original_filename=file_record.original_filename,
-                    file_type=file_record.file_type.value,
-                    file_size=file_record.file_size,
-                    mime_type=file_record.mime_type,
-                    description=file_record.description,
-                    tags=file_record.tags,
-                    status=file_record.status.value,
-                    is_private=file_record.is_private,
-                    processing_status=file_record.processing_status,
-                    processing_result=file_record.processing_result,
-                    download_count=file_record.download_count,
-                    created_at=file_record.created_at,
-                    updated_at=file_record.updated_at,
-                    expires_at=file_record.expires_at
-                ))
-            
+                file_responses.append(
+                    FileResponse(
+                        id=file_record.id,
+                        filename=file_record.filename,
+                        original_filename=file_record.original_filename,
+                        file_type=file_record.file_type.value,
+                        file_size=file_record.file_size,
+                        mime_type=file_record.mime_type,
+                        description=file_record.description,
+                        tags=file_record.tags,
+                        status=file_record.status.value,
+                        is_private=file_record.is_private,
+                        processing_status=file_record.processing_status,
+                        processing_result=file_record.processing_result,
+                        download_count=file_record.download_count,
+                        created_at=file_record.created_at,
+                        updated_at=file_record.updated_at,
+                        expires_at=file_record.expires_at,
+                    )
+                )
+
             return {
                 "files": file_responses,
                 "pagination": {
                     "total": total_files,
                     "limit": search_params.limit,
                     "offset": search_params.offset,
-                    "has_more": (search_params.offset + len(file_responses)) < total_files
-                }
+                    "has_more": (search_params.offset + len(file_responses)) < total_files,
+                },
             }
-            
+
         except Exception as e:
             self.logger.error("File search failed", error=str(e), user_id=user_id)
             raise HTTPException(status_code=500, detail="File search failed")
-    
-    async def delete_file(
-        self,
-        file_id: str,
-        user_id: str,
-        db: AsyncSession
-    ) -> Dict[str, Any]:
+
+    async def delete_file(self, file_id: str, user_id: str, db: AsyncSession) -> Dict[str, Any]:
         """Delete file with cleanup"""
         try:
             file_record = self._file_records.get(file_id)
-            
+
             if not file_record:
                 raise HTTPException(status_code=404, detail="File not found")
-            
+
             if file_record.user_id != user_id:
                 raise HTTPException(status_code=403, detail="Access denied")
-            
+
             # Mark as deleted
             file_record.status = FileStatus.DELETED
             file_record.updated_at = datetime.utcnow()
-            
+
             # Delete physical file
             file_path = self._storage_path / file_record.filename
             if file_path.exists():
                 file_path.unlink()
-            
+
             self.logger.info("File deleted", file_id=file_id, user_id=user_id)
-            
+
             return {
                 "message": "File deleted successfully",
                 "file_id": file_id,
-                "timestamp": datetime.utcnow().isoformat()
+                "timestamp": datetime.utcnow().isoformat(),
             }
-            
+
         except HTTPException:
             raise
         except Exception as e:
             self.logger.error("File deletion failed", error=str(e), file_id=file_id)
             raise HTTPException(status_code=500, detail="File deletion failed")
-    
+
     async def process_bulk_operation(
-        self,
-        operation_data: BulkOperation,
-        user_id: str,
-        db: AsyncSession
+        self, operation_data: BulkOperation, user_id: str, db: AsyncSession
     ) -> Dict[str, Any]:
         """Process bulk file operations"""
         try:
             results = {
                 "operation": operation_data.operation,
                 "total_files": len(operation_data.file_ids),
                 "successful": 0,
                 "failed": 0,
-                "errors": []
+                "errors": [],
             }
-            
+
             for file_id in operation_data.file_ids:
                 try:
                     file_record = self._file_records.get(file_id)
-                    
+
                     if not file_record or file_record.user_id != user_id:
                         results["failed"] += 1
-                        results["errors"].append({
-                            "file_id": file_id,
-                            "error": "File not found or access denied"
-                        })
+                        results["errors"].append(
+                            {"file_id": file_id, "error": "File not found or access denied"}
+                        )
                         continue
-                    
+
                     if operation_data.operation == "delete":
                         file_record.status = FileStatus.DELETED
                         file_record.updated_at = datetime.utcnow()
-                    
+
                     elif operation_data.operation == "update_tags":
                         new_tags = operation_data.parameters.get("tags", [])
                         file_record.tags = new_tags
                         file_record.updated_at = datetime.utcnow()
-                    
+
                     elif operation_data.operation == "update_privacy":
                         is_private = operation_data.parameters.get("is_private", True)
                         file_record.is_private = is_private
                         file_record.updated_at = datetime.utcnow()
-                    
+
                     results["successful"] += 1
-                    
+
                 except Exception as e:
                     results["failed"] += 1
-                    results["errors"].append({
-                        "file_id": file_id,
-                        "error": str(e)
-                    })
-            
+                    results["errors"].append({"file_id": file_id, "error": str(e)})
+
             self.logger.info(
                 "Bulk operation completed",
                 operation=operation_data.operation,
                 successful=results["successful"],
-                failed=results["failed"]
+                failed=results["failed"],
             )
-            
+
             return results
-            
+
         except Exception as e:
             self.logger.error("Bulk operation failed", error=str(e))
             raise HTTPException(status_code=500, detail="Bulk operation failed")
-    
+
     # Helper methods
-    
+
     async def _validate_file(self, file: UploadFile, user_id: str, db: AsyncSession):
         """Validate uploaded file"""
         # Check file extension
-        file_extension = Path(file.filename).suffix.lower().lstrip('.')
+        file_extension = Path(file.filename).suffix.lower().lstrip(".")
         if file_extension not in ALLOWED_EXTENSIONS:
             raise HTTPException(
-                status_code=415,
-                detail=f"File type '{file_extension}' is not allowed"
+                status_code=415, detail=f"File type '{file_extension}' is not allowed"
             )
-        
+
         # Check user file limit
-        user_files = [f for f in self._file_records.values() 
-                     if f.user_id == user_id and f.status != FileStatus.DELETED]
-        
+        user_files = [
+            f
+            for f in self._file_records.values()
+            if f.user_id == user_id and f.status != FileStatus.DELETED
+        ]
+
         if len(user_files) >= self.config.max_files_per_user:
             raise HTTPException(
                 status_code=400,
-                detail=f"Maximum {self.config.max_files_per_user} files per user exceeded"
+                detail=f"Maximum {self.config.max_files_per_user} files per user exceeded",
             )
-    
+
     async def _save_file_temporarily(self, file: UploadFile, temp_path: Path):
         """Save uploaded file to temporary location"""
-        async with aiofiles.open(temp_path, 'wb') as f:
+        async with aiofiles.open(temp_path, "wb") as f:
             while chunk := await file.read(CHUNK_SIZE):
                 await f.write(chunk)
-    
+
     async def _calculate_file_hash(self, file_path: Path) -> str:
         """Calculate SHA-256 hash of file"""
         hash_sha256 = hashlib.sha256()
-        async with aiofiles.open(file_path, 'rb') as f:
+        async with aiofiles.open(file_path, "rb") as f:
             while chunk := await f.read(CHUNK_SIZE):
                 hash_sha256.update(chunk)
         return hash_sha256.hexdigest()
-    
+
     def _determine_file_type(self, extension: str, mime_type: str) -> FileType:
         """Determine file type based on extension"""
-        if extension in {'txt', 'md', 'json', 'xml', 'csv', 'html', 'css', 'js'}:
+        if extension in {"txt", "md", "json", "xml", "csv", "html", "css", "js"}:
             return FileType.TEXT
-        elif extension in {'pdf', 'doc', 'docx', 'ppt', 'pptx', 'xls', 'xlsx'}:
+        elif extension in {"pdf", "doc", "docx", "ppt", "pptx", "xls", "xlsx"}:
             return FileType.DOCUMENT
-        elif extension in {'jpg', 'jpeg', 'png', 'gif', 'svg', 'bmp'}:
+        elif extension in {"jpg", "jpeg", "png", "gif", "svg", "bmp"}:
             return FileType.IMAGE
-        elif extension in {'mp4', 'avi', 'mov', 'wmv', 'flv'}:
+        elif extension in {"mp4", "avi", "mov", "wmv", "flv"}:
             return FileType.VIDEO
-        elif extension in {'mp3', 'wav', 'flac', 'aac'}:
+        elif extension in {"mp3", "wav", "flac", "aac"}:
             return FileType.AUDIO
-        elif extension in {'zip', 'rar', '7z', 'tar', 'gz'}:
+        elif extension in {"zip", "rar", "7z", "tar", "gz"}:
             return FileType.ARCHIVE
-        elif extension in {'py', 'java', 'cpp', 'c', 'h', 'cs', 'php', 'rb'}:
+        elif extension in {"py", "java", "cpp", "c", "h", "cs", "php", "rb"}:
             return FileType.CODE
         else:
             return FileType.OTHER
 
+
 # ===============================================================================
 # ROUTER SETUP
 # ===============================================================================
 
 router = APIRouter(prefix="/api/v1/files", tags=["files"])
 
 file_config = FileConfig()
 file_manager = FileManager(file_config)
+
 
 # Mock get_current_user
 async def get_current_user():
     """Mock current user (replace with actual auth)"""
     return User(id=str(uuid.uuid4()), email="user@example.com")
 
+
 # ===============================================================================
 # ROUTES
 # ===============================================================================
+
 
 @router.on_event("startup")
 async def startup_event():
     """Initialize file manager on startup"""
     await file_manager.initialize()
+
 
 @router.post("/upload", response_model=FileResponse)
 async def upload_file(
     file: UploadFile = File(...),
     description: Optional[str] = Form(None),
     tags: str = Form("[]"),
     processing_type: ProcessingType = Form(ProcessingType.NONE),
     is_private: bool = Form(True),
     expires_at: Optional[str] = Form(None),
     current_user: User = Depends(get_current_user),
-    db: AsyncSession = Depends(get_db_session)
+    db: AsyncSession = Depends(get_db_session),
 ) -> FileResponse:
     """Upload a file with metadata"""
     try:
         import json
+
         parsed_tags = json.loads(tags) if tags else []
         parsed_expires_at = None
         if expires_at:
-            parsed_expires_at = datetime.fromisoformat(expires_at.replace('Z', '+00:00'))
-        
+            parsed_expires_at = datetime.fromisoformat(expires_at.replace("Z", "+00:00"))
+
         metadata = FileUploadRequest(
             description=description,
             tags=parsed_tags,
             processing_type=processing_type,
             is_private=is_private,
-            expires_at=parsed_expires_at
+            expires_at=parsed_expires_at,
         )
-        
+
         return await file_manager.upload_file(file, metadata, current_user.id, db)
-        
+
     except Exception as e:
         logger.error("Upload endpoint failed", error=str(e))
         raise HTTPException(status_code=400, detail=str(e))
+
 
 @router.get("/", response_model=Dict[str, Any])
 async def list_files(
     limit: int = Query(default=50, ge=1, le=100),
     offset: int = Query(default=0, ge=0),
     file_type: Optional[FileType] = None,
     status: Optional[FileStatus] = None,
     current_user: User = Depends(get_current_user),
-    db: AsyncSession = Depends(get_db_session)
+    db: AsyncSession = Depends(get_db_session),
 ) -> Dict[str, Any]:
     """List user files with pagination"""
     search_params = FileSearchRequest(
-        file_type=file_type,
-        status=status,
-        limit=limit,
-        offset=offset
+        file_type=file_type, status=status, limit=limit, offset=offset
     )
     return await file_manager.search_files(search_params, current_user.id, db)
+
 
 @router.post("/search", response_model=Dict[str, Any])
 async def search_files(
     search_params: FileSearchRequest,
     current_user: User = Depends(get_current_user),
-    db: AsyncSession = Depends(get_db_session)
+    db: AsyncSession = Depends(get_db_session),
 ) -> Dict[str, Any]:
     """Advanced file search"""
     return await file_manager.search_files(search_params, current_user.id, db)
+
 
 @router.get("/{file_id}", response_model=FileResponse)
 async def get_file_info(
     file_id: str,
     current_user: User = Depends(get_current_user),
-    db: AsyncSession = Depends(get_db_session)
+    db: AsyncSession = Depends(get_db_session),
 ) -> FileResponse:
     """Get file information"""
     file_record = file_manager._file_records.get(file_id)
-    
+
     if not file_record:
         raise HTTPException(status_code=404, detail="File not found")
-    
+
     if file_record.is_private and file_record.user_id != current_user.id:
         raise HTTPException(status_code=403, detail="Access denied")
-    
+
     return FileResponse(
         id=file_record.id,
         filename=file_record.filename,
         original_filename=file_record.original_filename,
         file_type=file_record.file_type.value,
@@ -737,53 +775,54 @@
         processing_status=file_record.processing_status,
         processing_result=file_record.processing_result,
         download_count=file_record.download_count,
         created_at=file_record.created_at,
         updated_at=file_record.updated_at,
-        expires_at=file_record.expires_at
+        expires_at=file_record.expires_at,
     )
+
 
 @router.get("/{file_id}/download")
 async def download_file(
     file_id: str,
     current_user: User = Depends(get_current_user),
-    db: AsyncSession = Depends(get_db_session)
+    db: AsyncSession = Depends(get_db_session),
 ):
     """Download a file"""
     try:
         file_path = await file_manager.download_file(file_id, current_user.id, db)
         file_record = file_manager._file_records.get(file_id)
-        
+
         return FileResponse(
             path=str(file_path),
             filename=file_record.original_filename,
-            media_type=file_record.mime_type
+            media_type=file_record.mime_type,
         )
     except Exception as e:
         logger.error("Download failed", error=str(e))
         raise
 
+
 @router.delete("/{file_id}")
 async def delete_file(
     file_id: str,
     current_user: User = Depends(get_current_user),
-    db: AsyncSession = Depends(get_db_session)
+    db: AsyncSession = Depends(get_db_session),
 ) -> Dict[str, Any]:
     """Delete a file"""
     return await file_manager.delete_file(file_id, current_user.id, db)
+
 
 @router.post("/bulk", response_model=Dict[str, Any])
 async def bulk_file_operation(
     operation_data: BulkOperation,
     current_user: User = Depends(get_current_user),
-    db: AsyncSession = Depends(get_db_session)
+    db: AsyncSession = Depends(get_db_session),
 ) -> Dict[str, Any]:
     """Perform bulk operations on files"""
     return await file_manager.process_bulk_operation(operation_data, current_user.id, db)
 
+
 @router.get("/health")
 async def files_health_check() -> Dict[str, Any]:
     """File system health check"""
-    return {
-        "status": "ok",
-        "storage_path": str(file_manager._storage_path)
-    }
\ No newline at end of file
+    return {"status": "ok", "storage_path": str(file_manager._storage_path)}
would reformat /home/runner/work/ymera_y/ymera_y/complete_file_routes.py
--- /home/runner/work/ymera_y/ymera_y/config_manager.py	2025-10-19 22:47:02.798432+00:00
+++ /home/runner/work/ymera_y/ymera_y/config_manager.py	2025-10-19 23:09:04.574010+00:00
@@ -1,6 +1,5 @@
-
 """
 Configuration Manager for Multi-Agent Platform
 Provides centralized, dynamic configuration management for all agents.
 """
 
@@ -8,10 +7,11 @@
 import json
 import os
 from typing import Dict, Any, Optional
 
 from base_agent import BaseAgent, AgentConfig, TaskRequest, TaskResponse
+
 
 class ConfigManager(BaseAgent):
     """
     The ConfigManager is responsible for:
     - Storing and managing configuration for all agents and the platform.
@@ -28,22 +28,13 @@
     async def start(self):
         # Load initial configurations from DB or file
         await self._load_all_configurations()
 
         # Subscribe to configuration update requests
-        await self._subscribe(
-            "config.get",
-            self._handle_get_config_request
-        )
-        await self._subscribe(
-            "config.set",
-            self._handle_set_config_request
-        )
-        await self._subscribe(
-            "config.reload",
-            self._handle_reload_config_request
-        )
+        await self._subscribe("config.get", self._handle_get_config_request)
+        await self._subscribe("config.set", self._handle_set_config_request)
+        await self._subscribe("config.reload", self._handle_reload_config_request)
 
         self.logger.info("Config Manager started.")
 
     async def _execute_task_impl(self, request: TaskRequest) -> Dict[str, Any]:
         """Implement the actual task logic for the Config Manager agent"""
@@ -63,11 +54,11 @@
             elif task_type == "reload_all_configs":
                 await self._load_all_configurations()
                 result = {"status": "success", "message": "All configurations reloaded."}
             else:
                 raise ValueError(f"Unknown config manager task type: {task_type}")
-            
+
             return TaskResponse(task_id=request.task_id, success=True, result=result).dict()
 
         except Exception as e:
             self.logger.error(f"Error executing config manager task {task_type}", error=str(e))
             return TaskResponse(task_id=request.task_id, success=False, error=str(e)).dict()
@@ -92,19 +83,20 @@
         if not self.db_pool:
             self.logger.warning("Database pool not initialized. Cannot save configuration.")
             return
         try:
             async with self.db_pool.acquire() as conn:
-                await conn.execute("""
+                await conn.execute(
+                    """
                     INSERT INTO agent_configurations (agent_name, config_data)
                     VALUES ($1, $2)
                     ON CONFLICT (agent_name) DO UPDATE SET
                         config_data = EXCLUDED.config_data,
                         updated_at = CURRENT_TIMESTAMP
                 """,
-                agent_name,
-                json.dumps(config_data)
+                    agent_name,
+                    json.dumps(config_data),
                 )
             self.configurations[agent_name] = config_data
             self.last_config_update[agent_name] = time.time()
             self.logger.info(f"Configuration for {agent_name} updated and persisted.")
             # Notify the agent about the update
@@ -139,11 +131,16 @@
 
     async def _handle_reload_config_request(self, msg):
         try:
             await self._load_all_configurations()
             if msg.reply:
-                await self._publish(msg.reply, json.dumps({"status": "success", "message": "All configurations reloaded."}).encode())
+                await self._publish(
+                    msg.reply,
+                    json.dumps(
+                        {"status": "success", "message": "All configurations reloaded."}
+                    ).encode(),
+                )
         except Exception as e:
             self.logger.error(f"Error handling reload config request: {e}")
             if msg.reply:
                 await self._publish(msg.reply, json.dumps({"error": str(e)}).encode())
 
@@ -152,12 +149,13 @@
     config = AgentConfig(
         name="config_manager",
         agent_type="utility",
         capabilities=["configuration_management"],
         nats_url=os.getenv("NATS_URL", "nats://nats:4222"),
-        postgres_url=os.getenv("POSTGRES_URL", "postgresql://agent:secure_password@postgres:5432/ymera"),
-        log_level=os.getenv("LOG_LEVEL", "INFO")
+        postgres_url=os.getenv(
+            "POSTGRES_URL", "postgresql://agent:secure_password@postgres:5432/ymera"
+        ),
+        log_level=os.getenv("LOG_LEVEL", "INFO"),
     )
-    
+
     agent = ConfigManager(config)
     asyncio.run(agent.run())
-
would reformat /home/runner/work/ymera_y/ymera_y/config_manager.py
--- /home/runner/work/ymera_y/ymera_y/config_compat.py	2025-10-19 22:47:02.797432+00:00
+++ /home/runner/work/ymera_y/ymera_y/config_compat.py	2025-10-19 23:09:04.681060+00:00
@@ -9,278 +9,306 @@
 from pathlib import Path
 
 # ============================================================================
 # ENVIRONMENT VARIABLE HELPERS
 # ============================================================================
+
 
 def get_env(key: str, default: Any = None, required: bool = False) -> Any:
     """Get environment variable with optional default and required check"""
     value = os.getenv(key, default)
     if required and value is None:
         raise ValueError(f"Required environment variable {key} not set")
     return value
 
+
 def get_bool_env(key: str, default: bool = False) -> bool:
     """Get boolean environment variable"""
     value = os.getenv(key, str(default)).lower()
-    return value in ('true', '1', 'yes', 'on')
+    return value in ("true", "1", "yes", "on")
+
 
 def get_int_env(key: str, default: int = 0) -> int:
     """Get integer environment variable"""
     try:
         return int(os.getenv(key, str(default)))
     except ValueError:
         return default
 
+
 # ============================================================================
 # SETTINGS CLASS
 # ============================================================================
+
 
 @dataclass
 class Settings:
     """Unified settings class compatible with multiple patterns"""
-    
+
     # Application Settings
     APP_NAME: str = "YMERA Enterprise"
     APP_VERSION: str = "5.0"
     ENVIRONMENT: str = field(default_factory=lambda: get_env("ENVIRONMENT", "development"))
     DEBUG: bool = field(default_factory=lambda: get_bool_env("DEBUG", False))
-    
+
     # Database Settings
     DATABASE_URL: str = field(
-        default_factory=lambda: get_env(
-            "DATABASE_URL", 
-            "sqlite+aiosqlite:///./ymera.db"
-        )
+        default_factory=lambda: get_env("DATABASE_URL", "sqlite+aiosqlite:///./ymera.db")
     )
     DATABASE_ECHO: bool = field(default_factory=lambda: get_bool_env("DATABASE_ECHO", False))
     DATABASE_POOL_SIZE: int = field(default_factory=lambda: get_int_env("DATABASE_POOL_SIZE", 5))
-    DATABASE_MAX_OVERFLOW: int = field(default_factory=lambda: get_int_env("DATABASE_MAX_OVERFLOW", 10))
-    
+    DATABASE_MAX_OVERFLOW: int = field(
+        default_factory=lambda: get_int_env("DATABASE_MAX_OVERFLOW", 10)
+    )
+
     # Redis Settings
-    REDIS_URL: str = field(
-        default_factory=lambda: get_env(
-            "REDIS_URL", 
-            "redis://localhost:6379/0"
-        )
-    )
-    REDIS_MAX_CONNECTIONS: int = field(default_factory=lambda: get_int_env("REDIS_MAX_CONNECTIONS", 20))
+    REDIS_URL: str = field(default_factory=lambda: get_env("REDIS_URL", "redis://localhost:6379/0"))
+    REDIS_MAX_CONNECTIONS: int = field(
+        default_factory=lambda: get_int_env("REDIS_MAX_CONNECTIONS", 20)
+    )
     REDIS_ENABLED: bool = field(default_factory=lambda: get_bool_env("REDIS_ENABLED", True))
-    
+
     # Cache Settings
     CACHE_TTL: int = field(default_factory=lambda: get_int_env("CACHE_TTL", 300))
     CACHE_ENABLED: bool = field(default_factory=lambda: get_bool_env("CACHE_ENABLED", True))
-    
+
     # Knowledge Graph Settings
     KG_MAX_NODES: int = field(default_factory=lambda: get_int_env("KG_MAX_NODES", 1000000))
     KG_MAX_CONNECTIONS_PER_NODE: int = field(
         default_factory=lambda: get_int_env("KG_MAX_CONNECTIONS_PER_NODE", 1000)
     )
     KG_SIMILARITY_THRESHOLD: float = 0.8
     KG_RETENTION_DAYS: int = field(default_factory=lambda: get_int_env("KG_RETENTION_DAYS", 365))
-    KG_EMBEDDING_DIMENSION: int = field(default_factory=lambda: get_int_env("KG_EMBEDDING_DIMENSION", 128))
-    
+    KG_EMBEDDING_DIMENSION: int = field(
+        default_factory=lambda: get_int_env("KG_EMBEDDING_DIMENSION", 128)
+    )
+
     # Vector Database Settings (Pinecone)
     PINECONE_API_KEY: str = field(default_factory=lambda: get_env("PINECONE_API_KEY", ""))
-    PINECONE_ENVIRONMENT: str = field(default_factory=lambda: get_env("PINECONE_ENVIRONMENT", "us-west1-gcp"))
-    PINECONE_INDEX_NAME: str = field(default_factory=lambda: get_env("PINECONE_INDEX_NAME", "ymera-enterprise"))
+    PINECONE_ENVIRONMENT: str = field(
+        default_factory=lambda: get_env("PINECONE_ENVIRONMENT", "us-west1-gcp")
+    )
+    PINECONE_INDEX_NAME: str = field(
+        default_factory=lambda: get_env("PINECONE_INDEX_NAME", "ymera-enterprise")
+    )
     PINECONE_DIMENSION: int = field(default_factory=lambda: get_int_env("PINECONE_DIMENSION", 1536))
-    
+
     # Security Settings
     SECRET_KEY: str = field(
-        default_factory=lambda: get_env(
-            "SECRET_KEY", 
-            "dev-secret-key-change-in-production"
-        )
+        default_factory=lambda: get_env("SECRET_KEY", "dev-secret-key-change-in-production")
     )
     ENCRYPTION_KEY: str = field(default_factory=lambda: get_env("ENCRYPTION_KEY", ""))
-    
+
     # API Settings
     API_HOST: str = field(default_factory=lambda: get_env("API_HOST", "0.0.0.0"))
     API_PORT: int = field(default_factory=lambda: get_int_env("API_PORT", 8000))
     API_WORKERS: int = field(default_factory=lambda: get_int_env("API_WORKERS", 4))
     API_TIMEOUT: int = field(default_factory=lambda: get_int_env("API_TIMEOUT", 60))
-    
+
     # Logging Settings
     LOG_LEVEL: str = field(default_factory=lambda: get_env("LOG_LEVEL", "INFO"))
     LOG_FORMAT: str = "json"
     LOG_FILE: Optional[str] = field(default_factory=lambda: get_env("LOG_FILE"))
-    
+
     # Monitoring Settings
     METRICS_ENABLED: bool = field(default_factory=lambda: get_bool_env("METRICS_ENABLED", True))
     METRICS_PORT: int = field(default_factory=lambda: get_int_env("METRICS_PORT", 9090))
-    
+
     # Agent Settings
     MAX_AGENTS: int = field(default_factory=lambda: get_int_env("MAX_AGENTS", 100))
     AGENT_TIMEOUT: int = field(default_factory=lambda: get_int_env("AGENT_TIMEOUT", 300))
-    
+
     # File Storage Settings
     UPLOAD_DIR: str = field(default_factory=lambda: get_env("UPLOAD_DIR", "./uploads"))
-    MAX_UPLOAD_SIZE: int = field(default_factory=lambda: get_int_env("MAX_UPLOAD_SIZE", 104857600))  # 100MB
-    
+    MAX_UPLOAD_SIZE: int = field(
+        default_factory=lambda: get_int_env("MAX_UPLOAD_SIZE", 104857600)
+    )  # 100MB
+
     # Performance Settings
     ENABLE_PROFILING: bool = field(default_factory=lambda: get_bool_env("ENABLE_PROFILING", False))
     ENABLE_TRACING: bool = field(default_factory=lambda: get_bool_env("ENABLE_TRACING", False))
-    
+
     def __post_init__(self):
         """Post-initialization validation and setup"""
         # Ensure upload directory exists
         Path(self.UPLOAD_DIR).mkdir(parents=True, exist_ok=True)
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert settings to dictionary"""
         return {
             key: getattr(self, key)
             for key in dir(self)
-            if not key.startswith('_') and key.isupper()
+            if not key.startswith("_") and key.isupper()
         }
-    
+
     def get(self, key: str, default: Any = None) -> Any:
         """Get setting value with optional default"""
         return getattr(self, key, default)
 
+
 # ============================================================================
 # GLOBAL SETTINGS INSTANCE
 # ============================================================================
 
 _settings_instance: Optional[Settings] = None
+
 
 def get_settings() -> Settings:
     """Get or create global settings instance (singleton pattern)"""
     global _settings_instance
     if _settings_instance is None:
         _settings_instance = Settings()
     return _settings_instance
 
+
 def reset_settings():
     """Reset global settings instance (useful for testing)"""
     global _settings_instance
     _settings_instance = None
 
+
 # ============================================================================
 # LEGACY COMPATIBILITY FUNCTIONS
 # ============================================================================
+
 
 # For code expecting specific function names
 def get_config_settings() -> Settings:
     """Legacy function name compatibility"""
     return get_settings()
 
+
 def load_settings() -> Settings:
     """Alternative function name"""
     return get_settings()
 
+
 # ============================================================================
 # CONFIGURATION PROFILES
 # ============================================================================
+
 
 class DevelopmentConfig(Settings):
     """Development environment configuration"""
+
     ENVIRONMENT: str = "development"
     DEBUG: bool = True
     DATABASE_ECHO: bool = True
     LOG_LEVEL: str = "DEBUG"
 
+
 class ProductionConfig(Settings):
     """Production environment configuration"""
+
     ENVIRONMENT: str = "production"
     DEBUG: bool = False
     DATABASE_ECHO: bool = False
     LOG_LEVEL: str = "INFO"
     METRICS_ENABLED: bool = True
 
+
 class TestingConfig(Settings):
     """Testing environment configuration"""
+
     ENVIRONMENT: str = "testing"
     DEBUG: bool = True
     DATABASE_URL: str = "sqlite+aiosqlite:///:memory:"
     REDIS_ENABLED: bool = False
     CACHE_ENABLED: bool = False
 
+
 def get_config_by_environment(env: str = None) -> Settings:
     """Get configuration based on environment"""
     if env is None:
         env = os.getenv("ENVIRONMENT", "development")
-    
+
     configs = {
         "development": DevelopmentConfig,
         "production": ProductionConfig,
-        "testing": TestingConfig
+        "testing": TestingConfig,
     }
-    
+
     config_class = configs.get(env.lower(), Settings)
     return config_class()
 
+
 # ============================================================================
 # VALIDATORS
 # ============================================================================
+
 
 def validate_database_url(url: str) -> bool:
     """Validate database URL format"""
-    valid_prefixes = ['sqlite', 'postgresql', 'mysql', 'oracle']
+    valid_prefixes = ["sqlite", "postgresql", "mysql", "oracle"]
     return any(url.startswith(prefix) for prefix in valid_prefixes)
+
 
 def validate_redis_url(url: str) -> bool:
     """Validate Redis URL format"""
-    return url.startswith('redis://') or url.startswith('rediss://')
+    return url.startswith("redis://") or url.startswith("rediss://")
+
 
 def validate_settings(settings: Settings) -> tuple[bool, list[str]]:
     """Validate settings configuration"""
     errors = []
-    
+
     # Database validation
     if not validate_database_url(settings.DATABASE_URL):
         errors.append(f"Invalid database URL: {settings.DATABASE_URL}")
-    
+
     # Redis validation
     if settings.REDIS_ENABLED and not validate_redis_url(settings.REDIS_URL):
         errors.append(f"Invalid Redis URL: {settings.REDIS_URL}")
-    
+
     # Security validation
     if settings.ENVIRONMENT == "production":
         if settings.SECRET_KEY == "dev-secret-key-change-in-production":
             errors.append("Production environment requires custom SECRET_KEY")
-        
+
         if settings.DEBUG:
             errors.append("DEBUG should be False in production")
-    
+
     # Pinecone validation
     if settings.PINECONE_API_KEY and len(settings.PINECONE_API_KEY) < 10:
         errors.append("Invalid Pinecone API key")
-    
+
     return len(errors) == 0, errors
 
+
 # ============================================================================
 # CONTEXT MANAGERS
 # ============================================================================
+
 
 class TemporarySettings:
     """Context manager for temporary settings changes (useful for testing)"""
-    
+
     def __init__(self, **overrides):
         self.overrides = overrides
         self.original_values = {}
         self.settings = get_settings()
-    
+
     def __enter__(self):
         for key, value in self.overrides.items():
             self.original_values[key] = getattr(self.settings, key, None)
             setattr(self.settings, key, value)
         return self.settings
-    
+
     def __exit__(self, exc_type, exc_val, exc_tb):
         for key, value in self.original_values.items():
             setattr(self.settings, key, value)
 
+
 # ============================================================================
 # CONFIGURATION EXPORT
 # ============================================================================
+
 
 def export_config_to_env_file(filepath: str = ".env.example") -> None:
     """Export current configuration as .env file template"""
     settings = get_settings()
-    
+
     env_content = [
         "# YMERA Enterprise Configuration",
         "# Generated configuration template",
         "",
         "# Application Settings",
@@ -309,21 +337,23 @@
         "",
         "# Pinecone Settings",
         "PINECONE_API_KEY=your-pinecone-api-key",
         f"PINECONE_ENVIRONMENT={settings.PINECONE_ENVIRONMENT}",
         f"PINECONE_INDEX_NAME={settings.PINECONE_INDEX_NAME}",
-        ""
+        "",
     ]
-    
-    with open(filepath, 'w') as f:
-        f.write('\n'.join(env_content))
-    
+
+    with open(filepath, "w") as f:
+        f.write("\n".join(env_content))
+
     print(f"Configuration template exported to {filepath}")
 
+
 # ============================================================================
 # USAGE EXAMPLES
 # ============================================================================
+
 
 def print_configuration_examples():
     """Print usage examples"""
     examples = """
     
@@ -361,10 +391,11 @@
        export_config_to_env_file(".env.example")
     
     
     """
     print(examples)
+
 
 # ============================================================================
 # MODULE INITIALIZATION
 # ============================================================================
 
@@ -386,54 +417,54 @@
     "ProductionConfig",
     "TestingConfig",
     "validate_settings",
     "TemporarySettings",
     "export_config_to_env_file",
-    "settings"
+    "settings",
 ]
 
 # ============================================================================
 # TESTING
 # ============================================================================
 
 if __name__ == "__main__":
     print("Configuration Settings Module Test\n")
-    
+
     # Test 1: Get settings
     print("1. Testing get_settings()...")
     s = get_settings()
     print(f"    Environment: {s.ENVIRONMENT}")
     print(f"    Database URL: {s.DATABASE_URL}")
     print(f"    Redis URL: {s.REDIS_URL}")
-    
+
     # Test 2: Validation
     print("\n2. Testing validation...")
     is_valid, errors = validate_settings(s)
     print(f"    Valid: {is_valid}")
     if errors:
         for error in errors:
             print(f"    {error}")
-    
+
     # Test 3: Temporary settings
     print("\n3. Testing temporary settings...")
     print(f"   Original DEBUG: {s.DEBUG}")
     with TemporarySettings(DEBUG=True):
         print(f"   Temporary DEBUG: {s.DEBUG}")
     print(f"   Restored DEBUG: {s.DEBUG}")
-    
+
     # Test 4: Export template
     print("\n4. Testing export template...")
     try:
         export_config_to_env_file(".env.test")
         print("    Template exported successfully")
     except Exception as e:
         print(f"    Export failed: {e}")
-    
+
     # Test 5: Configuration dict
     print("\n5. Testing configuration dict...")
     config_dict = s.to_dict()
     print(f"    Configuration has {len(config_dict)} settings")
-    
+
     print("\n All tests completed!")
     print("\nFor usage examples, run:")
     print("  from config_settings import print_configuration_examples")
     print("  print_configuration_examples()")
would reformat /home/runner/work/ymera_y/ymera_y/config_compat.py
--- /home/runner/work/ymera_y/ymera_y/conftest.py	2025-10-19 22:47:02.798432+00:00
+++ /home/runner/work/ymera_y/ymera_y/conftest.py	2025-10-19 23:09:04.792991+00:00
@@ -20,10 +20,11 @@
 
 @pytest.fixture
 def test_settings():
     """Test settings"""
     import os
+
     os.environ["DATABASE_URL"] = "postgresql+asyncpg://test:test@localhost:5432/test_db"
     os.environ["REDIS_URL"] = "redis://localhost:6379/15"
     os.environ["DEBUG"] = "True"
     os.environ["JWT_SECRET_KEY"] = "test_secret_key_for_testing_purposes_only_32_chars_min"
     return Settings()
@@ -64,7 +65,7 @@
     """Sample metadata for testing"""
     return {
         "module_name": "test_module",
         "test_coverage": 85,
         "complexity": 5,
-        "language": "python"
+        "language": "python",
     }
would reformat /home/runner/work/ymera_y/ymera_y/conftest.py
--- /home/runner/work/ymera_y/ymera_y/configuration_manager.py	2025-10-19 22:47:02.798432+00:00
+++ /home/runner/work/ymera_y/ymera_y/configuration_manager.py	2025-10-19 23:09:05.051778+00:00
@@ -15,279 +15,275 @@
 import hashlib
 from cryptography.fernet import Fernet
 
 logger = logging.getLogger(__name__)
 
+
 class ConfigurationManager:
     """Enterprise configuration management system"""
-    
+
     def __init__(self, config_path: str, environment: str, redis_client: aioredis.Redis):
         self.config_path = config_path
         self.environment = environment
         self.redis = redis_client
         self.configs: Dict[str, Any] = {}
         self.secrets: Dict[str, str] = {}
         self.last_loaded: Dict[str, datetime] = {}
         self.watchers: Dict[str, asyncio.Task] = {}
-        
+
         # Setup encryption for secrets
         self.fernet = self._initialize_fernet()
-    
+
     def _initialize_fernet(self) -> Fernet:
         """Initialize encryption for secrets"""
-        key = os.environ.get('CONFIG_ENCRYPTION_KEY')
+        key = os.environ.get("CONFIG_ENCRYPTION_KEY")
         if not key:
-            key_file = os.path.join(self.config_path, '.encryption_key')
+            key_file = os.path.join(self.config_path, ".encryption_key")
             if os.path.exists(key_file):
-                with open(key_file, 'rb') as f:
+                with open(key_file, "rb") as f:
                     key = f.read()
             else:
                 key = Fernet.generate_key()
-                with open(key_file, 'wb') as f:
+                with open(key_file, "wb") as f:
                     f.write(key)
-        
+
         return Fernet(key)
-    
+
     async def initialize(self) -> None:
         """Initialize all configuration components"""
         await self.load_all_configs()
         await self.load_secrets()
-        
+
         # Start configuration watchers
         self._start_watchers()
-        
+
         # Subscribe to Redis configuration updates
         await self._subscribe_to_config_updates()
-        
+
         logger.info(f"Configuration manager initialized for environment: {self.environment}")
-    
+
     async def load_all_configs(self) -> Dict[str, Any]:
         """Load all configuration files for the current environment"""
         config_files = [
-            f for f in os.listdir(self.config_path) 
-            if f.endswith('.yaml') or f.endswith('.json')
+            f for f in os.listdir(self.config_path) if f.endswith(".yaml") or f.endswith(".json")
         ]
-        
+
         for filename in config_files:
             name = os.path.splitext(filename)[0]
             file_path = os.path.join(self.config_path, filename)
-            
+
             # Skip environment-specific configs that don't match current env
-            if '_' in name:
-                file_env = name.split('_', 1)[1]
+            if "_" in name:
+                file_env = name.split("_", 1)[1]
                 if file_env != self.environment:
                     continue
-            
+
             # Load the config
             self.configs[name] = await self._load_config_file(file_path)
             self.last_loaded[name] = datetime.utcnow()
-        
+
         # Load environment overrides
         env_dir = os.path.join(self.config_path, self.environment)
         if os.path.exists(env_dir):
             env_files = [
-                f for f in os.listdir(env_dir) 
-                if f.endswith('.yaml') or f.endswith('.json')
+                f for f in os.listdir(env_dir) if f.endswith(".yaml") or f.endswith(".json")
             ]
-            
+
             for filename in env_files:
                 name = os.path.splitext(filename)[0]
                 file_path = os.path.join(env_dir, filename)
-                
+
                 # Override existing config
                 env_config = await self._load_config_file(file_path)
                 if name in self.configs:
                     self.configs[name] = self._deep_merge(self.configs[name], env_config)
                 else:
                     self.configs[name] = env_config
-                
+
                 self.last_loaded[name] = datetime.utcnow()
-        
+
         return self.configs
-    
+
     async def _load_config_file(self, file_path: str) -> Dict[str, Any]:
         """Load configuration from file"""
         try:
-            with open(file_path, 'r') as f:
-                if file_path.endswith('.yaml') or file_path.endswith('.yml'):
+            with open(file_path, "r") as f:
+                if file_path.endswith(".yaml") or file_path.endswith(".yml"):
                     config = yaml.safe_load(f)
                 else:
                     config = json.load(f)
-            
+
             return config
         except Exception as e:
             logger.error(f"Failed to load config file {file_path}: {e}")
             return {}
-    
+
     async def load_secrets(self) -> Dict[str, str]:
         """Load secrets from secure storage"""
         secrets_file = os.path.join(self.config_path, f"secrets_{self.environment}.enc")
-        
+
         if os.path.exists(secrets_file):
             try:
-                with open(secrets_file, 'rb') as f:
+                with open(secrets_file, "rb") as f:
                     encrypted_data = f.read()
-                
+
                 decrypted_data = self.fernet.decrypt(encrypted_data)
                 self.secrets = json.loads(decrypted_data)
-                
+
                 logger.info(f"Loaded {len(self.secrets)} secrets from {secrets_file}")
-                
+
             except Exception as e:
                 logger.error(f"Failed to load secrets: {e}")
-        
+
         # Also check for environment variable secrets
         for key, value in os.environ.items():
-            if key.startswith('SECRET_'):
+            if key.startswith("SECRET_"):
                 secret_name = key[7:].lower()
                 self.secrets[secret_name] = value
-        
+
         return self.secrets
-    
+
     def _start_watchers(self) -> None:
         """Start watchers for configuration files"""
         for filename in self.configs.keys():
             file_path = os.path.join(self.config_path, f"{filename}.yaml")
             if not os.path.exists(file_path):
                 file_path = os.path.join(self.config_path, f"{filename}.json")
-            
+
             if os.path.exists(file_path):
                 task = asyncio.create_task(self._watch_config_file(file_path, filename))
                 self.watchers[filename] = task
-    
+
     async def _watch_config_file(self, file_path: str, config_name: str) -> None:
         """Watch configuration file for changes"""
         last_mtime = os.path.getmtime(file_path)
-        
+
         while True:
             try:
                 await asyncio.sleep(30)  # Check every 30 seconds
-                
+
                 current_mtime = os.path.getmtime(file_path)
                 if current_mtime > last_mtime:
                     # File changed, reload
                     config = await self._load_config_file(file_path)
                     self.configs[config_name] = config
                     self.last_loaded[config_name] = datetime.utcnow()
                     last_mtime = current_mtime
-                    
+
                     # Notify subscribers
                     await self._notify_config_change(config_name)
-                    
+
                     logger.info(f"Reloaded configuration: {config_name}")
             except Exception as e:
                 logger.error(f"Error watching config file {file_path}: {e}")
                 await asyncio.sleep(60)  # Retry after a minute
-    
+
     async def _subscribe_to_config_updates(self) -> None:
         """Subscribe to Redis configuration updates"""
         # Start subscription in background task
         asyncio.create_task(self._config_subscription_listener())
-    
+
     async def _config_subscription_listener(self) -> None:
         """Listen for configuration updates from Redis"""
         channel = self.redis.pubsub()
         await channel.subscribe("config_updates")
-        
+
         try:
             while True:
                 message = await channel.get_message(ignore_subscribe_messages=True)
                 if message:
                     data = json.loads(message["data"])
                     config_name = data.get("config_name")
                     config_value = data.get("config")
-                    
+
                     if config_name and config_value:
                         # Update configuration
                         self.configs[config_name] = config_value
                         self.last_loaded[config_name] = datetime.utcnow()
-                        
+
                         logger.info(f"Updated configuration from Redis: {config_name}")
-                
+
                 await asyncio.sleep(0.1)
         except Exception as e:
             logger.error(f"Config subscription error: {e}")
             # Restart subscription
             asyncio.create_task(self._config_subscription_listener())
-    
+
     async def _notify_config_change(self, config_name: str) -> None:
         """Notify other instances of configuration changes"""
-        message = {
-            "config_name": config_name,
-            "timestamp": datetime.utcnow().isoformat()
-        }
+        message = {"config_name": config_name, "timestamp": datetime.utcnow().isoformat()}
         await self.redis.publish("config_updates", json.dumps(message))
-    
+
     def _deep_merge(self, base: Dict, override: Dict) -> Dict:
         """Deep merge dictionaries"""
         result = base.copy()
-        
+
         for key, value in override.items():
             if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                 result[key] = self._deep_merge(result[key], value)
             else:
                 result[key] = value
-        
+
         return result
-    
+
     def get(self, path: str, default: Any = None) -> Any:
         """
         Get configuration value using dot notation
         Example: config_manager.get('database.connection.timeout', 30)
         """
-        parts = path.split('.')
+        parts = path.split(".")
         config_name = parts[0]
-        
+
         if config_name not in self.configs:
             return default
-        
+
         current = self.configs[config_name]
         for part in parts[1:]:
             if not isinstance(current, dict) or part not in current:
                 return default
             current = current[part]
-        
+
         return current
-    
+
     def get_secret(self, name: str, default: str = None) -> str:
         """Get secret value"""
         return self.secrets.get(name, default)
-    
+
     async def set(self, path: str, value: Any) -> bool:
         """Set configuration value using dot notation"""
-        parts = path.split('.')
+        parts = path.split(".")
         config_name = parts[0]
-        
+
         if config_name not in self.configs:
             self.configs[config_name] = {}
-        
+
         # Navigate to the right location
         current = self.configs[config_name]
         for part in parts[1:-1]:
             if part not in current:
                 current[part] = {}
             current = current[part]
-        
+
         # Set the value
         current[parts[-1]] = value
-        
+
         # Notify other instances
         await self._notify_config_change(config_name)
-        
+
         # Also persist to file if in development mode
         if self.environment == "development":
             await self._save_config_file(config_name)
-        
+
         return True
-    
+
     async def _save_config_file(self, config_name: str) -> bool:
         """Save configuration to file"""
         try:
             file_path = os.path.join(self.config_path, f"{config_name}.yaml")
-            
-            with open(file_path, 'w') as f:
+
+            with open(file_path, "w") as f:
                 yaml.dump(self.configs[config_name], f)
-            
+
             return True
         except Exception as e:
             logger.error(f"Failed to save config file {config_name}: {e}")
-            return False
\ No newline at end of file
+            return False
would reformat /home/runner/work/ymera_y/ymera_y/configuration_manager.py
--- /home/runner/work/ymera_y/ymera_y/connection_pool.py	2025-10-19 22:47:02.798432+00:00
+++ /home/runner/work/ymera_y/ymera_y/connection_pool.py	2025-10-19 23:09:05.306558+00:00
@@ -10,11 +10,11 @@
 
 from sqlalchemy.ext.asyncio import (
     create_async_engine,
     AsyncSession,
     AsyncEngine,
-    async_sessionmaker
+    async_sessionmaker,
 )
 from sqlalchemy.pool import NullPool, QueuePool
 from sqlalchemy.orm import declarative_base
 
 from shared.config.settings import Settings
@@ -24,84 +24,84 @@
 Base = declarative_base()
 
 
 class DatabaseManager:
     """Manages database connections and sessions"""
-    
+
     def __init__(self, settings: Settings):
         self.settings = settings
         self.engine: Optional[AsyncEngine] = None
         self.session_factory: Optional[async_sessionmaker] = None
         self._healthy = False
         self.logger = structlog.get_logger(__name__)
-    
+
     async def initialize(self):
         """Initialize database engine and connection pool"""
         try:
             self.logger.info("Initializing database connection pool...")
-            
+
             # Create async engine
             self.engine = create_async_engine(
                 self.settings.DATABASE_URL,
                 echo=self.settings.DB_ECHO,
                 pool_size=self.settings.DB_POOL_SIZE,
                 max_overflow=self.settings.DB_MAX_OVERFLOW,
                 pool_timeout=self.settings.DB_POOL_TIMEOUT,
                 pool_pre_ping=True,  # Verify connections before using
                 pool_recycle=3600,  # Recycle connections after 1 hour
-                poolclass=QueuePool if self.settings.DB_POOL_SIZE > 0 else NullPool
+                poolclass=QueuePool if self.settings.DB_POOL_SIZE > 0 else NullPool,
             )
-            
+
             # Create session factory
             self.session_factory = async_sessionmaker(
                 self.engine,
                 class_=AsyncSession,
                 expire_on_commit=False,
                 autocommit=False,
-                autoflush=False
+                autoflush=False,
             )
-            
+
             # Test connection
             async with self.engine.begin() as conn:
                 await conn.execute("SELECT 1")
-            
+
             self._healthy = True
             self.logger.info("Database connection pool initialized successfully")
-            
+
         except Exception as e:
             self.logger.error(f"Database initialization failed: {e}", exc_info=True)
             self._healthy = False
             raise
-    
+
     @asynccontextmanager
     async def get_session(self) -> AsyncSession:
         """Get a database session"""
         if not self.session_factory:
             raise RuntimeError("Database not initialized")
-        
+
         session = self.session_factory()
         try:
             yield session
             await session.commit()
         except Exception as e:
             await session.rollback()
             raise
         finally:
             await session.close()
-    
+
     async def close(self):
         """Close database connections"""
         if self.engine:
             self.logger.info("Closing database connections...")
             await self.engine.dispose()
             self._healthy = False
             self.logger.info("Database connections closed")
-    
+
     def is_healthy(self) -> bool:
         """Check if database is healthy"""
         return self._healthy and self.engine is not None
-    
+
     async def health_check(self) -> bool:
         """Perform health check"""
         try:
             async with self.engine.begin() as conn:
                 result = await conn.execute("SELECT 1")
would reformat /home/runner/work/ymera_y/ymera_y/connection_pool.py
--- /home/runner/work/ymera_y/ymera_y/connection_manager.py	2025-10-19 22:47:02.798432+00:00
+++ /home/runner/work/ymera_y/ymera_y/connection_manager.py	2025-10-19 23:09:05.306809+00:00
@@ -12,247 +12,265 @@
 import uuid
 from fastapi import WebSocket, WebSocketDisconnect, status
 
 logger = logging.getLogger(__name__)
 
+
 class ConnectionManager:
     """WebSocket connection manager with authentication and secure messaging"""
-    
+
     def __init__(self, redis_client):
         """Initialize with Redis for cross-instance communication"""
         self.redis = redis_client
         self.active_connections: Dict[str, Dict[str, WebSocket]] = {}
         self.connection_metadata: Dict[str, Dict[str, Any]] = {}
         self.user_connections: Dict[str, Set[str]] = {}
         self.agent_connections: Dict[str, Set[str]] = {}
         self.message_handlers: Dict[str, Callable] = {}
-        
+
         # Start pubsub listener for cross-instance communication
         asyncio.create_task(self._start_redis_listener())
-        
+
         logger.info("WebSocket ConnectionManager initialized")
-    
-    async def connect(self, websocket: WebSocket, entity_id: str, 
-                    connection_type: str, metadata: Dict[str, Any] = None) -> str:
+
+    async def connect(
+        self,
+        websocket: WebSocket,
+        entity_id: str,
+        connection_type: str,
+        metadata: Dict[str, Any] = None,
+    ) -> str:
         """Accept WebSocket connection and register it"""
         connection_id = str(uuid.uuid4())
-        
+
         await websocket.accept()
-        
+
         # Store connection
         if entity_id not in self.active_connections:
             self.active_connections[entity_id] = {}
         self.active_connections[entity_id][connection_id] = websocket
-        
+
         # Store metadata
         self.connection_metadata[connection_id] = {
             "entity_id": entity_id,
             "type": connection_type,
             "connected_at": datetime.utcnow(),
-            "metadata": metadata or {}
+            "metadata": metadata or {},
         }
-        
+
         # Map to appropriate collections
         if connection_type == "user":
             if entity_id not in self.user_connections:
                 self.user_connections[entity_id] = set()
             self.user_connections[entity_id].add(connection_id)
         elif connection_type == "agent":
             if entity_id not in self.agent_connections:
                 self.agent_connections[entity_id] = set()
             self.agent_connections[entity_id].add(connection_id)
-        
+
         # Send welcome message
-        await websocket.send_json({
-            "type": "connection_established",
-            "connection_id": connection_id,
-            "timestamp": datetime.utcnow().isoformat()
-        })
-        
-        logger.info(f"WebSocket connected: {connection_type}={entity_id}, connection={connection_id}")
+        await websocket.send_json(
+            {
+                "type": "connection_established",
+                "connection_id": connection_id,
+                "timestamp": datetime.utcnow().isoformat(),
+            }
+        )
+
+        logger.info(
+            f"WebSocket connected: {connection_type}={entity_id}, connection={connection_id}"
+        )
         return connection_id
-    
+
     async def disconnect(self, connection_id: str) -> None:
         """Disconnect and unregister WebSocket connection"""
         if connection_id not in self.connection_metadata:
             return
-        
+
         # Get connection info
         metadata = self.connection_metadata[connection_id]
         entity_id = metadata["entity_id"]
         connection_type = metadata["type"]
-        
+
         # Remove from mappings
         if connection_type == "user" and entity_id in self.user_connections:
             self.user_connections[entity_id].discard(connection_id)
             if not self.user_connections[entity_id]:
                 del self.user_connections[entity_id]
-        
+
         elif connection_type == "agent" and entity_id in self.agent_connections:
             self.agent_connections[entity_id].discard(connection_id)
             if not self.agent_connections[entity_id]:
                 del self.agent_connections[entity_id]
-        
+
         # Remove from active connections
         if entity_id in self.active_connections:
             if connection_id in self.active_connections[entity_id]:
                 del self.active_connections[entity_id][connection_id]
-            
+
             if not self.active_connections[entity_id]:
                 del self.active_connections[entity_id]
-        
+
         # Remove metadata
         if connection_id in self.connection_metadata:
             del self.connection_metadata[connection_id]
-        
-        logger.info(f"WebSocket disconnected: {connection_type}={entity_id}, connection={connection_id}")
-    
+
+        logger.info(
+            f"WebSocket disconnected: {connection_type}={entity_id}, connection={connection_id}"
+        )
+
     async def send_to_agent(self, agent_id: str, message: Dict) -> bool:
         """Send message to specific agent"""
         success = False
-        
+
         # First try direct send if agent is connected to this instance
         if agent_id in self.active_connections:
             disconnected = []
             for connection_id, websocket in self.active_connections[agent_id].items():
                 try:
                     await websocket.send_json(message)
                     success = True
                 except Exception as e:
                     logger.error(f"Failed to send to agent {agent_id}: {e}")
                     disconnected.append(connection_id)
-            
+
             # Clean up disconnected connections
             for conn_id in disconnected:
                 await self.disconnect(conn_id)
-        
+
         # Also publish to Redis for cross-instance delivery
-        await self.redis.publish(
-            f"agent_message:{agent_id}",
-            json.dumps(message)
-        )
-        
+        await self.redis.publish(f"agent_message:{agent_id}", json.dumps(message))
+
         return success
-    
+
     async def send_to_user(self, user_id: str, message: Dict) -> bool:
         """Send message to specific user"""
         success = False
-        
+
         # First try direct send if user is connected to this instance
         if user_id in self.active_connections:
             disconnected = []
             for connection_id, websocket in self.active_connections[user_id].items():
                 try:
                     await websocket.send_json(message)
                     success = True
                 except Exception as e:
                     logger.error(f"Failed to send to user {user_id}: {e}")
                     disconnected.append(connection_id)
-            
+
             # Clean up disconnected connections
             for conn_id in disconnected:
                 await self.disconnect(conn_id)
-        
+
         # Also publish to Redis for cross-instance delivery
-        await self.redis.publish(
-            f"user_message:{user_id}",
-            json.dumps(message)
-        )
-        
+        await self.redis.publish(f"user_message:{user_id}", json.dumps(message))
+
         return success
-    
+
     async def broadcast_to_all_agents(self, message: Dict) -> None:
         """Broadcast message to all connected agents"""
         for agent_id in list(self.agent_connections.keys()):
             await self.send_to_agent(agent_id, message)
-    
+
     async def broadcast_to_all_users(self, message: Dict) -> None:
         """Broadcast message to all connected users"""
         for user_id in list(self.user_connections.keys()):
             await self.send_to_user(user_id, message)
-    
+
     async def register_message_handler(self, message_type: str, handler: Callable) -> None:
         """Register handler for specific message types"""
         self.message_handlers[message_type] = handler
-    
+
     async def handle_message(self, connection_id: str, message: Dict) -> None:
         """Process incoming WebSocket message"""
         try:
             # Get connection metadata
             metadata = self.connection_metadata.get(connection_id)
             if not metadata:
                 logger.warning(f"Message from unknown connection: {connection_id}")
                 return
-            
+
             entity_id = metadata["entity_id"]
             connection_type = metadata["type"]
-            
+
             # Add metadata to message
             message["_connection_id"] = connection_id
             message["_entity_id"] = entity_id
             message["_type"] = connection_type
-            
+
             # Log message receipt
-            logger.debug(f"WebSocket message from {connection_type}={entity_id}: {message.get('type', 'unknown')}")
-            
+            logger.debug(
+                f"WebSocket message from {connection_type}={entity_id}: {message.get('type', 'unknown')}"
+            )
+
             # Process message based on type
             message_type = message.get("type", "unknown")
             if message_type in self.message_handlers:
                 await self.message_handlers[message_type](message)
             else:
                 logger.warning(f"No handler for message type: {message_type}")
-        
+
         except Exception as e:
             logger.error(f"Error handling WebSocket message: {e}")
-    
+
     async def _start_redis_listener(self) -> None:
         """Listen for messages from Redis pubsub"""
         try:
             # Create pubsub connection
             pubsub = self.redis.pubsub()
-            
+
             # Subscribe to relevant channels
             await pubsub.subscribe("broadcast_all")
             await pubsub.psubscribe("agent_message:*")
             await pubsub.psubscribe("user_message:*")
-            
+
             # Start listening
             while True:
                 message = await pubsub.get_message(ignore_subscribe_messages=True)
                 if message:
                     try:
                         # Parse message
                         channel = message["channel"].decode()
                         data = json.loads(message["data"].decode())
-                        
+
                         # Process based on channel
                         if channel == "broadcast_all":
                             if data.get("target") == "agents":
                                 await self.broadcast_to_all_agents(data["message"])
                             elif data.get("target") == "users":
                                 await self.broadcast_to_all_users(data["message"])
-                        
+
                         elif channel.startswith("agent_message:"):
-                            agent_id = channel[len("agent_message:"):]
+                            agent_id = channel[len("agent_message:") :]
                             if agent_id in self.active_connections:
-                                for connection_id, websocket in self.active_connections[agent_id].items():
-                                    if self.connection_metadata.get(connection_id, {}).get("type") == "agent":
+                                for connection_id, websocket in self.active_connections[
+                                    agent_id
+                                ].items():
+                                    if (
+                                        self.connection_metadata.get(connection_id, {}).get("type")
+                                        == "agent"
+                                    ):
                                         await websocket.send_json(data)
-                        
+
                         elif channel.startswith("user_message:"):
-                            user_id = channel[len("user_message:"):]
+                            user_id = channel[len("user_message:") :]
                             if user_id in self.active_connections:
-                                for connection_id, websocket in self.active_connections[user_id].items():
-                                    if self.connection_metadata.get(connection_id, {}).get("type") == "user":
+                                for connection_id, websocket in self.active_connections[
+                                    user_id
+                                ].items():
+                                    if (
+                                        self.connection_metadata.get(connection_id, {}).get("type")
+                                        == "user"
+                                    ):
                                         await websocket.send_json(data)
-                    
+
                     except Exception as e:
                         logger.error(f"Error processing Redis message: {e}")
-                
+
                 # Sleep briefly to reduce CPU usage
                 await asyncio.sleep(0.01)
-                
+
         except Exception as e:
             logger.error(f"Redis listener error: {e}")
             # Try to reconnect
             await asyncio.sleep(5)
-            asyncio.create_task(self._start_redis_listener())
\ No newline at end of file
+            asyncio.create_task(self._start_redis_listener())
would reformat /home/runner/work/ymera_y/ymera_y/connection_manager.py
--- /home/runner/work/ymera_y/ymera_y/core/__init__.py	2025-10-19 22:47:02.798432+00:00
+++ /home/runner/work/ymera_y/ymera_y/core/__init__.py	2025-10-19 23:09:05.344171+00:00
@@ -7,16 +7,16 @@
 from core.auth import AuthService
 from core.database import Database
 from core.sqlalchemy_models import Base, User, Agent, Task, AgentStatus, TaskStatus, TaskPriority
 
 __all__ = [
-    'Settings',
-    'AuthService',
-    'Database',
-    'Base',
-    'User',
-    'Agent',
-    'Task',
-    'AgentStatus',
-    'TaskStatus',
-    'TaskPriority',
+    "Settings",
+    "AuthService",
+    "Database",
+    "Base",
+    "User",
+    "Agent",
+    "Task",
+    "AgentStatus",
+    "TaskStatus",
+    "TaskPriority",
 ]
would reformat /home/runner/work/ymera_y/ymera_y/core/__init__.py
--- /home/runner/work/ymera_y/ymera_y/core/auth.py	2025-10-19 22:47:02.799797+00:00
+++ /home/runner/work/ymera_y/ymera_y/core/auth.py	2025-10-19 23:09:05.423094+00:00
@@ -9,44 +9,49 @@
 from core.config import Settings
 
 pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
 oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")
 
+
 class AuthService:
     def __init__(self, settings: Settings):
         self.settings = settings
-    
+
     def verify_password(self, plain_password: str, hashed_password: str) -> bool:
         return pwd_context.verify(plain_password, hashed_password)
-    
+
     def get_password_hash(self, password: str) -> str:
         return pwd_context.hash(password)
-    
-    def create_access_token(self, data: Dict[str, Any], expires_delta: Optional[timedelta] = None) -> str:
+
+    def create_access_token(
+        self, data: Dict[str, Any], expires_delta: Optional[timedelta] = None
+    ) -> str:
         to_encode = data.copy()
         if expires_delta:
             expire = datetime.utcnow() + expires_delta
         else:
             expire = datetime.utcnow() + timedelta(minutes=self.settings.jwt_expire_minutes)
         to_encode.update({"exp": expire, "sub": "access"})
-        encoded_jwt = jwt.encode(to_encode, self.settings.jwt_secret_key, algorithm=self.settings.jwt_algorithm)
+        encoded_jwt = jwt.encode(
+            to_encode, self.settings.jwt_secret_key, algorithm=self.settings.jwt_algorithm
+        )
         return encoded_jwt
-    
+
     def decode_access_token(self, token: str) -> Optional[Dict[str, Any]]:
         try:
-            decoded_token = jwt.decode(token, self.settings.jwt_secret_key, algorithms=[self.settings.jwt_algorithm])
+            decoded_token = jwt.decode(
+                token, self.settings.jwt_secret_key, algorithms=[self.settings.jwt_algorithm]
+            )
             return decoded_token
         except jwt.ExpiredSignatureError:
             raise HTTPException(status_code=401, detail="Token has expired")
         except jwt.InvalidTokenError:
             raise HTTPException(status_code=401, detail="Invalid token")
-    
+
     async def get_current_user(self, token: str = Security(oauth2_scheme)) -> Dict[str, Any]:
         payload = self.decode_access_token(token)
         if not payload or "user_id" not in payload:
             raise HTTPException(status_code=401, detail="Invalid authentication credentials")
-        
+
         # In a real application, you would fetch the user from the database here
         # For now, we'll return a mock user
         return {"id": UUID(payload["user_id"]), "email": payload["email"], "role": payload["role"]}
-
-
would reformat /home/runner/work/ymera_y/ymera_y/core/auth.py
--- /home/runner/work/ymera_y/ymera_y/continuous_learning.py	2025-10-19 22:47:02.798432+00:00
+++ /home/runner/work/ymera_y/ymera_y/continuous_learning.py	2025-10-19 23:09:05.741686+00:00
@@ -14,278 +14,297 @@
 logger = logging.getLogger(__name__)
 
 
 class DriftType(Enum):
     """Types of concept drift"""
+
     NONE = "none"
     GRADUAL = "gradual"
     SUDDEN = "sudden"
     RECURRING = "recurring"
 
 
 @dataclass
 class DriftDetection:
     """Represents detected concept drift"""
+
     drift_type: DriftType
     confidence: float
     detected_at: datetime
     affected_features: List[str]
     severity: float  # 0.0 to 1.0
-    
+
 
 class ContinuousLearningEngine:
     """
     Continuous Learning Engine with concept drift detection and incremental updates
     """
-    
+
     def __init__(self, config: Dict[str, Any]):
         """Initialize continuous learning engine"""
         self.config = config
         self.enabled = config.get("enabled", True)
         self.update_interval = config.get("update_interval_seconds", 300)
         self.drift_detection_enabled = config.get("drift_detection_enabled", True)
-        
+
         # Component references (injected)
         self.learning_engine = None
         self.knowledge_base = None
         self.pattern_recognizer = None
-        
+
         # State
         self.is_running = False
         self.update_task = None
         self.drift_history: List[DriftDetection] = []
         self.model_performance_history: List[Dict[str, float]] = []
-        
+
         # Drift detection parameters
         self.drift_window_size = config.get("drift_window_size", 100)
         self.drift_threshold = config.get("drift_threshold", 0.3)
-        
+
         logger.info(f"Continuous Learning Engine initialized")
-    
+
     def set_learning_engine(self, learning_engine):
         """Inject learning engine dependency"""
         self.learning_engine = learning_engine
-    
+
     def set_knowledge_base(self, knowledge_base):
         """Inject knowledge base dependency"""
         self.knowledge_base = knowledge_base
-    
+
     def set_pattern_recognizer(self, pattern_recognizer):
         """Inject pattern recognizer dependency"""
         self.pattern_recognizer = pattern_recognizer
-    
+
     async def start(self):
         """Start continuous learning"""
         if not self.enabled:
             logger.warning("Continuous learning is disabled")
             return
-        
+
         if self.is_running:
             logger.warning("Continuous learning already running")
             return
-        
+
         self.is_running = True
         self.update_task = asyncio.create_task(self._continuous_update_loop())
         logger.info("Continuous learning started")
-    
+
     async def stop(self):
         """Stop continuous learning"""
         self.is_running = False
         if self.update_task:
             self.update_task.cancel()
             try:
                 await self.update_task
             except asyncio.CancelledError:
                 pass
         logger.info("Continuous learning stopped")
-    
+
     async def _continuous_update_loop(self):
         """Main continuous learning loop"""
         while self.is_running:
             try:
                 await asyncio.sleep(self.update_interval)
-                
+
                 # Perform continuous learning update
                 await self._perform_update()
-                
+
             except asyncio.CancelledError:
                 break
             except Exception as e:
                 logger.error(f"Error in continuous learning loop: {str(e)}", exc_info=True)
-    
+
     async def _perform_update(self):
         """Perform a continuous learning update cycle"""
         logger.info("Performing continuous learning update")
-        
+
         # Check for concept drift
         if self.drift_detection_enabled:
             drift = await self._detect_drift()
             if drift and drift.drift_type != DriftType.NONE:
-                logger.warning(f"Drift detected: {drift.drift_type.value} (confidence: {drift.confidence:.2f})")
+                logger.warning(
+                    f"Drift detected: {drift.drift_type.value} (confidence: {drift.confidence:.2f})"
+                )
                 self.drift_history.append(drift)
-                
+
                 # Trigger model retraining if drift is significant
                 if drift.severity > 0.7:
                     await self._handle_drift(drift)
-        
+
         # Perform incremental update
         await self._incremental_update()
-        
+
         # Update knowledge base with new learnings
         if self.knowledge_base:
-            await self.knowledge_base.store({
-                "type": "continuous_learning_update",
-                "timestamp": datetime.utcnow().isoformat(),
-                "drift_detections": len(self.drift_history),
-                "performance_trend": self._calculate_performance_trend()
-            }, category="continuous_learning")
-    
+            await self.knowledge_base.store(
+                {
+                    "type": "continuous_learning_update",
+                    "timestamp": datetime.utcnow().isoformat(),
+                    "drift_detections": len(self.drift_history),
+                    "performance_trend": self._calculate_performance_trend(),
+                },
+                category="continuous_learning",
+            )
+
     async def _detect_drift(self) -> Optional[DriftDetection]:
         """
         Detect concept drift in the data stream
-        
+
         This implementation uses a simple statistical approach.
         In production, you might use more sophisticated methods like:
         - ADWIN (Adaptive Windowing)
         - DDM (Drift Detection Method)
         - EDDM (Early Drift Detection Method)
         - Page-Hinkley Test
         """
         if len(self.model_performance_history) < self.drift_window_size:
             return None
-        
+
         # Get recent performance metrics
-        recent_window = self.model_performance_history[-self.drift_window_size:]
-        
+        recent_window = self.model_performance_history[-self.drift_window_size :]
+
         # Calculate statistics
         recent_performance = [p.get("accuracy", 0.0) for p in recent_window]
         current_mean = np.mean(recent_performance)
         current_std = np.std(recent_performance)
-        
+
         # Compare with historical baseline
         if len(self.model_performance_history) > self.drift_window_size * 2:
-            baseline_window = self.model_performance_history[-self.drift_window_size*2:-self.drift_window_size]
+            baseline_window = self.model_performance_history[
+                -self.drift_window_size * 2 : -self.drift_window_size
+            ]
             baseline_performance = [p.get("accuracy", 0.0) for p in baseline_window]
             baseline_mean = np.mean(baseline_performance)
-            
+
             # Detect drift using statistical test
             performance_drop = baseline_mean - current_mean
-            
+
             if performance_drop > self.drift_threshold:
-                drift_type = DriftType.SUDDEN if performance_drop > self.drift_threshold * 2 else DriftType.GRADUAL
+                drift_type = (
+                    DriftType.SUDDEN
+                    if performance_drop > self.drift_threshold * 2
+                    else DriftType.GRADUAL
+                )
                 confidence = min(1.0, performance_drop / (self.drift_threshold * 3))
-                
+
                 return DriftDetection(
                     drift_type=drift_type,
                     confidence=confidence,
                     detected_at=datetime.utcnow(),
                     affected_features=[],  # Would need feature-level analysis
-                    severity=confidence
+                    severity=confidence,
                 )
-        
+
         return None
-    
+
     async def _handle_drift(self, drift: DriftDetection):
         """Handle detected drift"""
         logger.info(f"Handling drift: {drift.drift_type.value}")
-        
+
         # Strategy depends on drift type and severity
         if drift.drift_type == DriftType.SUDDEN and drift.severity > 0.8:
             # Trigger full model retraining
             logger.warning("Severe sudden drift detected - triggering full retraining")
             # In production, this would trigger a retraining pipeline
-        
+
         elif drift.drift_type == DriftType.GRADUAL:
             # Increase learning rate for faster adaptation
             logger.info("Gradual drift detected - increasing adaptation rate")
             # Adjust incremental learning parameters
-        
+
         # Store drift information in knowledge base
         if self.knowledge_base:
-            await self.knowledge_base.store({
-                "drift_type": drift.drift_type.value,
-                "confidence": drift.confidence,
-                "severity": drift.severity,
-                "detected_at": drift.detected_at.isoformat()
-            }, category="drift_detection", tags=["concept_drift", drift.drift_type.value])
-    
+            await self.knowledge_base.store(
+                {
+                    "drift_type": drift.drift_type.value,
+                    "confidence": drift.confidence,
+                    "severity": drift.severity,
+                    "detected_at": drift.detected_at.isoformat(),
+                },
+                category="drift_detection",
+                tags=["concept_drift", drift.drift_type.value],
+            )
+
     async def _incremental_update(self):
         """Perform incremental model update"""
         logger.debug("Performing incremental update")
-        
+
         # In a real implementation, this would:
         # 1. Fetch recent data
         # 2. Update model incrementally
         # 3. Validate performance
         # 4. Rollback if performance degrades
-        
+
         # Simulate performance metric
         current_performance = {
             "accuracy": 0.85 + np.random.normal(0, 0.05),
-            "timestamp": datetime.utcnow().isoformat()
+            "timestamp": datetime.utcnow().isoformat(),
         }
-        
+
         self.model_performance_history.append(current_performance)
-        
+
         # Keep history bounded
         if len(self.model_performance_history) > self.drift_window_size * 3:
-            self.model_performance_history = self.model_performance_history[-self.drift_window_size * 2:]
-    
+            self.model_performance_history = self.model_performance_history[
+                -self.drift_window_size * 2 :
+            ]
+
     def _calculate_performance_trend(self) -> str:
         """Calculate overall performance trend"""
         if len(self.model_performance_history) < 10:
             return "insufficient_data"
-        
+
         recent_perf = [p.get("accuracy", 0.0) for p in self.model_performance_history[-10:]]
         older_perf = [p.get("accuracy", 0.0) for p in self.model_performance_history[-20:-10]]
-        
+
         if not older_perf:
             return "stable"
-        
+
         recent_mean = np.mean(recent_perf)
         older_mean = np.mean(older_perf)
-        
+
         diff = recent_mean - older_mean
-        
+
         if diff > 0.05:
             return "improving"
         elif diff < -0.05:
             return "degrading"
         else:
             return "stable"
-    
+
     async def update_with_feedback(self, feedback_data: Dict[str, Any]):
         """Update models with user feedback"""
         logger.info("Updating with feedback")
-        
+
         # In production, this would update models based on user feedback
         # For now, we just store in knowledge base
         if self.knowledge_base:
             await self.knowledge_base.store(
-                feedback_data,
-                category="user_feedback",
-                tags=["feedback", "continuous_learning"]
+                feedback_data, category="user_feedback", tags=["feedback", "continuous_learning"]
             )
-    
+
     async def get_drift_history(self, limit: int = 100) -> List[Dict[str, Any]]:
         """Get drift detection history"""
         recent_drifts = self.drift_history[-limit:]
         return [
             {
                 "drift_type": d.drift_type.value,
                 "confidence": d.confidence,
                 "detected_at": d.detected_at.isoformat(),
-                "severity": d.severity
+                "severity": d.severity,
             }
             for d in recent_drifts
         ]
-    
+
     async def get_statistics(self) -> Dict[str, Any]:
         """Get continuous learning statistics"""
         return {
             "enabled": self.enabled,
             "is_running": self.is_running,
             "total_updates": len(self.model_performance_history),
             "drift_detections": len(self.drift_history),
             "performance_trend": self._calculate_performance_trend(),
-            "current_performance": self.model_performance_history[-1] if self.model_performance_history else None
+            "current_performance": (
+                self.model_performance_history[-1] if self.model_performance_history else None
+            ),
         }
would reformat /home/runner/work/ymera_y/ymera_y/continuous_learning.py
--- /home/runner/work/ymera_y/ymera_y/core/database.py	2025-10-19 22:47:02.799797+00:00
+++ /home/runner/work/ymera_y/ymera_y/core/database.py	2025-10-19 23:09:05.862902+00:00
@@ -6,76 +6,74 @@
 from datetime import datetime
 import logging
 
 logger = logging.getLogger(__name__)
 
+
 class Database:
     def __init__(self, database_url: str):
         self.database_url = database_url
         self.pool = None
-    
+
     async def initialize(self):
         """Initialize database connection pool"""
         try:
             self.pool = await asyncpg.create_pool(
                 self.database_url.replace("+asyncpg", ""),
                 min_size=5,
                 max_size=20,
                 command_timeout=60,
-                server_settings={
-                    'application_name': 'ymera_agent'
-                }
+                server_settings={"application_name": "ymera_agent"},
             )
             logger.info("Database pool initialized")
         except Exception as e:
             logger.error(f"Database initialization failed: {e}")
             raise
-    
+
     @asynccontextmanager
     async def get_connection(self):
         """Get database connection from pool"""
         if not self.pool:
             raise RuntimeError("Database not initialized")
-        
+
         async with self.pool.acquire() as connection:
             yield connection
-    
+
     async def execute_query(self, query: str, *args) -> List[Dict]:
         """Execute query and return results"""
         async with self.get_connection() as conn:
             try:
                 rows = await conn.fetch(query, *args)
                 return [dict(row) for row in rows]
             except Exception as e:
                 logger.error(f"Query execution failed: {query[:100]}... Error: {e}")
                 raise
-    
+
     async def execute_single(self, query: str, *args) -> Optional[Dict]:
         """Execute query and return single result"""
         results = await self.execute_query(query, *args)
         return results[0] if results else None
-    
+
     async def execute_command(self, command: str, *args) -> str:
         """Execute command (INSERT, UPDATE, DELETE)"""
         async with self.get_connection() as conn:
             try:
                 result = await conn.execute(command, *args)
                 return result
             except Exception as e:
                 logger.error(f"Command execution failed: {command[:100]}... Error: {e}")
                 raise
-    
+
     async def health_check(self) -> bool:
         """Check database health"""
         try:
             await self.execute_query("SELECT 1")
             return True
         except Exception as e:
             logger.error(f"Database health check failed: {e}")
             return False
-    
+
     async def close(self):
         """Close database connections"""
         if self.pool:
             await self.pool.close()
             logger.info("Database connections closed")
-
would reformat /home/runner/work/ymera_y/ymera_y/core/database.py
--- /home/runner/work/ymera_y/ymera_y/core/config.py	2025-10-19 22:47:02.799797+00:00
+++ /home/runner/work/ymera_y/ymera_y/core/config.py	2025-10-19 23:09:05.913392+00:00
@@ -13,242 +13,222 @@
 class ProjectAgentSettings(BaseSettings):
     """
     Project Agent Configuration Settings
     All settings loaded from environment variables or .env file
     """
-    
+
     # ============================================================================
     # SERVER CONFIGURATION
     # ============================================================================
     host: str = Field(default="0.0.0.0", env="PROJECT_AGENT_HOST")
     port: int = Field(default=8001, env="PROJECT_AGENT_PORT")
     environment: str = Field(default="production", env="ENVIRONMENT")
     debug: bool = Field(default=False, env="DEBUG")
     log_level: str = Field(default="INFO", env="LOG_LEVEL")
     worker_count: int = Field(default=4, env="WORKER_COUNT")
-    
+
     # ============================================================================
     # DATABASE CONFIGURATION
     # ============================================================================
     database_url: str = Field(..., env="DATABASE_URL")
     database_pool_size: int = Field(default=20, env="DATABASE_POOL_SIZE")
     database_max_overflow: int = Field(default=10, env="DATABASE_MAX_OVERFLOW")
     database_echo: bool = Field(default=False, env="DATABASE_ECHO")
-    
+
     # ============================================================================
     # REDIS CONFIGURATION
     # ============================================================================
     redis_url: str = Field(default="redis://localhost:6379/0", env="REDIS_URL")
     redis_password: Optional[str] = Field(default=None, env="REDIS_PASSWORD")
     redis_max_connections: int = Field(default=50, env="REDIS_MAX_CONNECTIONS")
-    
+
     # ============================================================================
     # KAFKA CONFIGURATION
     # ============================================================================
     kafka_bootstrap_servers: List[str] = Field(
-        default=["localhost:9092"],
-        env="KAFKA_BOOTSTRAP_SERVERS"
+        default=["localhost:9092"], env="KAFKA_BOOTSTRAP_SERVERS"
     )
     kafka_topic_prefix: str = Field(default="project_agent", env="KAFKA_TOPIC_PREFIX")
-    
+
     # ============================================================================
     # SECURITY CONFIGURATION
     # ============================================================================
     jwt_secret_key: str = Field(..., env="JWT_SECRET_KEY")
     jwt_algorithm: str = Field(default="RS256", env="JWT_ALGORITHM")
     jwt_expire_minutes: int = Field(default=60, env="JWT_EXPIRE_MINUTES")
     jwt_public_key_path: Optional[str] = Field(default=None, env="JWT_PUBLIC_KEY_PATH")
     jwt_private_key_path: Optional[str] = Field(default=None, env="JWT_PRIVATE_KEY_PATH")
-    
-    cors_origins: List[str] = Field(
-        default=["http://localhost:3000"],
-        env="CORS_ORIGINS"
-    )
-    trusted_hosts: List[str] = Field(
-        default=["localhost", "127.0.0.1"],
-        env="TRUSTED_HOSTS"
-    )
-    
+
+    cors_origins: List[str] = Field(default=["http://localhost:3000"], env="CORS_ORIGINS")
+    trusted_hosts: List[str] = Field(default=["localhost", "127.0.0.1"], env="TRUSTED_HOSTS")
+
     rate_limit_enabled: bool = Field(default=True, env="RATE_LIMIT_ENABLED")
-    rate_limit_requests_per_minute: int = Field(
-        default=100,
-        env="RATE_LIMIT_REQUESTS_PER_MINUTE"
-    )
-    
+    rate_limit_requests_per_minute: int = Field(default=100, env="RATE_LIMIT_REQUESTS_PER_MINUTE")
+
     # ============================================================================
     # QUALITY VERIFICATION
     # ============================================================================
     quality_threshold: float = Field(default=85.0, env="QUALITY_THRESHOLD")
     code_coverage_min: float = Field(default=80.0, env="CODE_COVERAGE_MIN")
     security_scan_enabled: bool = Field(default=True, env="SECURITY_SCAN_ENABLED")
-    performance_benchmark_enabled: bool = Field(
-        default=True,
-        env="PERFORMANCE_BENCHMARK_ENABLED"
-    )
-    
+    performance_benchmark_enabled: bool = Field(default=True, env="PERFORMANCE_BENCHMARK_ENABLED")
+
     quality_code_weight: float = Field(default=0.35, env="QUALITY_CODE_WEIGHT")
     quality_security_weight: float = Field(default=0.30, env="QUALITY_SECURITY_WEIGHT")
     quality_performance_weight: float = Field(default=0.20, env="QUALITY_PERFORMANCE_WEIGHT")
-    quality_documentation_weight: float = Field(
-        default=0.15,
-        env="QUALITY_DOCUMENTATION_WEIGHT"
-    )
-    
+    quality_documentation_weight: float = Field(default=0.15, env="QUALITY_DOCUMENTATION_WEIGHT")
+
     # ============================================================================
     # FILE STORAGE
     # ============================================================================
     storage_backend: str = Field(default="local", env="STORAGE_BACKEND")
     storage_path: str = Field(default="./uploads", env="STORAGE_PATH")
     max_upload_size_mb: int = Field(default=100, env="MAX_UPLOAD_SIZE_MB")
     file_versioning_enabled: bool = Field(default=True, env="FILE_VERSIONING_ENABLED")
-    
+
     # AWS S3
     aws_access_key_id: Optional[str] = Field(default=None, env="AWS_ACCESS_KEY_ID")
     aws_secret_access_key: Optional[str] = Field(default=None, env="AWS_SECRET_ACCESS_KEY")
     aws_region: Optional[str] = Field(default="us-east-1", env="AWS_REGION")
     s3_bucket: Optional[str] = Field(default=None, env="S3_BUCKET")
-    
+
     # ============================================================================
     # AGENT REGISTRY
     # ============================================================================
-    manager_agent_url: str = Field(
-        default="http://manager-agent:8000",
-        env="MANAGER_AGENT_URL"
-    )
-    coding_agent_url: str = Field(
-        default="http://coding-agent:8010",
-        env="CODING_AGENT_URL"
-    )
+    manager_agent_url: str = Field(default="http://manager-agent:8000", env="MANAGER_AGENT_URL")
+    coding_agent_url: str = Field(default="http://coding-agent:8010", env="CODING_AGENT_URL")
     examination_agent_url: str = Field(
-        default="http://examination-agent:8030",
-        env="EXAMINATION_AGENT_URL"
+        default="http://examination-agent:8030", env="EXAMINATION_AGENT_URL"
     )
     enhancement_agent_url: str = Field(
-        default="http://enhancement-agent:8020",
-        env="ENHANCEMENT_AGENT_URL"
-    )
-    
+        default="http://enhancement-agent:8020", env="ENHANCEMENT_AGENT_URL"
+    )
+
     agent_request_timeout: int = Field(default=30, env="AGENT_REQUEST_TIMEOUT")
     agent_max_retries: int = Field(default=3, env="AGENT_MAX_RETRIES")
-    
+
     # ============================================================================
     # MONITORING
     # ============================================================================
     prometheus_enabled: bool = Field(default=True, env="PROMETHEUS_ENABLED")
     prometheus_port: int = Field(default=9090, env="PROMETHEUS_PORT")
-    
+
     jaeger_enabled: bool = Field(default=True, env="JAEGER_ENABLED")
     jaeger_agent_host: str = Field(default="localhost", env="JAEGER_AGENT_HOST")
     jaeger_agent_port: int = Field(default=6831, env="JAEGER_AGENT_PORT")
-    
+
     log_format: str = Field(default="json", env="LOG_FORMAT")
     log_file: str = Field(default="./logs/project_agent.log", env="LOG_FILE")
-    
+
     # ============================================================================
     # FEATURE FLAGS
     # ============================================================================
     enable_chat_interface: bool = Field(default=True, env="ENABLE_CHAT_INTERFACE")
     enable_file_versioning: bool = Field(default=True, env="ENABLE_FILE_VERSIONING")
     enable_auto_integration: bool = Field(default=True, env="ENABLE_AUTO_INTEGRATION")
     enable_rollback: bool = Field(default=True, env="ENABLE_ROLLBACK")
-    
+
     # ============================================================================
     # VALIDATORS
     # ============================================================================
-    
-    @validator('jwt_secret_key')
+
+    @validator("jwt_secret_key")
     def validate_jwt_secret(cls, v):
         """Ensure JWT secret is strong enough"""
         if len(v) < 32:
-            raise ValueError('JWT_SECRET_KEY must be at least 32 characters')
+            raise ValueError("JWT_SECRET_KEY must be at least 32 characters")
         if v == "CHANGE_ME_TO_A_SECURE_256_BIT_SECRET_KEY_MINIMUM_32_CHARS":
-            raise ValueError('JWT_SECRET_KEY must be changed from default value!')
-        return v
-    
-    @validator('quality_threshold')
+            raise ValueError("JWT_SECRET_KEY must be changed from default value!")
+        return v
+
+    @validator("quality_threshold")
     def validate_quality_threshold(cls, v):
         """Ensure quality threshold is valid"""
         if not 0 <= v <= 100:
-            raise ValueError('QUALITY_THRESHOLD must be between 0 and 100')
-        return v
-    
-    @validator('quality_code_weight', 'quality_security_weight', 
-               'quality_performance_weight', 'quality_documentation_weight')
+            raise ValueError("QUALITY_THRESHOLD must be between 0 and 100")
+        return v
+
+    @validator(
+        "quality_code_weight",
+        "quality_security_weight",
+        "quality_performance_weight",
+        "quality_documentation_weight",
+    )
     def validate_quality_weights(cls, v):
         """Ensure quality weights are valid"""
         if not 0 <= v <= 1:
-            raise ValueError('Quality weights must be between 0 and 1')
-        return v
-    
-    @validator('cors_origins', pre=True, always=True)
+            raise ValueError("Quality weights must be between 0 and 1")
+        return v
+
+    @validator("cors_origins", pre=True, always=True)
     def parse_cors_origins(cls, v):
         """Parse CORS origins from string or list"""
         if v is None:
             return ["http://localhost:3000"]
         if isinstance(v, str):
             # Fallback to comma-separated values
-            return [s.strip() for s in v.split(',') if s.strip()]
+            return [s.strip() for s in v.split(",") if s.strip()]
         if isinstance(v, list):
             return v
         return ["http://localhost:3000"]
-    
-    @validator('kafka_bootstrap_servers', pre=True, always=True)
+
+    @validator("kafka_bootstrap_servers", pre=True, always=True)
     def parse_kafka_servers(cls, v):
         """Parse Kafka servers from string or list"""
         if v is None:
             return ["localhost:9092"]
         if isinstance(v, str):
-            return [s.strip() for s in v.split(',') if s.strip()]
+            return [s.strip() for s in v.split(",") if s.strip()]
         if isinstance(v, list):
             return v
         return ["localhost:9092"]
-    
-    @validator('trusted_hosts', pre=True, always=True)
+
+    @validator("trusted_hosts", pre=True, always=True)
     def parse_trusted_hosts(cls, v):
         """Parse trusted hosts from string or list"""
         if v is None:
             return ["localhost", "127.0.0.1"]
         if isinstance(v, str):
-            return [s.strip() for s in v.split(',') if s.strip()]
+            return [s.strip() for s in v.split(",") if s.strip()]
         if isinstance(v, list):
             return v
         return ["localhost", "127.0.0.1"]
-    
-    @validator('storage_path')
+
+    @validator("storage_path")
     def ensure_storage_path_exists(cls, v):
         """Create storage path if it doesn't exist"""
         path = Path(v)
         path.mkdir(parents=True, exist_ok=True)
         return str(path.absolute())
-    
-    @validator('log_file')
+
+    @validator("log_file")
     def ensure_log_directory_exists(cls, v):
         """Create log directory if it doesn't exist"""
         path = Path(v).parent
         path.mkdir(parents=True, exist_ok=True)
         return v
-    
+
     class Config:
         env_file = ".env"
         case_sensitive = False
         env_file_encoding = "utf-8"
         extra = "ignore"  # Ignore extra fields from .env
-    
+
     def get_agent_urls(self) -> dict:
         """Get all configured agent URLs"""
         return {
             "manager": self.manager_agent_url,
             "coding": self.coding_agent_url,
             "examination": self.examination_agent_url,
             "enhancement": self.enhancement_agent_url,
         }
-    
+
     @property
     def is_production(self) -> bool:
         """Check if running in production environment"""
         return self.environment.lower() == "production"
-    
+
     @property
     def is_development(self) -> bool:
         """Check if running in development environment"""
         return self.environment.lower() == "development"
 
would reformat /home/runner/work/ymera_y/ymera_y/core/config.py
--- /home/runner/work/ymera_y/ymera_y/core/manager_client.py	2025-10-19 22:47:02.799797+00:00
+++ /home/runner/work/ymera_y/ymera_y/core/manager_client.py	2025-10-19 23:09:05.955823+00:00
@@ -4,10 +4,11 @@
 from datetime import datetime
 
 from core.config import Settings
 
 logger = logging.getLogger(__name__)
+
 
 class ManagerClient:
     def __init__(self, settings: Settings):
         self.settings = settings
         self.client = httpx.AsyncClient()
@@ -15,48 +16,45 @@
 
     async def send_heartbeat(self, agent_id: str, status: str) -> bool:
         if not self.manager_url:
             logger.warning("Manager Agent URL not configured. Skipping heartbeat.")
             return False
-        
+
         try:
             response = await self.client.post(
                 f"{self.manager_url}/agents/{agent_id}/heartbeat",
-                json={
-                    "status": status,
-                    "timestamp": datetime.utcnow().isoformat()
-                }
+                json={"status": status, "timestamp": datetime.utcnow().isoformat()},
             )
             response.raise_for_status()
             logger.info(f"Heartbeat sent to Manager Agent for agent {agent_id}. Status: {status}")
             return True
         except httpx.RequestError as e:
             logger.error(f"Error sending heartbeat to Manager Agent: {e}")
             return False
         except httpx.HTTPStatusError as e:
-            logger.error(f"Manager Agent returned error for heartbeat: {e.response.status_code} - {e.response.text}")
+            logger.error(
+                f"Manager Agent returned error for heartbeat: {e.response.status_code} - {e.response.text}"
+            )
             return False
 
     async def register_agent(self, agent_data: Dict[str, Any]) -> Optional[str]:
         if not self.manager_url:
             logger.warning("Manager Agent URL not configured. Skipping agent registration.")
             return None
-        
+
         try:
-            response = await self.client.post(
-                f"{self.manager_url}/agents",
-                json=agent_data
-            )
+            response = await self.client.post(f"{self.manager_url}/agents", json=agent_data)
             response.raise_for_status()
             response_data = response.json()
             logger.info(f"Agent registered with Manager Agent. ID: {response_data.get('agent_id')}")
-            return response_data.get('agent_id')
+            return response_data.get("agent_id")
         except httpx.RequestError as e:
             logger.error(f"Error registering agent with Manager Agent: {e}")
             return None
         except httpx.HTTPStatusError as e:
-            logger.error(f"Manager Agent returned error for agent registration: {e.response.status_code} - {e.response.text}")
+            logger.error(
+                f"Manager Agent returned error for agent registration: {e.response.status_code} - {e.response.text}"
+            )
             return None
 
     async def shutdown(self):
         await self.client.aclose()
-
would reformat /home/runner/work/ymera_y/ymera_y/core/manager_client.py
--- /home/runner/work/ymera_y/ymera_y/core/sqlalchemy_models.py	2025-10-19 22:47:02.799797+00:00
+++ /home/runner/work/ymera_y/ymera_y/core/sqlalchemy_models.py	2025-10-19 23:09:06.051118+00:00
@@ -4,82 +4,87 @@
 from enum import Enum
 import uuid
 
 Base = declarative_base()
 
+
 class AgentStatus(str, Enum):
     ACTIVE = "active"
     INACTIVE = "inactive"
     BUSY = "busy"
     ERROR = "error"
+
 
 class TaskStatus(str, Enum):
     PENDING = "pending"
     RUNNING = "running"
     COMPLETED = "completed"
     FAILED = "failed"
 
+
 class TaskPriority(str, Enum):
     LOW = "low"
     NORMAL = "normal"
     HIGH = "high"
     CRITICAL = "critical"
 
+
 # Database Models
 class User(Base):
     __tablename__ = "users"
-    
+
     id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
     username = Column(String(50), unique=True, nullable=False, index=True)
     email = Column(String(255), unique=True, nullable=False, index=True)
     password_hash = Column(String(255), nullable=False)
     is_active = Column(Boolean, default=True)
     created_at = Column(DateTime, default=datetime.utcnow)
     updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
-    
+
     # Relationships
     agents = relationship("Agent", back_populates="owner")
     tasks = relationship("Task", back_populates="user")
 
+
 class Agent(Base):
     __tablename__ = "agents"
-    
+
     id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
     name = Column(String(100), nullable=False)
     description = Column(Text)
     capabilities = Column(JSON, default=list)
     status = Column(String(20), default=AgentStatus.INACTIVE)
     owner_id = Column(String, ForeignKey("users.id"), nullable=False)
     config = Column(JSON, default=dict)
     last_heartbeat = Column(DateTime)
     created_at = Column(DateTime, default=datetime.utcnow)
     updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
-    
+
     # Relationships
     owner = relationship("User", back_populates="agents")
     tasks = relationship("Task", back_populates="agent")
 
+
 class Task(Base):
     __tablename__ = "tasks"
-    
+
     id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
     name = Column(String(200), nullable=False)
     description = Column(Text)
     task_type = Column(String(50), nullable=False)
     parameters = Column(JSON, default=dict)
     priority = Column(String(20), default=TaskPriority.NORMAL)
     status = Column(String(20), default=TaskStatus.PENDING)
     result = Column(JSON)
     error_message = Column(Text)
-    
+
     user_id = Column(String, ForeignKey("users.id"), nullable=False)
     agent_id = Column(String, ForeignKey("agents.id"))
-    
+
     created_at = Column(DateTime, default=datetime.utcnow)
     updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
     started_at = Column(DateTime)
     completed_at = Column(DateTime)
-    
+
     # Relationships
     user = relationship("User", back_populates="tasks")
     agent = relationship("Agent", back_populates="tasks")
-
would reformat /home/runner/work/ymera_y/ymera_y/core/sqlalchemy_models.py
--- /home/runner/work/ymera_y/ymera_y/core_engine_init.py	2025-10-19 22:47:02.799797+00:00
+++ /home/runner/work/ymera_y/ymera_y/core_engine_init.py	2025-10-19 23:09:06.258073+00:00
@@ -13,10 +13,11 @@
 # ===============================================================================
 # STRUCTURED LOGGING
 # ===============================================================================
 try:
     import structlog
+
     logger = structlog.get_logger("ymera.core_engine")
 except ImportError:
     logger = logging.getLogger("ymera.core_engine")
     logger.warning("structlog not available, using standard logging")
 
@@ -24,26 +25,26 @@
 # CORE ENGINE IMPORTS
 # ===============================================================================
 try:
     # Core Engine main class
     from .core_engine import CoreEngine, LearningEngineConfig
-    
+
     # Utils module - ALL utility functions
     from . import utils
-    
+
     # Export utils submodules if they exist
-    if hasattr(utils, 'validators'):
+    if hasattr(utils, "validators"):
         from .utils import validators
-    if hasattr(utils, 'helpers'):
+    if hasattr(utils, "helpers"):
         from .utils import helpers
-    if hasattr(utils, 'encryption'):
+    if hasattr(utils, "encryption"):
         from .utils import encryption
-    if hasattr(utils, 'cache_manager'):
+    if hasattr(utils, "cache_manager"):
         from .utils import cache_manager
-    
+
     logger.info("Core Engine module loaded successfully", version="4.0.0")
-    
+
 except ImportError as e:
     error_msg = f"Failed to import Core Engine components: {e}"
     logger.error(error_msg)
     raise ImportError(error_msg) from e
 
@@ -57,66 +58,70 @@
 # ===============================================================================
 # PUBLIC API
 # ===============================================================================
 __all__ = [
     # Core classes
-    'CoreEngine',
-    'LearningEngineConfig',
-    
+    "CoreEngine",
+    "LearningEngineConfig",
     # Utils module
-    'utils',
-    
+    "utils",
     # Version info
-    '__version__',
+    "__version__",
 ]
+
 
 # ===============================================================================
 # MODULE INITIALIZATION
 # ===============================================================================
 def get_version() -> str:
     """Get Core Engine version"""
     return __version__
+
 
 def check_dependencies() -> Dict[str, Any]:
     """Check if all required dependencies are available"""
     dependencies_status = {
         "core_engine": True,
         "utils": True,
         "structlog": False,
         "aioredis": False,
         "asyncio": False,
     }
-    
+
     # Check optional dependencies
     try:
         import structlog
+
         dependencies_status["structlog"] = True
     except ImportError:
         pass
-    
+
     try:
         import aioredis
+
         dependencies_status["aioredis"] = True
     except ImportError:
         pass
-    
+
     try:
         import asyncio
+
         dependencies_status["asyncio"] = True
     except ImportError:
         pass
-    
+
     return {
         "status": "ready" if all(dependencies_status.values()) else "partial",
         "dependencies": dependencies_status,
-        "version": __version__
+        "version": __version__,
     }
+
 
 # ===============================================================================
 # AUTOMATIC HEALTH CHECK ON IMPORT
 # ===============================================================================
 _health_status = check_dependencies()
 if _health_status["status"] != "ready":
     logger.warning(
         "Core Engine loaded with missing optional dependencies",
-        missing=[k for k, v in _health_status["dependencies"].items() if not v]
+        missing=[k for k, v in _health_status["dependencies"].items() if not v],
     )
would reformat /home/runner/work/ymera_y/ymera_y/core_engine_init.py
--- /home/runner/work/ymera_y/ymera_y/core_engine_complete.py	2025-10-19 22:47:02.799797+00:00
+++ /home/runner/work/ymera_y/ymera_y/core_engine_complete.py	2025-10-19 23:09:06.765026+00:00
@@ -16,16 +16,18 @@
 # ===============================================================================
 # THIRD-PARTY IMPORTS
 # ===============================================================================
 try:
     import structlog
+
     logger = structlog.get_logger("ymera.core_engine")
 except ImportError:
     logger = logging.getLogger("ymera.core_engine")
 
 try:
     import aioredis
+
     REDIS_AVAILABLE = True
 except ImportError:
     logger.warning("aioredis not available, Redis features disabled")
     REDIS_AVAILABLE = False
 
@@ -35,465 +37,462 @@
 from . import utils
 
 # Try to import settings, but don't fail if not available
 try:
     from CORE_CONFIGURATION.config_settings import get_settings
+
     settings = get_settings()
 except ImportError:
     logger.warning("Settings not available, using defaults")
     settings = None
 
 # ===============================================================================
 # CONFIGURATION CLASSES
 # ===============================================================================
 
+
 @dataclass
 class LearningEngineConfig:
     """Configuration for the Core Learning Engine"""
-    
+
     # Learning cycle settings
     learning_cycle_interval: int = 60  # seconds
     knowledge_sync_interval: int = 300  # seconds
     pattern_discovery_interval: int = 900  # seconds
     memory_consolidation_interval: int = 3600  # seconds
-    
+
     # Performance settings
     max_learning_batch_size: int = 1000
     learning_thread_pool_size: int = 4
     knowledge_retention_days: int = 90
     pattern_significance_threshold: float = 0.75
-    
+
     # Inter-agent settings
     inter_agent_sync_enabled: bool = True
     knowledge_transfer_timeout: int = 30
     collaboration_score_threshold: float = 0.6
-    
+
     # External learning
     external_learning_enabled: bool = True
     external_knowledge_validation: bool = True
     knowledge_confidence_threshold: float = 0.8
-    
+
     # Background tasks
     auto_start_background_tasks: bool = True
     enable_health_monitoring: bool = True
-    
+
     # Redis settings
     redis_url: str = "redis://localhost:6379/0"
     redis_max_connections: int = 20
     redis_retry_on_timeout: bool = True
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert config to dictionary"""
         return {
-            'learning_cycle_interval': self.learning_cycle_interval,
-            'knowledge_sync_interval': self.knowledge_sync_interval,
-            'pattern_discovery_interval': self.pattern_discovery_interval,
-            'memory_consolidation_interval': self.memory_consolidation_interval,
-            'max_learning_batch_size': self.max_learning_batch_size,
-            'learning_thread_pool_size': self.learning_thread_pool_size,
-            'knowledge_retention_days': self.knowledge_retention_days,
-            'pattern_significance_threshold': self.pattern_significance_threshold,
-            'inter_agent_sync_enabled': self.inter_agent_sync_enabled,
-            'knowledge_transfer_timeout': self.knowledge_transfer_timeout,
-            'collaboration_score_threshold': self.collaboration_score_threshold,
-            'external_learning_enabled': self.external_learning_enabled,
-            'external_knowledge_validation': self.external_knowledge_validation,
-            'knowledge_confidence_threshold': self.knowledge_confidence_threshold,
-            'auto_start_background_tasks': self.auto_start_background_tasks,
-            'enable_health_monitoring': self.enable_health_monitoring,
+            "learning_cycle_interval": self.learning_cycle_interval,
+            "knowledge_sync_interval": self.knowledge_sync_interval,
+            "pattern_discovery_interval": self.pattern_discovery_interval,
+            "memory_consolidation_interval": self.memory_consolidation_interval,
+            "max_learning_batch_size": self.max_learning_batch_size,
+            "learning_thread_pool_size": self.learning_thread_pool_size,
+            "knowledge_retention_days": self.knowledge_retention_days,
+            "pattern_significance_threshold": self.pattern_significance_threshold,
+            "inter_agent_sync_enabled": self.inter_agent_sync_enabled,
+            "knowledge_transfer_timeout": self.knowledge_transfer_timeout,
+            "collaboration_score_threshold": self.collaboration_score_threshold,
+            "external_learning_enabled": self.external_learning_enabled,
+            "external_knowledge_validation": self.external_knowledge_validation,
+            "knowledge_confidence_threshold": self.knowledge_confidence_threshold,
+            "auto_start_background_tasks": self.auto_start_background_tasks,
+            "enable_health_monitoring": self.enable_health_monitoring,
         }
 
 
 @dataclass
 class LearningCycle:
     """Represents a single learning cycle"""
+
     cycle_id: str
     start_time: datetime
     end_time: Optional[datetime] = None
     status: str = "pending"  # pending, running, completed, failed
     knowledge_items_processed: int = 0
     patterns_discovered: int = 0
     agents_synced: int = 0
     errors: List[str] = field(default_factory=list)
     metrics: Dict[str, Any] = field(default_factory=dict)
 
+
 # ===============================================================================
 # BASE ENGINE CLASS
 # ===============================================================================
+
 
 class BaseEngine:
     """Base class for engine components with lifecycle management"""
-    
+
     def __init__(self, config: Dict[str, Any]):
         self.config = config
         self.logger = logger.bind(component=self.__class__.__name__)
         self._is_initialized = False
         self._is_running = False
         self._health_status = "unknown"
         self._last_health_check = None
-    
+
     async def initialize(self) -> None:
         """Initialize the engine component"""
         if self._is_initialized:
             self.logger.warning("Engine already initialized")
             return
-        
+
         await self._initialize_resources()
         self._is_initialized = True
         self.logger.info("Engine initialized successfully")
-    
+
     async def _initialize_resources(self) -> None:
         """Override this to initialize specific resources"""
         pass
-    
+
     async def start(self) -> None:
         """Start the engine component"""
         if not self._is_initialized:
             await self.initialize()
-        
+
         self._is_running = True
         self.logger.info("Engine started")
-    
+
     async def stop(self) -> None:
         """Stop the engine component"""
         self._is_running = False
         await self._cleanup_resources()
         self.logger.info("Engine stopped")
-    
+
     async def _cleanup_resources(self) -> None:
         """Override this to cleanup specific resources"""
         pass
-    
+
     async def health_check(self) -> Dict[str, Any]:
         """Perform health check"""
         self._last_health_check = datetime.utcnow()
         return {
             "status": self._health_status,
             "initialized": self._is_initialized,
             "running": self._is_running,
-            "last_check": self._last_health_check.isoformat()
+            "last_check": self._last_health_check.isoformat(),
         }
 
+
 # ===============================================================================
 # CORE LEARNING ENGINE
 # ===============================================================================
+
 
 class CoreEngine(BaseEngine):
     """
     Core Learning Engine for YMERA platform.
     Coordinates learning cycles, knowledge management, and agent collaboration.
     """
-    
+
     def __init__(
         self,
         config: LearningEngineConfig,
         knowledge_graph=None,
         pattern_engine=None,
         agent_integration=None,
         external_learning=None,
         memory_consolidation=None,
-        metrics_collector=None
+        metrics_collector=None,
     ):
         """
         Initialize Core Learning Engine.
-        
+
         Args:
             config: Engine configuration
             knowledge_graph: Knowledge graph component (optional)
             pattern_engine: Pattern recognition engine (optional)
             agent_integration: Agent integration component (optional)
             external_learning: External learning component (optional)
             memory_consolidation: Memory consolidation component (optional)
             metrics_collector: Metrics collection component (optional)
         """
-        super().__init__(config.to_dict() if hasattr(config, 'to_dict') else config.__dict__)
-        
+        super().__init__(config.to_dict() if hasattr(config, "to_dict") else config.__dict__)
+
         # Store configuration
         self.config = config
-        
+
         # Store component references
         self.knowledge_graph = knowledge_graph
         self.pattern_engine = pattern_engine
         self.agent_integration = agent_integration
         self.external_learning = external_learning
         self.memory_consolidation = memory_consolidation
         self.metrics_collector = metrics_collector
-        
+
         # Learning state management
         self._learning_queue = None
         self._active_cycles: Dict[str, LearningCycle] = {}
         self._completed_cycles: List[LearningCycle] = []
         self._background_tasks: List[asyncio.Task] = []
-        
+
         # Performance metrics
         self._learning_velocity = 0.0
         self._total_knowledge_items = 0
         self._system_intelligence_score = 0.0
         self._cycle_performance_history: List[Dict[str, Any]] = []
         self._learning_effectiveness_score = 0.0
-        
+
         # Redis client placeholder
         self._redis_client = None
-        
+
         self.logger.info(
             "Core Engine initialized",
             config_summary={
-                'learning_cycle_interval': config.learning_cycle_interval,
-                'auto_start': config.auto_start_background_tasks
-            }
+                "learning_cycle_interval": config.learning_cycle_interval,
+                "auto_start": config.auto_start_background_tasks,
+            },
         )
-    
+
     async def _initialize_resources(self) -> None:
         """Initialize core learning engine resources"""
         try:
             self.logger.info("Initializing core learning engine resources")
-            
+
             # Initialize Redis connection if available
             if REDIS_AVAILABLE:
                 await self._initialize_redis()
-            
+
             # Initialize learning event queue
             await self._initialize_learning_queue()
-            
+
             # Initialize learning metrics
             await self._initialize_learning_metrics()
-            
+
             # Start background learning tasks if enabled
             if self.config.auto_start_background_tasks:
                 await self._start_background_learning_tasks()
-            
+
             self._is_initialized = True
             self._health_status = "healthy"
-            
+
             self.logger.info("Core learning engine initialized successfully")
-            
+
         except Exception as e:
             self._health_status = "unhealthy"
             self.logger.error("Failed to initialize core learning engine", error=str(e))
             raise
-    
+
     async def _initialize_redis(self) -> None:
         """Initialize Redis connection for learning coordination"""
         try:
             if REDIS_AVAILABLE:
                 self._redis_client = await aioredis.from_url(
                     self.config.redis_url,
                     max_connections=self.config.redis_max_connections,
-                    retry_on_timeout=self.config.redis_retry_on_timeout
+                    retry_on_timeout=self.config.redis_retry_on_timeout,
                 )
                 self.logger.info("Redis connection established")
             else:
                 self.logger.warning("Redis not available, using in-memory storage")
         except Exception as e:
             self.logger.warning(f"Failed to initialize Redis: {e}, using in-memory storage")
             self._redis_client = None
-    
+
     async def _initialize_learning_queue(self) -> None:
         """Initialize the learning event queue"""
         self._learning_queue = asyncio.Queue(maxsize=1000)
         self.logger.debug("Learning queue initialized")
-    
+
     async def _initialize_learning_metrics(self) -> None:
         """Initialize learning performance metrics"""
         if self.metrics_collector:
-            await self.metrics_collector.initialize_core_metrics({
-                "learning_velocity": 0.0,
-                "knowledge_retention_rate": 0.0,
-                "pattern_discovery_rate": 0.0,
-                "agent_collaboration_score": 0.0,
-                "system_intelligence_score": 0.0
-            })
-        
+            await self.metrics_collector.initialize_core_metrics(
+                {
+                    "learning_velocity": 0.0,
+                    "knowledge_retention_rate": 0.0,
+                    "pattern_discovery_rate": 0.0,
+                    "agent_collaboration_score": 0.0,
+                    "system_intelligence_score": 0.0,
+                }
+            )
+
         self.logger.debug("Learning metrics initialized")
-    
+
     async def _start_background_learning_tasks(self) -> None:
         """Start all background learning processes"""
         self.logger.info("Starting background learning tasks")
-        
+
         # Core learning cycle task
         learning_task = asyncio.create_task(
-            self._continuous_learning_loop(),
-            name="continuous_learning_loop"
+            self._continuous_learning_loop(), name="continuous_learning_loop"
         )
         self._background_tasks.append(learning_task)
-        
+
         # Knowledge synchronization task
         if self.config.inter_agent_sync_enabled:
             sync_task = asyncio.create_task(
-                self._inter_agent_knowledge_synchronization(),
-                name="knowledge_synchronization"
+                self._inter_agent_knowledge_synchronization(), name="knowledge_synchronization"
             )
             self._background_tasks.append(sync_task)
-        
+
         # Pattern discovery task
-        pattern_task = asyncio.create_task(
-            self._pattern_discovery_loop(),
-            name="pattern_discovery"
-        )
+        pattern_task = asyncio.create_task(self._pattern_discovery_loop(), name="pattern_discovery")
         self._background_tasks.append(pattern_task)
-        
+
         # Memory consolidation task
         memory_task = asyncio.create_task(
-            self._memory_consolidation_loop(),
-            name="memory_consolidation"
+            self._memory_consolidation_loop(), name="memory_consolidation"
         )
         self._background_tasks.append(memory_task)
-        
+
         # Health monitoring task
         if self.config.enable_health_monitoring:
             health_task = asyncio.create_task(
-                self._health_monitoring_loop(),
-                name="health_monitoring"
+                self._health_monitoring_loop(), name="health_monitoring"
             )
             self._background_tasks.append(health_task)
-        
+
         self.logger.info(f"Started {len(self._background_tasks)} background tasks")
-    
+
     async def _continuous_learning_loop(self) -> None:
         """Continuous learning cycle execution"""
         self.logger.info("Starting continuous learning loop")
-        
+
         while self._is_running:
             try:
                 # Create new learning cycle
                 cycle_id = utils.generate_cycle_id()
                 cycle = LearningCycle(
-                    cycle_id=cycle_id,
-                    start_time=datetime.utcnow(),
-                    status="running"
+                    cycle_id=cycle_id, start_time=datetime.utcnow(), status="running"
                 )
-                
+
                 self._active_cycles[cycle_id] = cycle
-                
+
                 # Execute learning cycle
                 await self._execute_learning_cycle(cycle)
-                
+
                 # Mark cycle as completed
                 cycle.end_time = datetime.utcnow()
                 cycle.status = "completed"
-                
+
                 # Move to completed cycles
                 self._completed_cycles.append(cycle)
                 del self._active_cycles[cycle_id]
-                
+
                 # Update metrics
                 await self._update_learning_metrics(cycle)
-                
+
                 # Wait for next cycle
                 await asyncio.sleep(self.config.learning_cycle_interval)
-                
+
             except asyncio.CancelledError:
                 self.logger.info("Learning loop cancelled")
                 break
             except Exception as e:
                 self.logger.error(f"Error in learning loop: {e}")
                 await asyncio.sleep(self.config.learning_cycle_interval)
-    
+
     async def _execute_learning_cycle(self, cycle: LearningCycle) -> None:
         """Execute a single learning cycle"""
         try:
             self.logger.debug(f"Executing learning cycle: {cycle.cycle_id}")
-            
+
             # Process knowledge items
             if self.knowledge_graph:
                 knowledge_count = await self._process_knowledge_items(cycle)
                 cycle.knowledge_items_processed = knowledge_count
-            
+
             # Discover patterns
             if self.pattern_engine:
                 patterns_count = await self._discover_patterns(cycle)
                 cycle.patterns_discovered = patterns_count
-            
+
             # Update metrics
             cycle.metrics = {
-                'duration_seconds': (datetime.utcnow() - cycle.start_time).total_seconds(),
-                'knowledge_items': cycle.knowledge_items_processed,
-                'patterns': cycle.patterns_discovered,
+                "duration_seconds": (datetime.utcnow() - cycle.start_time).total_seconds(),
+                "knowledge_items": cycle.knowledge_items_processed,
+                "patterns": cycle.patterns_discovered,
             }
-            
-            self.logger.info(
-                f"Learning cycle completed: {cycle.cycle_id}",
-                metrics=cycle.metrics
-            )
-            
+
+            self.logger.info(f"Learning cycle completed: {cycle.cycle_id}", metrics=cycle.metrics)
+
         except Exception as e:
             cycle.status = "failed"
             cycle.errors.append(str(e))
             self.logger.error(f"Learning cycle failed: {e}", cycle_id=cycle.cycle_id)
             raise
-    
+
     async def _process_knowledge_items(self, cycle: LearningCycle) -> int:
         """Process knowledge items in the current cycle"""
         # Placeholder implementation
         # In production, this would interact with knowledge graph
         return 0
-    
+
     async def _discover_patterns(self, cycle: LearningCycle) -> int:
         """Discover patterns in the current cycle"""
         # Placeholder implementation
         # In production, this would use pattern engine
         return 0
-    
+
     async def _inter_agent_knowledge_synchronization(self) -> None:
         """Synchronize knowledge between agents"""
         self.logger.info("Starting inter-agent knowledge synchronization")
-        
+
         while self._is_running:
             try:
                 if self.agent_integration:
                     # Perform knowledge synchronization
                     await self._sync_agent_knowledge()
-                
+
                 await asyncio.sleep(self.config.knowledge_sync_interval)
-                
+
             except asyncio.CancelledError:
                 break
             except Exception as e:
                 self.logger.error(f"Knowledge sync error: {e}")
                 await asyncio.sleep(self.config.knowledge_sync_interval)
-    
+
     async def _sync_agent_knowledge(self) -> None:
         """Synchronize knowledge across agents"""
         # Placeholder implementation
         pass
-    
+
     async def _pattern_discovery_loop(self) -> None:
         """Pattern discovery background task"""
         self.logger.info("Starting pattern discovery loop")
-        
+
         while self._is_running:
             try:
                 if self.pattern_engine:
                     # Run pattern discovery
                     pass
-                
+
                 await asyncio.sleep(self.config.pattern_discovery_interval)
-                
+
             except asyncio.CancelledError:
                 break
             except Exception as e:
                 self.logger.error(f"Pattern discovery error: {e}")
                 await asyncio.sleep(self.config.pattern_discovery_interval)
-    
+
     async def _memory_consolidation_loop(self) -> None:
         """Memory consolidation background task"""
         self.logger.info("Starting memory consolidation loop")
-        
+
         while self._is_running:
             try:
                 if self.memory_consolidation:
                     # Consolidate memories
                     pass
-                
+
                 await asyncio.sleep(self.config.memory_consolidation_interval)
-                
+
             except asyncio.CancelledError:
                 break
             except Exception as e:
                 self.logger.error(f"Memory consolidation error: {e}")
                 await asyncio.sleep(self.config.memory_consolidation_interval)
-    
+
     async def _health_monitoring_loop(self) -> None:
         """Health monitoring background task"""
         while self._is_running:
             try:
                 await self.health_check()
@@ -501,96 +500,100 @@
             except asyncio.CancelledError:
                 break
             except Exception as e:
                 self.logger.error(f"Health monitoring error: {e}")
                 await asyncio.sleep(60)
-    
+
     async def _update_learning_metrics(self, cycle: LearningCycle) -> None:
         """Update learning performance metrics"""
         # Calculate learning velocity
         if cycle.metrics:
-            duration = cycle.metrics.get('duration_seconds', 1)
+            duration = cycle.metrics.get("duration_seconds", 1)
             items_processed = cycle.knowledge_items_processed
             self._learning_velocity = items_processed / duration if duration > 0 else 0
-        
+
         # Store cycle performance
-        self._cycle_performance_history.append({
-            'cycle_id': cycle.cycle_id,
-            'timestamp': cycle.start_time.isoformat(),
-            'duration': cycle.metrics.get('duration_seconds', 0),
-            'items_processed': cycle.knowledge_items_processed,
-            'patterns_discovered': cycle.patterns_discovered,
-            'velocity': self._learning_velocity
-        })
-        
+        self._cycle_performance_history.append(
+            {
+                "cycle_id": cycle.cycle_id,
+                "timestamp": cycle.start_time.isoformat(),
+                "duration": cycle.metrics.get("duration_seconds", 0),
+                "items_processed": cycle.knowledge_items_processed,
+                "patterns_discovered": cycle.patterns_discovered,
+                "velocity": self._learning_velocity,
+            }
+        )
+
         # Keep only last 100 cycles
         if len(self._cycle_performance_history) > 100:
             self._cycle_performance_history = self._cycle_performance_history[-100:]
-        
+
         # Update metrics collector if available
         if self.metrics_collector:
-            await self.metrics_collector.record_metric(
-                "learning_velocity",
-                self._learning_velocity
-            )
-    
+            await self.metrics_collector.record_metric("learning_velocity", self._learning_velocity)
+
     async def _cleanup_resources(self) -> None:
         """Cleanup engine resources"""
         try:
             self.logger.info("Cleaning up core engine resources")
-            
+
             # Cancel background tasks
             for task in self._background_tasks:
                 if not task.done():
                     task.cancel()
-            
+
             # Wait for tasks to complete
             if self._background_tasks:
                 await asyncio.gather(*self._background_tasks, return_exceptions=True)
-            
+
             # Close Redis connection
             if self._redis_client:
                 await self._redis_client.close()
-            
+
             self.logger.info("Core engine cleanup completed")
-            
+
         except Exception as e:
             self.logger.error(f"Error during cleanup: {e}")
-    
+
     async def health_check(self) -> Dict[str, Any]:
         """Perform comprehensive health check"""
         health = await super().health_check()
-        
-        health.update({
-            "active_cycles": len(self._active_cycles),
-            "completed_cycles": len(self._completed_cycles),
-            "learning_velocity": self._learning_velocity,
-            "total_knowledge_items": self._total_knowledge_items,
-            "system_intelligence_score": self._system_intelligence_score,
-            "background_tasks_running": sum(1 for t in self._background_tasks if not t.done()),
-            "redis_connected": self._redis_client is not None
-        })
-        
+
+        health.update(
+            {
+                "active_cycles": len(self._active_cycles),
+                "completed_cycles": len(self._completed_cycles),
+                "learning_velocity": self._learning_velocity,
+                "total_knowledge_items": self._total_knowledge_items,
+                "system_intelligence_score": self._system_intelligence_score,
+                "background_tasks_running": sum(1 for t in self._background_tasks if not t.done()),
+                "redis_connected": self._redis_client is not None,
+            }
+        )
+
         return health
-    
+
     def get_statistics(self) -> Dict[str, Any]:
         """Get engine statistics"""
         return {
             "total_cycles": len(self._completed_cycles),
             "active_cycles": len(self._active_cycles),
             "learning_velocity": self._learning_velocity,
             "total_knowledge_items": self._total_knowledge_items,
             "system_intelligence_score": self._system_intelligence_score,
             "learning_effectiveness_score": self._learning_effectiveness_score,
-            "recent_performance": self._cycle_performance_history[-10:] if self._cycle_performance_history else []
+            "recent_performance": (
+                self._cycle_performance_history[-10:] if self._cycle_performance_history else []
+            ),
         }
 
+
 # ===============================================================================
 # EXPORTS
 # ===============================================================================
 
 __all__ = [
-    'CoreEngine',
-    'LearningEngineConfig',
-    'LearningCycle',
-    'BaseEngine',
+    "CoreEngine",
+    "LearningEngineConfig",
+    "LearningCycle",
+    "BaseEngine",
 ]
would reformat /home/runner/work/ymera_y/ymera_y/core_engine_complete.py
--- /home/runner/work/ymera_y/ymera_y/core_engine_utils.py	2025-10-19 22:47:02.799797+00:00
+++ /home/runner/work/ymera_y/ymera_y/core_engine_utils.py	2025-10-19 23:09:06.778751+00:00
@@ -20,10 +20,11 @@
 # ===============================================================================
 # STRUCTURED LOGGING
 # ===============================================================================
 try:
     import structlog
+
     logger = structlog.get_logger("ymera.core_engine.utils")
 except ImportError:
     logger = logging.getLogger("ymera.core_engine.utils")
 
 # ===============================================================================
@@ -35,28 +36,29 @@
 
 # ===============================================================================
 # IDENTIFIER GENERATION
 # ===============================================================================
 
+
 def generate_unique_id(prefix: str = "", length: int = 8) -> str:
     """
     Generate a unique identifier for Core Engine operations.
-    
+
     Args:
         prefix: Optional prefix for the ID
         length: Length of the random portion (default: 8)
-        
+
     Returns:
         Unique identifier string
-        
+
     Example:
         >>> generate_unique_id("task", 8)
         'task_172334_a3f4c2d1'
     """
-    unique_part = str(uuid.uuid4()).replace('-', '')[:length]
+    unique_part = str(uuid.uuid4()).replace("-", "")[:length]
     timestamp = str(int(time.time() * 1000))[-6:]
-    
+
     if prefix:
         return f"{prefix}_{timestamp}_{unique_part}"
     return f"{timestamp}_{unique_part}"
 
 
@@ -67,391 +69,377 @@
 
 def generate_task_id() -> str:
     """Generate unique ID for async task"""
     return generate_unique_id("task", 10)
 
+
 # ===============================================================================
 # TIMESTAMP UTILITIES
 # ===============================================================================
+
 
 def get_utc_timestamp() -> datetime:
     """Get current UTC timestamp"""
     return datetime.now(timezone.utc)
 
 
-def format_timestamp(
-    dt: Optional[datetime] = None,
-    format_type: str = "iso"
-) -> str:
+def format_timestamp(dt: Optional[datetime] = None, format_type: str = "iso") -> str:
     """
     Format timestamp in various formats.
-    
+
     Args:
         dt: DateTime object (defaults to current time)
         format_type: Format type ('iso', 'human', 'compact')
-        
+
     Returns:
         Formatted timestamp string
     """
     if dt is None:
         dt = get_utc_timestamp()
-    
+
     formats = {
-        'iso': lambda d: d.isoformat(),
-        'human': lambda d: d.strftime('%Y-%m-%d %H:%M:%S UTC'),
-        'compact': lambda d: d.strftime('%Y%m%d_%H%M%S'),
+        "iso": lambda d: d.isoformat(),
+        "human": lambda d: d.strftime("%Y-%m-%d %H:%M:%S UTC"),
+        "compact": lambda d: d.strftime("%Y%m%d_%H%M%S"),
     }
-    
-    formatter = formats.get(format_type, formats['iso'])
+
+    formatter = formats.get(format_type, formats["iso"])
     return formatter(dt)
 
 
 def timestamp_to_seconds(dt: datetime) -> float:
     """Convert datetime to seconds since epoch"""
     return dt.timestamp()
 
+
 # ===============================================================================
 # HASHING UTILITIES
 # ===============================================================================
 
-def calculate_hash(
-    data: Union[str, bytes, Dict[str, Any]],
-    algorithm: str = "sha256"
-) -> str:
+
+def calculate_hash(data: Union[str, bytes, Dict[str, Any]], algorithm: str = "sha256") -> str:
     """
     Calculate hash of data for integrity checking.
-    
+
     Args:
         data: Data to hash
         algorithm: Hash algorithm (default: sha256)
-        
+
     Returns:
         Hexadecimal hash string
     """
     if algorithm not in hashlib.algorithms_available:
         raise ValueError(f"Unsupported hash algorithm: {algorithm}")
-    
+
     hasher = hashlib.new(algorithm)
-    
+
     if isinstance(data, dict):
-        data_str = json.dumps(data, sort_keys=True, separators=(',', ':'))
-        hasher.update(data_str.encode('utf-8'))
+        data_str = json.dumps(data, sort_keys=True, separators=(",", ":"))
+        hasher.update(data_str.encode("utf-8"))
     elif isinstance(data, str):
-        hasher.update(data.encode('utf-8'))
+        hasher.update(data.encode("utf-8"))
     elif isinstance(data, bytes):
         hasher.update(data)
     else:
-        hasher.update(str(data).encode('utf-8'))
-    
+        hasher.update(str(data).encode("utf-8"))
+
     return hasher.hexdigest()
 
 
 def verify_integrity(data: Any, expected_hash: str) -> bool:
     """Verify data integrity against hash"""
     actual_hash = calculate_hash(data)
     return actual_hash == expected_hash
 
+
 # ===============================================================================
 # JSON UTILITIES
 # ===============================================================================
 
+
 def safe_json_loads(json_str: str, default: Any = None) -> Any:
     """
     Safely parse JSON string with error handling.
-    
+
     Args:
         json_str: JSON string to parse
         default: Default value if parsing fails
-        
+
     Returns:
         Parsed data or default value
     """
     try:
         return json.loads(json_str)
     except (json.JSONDecodeError, TypeError, ValueError) as e:
         logger.warning("JSON parsing failed", error=str(e))
         return default
 
 
-def safe_json_dumps(
-    data: Any,
-    ensure_ascii: bool = False,
-    indent: Optional[int] = None
-) -> str:
+def safe_json_dumps(data: Any, ensure_ascii: bool = False, indent: Optional[int] = None) -> str:
     """
     Safely serialize data to JSON with error handling.
-    
+
     Args:
         data: Data to serialize
         ensure_ascii: Whether to escape non-ASCII characters
         indent: Indentation for pretty printing
-        
+
     Returns:
         JSON string or empty string on error
     """
+
     def json_serializer(obj):
         """Default serializer for non-standard types"""
         if isinstance(obj, datetime):
             return obj.isoformat()
         elif isinstance(obj, uuid.UUID):
             return str(obj)
-        elif hasattr(obj, '__dict__'):
+        elif hasattr(obj, "__dict__"):
             return obj.__dict__
         return str(obj)
-    
+
     try:
         return json.dumps(
             data,
             default=json_serializer,
             ensure_ascii=ensure_ascii,
             indent=indent,
-            separators=(',', ':') if indent is None else None
+            separators=(",", ":") if indent is None else None,
         )
     except (TypeError, ValueError) as e:
         logger.warning("JSON serialization failed", error=str(e))
         return ""
 
+
 # ===============================================================================
 # DICTIONARY UTILITIES
 # ===============================================================================
 
+
 def deep_merge_dict(dict1: Dict, dict2: Dict) -> Dict:
     """
     Deep merge two dictionaries.
-    
+
     Args:
         dict1: Base dictionary
         dict2: Dictionary to merge into dict1
-        
+
     Returns:
         Merged dictionary
     """
     result = dict1.copy()
-    
+
     for key, value in dict2.items():
-        if (key in result and 
-            isinstance(result[key], dict) and 
-            isinstance(value, dict)):
+        if key in result and isinstance(result[key], dict) and isinstance(value, dict):
             result[key] = deep_merge_dict(result[key], value)
         else:
             result[key] = value
-    
+
     return result
 
 
-def flatten_dict(
-    data: Dict[str, Any],
-    separator: str = ".",
-    prefix: str = ""
-) -> Dict[str, Any]:
+def flatten_dict(data: Dict[str, Any], separator: str = ".", prefix: str = "") -> Dict[str, Any]:
     """
     Flatten nested dictionary.
-    
+
     Args:
         data: Dictionary to flatten
         separator: Separator for nested keys
         prefix: Prefix for keys
-        
+
     Returns:
         Flattened dictionary
     """
     items = []
-    
+
     for key, value in data.items():
         new_key = f"{prefix}{separator}{key}" if prefix else key
-        
+
         if isinstance(value, dict):
             items.extend(flatten_dict(value, separator, new_key).items())
         elif isinstance(value, list):
             for i, item in enumerate(value):
                 if isinstance(item, dict):
-                    items.extend(
-                        flatten_dict(item, separator, f"{new_key}[{i}]").items()
-                    )
+                    items.extend(flatten_dict(item, separator, f"{new_key}[{i}]").items())
                 else:
                     items.append((f"{new_key}[{i}]", item))
         else:
             items.append((new_key, value))
-    
+
     return dict(items)
 
+
 # ===============================================================================
 # ASYNC UTILITIES
 # ===============================================================================
+
 
 async def retry_async_operation(
     operation: Callable,
     max_attempts: int = 3,
     base_delay: float = 1.0,
     backoff_factor: float = 2.0,
     *args,
-    **kwargs
+    **kwargs,
 ) -> Any:
     """
     Retry an async operation with exponential backoff.
-    
+
     Args:
         operation: Async function to retry
         max_attempts: Maximum number of attempts
         base_delay: Initial delay between retries
         backoff_factor: Multiplier for delay on each retry
         *args: Arguments for the operation
         **kwargs: Keyword arguments for the operation
-        
+
     Returns:
         Result of the operation
-        
+
     Raises:
         Last exception if all retries fail
     """
     last_error = None
-    
+
     for attempt in range(max_attempts):
         try:
             return await operation(*args, **kwargs)
         except Exception as e:
             last_error = e
-            
+
             if attempt < max_attempts - 1:
-                delay = base_delay * (backoff_factor ** attempt)
-                
+                delay = base_delay * (backoff_factor**attempt)
+
                 logger.debug(
-                    "Operation failed, retrying",
-                    attempt=attempt + 1,
-                    delay=delay,
-                    error=str(e)
+                    "Operation failed, retrying", attempt=attempt + 1, delay=delay, error=str(e)
                 )
-                
+
                 await asyncio.sleep(delay)
             else:
                 logger.error(
-                    "Operation failed after all retries",
-                    attempts=max_attempts,
-                    error=str(e)
+                    "Operation failed after all retries", attempts=max_attempts, error=str(e)
                 )
-    
+
     raise last_error
 
 
-async def run_with_timeout(
-    coro: Callable,
-    timeout: float,
-    *args,
-    **kwargs
-) -> Any:
+async def run_with_timeout(coro: Callable, timeout: float, *args, **kwargs) -> Any:
     """
     Run async operation with timeout.
-    
+
     Args:
         coro: Coroutine function to run
         timeout: Timeout in seconds
         *args: Arguments for the coroutine
         **kwargs: Keyword arguments for the coroutine
-        
+
     Returns:
         Result of the operation
-        
+
     Raises:
         asyncio.TimeoutError if operation times out
     """
     return await asyncio.wait_for(coro(*args, **kwargs), timeout=timeout)
 
+
 # ===============================================================================
 # PERFORMANCE UTILITIES
 # ===============================================================================
 
+
 def measure_time(func: Callable) -> Callable:
     """
     Decorator to measure function execution time.
-    
+
     Args:
         func: Function to measure
-        
+
     Returns:
         Wrapped function that logs execution time
     """
+
     @wraps(func)
     async def async_wrapper(*args, **kwargs):
         start_time = time.time()
         try:
             result = await func(*args, **kwargs)
             execution_time = time.time() - start_time
             logger.debug(
-                f"Function {func.__name__} executed",
-                execution_time=f"{execution_time:.3f}s"
+                f"Function {func.__name__} executed", execution_time=f"{execution_time:.3f}s"
             )
             return result
         except Exception as e:
             execution_time = time.time() - start_time
             logger.error(
                 f"Function {func.__name__} failed",
                 execution_time=f"{execution_time:.3f}s",
-                error=str(e)
+                error=str(e),
             )
             raise
-    
+
     @wraps(func)
     def sync_wrapper(*args, **kwargs):
         start_time = time.time()
         try:
             result = func(*args, **kwargs)
             execution_time = time.time() - start_time
             logger.debug(
-                f"Function {func.__name__} executed",
-                execution_time=f"{execution_time:.3f}s"
+                f"Function {func.__name__} executed", execution_time=f"{execution_time:.3f}s"
             )
             return result
         except Exception as e:
             execution_time = time.time() - start_time
             logger.error(
                 f"Function {func.__name__} failed",
                 execution_time=f"{execution_time:.3f}s",
-                error=str(e)
+                error=str(e),
             )
             raise
-    
+
     if asyncio.iscoroutinefunction(func):
         return async_wrapper
     return sync_wrapper
 
+
 # ===============================================================================
 # FORMAT UTILITIES
 # ===============================================================================
 
+
 def format_file_size(size_bytes: int) -> str:
     """
     Format file size in human-readable format.
-    
+
     Args:
         size_bytes: Size in bytes
-        
+
     Returns:
         Formatted size string (e.g., "1.23 MB")
     """
     if size_bytes == 0:
         return "0 B"
-    
-    units = ['B', 'KB', 'MB', 'GB', 'TB']
+
+    units = ["B", "KB", "MB", "GB", "TB"]
     unit_index = 0
     size = float(size_bytes)
-    
+
     while size >= 1024 and unit_index < len(units) - 1:
         size /= 1024
         unit_index += 1
-    
+
     if unit_index == 0:
         return f"{int(size)} {units[unit_index]}"
     return f"{size:.2f} {units[unit_index]}"
 
 
 def format_duration(seconds: float) -> str:
     """
     Format duration in human-readable format.
-    
+
     Args:
         seconds: Duration in seconds
-        
+
     Returns:
         Formatted duration string (e.g., "2.5h", "45m")
     """
     if seconds < 60:
         return f"{seconds:.2f}s"
@@ -463,93 +451,85 @@
         return f"{hours:.1f}h"
     else:
         days = seconds / 86400
         return f"{days:.1f}d"
 
+
 # ===============================================================================
 # VALIDATION UTILITIES
 # ===============================================================================
 
+
 def validate_config(config: Dict[str, Any], required_keys: List[str]) -> bool:
     """
     Validate configuration has required keys.
-    
+
     Args:
         config: Configuration dictionary
         required_keys: List of required key names
-        
+
     Returns:
         True if valid, False otherwise
     """
     missing_keys = [key for key in required_keys if key not in config]
-    
+
     if missing_keys:
-        logger.error(
-            "Configuration validation failed",
-            missing_keys=missing_keys
-        )
+        logger.error("Configuration validation failed", missing_keys=missing_keys)
         return False
-    
+
     return True
 
 
 def sanitize_string(text: str, max_length: Optional[int] = None) -> str:
     """
     Sanitize string by removing dangerous characters.
-    
+
     Args:
         text: Text to sanitize
         max_length: Maximum length to truncate to
-        
+
     Returns:
         Sanitized string
     """
     # Remove null bytes and control characters
-    sanitized = ''.join(char for char in text if ord(char) >= 32 or char in '\n\r\t')
-    
+    sanitized = "".join(char for char in text if ord(char) >= 32 or char in "\n\r\t")
+
     if max_length and len(sanitized) > max_length:
         sanitized = sanitized[:max_length]
-    
+
     return sanitized
+
 
 # ===============================================================================
 # EXPORTS
 # ===============================================================================
 
 __all__ = [
     # ID generation
-    'generate_unique_id',
-    'generate_cycle_id',
-    'generate_task_id',
-    
+    "generate_unique_id",
+    "generate_cycle_id",
+    "generate_task_id",
     # Timestamp utilities
-    'get_utc_timestamp',
-    'format_timestamp',
-    'timestamp_to_seconds',
-    
+    "get_utc_timestamp",
+    "format_timestamp",
+    "timestamp_to_seconds",
     # Hashing
-    'calculate_hash',
-    'verify_integrity',
-    
+    "calculate_hash",
+    "verify_integrity",
     # JSON utilities
-    'safe_json_loads',
-    'safe_json_dumps',
-    
+    "safe_json_loads",
+    "safe_json_dumps",
     # Dictionary utilities
-    'deep_merge_dict',
-    'flatten_dict',
-    
+    "deep_merge_dict",
+    "flatten_dict",
     # Async utilities
-    'retry_async_operation',
-    'run_with_timeout',
-    
+    "retry_async_operation",
+    "run_with_timeout",
     # Performance
-    'measure_time',
-    
+    "measure_time",
     # Formatting
-    'format_file_size',
-    'format_duration',
-    
+    "format_file_size",
+    "format_duration",
     # Validation
-    'validate_config',
-    'sanitize_string',
+    "validate_config",
+    "sanitize_string",
 ]
would reformat /home/runner/work/ymera_y/ymera_y/core_engine_utils.py
error: cannot format /home/runner/work/ymera_y/ymera_y/data_pipeline.etl_processor.py: Cannot parse for target version Python 3.12: 391:0: class MLOpsPipeline:
--- /home/runner/work/ymera_y/ymera_y/database.py	2025-10-19 22:47:02.799797+00:00
+++ /home/runner/work/ymera_y/ymera_y/database.py	2025-10-19 23:09:07.178279+00:00
@@ -1,82 +1,79 @@
-
 import asyncio
 from contextlib import asynccontextmanager
 from typing import Dict, List, Any, Optional
 import asyncpg
 import json
 from datetime import datetime
 import logging
 
 logger = logging.getLogger(__name__)
 
+
 class Database:
     def __init__(self, database_url: str):
         self.database_url = database_url
         self.pool = None
-    
+
     async def initialize(self):
         """Initialize database connection pool"""
         try:
             self.pool = await asyncpg.create_pool(
                 self.database_url.replace("+asyncpg", ""),
                 min_size=5,
                 max_size=20,
                 command_timeout=60,
-                server_settings={
-                    'application_name': 'ymera_agent'
-                }
+                server_settings={"application_name": "ymera_agent"},
             )
             logger.info("Database pool initialized")
         except Exception as e:
             logger.error(f"Database initialization failed: {e}")
             raise
-    
+
     @asynccontextmanager
     async def get_connection(self):
         """Get database connection from pool"""
         if not self.pool:
             raise RuntimeError("Database not initialized")
-        
+
         async with self.pool.acquire() as connection:
             yield connection
-    
+
     async def execute_query(self, query: str, *args) -> List[Dict]:
         """Execute query and return results"""
         async with self.get_connection() as conn:
             try:
                 rows = await conn.fetch(query, *args)
                 return [dict(row) for row in rows]
             except Exception as e:
                 logger.error(f"Query execution failed: {query[:100]}... Error: {e}")
                 raise
-    
+
     async def execute_single(self, query: str, *args) -> Optional[Dict]:
         """Execute query and return single result"""
         results = await self.execute_query(query, *args)
         return results[0] if results else None
-    
+
     async def execute_command(self, command: str, *args) -> str:
         """Execute command (INSERT, UPDATE, DELETE)"""
         async with self.get_connection() as conn:
             try:
                 result = await conn.execute(command, *args)
                 return result
             except Exception as e:
                 logger.error(f"Command execution failed: {command[:100]}... Error: {e}")
                 raise
-    
+
     async def health_check(self) -> bool:
         """Check database health"""
         try:
             await self.execute_query("SELECT 1")
             return True
         except Exception as e:
             logger.error(f"Database health check failed: {e}")
             return False
-    
+
     async def close(self):
         """Close database connections"""
         if self.pool:
             await self.pool.close()
             logger.info("Database connections closed")
-
would reformat /home/runner/work/ymera_y/ymera_y/database.py
--- /home/runner/work/ymera_y/ymera_y/data_flow_validator.py	2025-10-19 22:47:02.799797+00:00
+++ /home/runner/work/ymera_y/ymera_y/data_flow_validator.py	2025-10-19 23:09:07.313053+00:00
@@ -15,335 +15,354 @@
 import base64
 from dataclasses import dataclass
 
 logger = logging.getLogger(__name__)
 
+
 class DataValidationLevel(str, Enum):
     """Validation levels for data flows"""
+
     BASIC = "basic"
     STANDARD = "standard"
     STRICT = "strict"
     PARANOID = "paranoid"
 
+
 class DataClassification(str, Enum):
     """Data classification levels"""
+
     PUBLIC = "public"
     INTERNAL = "internal"
     CONFIDENTIAL = "confidential"
     RESTRICTED = "restricted"
 
+
 @dataclass
 class ValidationRule:
     """Data validation rule definition"""
+
     name: str
     pattern: str
     description: str
     validation_type: str  # regex, format, schema, semantic
     severity: str  # low, medium, high, critical
     enabled: bool = True
 
+
 class DataFlowValidator:
     """Advanced data flow validation and security"""
-    
+
     def __init__(self, manager):
         self.manager = manager
         self.validation_rules = self._load_validation_rules()
         self.sensitive_data_patterns = self._load_sensitive_patterns()
         self.schema_validators = {}
         self.validation_callbacks = {}
-        
+
         # Initialize metrics
         self.validation_count = 0
         self.validation_failures = 0
         self.validation_time_total = 0
-    
+
     def _load_validation_rules(self) -> Dict[str, List[ValidationRule]]:
         """Load validation rules for different flow types"""
         rules = {}
-        
+
         # Agent report validation rules
         rules["agent.report"] = [
             ValidationRule(
                 name="required_fields",
-                pattern=r'(status|health)',
+                pattern=r"(status|health)",
                 description="Report must contain status and health fields",
                 validation_type="regex",
-                severity="high"
+                severity="high",
             ),
             ValidationRule(
                 name="valid_status",
-                pattern=r'^(active|inactive|busy|error|suspended|isolated)$',
+                pattern=r"^(active|inactive|busy|error|suspended|isolated)$",
                 description="Status must be a valid status value",
                 validation_type="regex",
-                severity="medium"
-            )
+                severity="medium",
+            ),
         ]
-        
+
         # Task result validation rules
         rules["task.result"] = [
             ValidationRule(
                 name="max_result_size",
-                pattern=r'^.{1,1048576}$',  # Max 1MB result
+                pattern=r"^.{1,1048576}$",  # Max 1MB result
                 description="Task result exceeds maximum size",
                 validation_type="regex",
-                severity="medium"
+                severity="medium",
             )
         ]
-        
+
         # Task creation validation rules
         rules["task.create"] = [
             ValidationRule(
                 name="task_type_whitelist",
-                pattern=r'^[a-zA-Z0-9_\-\.]+$',
+                pattern=r"^[a-zA-Z0-9_\-\.]+$",
                 description="Task type contains invalid characters",
                 validation_type="regex",
-                severity="high"
+                severity="high",
             )
         ]
-        
+
         return rules
-    
+
     def _load_sensitive_patterns(self) -> Dict[str, re.Pattern]:
         """Load patterns for sensitive data detection"""
         return {
-            "credit_card": re.compile(r'\b(?:\d{4}[- ]?){3}\d{4}\b'),
-            "ssn": re.compile(r'\b\d{3}-\d{2}-\d{4}\b'),
-            "api_key": re.compile(r'\b[A-Za-z0-9_\-]{20,}\b'),
+            "credit_card": re.compile(r"\b(?:\d{4}[- ]?){3}\d{4}\b"),
+            "ssn": re.compile(r"\b\d{3}-\d{2}-\d{4}\b"),
+            "api_key": re.compile(r"\b[A-Za-z0-9_\-]{20,}\b"),
             "password": re.compile(r'password["\s:=]+[^"\s]+'),
-            "jwt": re.compile(r'eyJ[a-zA-Z0-9_-]+\.eyJ[a-zA-Z0-9_-]+\.[a-zA-Z0-9_-]+')
+            "jwt": re.compile(r"eyJ[a-zA-Z0-9_-]+\.eyJ[a-zA-Z0-9_-]+\.[a-zA-Z0-9_-]+"),
         }
-    
-    async def validate_data_flow(self, flow_type: str, data: Dict[str, Any], 
-                               context: Dict[str, Any] = None) -> Tuple[bool, str]:
+
+    async def validate_data_flow(
+        self, flow_type: str, data: Dict[str, Any], context: Dict[str, Any] = None
+    ) -> Tuple[bool, str]:
         """
         Validate data flow with comprehensive security checks
         Returns: (valid, message)
         """
         start_time = datetime.utcnow()
         context = context or {}
         validation_level = context.get("validation_level", DataValidationLevel.STANDARD)
-        
+
         try:
             # Validation result (valid, message)
             result = (True, "Data validated successfully")
-            
+
             # 1. Apply basic validation
             basic_validation = self._perform_basic_validation(flow_type, data, context)
             if not basic_validation[0]:
                 return basic_validation
-            
+
             # 2. Apply schema validation
             schema_validation = await self._perform_schema_validation(flow_type, data, context)
             if not schema_validation[0]:
                 return schema_validation
-            
+
             # 3. Perform sensitive data check
             if validation_level in [DataValidationLevel.STRICT, DataValidationLevel.PARANOID]:
                 sensitive_check = self._check_sensitive_data(data)
                 if sensitive_check[0]:  # Contains sensitive data
                     result = (False, f"Sensitive data detected: {sensitive_check[1]}")
                     return result
-            
+
             # 4. Perform custom validation rules
             rule_validation = self._validate_against_rules(flow_type, data)
             if not rule_validation[0]:
                 return rule_validation
-            
+
             # 5. Apply custom validation callbacks if registered
             if flow_type in self.validation_callbacks:
                 for callback in self.validation_callbacks[flow_type]:
                     callback_result = await callback(data, context)
                     if not callback_result[0]:
                         return callback_result
-            
+
             # Update metrics
             self.validation_count += 1
             duration = (datetime.utcnow() - start_time).total_seconds()
             self.validation_time_total += duration
-            
+
             # 6. Log data access for auditing
             await self._log_data_access(flow_type, data, context, True)
-            
+
             return result
-            
+
         except Exception as e:
             logger.error(f"Data validation error: {e}")
             self.validation_failures += 1
-            
+
             # Log failed validation
             await self._log_data_access(flow_type, data, context, False, str(e))
-            
+
             return (False, f"Validation error: {str(e)}")
-    
-    def _perform_basic_validation(self, flow_type: str, data: Dict[str, Any], 
-                                context: Dict[str, Any]) -> Tuple[bool, str]:
+
+    def _perform_basic_validation(
+        self, flow_type: str, data: Dict[str, Any], context: Dict[str, Any]
+    ) -> Tuple[bool, str]:
         """Perform basic data validation"""
         # Check for null/empty data
         if not data:
             return (False, "Data cannot be empty")
-        
+
         # Check for payload size limits
         data_size = len(json.dumps(data))
         if data_size > 10 * 1024 * 1024:  # 10MB limit
             return (False, f"Data exceeds maximum size: {data_size} bytes")
-        
+
         # Check for malicious input patterns
         if self._contains_malicious_patterns(data):
             return (False, "Potential security threat detected in data")
-        
+
         return (True, "Basic validation passed")
-    
-    async def _perform_schema_validation(self, flow_type: str, data: Dict[str, Any],
-                                      context: Dict[str, Any]) -> Tuple[bool, str]:
+
+    async def _perform_schema_validation(
+        self, flow_type: str, data: Dict[str, Any], context: Dict[str, Any]
+    ) -> Tuple[bool, str]:
         """Validate data against defined schemas"""
         if flow_type in self.schema_validators:
             validator = self.schema_validators[flow_type]
             try:
                 result = validator(data)
                 if not result:
                     return (False, f"Schema validation failed for {flow_type}")
             except Exception as e:
                 return (False, f"Schema validation error: {str(e)}")
-        
+
         return (True, "Schema validation passed")
-    
+
     def _validate_against_rules(self, flow_type: str, data: Dict[str, Any]) -> Tuple[bool, str]:
         """Validate data against defined rules for flow type"""
         if flow_type not in self.validation_rules:
             return (True, "No specific rules for this flow type")
-        
+
         rules = self.validation_rules[flow_type]
         data_str = json.dumps(data)
-        
+
         for rule in rules:
             if not rule.enabled:
                 continue
-                
+
             if rule.validation_type == "regex":
                 pattern = re.compile(rule.pattern)
-                
+
                 # For required fields, check if fields exist
                 if rule.name == "required_fields":
                     missing_fields = []
                     for field in pattern.findall(rule.pattern):
                         if field not in data:
                             missing_fields.append(field)
-                    
+
                     if missing_fields:
                         return (False, f"Required fields missing: {', '.join(missing_fields)}")
-                
+
                 # For value validation, check if values match pattern
                 elif "valid_" in rule.name:
                     field_name = rule.name.replace("valid_", "")
                     if field_name in data:
                         value = str(data[field_name])
                         if not pattern.match(value):
                             return (False, rule.description)
-        
+
         return (True, "Rule validation passed")
-    
+
     def _check_sensitive_data(self, data: Dict[str, Any]) -> Tuple[bool, str]:
         """Check for sensitive data patterns"""
         data_str = json.dumps(data)
-        
+
         for pattern_name, pattern in self.sensitive_data_patterns.items():
             if pattern.search(data_str):
                 return (True, pattern_name)
-        
+
         return (False, "")
-    
+
     def _contains_malicious_patterns(self, data: Dict[str, Any]) -> bool:
         """Check for potentially malicious patterns"""
         data_str = json.dumps(data)
-        
+
         # Check for common attack patterns
         malicious_patterns = [
             # SQL injection
             r"(?i)'\s*OR\s*'1'='1",
             r"(?i);\s*DROP\s+TABLE",
             r"(?i);\s*DELETE\s+FROM",
-            
             # Command injection
             r"(?i)`.*`",
             r"(?i)\|\s*.*",
             r"(?i);\s*.*",
-            
             # XSS
             r"(?i)<script.*>",
             r"(?i)javascript:",
             r"(?i)onerror=",
-            
             # Path traversal
-            r"(?i)\.\./"
+            r"(?i)\.\./",
         ]
-        
+
         for pattern in malicious_patterns:
             if re.search(pattern, data_str):
                 return True
-        
+
         return False
-    
-    async def _log_data_access(self, flow_type: str, data: Dict[str, Any], 
-                             context: Dict[str, Any], success: bool, 
-                             error: str = None) -> None:
+
+    async def _log_data_access(
+        self,
+        flow_type: str,
+        data: Dict[str, Any],
+        context: Dict[str, Any],
+        success: bool,
+        error: str = None,
+    ) -> None:
         """Log data access for auditing"""
         try:
             # Create data access log entry
             log_entry = {
                 "flow_type": flow_type,
                 "timestamp": datetime.utcnow().isoformat(),
                 "context": {
                     "user_id": context.get("user_id"),
                     "agent_id": context.get("agent_id"),
-                    "ip_address": context.get("ip_address")
+                    "ip_address": context.get("ip_address"),
                 },
                 "success": success,
                 "data_size": len(json.dumps(data)),
-                "error": error
+                "error": error,
             }
-            
+
             # Store in database
             async with self.manager.get_db_session() as session:
                 from models import DataAccessLog
+
                 log = DataAccessLog(
                     user_id=context.get("user_id"),
                     agent_id=context.get("agent_id"),
                     resource_type=flow_type,
                     resource_id=context.get("resource_id", str(hash(json.dumps(data)))),
                     operation="read",
                     success=success,
                     details=log_entry,
                     ip_address=context.get("ip_address"),
-                    data_size=log_entry["data_size"]
+                    data_size=log_entry["data_size"],
                 )
                 session.add(log)
                 await session.commit()
-                
+
         except Exception as e:
             logger.error(f"Failed to log data access: {e}")
-    
+
     def register_schema_validator(self, flow_type: str, validator: Callable) -> None:
         """Register schema validator for flow type"""
         self.schema_validators[flow_type] = validator
-    
+
     def register_validation_callback(self, flow_type: str, callback: Callable) -> None:
         """Register custom validation callback for flow type"""
         if flow_type not in self.validation_callbacks:
             self.validation_callbacks[flow_type] = []
         self.validation_callbacks[flow_type].append(callback)
 
     async def classify_data(self, data: Dict[str, Any]) -> DataClassification:
         """Classify data sensitivity level"""
         data_str = json.dumps(data)
-        
+
         # Check for most restricted patterns first
-        if any(pattern.search(data_str) for name, pattern in 
-               self.sensitive_data_patterns.items() if name in ['ssn', 'credit_card']):
+        if any(
+            pattern.search(data_str)
+            for name, pattern in self.sensitive_data_patterns.items()
+            if name in ["ssn", "credit_card"]
+        ):
             return DataClassification.RESTRICTED
-        
+
         # Check for confidential patterns
-        if any(pattern.search(data_str) for name, pattern in 
-               self.sensitive_data_patterns.items() if name in ['api_key', 'password', 'jwt']):
+        if any(
+            pattern.search(data_str)
+            for name, pattern in self.sensitive_data_patterns.items()
+            if name in ["api_key", "password", "jwt"]
+        ):
             return DataClassification.CONFIDENTIAL
-        
+
         # Default to internal
-        return DataClassification.INTERNAL
\ No newline at end of file
+        return DataClassification.INTERNAL
would reformat /home/runner/work/ymera_y/ymera_y/data_flow_validator.py
--- /home/runner/work/ymera_y/ymera_y/database_monitor.py	2025-10-19 22:47:02.799797+00:00
+++ /home/runner/work/ymera_y/ymera_y/database_monitor.py	2025-10-19 23:09:08.198383+00:00
@@ -16,539 +16,505 @@
 
 
 @dataclass
 class HealthCheckResult:
     """Health check result"""
+
     timestamp: datetime
     status: str  # healthy, degraded, unhealthy
     checks: Dict[str, Any]
     metrics: Dict[str, Any]
     alerts: List[str]
-    
+
     def to_dict(self) -> Dict[str, Any]:
         data = asdict(self)
-        data['timestamp'] = self.timestamp.isoformat()
+        data["timestamp"] = self.timestamp.isoformat()
         return data
 
 
 @dataclass
 class PerformanceMetrics:
     """Performance metrics"""
+
     timestamp: datetime
     query_count: int
     avg_query_time_ms: float
     slow_queries: int
     active_connections: int
     pool_size: int
     pool_overflow: int
     cache_hit_rate: float
-    
+
     def to_dict(self) -> Dict[str, Any]:
         data = asdict(self)
-        data['timestamp'] = self.timestamp.isoformat()
+        data["timestamp"] = self.timestamp.isoformat()
         return data
 
 
 class DatabaseMonitor:
     """Database monitoring and health checks"""
-    
+
     def __init__(self):
         self.config = DatabaseConfig()
         self.metrics_history: List[PerformanceMetrics] = []
         self.health_history: List[HealthCheckResult] = []
-        
+
         # Alert thresholds
         self.thresholds = {
-            'slow_query_ms': 1000,
-            'pool_usage_percent': 80,
-            'connection_errors_percent': 10,
-            'response_time_ms': 500,
+            "slow_query_ms": 1000,
+            "pool_usage_percent": 80,
+            "connection_errors_percent": 10,
+            "response_time_ms": 500,
         }
-    
+
     async def comprehensive_health_check(self) -> HealthCheckResult:
         """Perform comprehensive health check"""
         checks = {}
         alerts = []
-        
+
         db_manager = await get_database_manager()
-        
+
         # 1. Database connectivity
-        checks['database_connectivity'] = await self._check_connectivity(db_manager)
-        if not checks['database_connectivity']['healthy']:
+        checks["database_connectivity"] = await self._check_connectivity(db_manager)
+        if not checks["database_connectivity"]["healthy"]:
             alerts.append("Database connectivity failed")
-        
+
         # 2. Connection pool status
-        checks['connection_pool'] = await self._check_connection_pool(db_manager)
-        if checks['connection_pool'].get('usage_percent', 0) > self.thresholds['pool_usage_percent']:
-            alerts.append(f"Connection pool usage high: {checks['connection_pool']['usage_percent']:.1f}%")
-        
+        checks["connection_pool"] = await self._check_connection_pool(db_manager)
+        if (
+            checks["connection_pool"].get("usage_percent", 0)
+            > self.thresholds["pool_usage_percent"]
+        ):
+            alerts.append(
+                f"Connection pool usage high: {checks['connection_pool']['usage_percent']:.1f}%"
+            )
+
         # 3. Query performance
-        checks['query_performance'] = await self._check_query_performance(db_manager)
-        if checks['query_performance'].get('slow_queries', 0) > 0:
+        checks["query_performance"] = await self._check_query_performance(db_manager)
+        if checks["query_performance"].get("slow_queries", 0) > 0:
             alerts.append(f"Slow queries detected: {checks['query_performance']['slow_queries']}")
-        
+
         # 4. Table statistics
-        checks['table_statistics'] = await self._check_table_statistics(db_manager)
-        
+        checks["table_statistics"] = await self._check_table_statistics(db_manager)
+
         # 5. Disk space (if applicable)
-        checks['disk_space'] = await self._check_disk_space()
-        if checks['disk_space'].get('usage_percent', 0) > 90:
+        checks["disk_space"] = await self._check_disk_space()
+        if checks["disk_space"].get("usage_percent", 0) > 90:
             alerts.append(f"Disk space critical: {checks['disk_space']['usage_percent']:.1f}% used")
-        
+
         # 6. Replication status (if applicable)
-        checks['replication'] = await self._check_replication(db_manager)
-        
+        checks["replication"] = await self._check_replication(db_manager)
+
         # Determine overall status
-        if any(not check.get('healthy', True) for check in checks.values()):
+        if any(not check.get("healthy", True) for check in checks.values()):
             status = "unhealthy"
         elif alerts:
             status = "degraded"
         else:
             status = "healthy"
-        
+
         # Collect metrics
         metrics = await db_manager.get_statistics()
-        
+
         result = HealthCheckResult(
             timestamp=datetime.utcnow(),
             status=status,
             checks=checks,
             metrics=metrics,
-            alerts=alerts
+            alerts=alerts,
         )
-        
+
         self.health_history.append(result)
-        
+
         return result
-    
+
     async def _check_connectivity(self, db_manager) -> Dict[str, Any]:
         """Check database connectivity"""
         try:
             start_time = time.time()
             health = await db_manager.health_check()
             response_time_ms = (time.time() - start_time) * 1000
-            
+
             return {
-                'healthy': health.get('healthy', False),
-                'response_time_ms': response_time_ms,
-                'database_type': health.get('database_type'),
-                'timestamp': datetime.utcnow().isoformat()
+                "healthy": health.get("healthy", False),
+                "response_time_ms": response_time_ms,
+                "database_type": health.get("database_type"),
+                "timestamp": datetime.utcnow().isoformat(),
             }
         except Exception as e:
-            return {
-                'healthy': False,
-                'error': str(e),
-                'timestamp': datetime.utcnow().isoformat()
-            }
-    
+            return {"healthy": False, "error": str(e), "timestamp": datetime.utcnow().isoformat()}
+
     async def _check_connection_pool(self, db_manager) -> Dict[str, Any]:
         """Check connection pool status"""
         try:
             health = await db_manager.health_check()
-            pool_stats = health.get('pool_stats', {})
-            
+            pool_stats = health.get("pool_stats", {})
+
             if pool_stats:
-                pool_size = pool_stats.get('pool_size', 0)
-                checked_out = pool_stats.get('checked_out', 0)
+                pool_size = pool_stats.get("pool_size", 0)
+                checked_out = pool_stats.get("checked_out", 0)
                 usage_percent = (checked_out / pool_size * 100) if pool_size > 0 else 0
-                
+
                 return {
-                    'healthy': usage_percent < self.thresholds['pool_usage_percent'],
-                    'pool_size': pool_size,
-                    'checked_in': pool_stats.get('checked_in', 0),
-                    'checked_out': checked_out,
-                    'overflow': pool_stats.get('overflow', 0),
-                    'usage_percent': usage_percent
+                    "healthy": usage_percent < self.thresholds["pool_usage_percent"],
+                    "pool_size": pool_size,
+                    "checked_in": pool_stats.get("checked_in", 0),
+                    "checked_out": checked_out,
+                    "overflow": pool_stats.get("overflow", 0),
+                    "usage_percent": usage_percent,
                 }
             else:
-                return {
-                    'healthy': True,
-                    'message': 'Pool statistics not available (SQLite)'
-                }
+                return {"healthy": True, "message": "Pool statistics not available (SQLite)"}
         except Exception as e:
-            return {
-                'healthy': False,
-                'error': str(e)
-            }
-    
+            return {"healthy": False, "error": str(e)}
+
     async def _check_query_performance(self, db_manager) -> Dict[str, Any]:
         """Check query performance"""
         try:
             async with db_manager.get_session() as session:
                 # Test query performance
                 queries = [
                     ("SELECT COUNT(*) FROM users", "users_count"),
                     ("SELECT COUNT(*) FROM projects", "projects_count"),
                     ("SELECT COUNT(*) FROM tasks", "tasks_count"),
                 ]
-                
+
                 query_times = []
                 slow_queries = 0
-                
+
                 for query, name in queries:
                     start_time = time.time()
                     await session.execute(text(query))
                     query_time_ms = (time.time() - start_time) * 1000
                     query_times.append(query_time_ms)
-                    
-                    if query_time_ms > self.thresholds['slow_query_ms']:
+
+                    if query_time_ms > self.thresholds["slow_query_ms"]:
                         slow_queries += 1
-                
+
                 avg_query_time_ms = sum(query_times) / len(query_times) if query_times else 0
-                
+
                 return {
-                    'healthy': slow_queries == 0,
-                    'avg_query_time_ms': avg_query_time_ms,
-                    'max_query_time_ms': max(query_times) if query_times else 0,
-                    'slow_queries': slow_queries,
-                    'queries_tested': len(queries)
+                    "healthy": slow_queries == 0,
+                    "avg_query_time_ms": avg_query_time_ms,
+                    "max_query_time_ms": max(query_times) if query_times else 0,
+                    "slow_queries": slow_queries,
+                    "queries_tested": len(queries),
                 }
         except Exception as e:
-            return {
-                'healthy': False,
-                'error': str(e)
-            }
-    
+            return {"healthy": False, "error": str(e)}
+
     async def _check_table_statistics(self, db_manager) -> Dict[str, Any]:
         """Check table statistics"""
         try:
             stats = await db_manager.get_statistics()
-            
+
             return {
-                'healthy': True,
-                'users_count': stats.get('users_count', 0),
-                'projects_count': stats.get('projects_count', 0),
-                'agents_count': stats.get('agents_count', 0),
-                'tasks_count': stats.get('tasks_count', 0),
-                'files_count': stats.get('files_count', 0),
-                'audit_logs_count': stats.get('audit_logs_count', 0),
-                'recent_activity_24h': stats.get('recent_activity_24h', 0)
+                "healthy": True,
+                "users_count": stats.get("users_count", 0),
+                "projects_count": stats.get("projects_count", 0),
+                "agents_count": stats.get("agents_count", 0),
+                "tasks_count": stats.get("tasks_count", 0),
+                "files_count": stats.get("files_count", 0),
+                "audit_logs_count": stats.get("audit_logs_count", 0),
+                "recent_activity_24h": stats.get("recent_activity_24h", 0),
             }
         except Exception as e:
-            return {
-                'healthy': False,
-                'error': str(e)
-            }
-    
+            return {"healthy": False, "error": str(e)}
+
     async def _check_disk_space(self) -> Dict[str, Any]:
         """Check disk space"""
         try:
             if self.config.is_sqlite:
                 # Check SQLite database file size
-                db_path = self.config.database_url.replace("sqlite+aiosqlite:///", "").replace("./", "")
+                db_path = self.config.database_url.replace("sqlite+aiosqlite:///", "").replace(
+                    "./", ""
+                )
                 file_path = Path(db_path)
-                
+
                 if file_path.exists():
                     import psutil
-                    
+
                     disk_usage = psutil.disk_usage(file_path.parent)
-                    
+
                     return {
-                        'healthy': disk_usage.percent < 90,
-                        'total_gb': disk_usage.total / (1024**3),
-                        'used_gb': disk_usage.used / (1024**3),
-                        'free_gb': disk_usage.free / (1024**3),
-                        'usage_percent': disk_usage.percent,
-                        'database_size_mb': file_path.stat().st_size / (1024**2)
+                        "healthy": disk_usage.percent < 90,
+                        "total_gb": disk_usage.total / (1024**3),
+                        "used_gb": disk_usage.used / (1024**3),
+                        "free_gb": disk_usage.free / (1024**3),
+                        "usage_percent": disk_usage.percent,
+                        "database_size_mb": file_path.stat().st_size / (1024**2),
                     }
-            
-            return {
-                'healthy': True,
-                'message': 'Disk space check not applicable'
-            }
+
+            return {"healthy": True, "message": "Disk space check not applicable"}
         except Exception as e:
-            return {
-                'healthy': True,
-                'error': str(e),
-                'message': 'Could not check disk space'
-            }
-    
+            return {"healthy": True, "error": str(e), "message": "Could not check disk space"}
+
     async def _check_replication(self, db_manager) -> Dict[str, Any]:
         """Check replication status (PostgreSQL)"""
         try:
             if self.config.is_postgres:
                 async with db_manager.get_session() as session:
-                    result = await session.execute(text(
-                        "SELECT count(*) FROM pg_stat_replication"
-                    ))
+                    result = await session.execute(text("SELECT count(*) FROM pg_stat_replication"))
                     replicas = result.scalar()
-                    
-                    return {
-                        'healthy': True,
-                        'replicas_count': replicas,
-                        'enabled': replicas > 0
-                    }
-            
-            return {
-                'healthy': True,
-                'enabled': False,
-                'message': 'Replication not applicable'
-            }
+
+                    return {"healthy": True, "replicas_count": replicas, "enabled": replicas > 0}
+
+            return {"healthy": True, "enabled": False, "message": "Replication not applicable"}
         except Exception as e:
-            return {
-                'healthy': True,
-                'enabled': False,
-                'error': str(e)
-            }
-    
+            return {"healthy": True, "enabled": False, "error": str(e)}
+
     async def collect_performance_metrics(self) -> PerformanceMetrics:
         """Collect performance metrics"""
         db_manager = await get_database_manager()
-        
+
         async with db_manager.get_session() as session:
             # Simulate metrics collection (in production, use actual query stats)
             health = await db_manager.health_check()
-            pool_stats = health.get('pool_stats', {})
-            
+            pool_stats = health.get("pool_stats", {})
+
             metrics = PerformanceMetrics(
                 timestamp=datetime.utcnow(),
                 query_count=0,  # Would come from query log
                 avg_query_time_ms=0.0,  # Would come from query log
                 slow_queries=0,  # Would come from query log
-                active_connections=pool_stats.get('checked_out', 0),
-                pool_size=pool_stats.get('pool_size', 0),
-                pool_overflow=pool_stats.get('overflow', 0),
-                cache_hit_rate=0.0  # Would come from cache stats
+                active_connections=pool_stats.get("checked_out", 0),
+                pool_size=pool_stats.get("pool_size", 0),
+                pool_overflow=pool_stats.get("overflow", 0),
+                cache_hit_rate=0.0,  # Would come from cache stats
             )
-            
+
             self.metrics_history.append(metrics)
-            
+
             return metrics
-    
+
     async def generate_report(self, hours: int = 24) -> Dict[str, Any]:
         """Generate monitoring report"""
         cutoff_time = datetime.utcnow() - timedelta(hours=hours)
-        
+
         # Filter recent health checks
-        recent_health = [
-            h for h in self.health_history
-            if h.timestamp > cutoff_time
-        ]
-        
+        recent_health = [h for h in self.health_history if h.timestamp > cutoff_time]
+
         # Calculate health statistics
         if recent_health:
             healthy_count = sum(1 for h in recent_health if h.status == "healthy")
             degraded_count = sum(1 for h in recent_health if h.status == "degraded")
             unhealthy_count = sum(1 for h in recent_health if h.status == "unhealthy")
-            
+
             uptime_percent = (healthy_count / len(recent_health)) * 100
         else:
             healthy_count = degraded_count = unhealthy_count = 0
             uptime_percent = 0.0
-        
+
         # Get current statistics
         db_manager = await get_database_manager()
         current_stats = await db_manager.get_statistics()
-        
+
         report = {
-            'report_period_hours': hours,
-            'generated_at': datetime.utcnow().isoformat(),
-            'health_summary': {
-                'total_checks': len(recent_health),
-                'healthy_count': healthy_count,
-                'degraded_count': degraded_count,
-                'unhealthy_count': unhealthy_count,
-                'uptime_percent': uptime_percent
+            "report_period_hours": hours,
+            "generated_at": datetime.utcnow().isoformat(),
+            "health_summary": {
+                "total_checks": len(recent_health),
+                "healthy_count": healthy_count,
+                "degraded_count": degraded_count,
+                "unhealthy_count": unhealthy_count,
+                "uptime_percent": uptime_percent,
             },
-            'current_statistics': current_stats,
-            'alerts': [
-                alert
-                for health in recent_health[-10:]  # Last 10 checks
-                for alert in health.alerts
+            "current_statistics": current_stats,
+            "alerts": [
+                alert for health in recent_health[-10:] for alert in health.alerts  # Last 10 checks
             ],
-            'recommendations': self._generate_recommendations(recent_health)
+            "recommendations": self._generate_recommendations(recent_health),
         }
-        
+
         return report
-    
+
     def _generate_recommendations(self, health_checks: List[HealthCheckResult]) -> List[str]:
         """Generate recommendations based on health checks"""
         recommendations = []
-        
+
         if not health_checks:
             return recommendations
-        
+
         # Check for recurring issues
         pool_warnings = sum(
-            1 for h in health_checks
-            if any('Connection pool' in alert for alert in h.alerts)
+            1 for h in health_checks if any("Connection pool" in alert for alert in h.alerts)
         )
-        
+
         if pool_warnings > len(health_checks) * 0.3:
             recommendations.append(
                 "Consider increasing connection pool size (DB_POOL_SIZE environment variable)"
             )
-        
+
         slow_query_warnings = sum(
-            1 for h in health_checks
-            if any('Slow queries' in alert for alert in h.alerts)
+            1 for h in health_checks if any("Slow queries" in alert for alert in h.alerts)
         )
-        
+
         if slow_query_warnings > 0:
-            recommendations.append(
-                "Review and optimize slow queries, consider adding indexes"
-            )
-        
+            recommendations.append("Review and optimize slow queries, consider adding indexes")
+
         disk_warnings = sum(
-            1 for h in health_checks
-            if any('Disk space' in alert for alert in h.alerts)
+            1 for h in health_checks if any("Disk space" in alert for alert in h.alerts)
         )
-        
+
         if disk_warnings > 0:
-            recommendations.append(
-                "Disk space running low - consider cleanup or expanding storage"
-            )
-        
+            recommendations.append("Disk space running low - consider cleanup or expanding storage")
+
         return recommendations
-    
+
     async def export_metrics(self, output_path: Path) -> None:
         """Export metrics to JSON file"""
         data = {
-            'exported_at': datetime.utcnow().isoformat(),
-            'health_checks': [h.to_dict() for h in self.health_history],
-            'performance_metrics': [m.to_dict() for m in self.metrics_history]
+            "exported_at": datetime.utcnow().isoformat(),
+            "health_checks": [h.to_dict() for h in self.health_history],
+            "performance_metrics": [m.to_dict() for m in self.metrics_history],
         }
-        
-        with open(output_path, 'w') as f:
+
+        with open(output_path, "w") as f:
             json.dump(data, f, indent=2)
-        
+
         print(f" Metrics exported to {output_path}")
 
 
 async def main():
     """CLI for database monitor"""
     import argparse
-    
+
     parser = argparse.ArgumentParser(description="YMERA Database Monitor")
-    subparsers = parser.add_subparsers(dest='command', help='Command to execute')
-    
+    subparsers = parser.add_subparsers(dest="command", help="Command to execute")
+
     # health command
-    subparsers.add_parser('health', help='Run comprehensive health check')
-    
+    subparsers.add_parser("health", help="Run comprehensive health check")
+
     # metrics command
-    subparsers.add_parser('metrics', help='Collect performance metrics')
-    
+    subparsers.add_parser("metrics", help="Collect performance metrics")
+
     # report command
-    report_parser = subparsers.add_parser('report', help='Generate monitoring report')
-    report_parser.add_argument('--hours', type=int, default=24, help='Report period in hours')
-    
+    report_parser = subparsers.add_parser("report", help="Generate monitoring report")
+    report_parser.add_argument("--hours", type=int, default=24, help="Report period in hours")
+
     # monitor command
-    monitor_parser = subparsers.add_parser('monitor', help='Continuous monitoring')
-    monitor_parser.add_argument('--interval', type=int, default=60, help='Check interval in seconds')
-    monitor_parser.add_argument('--duration', type=int, help='Monitoring duration in minutes')
-    
+    monitor_parser = subparsers.add_parser("monitor", help="Continuous monitoring")
+    monitor_parser.add_argument(
+        "--interval", type=int, default=60, help="Check interval in seconds"
+    )
+    monitor_parser.add_argument("--duration", type=int, help="Monitoring duration in minutes")
+
     # export command
-    export_parser = subparsers.add_parser('export', help='Export metrics to file')
-    export_parser.add_argument('output_file', help='Output file path')
-    
+    export_parser = subparsers.add_parser("export", help="Export metrics to file")
+    export_parser.add_argument("output_file", help="Output file path")
+
     args = parser.parse_args()
-    
+
     monitor = DatabaseMonitor()
-    
-    if args.command == 'health':
+
+    if args.command == "health":
         result = await monitor.comprehensive_health_check()
-        
-        print("\n" + "="*80)
+
+        print("\n" + "=" * 80)
         print("DATABASE HEALTH CHECK REPORT")
-        print("="*80)
+        print("=" * 80)
         print(f"\nStatus: {result.status.upper()}")
         print(f"Timestamp: {result.timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
-        
+
         print("\nChecks:")
         for check_name, check_data in result.checks.items():
-            status = "" if check_data.get('healthy', True) else ""
+            status = "" if check_data.get("healthy", True) else ""
             print(f"  {status} {check_name.replace('_', ' ').title()}")
-        
+
         if result.alerts:
             print("\nAlerts:")
             for alert in result.alerts:
                 print(f"   {alert}")
-        
+
         print("\nKey Metrics:")
         for key, value in result.metrics.items():
             print(f"  {key}: {value}")
-        
-        print("\n" + "="*80 + "\n")
-    
-    elif args.command == 'metrics':
+
+        print("\n" + "=" * 80 + "\n")
+
+    elif args.command == "metrics":
         metrics = await monitor.collect_performance_metrics()
-        
+
         print("\nPerformance Metrics:")
-        print("="*60)
+        print("=" * 60)
         print(f"Timestamp: {metrics.timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
         print(f"Active Connections: {metrics.active_connections}")
         print(f"Pool Size: {metrics.pool_size}")
         print(f"Pool Overflow: {metrics.pool_overflow}")
         print(f"Avg Query Time: {metrics.avg_query_time_ms:.2f}ms")
-        print("="*60 + "\n")
-    
-    elif args.command == 'report':
+        print("=" * 60 + "\n")
+
+    elif args.command == "report":
         report = await monitor.generate_report(args.hours)
-        
-        print("\n" + "="*80)
+
+        print("\n" + "=" * 80)
         print(f"MONITORING REPORT - Last {args.hours} hours")
-        print("="*80)
-        
+        print("=" * 80)
+
         print("\nHealth Summary:")
-        for key, value in report['health_summary'].items():
+        for key, value in report["health_summary"].items():
             print(f"  {key}: {value}")
-        
+
         print("\nCurrent Statistics:")
-        for key, value in report['current_statistics'].items():
+        for key, value in report["current_statistics"].items():
             print(f"  {key}: {value}")
-        
-        if report['alerts']:
+
+        if report["alerts"]:
             print("\nRecent Alerts:")
-            for alert in report['alerts'][:10]:  # Show last 10
+            for alert in report["alerts"][:10]:  # Show last 10
                 print(f"   {alert}")
-        
-        if report['recommendations']:
+
+        if report["recommendations"]:
             print("\nRecommendations:")
-            for rec in report['recommendations']:
+            for rec in report["recommendations"]:
                 print(f"   {rec}")
-        
-        print("\n" + "="*80 + "\n")
-    
-    elif args.command == 'monitor':
+
+        print("\n" + "=" * 80 + "\n")
+
+    elif args.command == "monitor":
         print(f"\nStarting continuous monitoring (interval: {args.interval}s)")
         print("Press Ctrl+C to stop\n")
-        
+
         try:
             start_time = time.time()
-            
+
             while True:
                 result = await monitor.comprehensive_health_check()
-                
-                timestamp = result.timestamp.strftime('%H:%M:%S')
-                status_icon = "" if result.status == "healthy" else "" if result.status == "degraded" else ""
-                
+
+                timestamp = result.timestamp.strftime("%H:%M:%S")
+                status_icon = (
+                    ""
+                    if result.status == "healthy"
+                    else "" if result.status == "degraded" else ""
+                )
+
                 print(f"[{timestamp}] {status_icon} Status: {result.status.upper()}", end="")
-                
+
                 if result.alerts:
                     print(f" - Alerts: {len(result.alerts)}")
                 else:
                     print()
-                
+
                 # Check duration
                 if args.duration:
                     elapsed_minutes = (time.time() - start_time) / 60
                     if elapsed_minutes >= args.duration:
                         break
-                
+
                 await asyncio.sleep(args.interval)
-                
+
         except KeyboardInterrupt:
             print("\n\nMonitoring stopped")
-            
+
             # Generate final report
             report = await monitor.generate_report(hours=24)
             print(f"\nTotal checks performed: {report['health_summary']['total_checks']}")
             print(f"Uptime: {report['health_summary']['uptime_percent']:.2f}%\n")
-    
-    elif args.command == 'export':
+
+    elif args.command == "export":
         await monitor.export_metrics(Path(args.output_file))
-    
+
     else:
         parser.print_help()
 
 
 if __name__ == "__main__":
would reformat /home/runner/work/ymera_y/ymera_y/database_monitor.py
--- /home/runner/work/ymera_y/ymera_y/database_wrapper.py	2025-10-19 22:47:02.799797+00:00
+++ /home/runner/work/ymera_y/ymera_y/database_wrapper.py	2025-10-19 23:09:08.569024+00:00
@@ -14,11 +14,11 @@
 from typing import AsyncGenerator, Optional
 from sqlalchemy.ext.asyncio import (
     AsyncSession,
     AsyncEngine,
     create_async_engine,
-    async_sessionmaker
+    async_sessionmaker,
 )
 from sqlalchemy.orm import declarative_base
 from sqlalchemy.pool import NullPool, QueuePool
 import structlog
 
@@ -36,52 +36,56 @@
 
 # ===============================================================================
 # DATABASE ENGINE CONFIGURATION
 # ===============================================================================
 
+
 class DatabaseConfig:
     """Database configuration with environment-based settings"""
-    
+
     def __init__(self):
         # Try to import settings, fallback to environment variables
         try:
             from app.CORE_CONFIGURATION.config_settings import get_settings
+
             settings = get_settings()
             self.database_url = settings.DATABASE_URL
             self.echo = settings.DATABASE_ECHO
             self.pool_size = settings.DATABASE_POOL_SIZE
             self.max_overflow = settings.DATABASE_MAX_OVERFLOW
         except ImportError:
             import os
+
             self.database_url = os.getenv(
-                "DATABASE_URL",
-                "postgresql+asyncpg://postgres:postgres@localhost:5432/ymera"
+                "DATABASE_URL", "postgresql+asyncpg://postgres:postgres@localhost:5432/ymera"
             )
             self.echo = os.getenv("DATABASE_ECHO", "False").lower() == "true"
             self.pool_size = int(os.getenv("DATABASE_POOL_SIZE", "5"))
             self.max_overflow = int(os.getenv("DATABASE_MAX_OVERFLOW", "10"))
 
+
 # ===============================================================================
 # DATABASE ENGINE AND SESSION MANAGEMENT
 # ===============================================================================
+
 
 class DatabaseManager:
     """Centralized database manager for all API routes"""
-    
+
     def __init__(self):
         self.config = DatabaseConfig()
         self._engine: Optional[AsyncEngine] = None
         self._session_factory: Optional[async_sessionmaker] = None
         self._initialized = False
         self.logger = logger.bind(component="database_manager")
-    
+
     async def initialize(self) -> None:
         """Initialize database engine and session factory"""
         if self._initialized:
             self.logger.warning("Database already initialized")
             return
-        
+
         try:
             # Create async engine with proper pooling
             self._engine = create_async_engine(
                 self.config.database_url,
                 echo=self.config.echo,
@@ -89,86 +93,91 @@
                 pool_size=self.config.pool_size,
                 max_overflow=self.config.max_overflow,
                 pool_pre_ping=True,
                 pool_recycle=3600,
             )
-            
+
             # Create session factory
             self._session_factory = async_sessionmaker(
                 self._engine,
                 class_=AsyncSession,
                 expire_on_commit=False,
                 autocommit=False,
                 autoflush=False,
             )
-            
+
             self._initialized = True
             self.logger.info(
                 "Database initialized successfully",
-                url=self.config.database_url.split('@')[1] if '@' in self.config.database_url else "local",
-                pool_size=self.config.pool_size
+                url=(
+                    self.config.database_url.split("@")[1]
+                    if "@" in self.config.database_url
+                    else "local"
+                ),
+                pool_size=self.config.pool_size,
             )
-            
+
         except Exception as e:
             self.logger.error("Failed to initialize database", error=str(e))
             raise RuntimeError(f"Database initialization failed: {e}") from e
-    
+
     async def dispose(self) -> None:
         """Dispose of database engine and cleanup resources"""
         if self._engine:
             await self._engine.dispose()
             self._initialized = False
             self.logger.info("Database disposed successfully")
-    
+
     @asynccontextmanager
     async def get_session(self) -> AsyncGenerator[AsyncSession, None]:
         """
         Get database session with automatic cleanup.
-        
+
         Yields:
             AsyncSession: Database session
-            
+
         Example:
             async with db_manager.get_session() as session:
                 result = await session.execute(query)
         """
         if not self._initialized:
             await self.initialize()
-        
+
         if not self._session_factory:
             raise RuntimeError("Database session factory not initialized")
-        
+
         session = self._session_factory()
         try:
             yield session
             await session.commit()
         except Exception as e:
             await session.rollback()
             self.logger.error("Database session error", error=str(e))
             raise
         finally:
             await session.close()
-    
+
     async def health_check(self) -> bool:
         """
         Check database connection health.
-        
+
         Returns:
             bool: True if database is healthy
         """
         try:
             if not self._initialized:
                 return False
-            
+
             async with self.get_session() as session:
                 await session.execute("SELECT 1")
                 return True
-                
+
         except Exception as e:
             self.logger.error("Database health check failed", error=str(e))
             return False
 
+
 # ===============================================================================
 # GLOBAL DATABASE MANAGER INSTANCE
 # ===============================================================================
 
 # Create singleton instance
@@ -176,85 +185,93 @@
 
 # ===============================================================================
 # DEPENDENCY INJECTION FUNCTIONS
 # ===============================================================================
 
+
 async def get_db_session() -> AsyncGenerator[AsyncSession, None]:
     """
     FastAPI dependency for getting database session.
-    
+
     Yields:
         AsyncSession: Database session
-        
+
     Example:
         @router.get("/items")
         async def get_items(db: AsyncSession = Depends(get_db_session)):
             result = await db.execute(select(Item))
             return result.scalars().all()
     """
     async with _db_manager.get_session() as session:
         yield session
 
+
 async def init_database() -> None:
     """Initialize database on application startup"""
     await _db_manager.initialize()
 
+
 async def close_database() -> None:
     """Close database on application shutdown"""
     await _db_manager.dispose()
 
+
 # ===============================================================================
 # DATABASE UTILITIES
 # ===============================================================================
 
+
 async def create_tables() -> None:
     """
     Create all database tables.
-    
+
     Warning: This should only be used in development.
     Use Alembic migrations in production.
     """
     if not _db_manager._initialized:
         await _db_manager.initialize()
-    
+
     if not _db_manager._engine:
         raise RuntimeError("Database engine not initialized")
-    
+
     async with _db_manager._engine.begin() as conn:
         await conn.run_sync(Base.metadata.create_all)
-    
+
     logger.info("Database tables created successfully")
 
+
 async def drop_tables() -> None:
     """
     Drop all database tables.
-    
+
     Warning: This is destructive and should only be used in development.
     """
     if not _db_manager._initialized:
         await _db_manager.initialize()
-    
+
     if not _db_manager._engine:
         raise RuntimeError("Database engine not initialized")
-    
+
     async with _db_manager._engine.begin() as conn:
         await conn.run_sync(Base.metadata.drop_all)
-    
+
     logger.warning("Database tables dropped")
 
+
 # ===============================================================================
 # TRANSACTION MANAGEMENT
 # ===============================================================================
+
 
 @asynccontextmanager
 async def transaction(session: AsyncSession):
     """
     Context manager for explicit transaction handling.
-    
+
     Args:
         session: Database session
-        
+
     Example:
         async with transaction(session):
             # Perform multiple operations
             session.add(item1)
             session.add(item2)
@@ -266,54 +283,54 @@
     except Exception as e:
         await session.rollback()
         logger.error("Transaction failed", error=str(e))
         raise
 
+
 # ===============================================================================
 # HEALTH CHECK ENDPOINT HELPER
 # ===============================================================================
 
+
 async def get_database_stats() -> dict:
     """
     Get database connection pool statistics.
-    
+
     Returns:
         dict: Database statistics
     """
     if not _db_manager._engine:
         return {"status": "not_initialized"}
-    
+
     pool = _db_manager._engine.pool
-    
+
     return {
         "status": "initialized",
         "pool_size": pool.size(),
         "checked_in_connections": pool.checkedin(),
         "checked_out_connections": pool.checkedout(),
         "overflow_connections": pool.overflow(),
         "total_connections": pool.size() + pool.overflow(),
     }
 
+
 # ===============================================================================
 # EXPORTS
 # ===============================================================================
 
 __all__ = [
     # Core components
     "Base",
     "DatabaseManager",
     "DatabaseConfig",
-    
     # Dependency injection
     "get_db_session",
     "init_database",
     "close_database",
-    
     # Utilities
     "create_tables",
     "drop_tables",
     "transaction",
     "get_database_stats",
-    
     # Global instance (use with caution)
     "_db_manager",
 ]
would reformat /home/runner/work/ymera_y/ymera_y/database_wrapper.py
--- /home/runner/work/ymera_y/ymera_y/database_core_integrated.py	2025-10-19 22:47:02.799797+00:00
+++ /home/runner/work/ymera_y/ymera_y/database_core_integrated.py	2025-10-19 23:09:08.685200+00:00
@@ -18,18 +18,38 @@
 from abc import ABC, abstractmethod
 
 # Third-party imports
 import structlog
 from sqlalchemy import (
-    create_engine, MetaData, Table, Column, Integer, String,
-    DateTime, Text, Boolean, Float, JSON, ForeignKey, Index,
-    UniqueConstraint, CheckConstraint, event, text, func,
-    select, insert, update, delete
+    create_engine,
+    MetaData,
+    Table,
+    Column,
+    Integer,
+    String,
+    DateTime,
+    Text,
+    Boolean,
+    Float,
+    JSON,
+    ForeignKey,
+    Index,
+    UniqueConstraint,
+    CheckConstraint,
+    event,
+    text,
+    func,
+    select,
+    insert,
+    update,
+    delete,
 )
 from sqlalchemy.ext.asyncio import (
-    create_async_engine, AsyncSession, async_sessionmaker,
-    AsyncEngine
+    create_async_engine,
+    AsyncSession,
+    async_sessionmaker,
+    AsyncEngine,
 )
 from sqlalchemy.orm import declarative_base, relationship, selectinload, joinedload
 from sqlalchemy.dialects.postgresql import UUID as PG_UUID, ARRAY, JSONB
 from sqlalchemy.pool import QueuePool, NullPool, StaticPool
 from sqlalchemy.exc import SQLAlchemyError, IntegrityError
@@ -37,72 +57,71 @@
 # ===============================================================================
 # LOGGING CONFIGURATION
 # ===============================================================================
 
 logging.basicConfig(
-    level=logging.INFO,
-    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
+    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
 )
 logger = structlog.get_logger("ymera.database.core")
 
 # ===============================================================================
 # BASE CONFIGURATION
 # ===============================================================================
 
+
 class DatabaseConfig:
     """Centralized database configuration"""
-    
+
     def __init__(self):
         # Database connection
-        self.database_url = os.getenv(
-            "DATABASE_URL",
-            "sqlite+aiosqlite:///./ymera_enterprise.db"
-        )
-        
+        self.database_url = os.getenv("DATABASE_URL", "sqlite+aiosqlite:///./ymera_enterprise.db")
+
         # Connection pooling
         self.pool_size = int(os.getenv("DB_POOL_SIZE", "20"))
         self.max_overflow = int(os.getenv("DB_MAX_OVERFLOW", "40"))
         self.pool_timeout = int(os.getenv("DB_POOL_TIMEOUT", "30"))
         self.pool_recycle = int(os.getenv("DB_POOL_RECYCLE", "3600"))
-        
+
         # Performance
         self.echo = os.getenv("DB_ECHO", "false").lower() == "true"
         self.echo_pool = os.getenv("DB_ECHO_POOL", "false").lower() == "true"
-        
+
         # Migrations
         self.migrations_dir = Path(os.getenv("DB_MIGRATIONS_DIR", "./database/migrations"))
         self.migration_table = "schema_migrations"
-        
+
         # Determine database type
         self.db_type = self._determine_db_type()
-    
+
     def _determine_db_type(self) -> str:
         """Determine database type from URL"""
         if "postgresql" in self.database_url:
             return "postgresql"
         elif "mysql" in self.database_url:
             return "mysql"
         elif "sqlite" in self.database_url:
             return "sqlite"
         else:
             return "sqlite"  # Default fallback
-    
+
     @property
     def is_postgres(self) -> bool:
         return self.db_type == "postgresql"
-    
+
     @property
     def is_sqlite(self) -> bool:
         return self.db_type == "sqlite"
 
+
 # ===============================================================================
 # DATABASE MODELS - ENHANCED BASE
 # ===============================================================================
+
 
 class EnhancedBase:
     """Enhanced base class with utility methods"""
-    
+
     def to_dict(self, include_relations: bool = False) -> Dict[str, Any]:
         """Convert model instance to dictionary"""
         result = {}
         for column in self.__table__.columns:
             value = getattr(self, column.name)
@@ -110,454 +129,491 @@
                 result[column.name] = value.isoformat()
             elif isinstance(value, uuid.UUID):
                 result[column.name] = str(value)
             else:
                 result[column.name] = value
-        
+
         if include_relations:
             for rel in self.__mapper__.relationships:
                 value = getattr(self, rel.key)
                 if value is not None:
                     if rel.collection_class is None:
-                        result[rel.key] = value.to_dict() if hasattr(value, 'to_dict') else str(value)
+                        result[rel.key] = (
+                            value.to_dict() if hasattr(value, "to_dict") else str(value)
+                        )
                     else:
                         result[rel.key] = [
-                            item.to_dict() if hasattr(item, 'to_dict') else str(item)
+                            item.to_dict() if hasattr(item, "to_dict") else str(item)
                             for item in value
                         ]
-        
+
         return result
-    
+
     def update_from_dict(self, data: Dict[str, Any], exclude: List[str] = None):
         """Update model instance from dictionary"""
-        exclude = exclude or ['id', 'created_at']
+        exclude = exclude or ["id", "created_at"]
         for key, value in data.items():
             if key not in exclude and hasattr(self, key):
                 setattr(self, key, value)
-    
+
     @classmethod
     def create_from_dict(cls, data: Dict[str, Any]):
         """Create model instance from dictionary"""
         return cls(**{k: v for k, v in data.items() if hasattr(cls, k)})
 
+
 Base = declarative_base(cls=EnhancedBase)
 
 # ===============================================================================
 # CORE MODELS
 # ===============================================================================
+
 
 class TimestampMixin:
     """Mixin for automatic timestamp management"""
+
     created_at = Column(DateTime, default=datetime.utcnow, nullable=False, index=True)
     updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)
 
+
 class SoftDeleteMixin:
     """Mixin for soft deletion support"""
+
     is_deleted = Column(Boolean, default=False, nullable=False, index=True)
     deleted_at = Column(DateTime, nullable=True)
+
 
 # Helper function for JSON/JSONB columns
 def get_json_column():
     """Return appropriate JSON column type based on database"""
     config = DatabaseConfig()
     return JSONB if config.is_postgres else JSON
 
+
 # Association Tables
 project_agents_table = Table(
-    'project_agents',
+    "project_agents",
     Base.metadata,
-    Column('project_id', String, ForeignKey('projects.id', ondelete='CASCADE'), primary_key=True),
-    Column('agent_id', String, ForeignKey('agents.id', ondelete='CASCADE'), primary_key=True),
-    Column('role', String(50), default='member'),
-    Column('assigned_at', DateTime, default=datetime.utcnow)
+    Column("project_id", String, ForeignKey("projects.id", ondelete="CASCADE"), primary_key=True),
+    Column("agent_id", String, ForeignKey("agents.id", ondelete="CASCADE"), primary_key=True),
+    Column("role", String(50), default="member"),
+    Column("assigned_at", DateTime, default=datetime.utcnow),
 )
+
 
 class User(Base, TimestampMixin, SoftDeleteMixin):
     """Enhanced User model"""
+
     __tablename__ = "users"
-    
+
     id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
     username = Column(String(255), unique=True, nullable=False, index=True)
     email = Column(String(255), unique=True, nullable=False, index=True)
     password_hash = Column(String(255), nullable=False)
     first_name = Column(String(100))
     last_name = Column(String(100))
-    
+
     # Profile
     avatar_url = Column(String(500))
     bio = Column(Text)
     location = Column(String(100))
-    timezone = Column(String(50), default='UTC')
-    language = Column(String(10), default='en')
-    
+    timezone = Column(String(50), default="UTC")
+    language = Column(String(10), default="en")
+
     # Authentication & Security
     email_verified = Column(Boolean, default=False)
     two_factor_enabled = Column(Boolean, default=False)
     two_factor_secret = Column(String(32))
-    
+
     # Access Control
-    role = Column(String(50), default='user', index=True)
+    role = Column(String(50), default="user", index=True)
     permissions = Column(get_json_column(), default=list)
     is_active = Column(Boolean, default=True, index=True)
     last_login = Column(DateTime)
     login_count = Column(Integer, default=0)
-    
+
     # Preferences
     preferences = Column(get_json_column(), default=dict)
     api_key = Column(String(64), unique=True, index=True)
     api_key_expires = Column(DateTime)
-    
+
     # Relationships
     projects = relationship("Project", back_populates="owner", foreign_keys="Project.owner_id")
     tasks = relationship("Task", back_populates="user", foreign_keys="Task.user_id")
     files = relationship("File", back_populates="user")
     audit_logs = relationship("AuditLog", back_populates="user")
-    
+
     __table_args__ = (
-        CheckConstraint('length(username) >= 3', name='username_min_length'),
-        Index('idx_user_email_active', 'email', 'is_active'),
+        CheckConstraint("length(username) >= 3", name="username_min_length"),
+        Index("idx_user_email_active", "email", "is_active"),
     )
+
 
 class Project(Base, TimestampMixin, SoftDeleteMixin):
     """Enhanced Project model"""
+
     __tablename__ = "projects"
-    
+
     id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
     name = Column(String(255), nullable=False, index=True)
     description = Column(Text)
-    owner_id = Column(String, ForeignKey('users.id', ondelete='CASCADE'), nullable=False, index=True)
-    
+    owner_id = Column(
+        String, ForeignKey("users.id", ondelete="CASCADE"), nullable=False, index=True
+    )
+
     # Project details
     github_url = Column(String(500))
-    project_type = Column(String(50), default='general', index=True)
+    project_type = Column(String(50), default="general", index=True)
     programming_language = Column(String(50))
     framework = Column(String(100))
-    
+
     # Status & Progress
     status = Column(String(50), default="active", index=True)
-    priority = Column(String(20), default='medium', index=True)
+    priority = Column(String(20), default="medium", index=True)
     progress = Column(Float, default=0.0)
     estimated_completion = Column(DateTime)
-    
+
     # Configuration
     settings = Column(get_json_column(), default=dict)
     environment_variables = Column(get_json_column(), default=dict)
     dependencies = Column(get_json_column(), default=list)
-    
+
     # Metrics
     total_tasks = Column(Integer, default=0)
     completed_tasks = Column(Integer, default=0)
     success_rate = Column(Float, default=0.0)
     total_lines_of_code = Column(Integer, default=0)
     test_coverage = Column(Float, default=0.0)
-    
+
     # Metadata
     tags = Column(get_json_column(), default=list)
     metadata_info = Column(get_json_column(), default=dict)
-    
+
     # Relationships
     owner = relationship("User", back_populates="projects", foreign_keys=[owner_id])
     tasks = relationship("Task", back_populates="project", cascade="all, delete-orphan")
     files = relationship("File", back_populates="project")
     agents = relationship("Agent", secondary=project_agents_table, back_populates="projects")
-    
+
     __table_args__ = (
-        CheckConstraint('progress >= 0 AND progress <= 100', name='progress_range'),
-        CheckConstraint('success_rate >= 0 AND success_rate <= 100', name='success_rate_range'),
-        Index('idx_project_owner_status', 'owner_id', 'status'),
+        CheckConstraint("progress >= 0 AND progress <= 100", name="progress_range"),
+        CheckConstraint("success_rate >= 0 AND success_rate <= 100", name="success_rate_range"),
+        Index("idx_project_owner_status", "owner_id", "status"),
     )
+
 
 class Agent(Base, TimestampMixin, SoftDeleteMixin):
     """Enhanced Agent model"""
+
     __tablename__ = "agents"
-    
+
     id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
     name = Column(String(255), nullable=False, index=True)
     agent_type = Column(String(100), nullable=False, index=True)
     description = Column(Text)
-    
+
     # Configuration
     capabilities = Column(get_json_column(), default=list)
     configuration = Column(get_json_column(), default=dict)
     api_endpoints = Column(get_json_column(), default=dict)
-    
+
     # Status & Performance
     status = Column(String(50), default="active", index=True)
     health_status = Column(String(50), default="healthy", index=True)
     load_factor = Column(Float, default=0.0)
     response_time_avg = Column(Float, default=0.0)
     success_rate = Column(Float, default=100.0)
-    
+
     # Learning & Intelligence
     learning_model_version = Column(String(50), default="1.0")
     knowledge_base = Column(get_json_column(), default=dict)
     learning_history = Column(get_json_column(), default=list)
     performance_metrics = Column(get_json_column(), default=dict)
-    
+
     # Statistics
     tasks_completed = Column(Integer, default=0)
     tasks_failed = Column(Integer, default=0)
     total_execution_time = Column(Float, default=0.0)
     last_active = Column(DateTime, default=datetime.utcnow)
-    
+
     # Relationships
     tasks = relationship("Task", back_populates="agent")
     projects = relationship("Project", secondary=project_agents_table, back_populates="agents")
-    
+
     __table_args__ = (
-        CheckConstraint('load_factor >= 0 AND load_factor <= 100', name='load_factor_range'),
-        CheckConstraint('success_rate >= 0 AND success_rate <= 100', name='agent_success_rate_range'),
-        Index('idx_agent_type_status', 'agent_type', 'status'),
+        CheckConstraint("load_factor >= 0 AND load_factor <= 100", name="load_factor_range"),
+        CheckConstraint(
+            "success_rate >= 0 AND success_rate <= 100", name="agent_success_rate_range"
+        ),
+        Index("idx_agent_type_status", "agent_type", "status"),
     )
+
 
 class Task(Base, TimestampMixin, SoftDeleteMixin):
     """Enhanced Task model"""
+
     __tablename__ = "tasks"
-    
+
     id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
     title = Column(String(500), nullable=False, index=True)
     description = Column(Text)
     task_type = Column(String(100), nullable=False, index=True)
-    
+
     # Relationships
-    user_id = Column(String, ForeignKey('users.id', ondelete='CASCADE'), nullable=False, index=True)
-    project_id = Column(String, ForeignKey('projects.id', ondelete='CASCADE'), nullable=True, index=True)
-    agent_id = Column(String, ForeignKey('agents.id', ondelete='SET NULL'), nullable=True, index=True)
-    parent_task_id = Column(String, ForeignKey('tasks.id', ondelete='SET NULL'), nullable=True)
-    
+    user_id = Column(String, ForeignKey("users.id", ondelete="CASCADE"), nullable=False, index=True)
+    project_id = Column(
+        String, ForeignKey("projects.id", ondelete="CASCADE"), nullable=True, index=True
+    )
+    agent_id = Column(
+        String, ForeignKey("agents.id", ondelete="SET NULL"), nullable=True, index=True
+    )
+    parent_task_id = Column(String, ForeignKey("tasks.id", ondelete="SET NULL"), nullable=True)
+
     # Status & Priority
     status = Column(String(50), default="pending", index=True)
     priority = Column(String(20), default="medium", index=True)
     urgency = Column(Integer, default=5)
-    
+
     # Execution Details
     input_data = Column(get_json_column(), default=dict)
     output_data = Column(get_json_column(), default=dict)
     execution_config = Column(get_json_column(), default=dict)
     error_details = Column(get_json_column(), default=dict)
-    
+
     # Timing
     scheduled_at = Column(DateTime, nullable=True, index=True)
     started_at = Column(DateTime, nullable=True)
     completed_at = Column(DateTime, nullable=True)
     execution_time = Column(Float, default=0.0)
     timeout = Column(Integer, default=3600)
-    
+
     # Progress & Results
     progress = Column(Float, default=0.0)
     result_summary = Column(Text)
     quality_score = Column(Float, default=0.0)
     retry_count = Column(Integer, default=0)
     max_retries = Column(Integer, default=3)
-    
+
     # Dependencies & Context
     dependencies = Column(get_json_column(), default=list)
     context_data = Column(get_json_column(), default=dict)
     tags = Column(get_json_column(), default=list)
-    
+
     # Relationships
     user = relationship("User", back_populates="tasks", foreign_keys=[user_id])
     project = relationship("Project", back_populates="tasks")
     agent = relationship("Agent", back_populates="tasks")
     parent_task = relationship("Task", remote_side=[id], foreign_keys=[parent_task_id])
-    
+
     __table_args__ = (
-        CheckConstraint('progress >= 0 AND progress <= 100', name='task_progress_range'),
-        CheckConstraint('urgency >= 1 AND urgency <= 10', name='urgency_range'),
-        Index('idx_task_status_priority', 'status', 'priority'),
-        Index('idx_task_agent_status', 'agent_id', 'status'),
+        CheckConstraint("progress >= 0 AND progress <= 100", name="task_progress_range"),
+        CheckConstraint("urgency >= 1 AND urgency <= 10", name="urgency_range"),
+        Index("idx_task_status_priority", "status", "priority"),
+        Index("idx_task_agent_status", "agent_id", "status"),
     )
+
 
 class File(Base, TimestampMixin, SoftDeleteMixin):
     """Enhanced File model"""
+
     __tablename__ = "files"
-    
+
     id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
     filename = Column(String(500), nullable=False, index=True)
     original_filename = Column(String(500), nullable=False)
     file_path = Column(String(1000), nullable=False)
-    
+
     # File Properties
     file_size = Column(Integer, nullable=False)
     mime_type = Column(String(200), index=True)
     file_extension = Column(String(20), index=True)
     encoding = Column(String(50))
-    
+
     # Security
     checksum_md5 = Column(String(32), index=True)
     checksum_sha256 = Column(String(64), index=True)
     virus_scan_status = Column(String(50), default="pending")
-    
+
     # Access Control
-    user_id = Column(String, ForeignKey('users.id', ondelete='CASCADE'), nullable=False, index=True)
-    project_id = Column(String, ForeignKey('projects.id', ondelete='CASCADE'), nullable=True, index=True)
+    user_id = Column(String, ForeignKey("users.id", ondelete="CASCADE"), nullable=False, index=True)
+    project_id = Column(
+        String, ForeignKey("projects.id", ondelete="CASCADE"), nullable=True, index=True
+    )
     access_level = Column(String(50), default="private", index=True)
-    
+
     # Metadata
     file_category = Column(String(50), index=True)
     tags = Column(get_json_column(), default=list)
     metadata_extracted = Column(get_json_column(), default=dict)
     content_summary = Column(Text)
-    
+
     # Usage Statistics
     download_count = Column(Integer, default=0)
     last_accessed = Column(DateTime)
-    
+
     # Relationships
     user = relationship("User", back_populates="files")
     project = relationship("Project", back_populates="files")
-    
+
     __table_args__ = (
-        CheckConstraint('file_size > 0', name='positive_file_size'),
-        Index('idx_file_user_project', 'user_id', 'project_id'),
+        CheckConstraint("file_size > 0", name="positive_file_size"),
+        Index("idx_file_user_project", "user_id", "project_id"),
     )
+
 
 class AuditLog(Base, TimestampMixin):
     """Comprehensive audit logging"""
+
     __tablename__ = "audit_logs"
-    
+
     id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
-    user_id = Column(String, ForeignKey('users.id', ondelete='SET NULL'), nullable=True, index=True)
-    
+    user_id = Column(String, ForeignKey("users.id", ondelete="SET NULL"), nullable=True, index=True)
+
     # Action Details
     action = Column(String(200), nullable=False, index=True)
     resource_type = Column(String(100), nullable=False, index=True)
     resource_id = Column(String, index=True)
-    
+
     # Context
     ip_address = Column(String(45))
     user_agent = Column(String(500))
     session_id = Column(String(100), index=True)
     request_id = Column(String(100), index=True)
-    
+
     # Details
     action_details = Column(get_json_column(), default=dict)
     before_state = Column(get_json_column(), default=dict)
     after_state = Column(get_json_column(), default=dict)
-    
+
     # Result
     success = Column(Boolean, nullable=False, index=True)
     error_message = Column(Text)
     execution_time = Column(Float)
-    
+
     # Relationships
     user = relationship("User", back_populates="audit_logs")
-    
+
     __table_args__ = (
-        Index('idx_audit_action_time', 'action', 'created_at'),
-        Index('idx_audit_resource', 'resource_type', 'resource_id'),
+        Index("idx_audit_action_time", "action", "created_at"),
+        Index("idx_audit_resource", "resource_type", "resource_id"),
     )
 
+
 # ===============================================================================
 # MIGRATION SYSTEM
 # ===============================================================================
+
 
 @dataclass
 class MigrationInfo:
     """Information about a migration"""
+
     version: int
     name: str
     filename: str
     filepath: Path
     checksum: str
     description: Optional[str] = None
     dependencies: List[int] = field(default_factory=list)
 
+
 class BaseMigration(ABC):
     """Abstract base class for migrations"""
-    
+
     def __init__(self):
         self.version: int = 0
         self.name: str = ""
         self.description: str = ""
         self.dependencies: List[int] = []
-    
+
     @abstractmethod
     async def up(self, session: AsyncSession) -> None:
         """Execute the migration"""
         pass
-    
+
     @abstractmethod
     async def down(self, session: AsyncSession) -> None:
         """Rollback the migration"""
         pass
-    
+
     async def validate_preconditions(self, session: AsyncSession) -> bool:
         """Validate migration preconditions"""
         return True
-    
+
     async def validate_postconditions(self, session: AsyncSession) -> bool:
         """Validate migration postconditions"""
         return True
 
+
 # ===============================================================================
 # DATABASE MANAGER - INTEGRATED
 # ===============================================================================
+
 
 class IntegratedDatabaseManager:
     """Fully integrated production-ready database manager"""
-    
+
     def __init__(self, config: Optional[DatabaseConfig] = None):
         self.config = config or DatabaseConfig()
         self.engine: Optional[AsyncEngine] = None
         self.session_factory: Optional[async_sessionmaker] = None
         self._initialized = False
         self.logger = logger.bind(component="database_manager")
-    
+
     async def initialize(self) -> None:
         """Initialize database with all components"""
         if self._initialized:
             self.logger.warning("Database already initialized")
             return
-        
+
         try:
             self.logger.info("Initializing integrated database manager")
-            
+
             # Create async engine
             await self._create_engine()
-            
+
             # Create session factory
             self._create_session_factory()
-            
+
             # Initialize database schema
             await self._initialize_schema()
-            
+
             # Setup migration system
             await self._setup_migrations()
-            
+
             # Verify connectivity
             await self._verify_connection()
-            
+
             self._initialized = True
             self.logger.info(
                 "Database initialized successfully",
                 db_type=self.config.db_type,
-                pool_size=self.config.pool_size
+                pool_size=self.config.pool_size,
             )
-            
+
         except Exception as e:
             self.logger.error("Failed to initialize database", error=str(e))
             raise RuntimeError(f"Database initialization failed: {e}") from e
-    
+
     async def _create_engine(self) -> None:
         """Create async database engine"""
         database_url = self.config.database_url
-        
+
         # Ensure async driver
         if "postgresql://" in database_url and "asyncpg" not in database_url:
             database_url = database_url.replace("postgresql://", "postgresql+asyncpg://")
         elif "sqlite://" in database_url and "aiosqlite" not in database_url:
             database_url = database_url.replace("sqlite://", "sqlite+aiosqlite://")
-        
+
         # Configure pooling based on database type
         if self.config.is_sqlite:
             self.engine = create_async_engine(
                 database_url,
                 echo=self.config.echo,
                 poolclass=StaticPool,
                 connect_args={"check_same_thread": False},
-                future=True
+                future=True,
             )
         else:
             self.engine = create_async_engine(
                 database_url,
                 echo=self.config.echo,
@@ -565,32 +621,32 @@
                 pool_size=self.config.pool_size,
                 max_overflow=self.config.max_overflow,
                 pool_timeout=self.config.pool_timeout,
                 pool_recycle=self.config.pool_recycle,
                 pool_pre_ping=True,
-                future=True
+                future=True,
             )
-        
+
         self.logger.debug("Database engine created", db_type=self.config.db_type)
-    
+
     def _create_session_factory(self) -> None:
         """Create session factory"""
         self.session_factory = async_sessionmaker(
             bind=self.engine,
             class_=AsyncSession,
             expire_on_commit=False,
             autocommit=False,
-            autoflush=False
+            autoflush=False,
         )
         self.logger.debug("Session factory created")
-    
+
     async def _initialize_schema(self) -> None:
         """Initialize database schema"""
         async with self.engine.begin() as conn:
             await conn.run_sync(Base.metadata.create_all)
         self.logger.info("Database schema initialized")
-    
+
     async def _setup_migrations(self) -> None:
         """Setup migration tracking table"""
         async with self.get_session() as session:
             # Create migration tracking table
             migration_table_sql = f"""
@@ -610,111 +666,110 @@
                 updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
             );
             """
             await session.execute(text(migration_table_sql))
             await session.commit()
-        
+
         self.logger.debug("Migration system initialized")
-    
+
     async def _verify_connection(self) -> None:
         """Verify database connection"""
         async with self.get_session() as session:
             await session.execute(text("SELECT 1"))
         self.logger.debug("Database connection verified")
-    
+
     @asynccontextmanager
     async def get_session(self) -> AsyncGenerator[AsyncSession, None]:
         """Get database session with automatic cleanup"""
         if not self._initialized:
             await self.initialize()
-        
+
         if not self.session_factory:
             raise RuntimeError("Database session factory not initialized")
-        
+
         session = self.session_factory()
         try:
             yield session
             await session.commit()
         except Exception as e:
             await session.rollback()
             self.logger.error("Session error, rolled back", error=str(e))
             raise
         finally:
             await session.close()
-    
+
     async def health_check(self) -> Dict[str, Any]:
         """Comprehensive health check"""
         try:
             async with self.get_session() as session:
                 result = await session.execute(text("SELECT 1 as healthcheck"))
                 health_value = result.scalar()
-                
+
                 # Get pool statistics if available
                 pool_stats = {}
-                if hasattr(self.engine.pool, 'size'):
+                if hasattr(self.engine.pool, "size"):
                     pool_stats = {
                         "pool_size": self.engine.pool.size(),
                         "checked_in": self.engine.pool.checkedin(),
                         "checked_out": self.engine.pool.checkedout(),
-                        "overflow": self.engine.pool.overflow()
+                        "overflow": self.engine.pool.overflow(),
                     }
-                
+
                 return {
                     "status": "healthy",
                     "healthy": True,
                     "database_type": self.config.db_type,
                     "pool_stats": pool_stats,
-                    "timestamp": datetime.utcnow().isoformat()
+                    "timestamp": datetime.utcnow().isoformat(),
                 }
         except Exception as e:
             self.logger.error("Health check failed", error=str(e))
             return {
                 "status": "unhealthy",
                 "healthy": False,
                 "error": str(e),
-                "timestamp": datetime.utcnow().isoformat()
+                "timestamp": datetime.utcnow().isoformat(),
             }
-    
+
     async def get_statistics(self) -> Dict[str, Any]:
         """Get comprehensive database statistics"""
         async with self.get_session() as session:
             stats = {}
-            
+
             # Count records in each table
             for table_name, model_class in {
-                'users': User,
-                'projects': Project,
-                'agents': Agent,
-                'tasks': Task,
-                'files': File,
-                'audit_logs': AuditLog
+                "users": User,
+                "projects": Project,
+                "agents": Agent,
+                "tasks": Task,
+                "files": File,
+                "audit_logs": AuditLog,
             }.items():
                 try:
                     result = await session.execute(select(func.count(model_class.id)))
                     stats[f"{table_name}_count"] = result.scalar() or 0
                 except Exception as e:
                     stats[f"{table_name}_count"] = 0
                     self.logger.warning(f"Could not count {table_name}", error=str(e))
-            
+
             # Get recent activity (last 24 hours)
             try:
                 cutoff = datetime.utcnow() - timedelta(hours=24)
                 result = await session.execute(
-                    select(func.count(AuditLog.id))
-                    .where(AuditLog.created_at > cutoff)
+                    select(func.count(AuditLog.id)).where(AuditLog.created_at > cutoff)
                 )
-                stats['recent_activity_24h'] = result.scalar() or 0
+                stats["recent_activity_24h"] = result.scalar() or 0
             except:
-                stats['recent_activity_24h'] = 0
-            
-            stats['timestamp'] = datetime.utcnow().isoformat()
+                stats["recent_activity_24h"] = 0
+
+            stats["timestamp"] = datetime.utcnow().isoformat()
             return stats
-    
+
     async def optimize_database(self) -> Dict[str, Any]:
         """Run database optimization tasks"""
         results = {"optimizations": [], "errors": []}
-        
+
         async with self.get_session() as session:
             try:
                 # Update project statistics
                 update_projects_sql = """
                 UPDATE projects SET
@@ -727,11 +782,11 @@
                     END,
                     updated_at = CURRENT_TIMESTAMP
                 """
                 await session.execute(text(update_projects_sql))
                 results["optimizations"].append("Updated project statistics")
-                
+
                 # Update agent statistics
                 update_agents_sql = """
                 UPDATE agents SET
                     tasks_completed = (SELECT COUNT(*) FROM tasks WHERE agent_id = agents.id AND status = 'completed'),
                     tasks_failed = (SELECT COUNT(*) FROM tasks WHERE agent_id = agents.id AND status = 'failed'),
@@ -743,208 +798,211 @@
                     total_execution_time = COALESCE((SELECT SUM(execution_time) FROM tasks WHERE agent_id = agents.id AND status = 'completed'), 0),
                     updated_at = CURRENT_TIMESTAMP
                 """
                 await session.execute(text(update_agents_sql))
                 results["optimizations"].append("Updated agent statistics")
-                
+
                 await session.commit()
-                
+
             except Exception as e:
                 results["errors"].append(str(e))
                 self.logger.error("Optimization failed", error=str(e))
                 await session.rollback()
-        
+
         results["timestamp"] = datetime.utcnow().isoformat()
         results["success"] = len(results["errors"]) == 0
         return results
-    
+
     async def cleanup_old_data(self, days_to_keep: int = 90) -> Dict[str, Any]:
         """Clean up old data"""
         results = {"cleaned": {}, "errors": []}
-        
+
         async with self.get_session() as session:
             try:
                 cutoff_date = datetime.utcnow() - timedelta(days=days_to_keep)
-                
+
                 # Clean old audit logs
                 result = await session.execute(
                     delete(AuditLog).where(AuditLog.created_at < cutoff_date)
                 )
                 results["cleaned"]["audit_logs"] = result.rowcount
-                
+
                 # Clean old soft-deleted records
                 for model_name, model_class in [
-                    ('users', User),
-                    ('projects', Project),
-                    ('agents', Agent),
-                    ('tasks', Task),
-                    ('files', File)
+                    ("users", User),
+                    ("projects", Project),
+                    ("agents", Agent),
+                    ("tasks", Task),
+                    ("files", File),
                 ]:
-                    if hasattr(model_class, 'is_deleted'):
+                    if hasattr(model_class, "is_deleted"):
                         result = await session.execute(
                             delete(model_class).where(
-                                model_class.is_deleted == True,
-                                model_class.deleted_at < cutoff_date
+                                model_class.is_deleted == True, model_class.deleted_at < cutoff_date
                             )
                         )
                         results["cleaned"][model_name] = result.rowcount
-                
+
                 await session.commit()
-                
+
             except Exception as e:
                 results["errors"].append(str(e))
                 self.logger.error("Cleanup failed", error=str(e))
                 await session.rollback()
-        
+
         results["timestamp"] = datetime.utcnow().isoformat()
         results["success"] = len(results["errors"]) == 0
         return results
-    
+
     async def close(self) -> None:
         """Close database connections"""
         if self.engine:
             await self.engine.dispose()
             self._initialized = False
             self.logger.info("Database connections closed")
 
+
 # ===============================================================================
 # GLOBAL INSTANCE AND UTILITIES
 # ===============================================================================
 
 _global_db_manager: Optional[IntegratedDatabaseManager] = None
+
 
 async def get_database_manager() -> IntegratedDatabaseManager:
     """Get or create global database manager instance"""
     global _global_db_manager
-    
+
     if _global_db_manager is None:
         _global_db_manager = IntegratedDatabaseManager()
         await _global_db_manager.initialize()
-    
+
     return _global_db_manager
+
 
 async def get_db_session() -> AsyncGenerator[AsyncSession, None]:
     """FastAPI dependency for database sessions"""
     db_manager = await get_database_manager()
     async with db_manager.get_session() as session:
         yield session
 
+
 async def init_database(config: Optional[DatabaseConfig] = None) -> IntegratedDatabaseManager:
     """Initialize database for application startup"""
     global _global_db_manager
-    
+
     if _global_db_manager is None:
         _global_db_manager = IntegratedDatabaseManager(config)
         await _global_db_manager.initialize()
-    
+
     return _global_db_manager
+
 
 async def close_database() -> None:
     """Close database for application shutdown"""
     global _global_db_manager
-    
+
     if _global_db_manager is not None:
         await _global_db_manager.close()
         _global_db_manager = None
 
+
 # ===============================================================================
 # REPOSITORY PATTERN
 # ===============================================================================
+
 
 class BaseRepository:
     """Base repository with common CRUD operations"""
-    
+
     def __init__(self, session: AsyncSession, model_class):
         self.session = session
         self.model_class = model_class
-    
+
     async def create(self, **kwargs) -> Any:
         """Create new record"""
         instance = self.model_class(**kwargs)
         self.session.add(instance)
         await self.session.flush()
         await self.session.refresh(instance)
         return instance
-    
+
     async def get_by_id(self, id: str) -> Optional[Any]:
         """Get record by ID"""
         result = await self.session.execute(
             select(self.model_class).where(self.model_class.id == id)
         )
         return result.scalar_one_or_none()
-    
+
     async def get_all(self, limit: int = 100, offset: int = 0, filters: Dict = None) -> List[Any]:
         """Get all records with pagination and filters"""
         query = select(self.model_class)
-        
+
         if filters:
             for key, value in filters.items():
                 if hasattr(self.model_class, key):
                     query = query.where(getattr(self.model_class, key) == value)
-        
+
         query = query.limit(limit).offset(offset)
         result = await self.session.execute(query)
         return result.scalars().all()
-    
+
     async def update(self, id: str, **kwargs) -> Optional[Any]:
         """Update record"""
         instance = await self.get_by_id(id)
         if instance:
             for key, value in kwargs.items():
                 if hasattr(instance, key):
                     setattr(instance, key, value)
             await self.session.flush()
             await self.session.refresh(instance)
         return instance
-    
+
     async def delete(self, id: str) -> bool:
         """Delete record (hard delete)"""
         instance = await self.get_by_id(id)
         if instance:
             await self.session.delete(instance)
             await self.session.flush()
             return True
         return False
-    
+
     async def soft_delete(self, id: str) -> bool:
         """Soft delete record if supported"""
-        if hasattr(self.model_class, 'is_deleted'):
+        if hasattr(self.model_class, "is_deleted"):
             instance = await self.get_by_id(id)
             if instance:
                 instance.is_deleted = True
                 instance.deleted_at = datetime.utcnow()
                 await self.session.flush()
                 return True
         return False
 
+
 # ===============================================================================
 # EXPORTS
 # ===============================================================================
 
 __all__ = [
     # Core classes
     "Base",
     "DatabaseConfig",
     "IntegratedDatabaseManager",
     "BaseRepository",
-    
     # Models
     "User",
     "Project",
     "Agent",
     "Task",
     "File",
     "AuditLog",
-    
     # Mixins
     "TimestampMixin",
     "SoftDeleteMixin",
-    
     # Migration
     "BaseMigration",
     "MigrationInfo",
-    
     # Utilities
     "get_database_manager",
     "get_db_session",
     "init_database",
     "close_database",
would reformat /home/runner/work/ymera_y/ymera_y/database_core_integrated.py
error: cannot format /home/runner/work/ymera_y/ymera_y/db_monitoring.py: Cannot parse for target version Python 3.12: 207:8:         storage_metrics = metrics.get('storage', {})
--- /home/runner/work/ymera_y/ymera_y/db_config.py	2025-10-19 22:47:02.799797+00:00
+++ /home/runner/work/ymera_y/ymera_y/db_config.py	2025-10-19 23:09:08.874708+00:00
@@ -1,8 +1,9 @@
 """
 Production-Ready Database Configuration with Connection Pooling
 """
+
 from sqlalchemy import create_engine, event, pool
 from sqlalchemy.orm import sessionmaker, Session
 from sqlalchemy.ext.declarative import declarative_base
 from contextlib import contextmanager
 import logging
@@ -10,38 +11,39 @@
 import os
 from urllib.parse import quote_plus
 
 logger = logging.getLogger(__name__)
 
+
 # Database Configuration
 class DatabaseConfig:
     """Production database configuration"""
-    
+
     def __init__(self):
         # Azure SQL Server connection details
-        self.server = os.getenv('AZURE_SQL_SERVER', 'ymera-sql.database.windows.net')
-        self.database = os.getenv('AZURE_SQL_DATABASE', 'ymeradb')
-        self.username = os.getenv('AZURE_SQL_USER', 'ymeraadmin')
-        self.password = os.getenv('AZURE_SQL_PASSWORD')
-        self.driver = 'ODBC Driver 18 for SQL Server'
-        
+        self.server = os.getenv("AZURE_SQL_SERVER", "ymera-sql.database.windows.net")
+        self.database = os.getenv("AZURE_SQL_DATABASE", "ymeradb")
+        self.username = os.getenv("AZURE_SQL_USER", "ymeraadmin")
+        self.password = os.getenv("AZURE_SQL_PASSWORD")
+        self.driver = "ODBC Driver 18 for SQL Server"
+
         # Connection Pool Settings
-        self.pool_size = int(os.getenv('DB_POOL_SIZE', '20'))
-        self.max_overflow = int(os.getenv('DB_MAX_OVERFLOW', '40'))
-        self.pool_timeout = int(os.getenv('DB_POOL_TIMEOUT', '30'))
-        self.pool_recycle = int(os.getenv('DB_POOL_RECYCLE', '3600'))
+        self.pool_size = int(os.getenv("DB_POOL_SIZE", "20"))
+        self.max_overflow = int(os.getenv("DB_MAX_OVERFLOW", "40"))
+        self.pool_timeout = int(os.getenv("DB_POOL_TIMEOUT", "30"))
+        self.pool_recycle = int(os.getenv("DB_POOL_RECYCLE", "3600"))
         self.pool_pre_ping = True
-        
+
         # Connection retry settings
         self.connect_args = {
-            'connect_timeout': 30,
-            'autocommit': False,
-            'TrustServerCertificate': 'no',
-            'Encrypt': 'yes',
-            'Connection Timeout': 30,
+            "connect_timeout": 30,
+            "autocommit": False,
+            "TrustServerCertificate": "no",
+            "Encrypt": "yes",
+            "Connection Timeout": 30,
         }
-    
+
     def get_connection_string(self) -> str:
         """Build Azure SQL connection string"""
         password_encoded = quote_plus(self.password)
         return (
             f"mssql+pyodbc://{self.username}:{password_encoded}@"
@@ -51,17 +53,17 @@
         )
 
 
 class DatabaseManager:
     """Database connection manager with health checks"""
-    
+
     def __init__(self):
         self.config = DatabaseConfig()
         self._engine = None
         self._session_factory = None
         self.Base = declarative_base()
-    
+
     def initialize(self):
         """Initialize database engine and session factory"""
         try:
             self._engine = create_engine(
                 self.config.get_connection_string(),
@@ -69,86 +71,80 @@
                 pool_size=self.config.pool_size,
                 max_overflow=self.config.max_overflow,
                 pool_timeout=self.config.pool_timeout,
                 pool_recycle=self.config.pool_recycle,
                 pool_pre_ping=self.config.pool_pre_ping,
-                echo=os.getenv('SQL_ECHO', 'false').lower() == 'true',
+                echo=os.getenv("SQL_ECHO", "false").lower() == "true",
                 connect_args=self.config.connect_args,
             )
-            
+
             # Register event listeners
             self._register_event_listeners()
-            
+
             # Create session factory
             self._session_factory = sessionmaker(
-                bind=self._engine,
-                autocommit=False,
-                autoflush=False,
-                expire_on_commit=False
+                bind=self._engine, autocommit=False, autoflush=False, expire_on_commit=False
             )
-            
+
             logger.info("Database initialized successfully")
-            
+
         except Exception as e:
             logger.error(f"Failed to initialize database: {e}")
             raise
-    
+
     def _register_event_listeners(self):
         """Register SQLAlchemy event listeners for monitoring"""
-        
+
         @event.listens_for(self._engine, "connect")
         def receive_connect(dbapi_conn, connection_record):
             logger.debug("Database connection established")
-        
+
         @event.listens_for(self._engine, "checkout")
         def receive_checkout(dbapi_conn, connection_record, connection_proxy):
             logger.debug("Connection checked out from pool")
-        
+
         @event.listens_for(self._engine, "checkin")
         def receive_checkin(dbapi_conn, connection_record):
             logger.debug("Connection returned to pool")
-    
+
     @contextmanager
     def get_session(self) -> Generator[Session, None, None]:
         """Get database session with automatic cleanup"""
         if not self._session_factory:
             raise RuntimeError("Database not initialized")
-        
+
         session = self._session_factory()
         try:
             yield session
             session.commit()
         except Exception as e:
             session.rollback()
             logger.error(f"Session error: {e}")
             raise
         finally:
             session.close()
-    
+
     def health_check(self) -> dict:
         """Perform database health check"""
         try:
             with self.get_session() as session:
                 session.execute("SELECT 1")
-            
+
             # Get pool status
             pool_status = self._engine.pool.status()
-            
+
             return {
                 "status": "healthy",
                 "pool_size": self._engine.pool.size(),
                 "checked_out": self._engine.pool.checkedout(),
                 "overflow": self._engine.pool.overflow(),
-                "pool_status": pool_status
+                "pool_status": pool_status,
             }
         except Exception as e:
             logger.error(f"Health check failed: {e}")
-            return {
-                "status": "unhealthy",
-                "error": str(e)
-            }
-    
+            return {"status": "unhealthy", "error": str(e)}
+
     def dispose(self):
         """Dispose database engine and cleanup connections"""
         if self._engine:
             self._engine.dispose()
             logger.info("Database connections disposed")
would reformat /home/runner/work/ymera_y/ymera_y/db_config.py
--- /home/runner/work/ymera_y/ymera_y/demo_expansion_framework.py	2025-10-19 22:47:02.799797+00:00
+++ /home/runner/work/ymera_y/ymera_y/demo_expansion_framework.py	2025-10-19 23:09:08.947801+00:00
@@ -16,163 +16,162 @@
     print("=" * 70 + "\n")
 
 
 async def main():
     """Main demonstration function"""
-    
+
     print("\n EXPANSION FRAMEWORK DEMONSTRATION")
     print("=" * 70)
-    
+
     # Step 1: Initialize ExpansionManager
     print_section("Step 1: Initialize Expansion Manager")
     from expansion_readiness import ExpansionManager
-    
+
     manager = ExpansionManager()
     print(" ExpansionManager initialized")
     print(f"   Expansion framework: {manager.expansion_framework}")
-    
+
     # Step 2: Setup Expansion Framework
     print_section("Step 2: Setup Expansion Framework")
     manager.setup_expansion_framework()
     print("\n All expansion framework components created")
-    
+
     # Step 3: Demonstrate Plugin Architecture
     print_section("Step 3: Plugin Architecture Demo")
     from enhanced_workspace.integration.plugin_architecture import PluginManager, BasePlugin
-    
+
     # Create a custom plugin
     class AnalyticsPlugin(BasePlugin):
         def __init__(self):
             super().__init__(name="Analytics", version="1.0.0")
             self.data = []
-        
+
         async def initialize(self):
             print(f"    Initializing {self.name} plugin v{self.version}")
             return True
-        
+
         async def execute(self, context):
             print(f"    Executing {self.name} plugin")
             self.data.append(context)
             return {"status": "success", "data_count": len(self.data)}
-    
+
     # Register and use the plugin
     plugin_mgr = PluginManager()
     plugin_mgr.register_plugin("analytics", AnalyticsPlugin)
-    
+
     # Initialize and execute
     analytics = AnalyticsPlugin()
     await analytics.initialize()
     result = await analytics.execute({"event": "user_action", "type": "click"})
     print(f"   Result: {result}")
-    
+
     # Demonstrate hooks
     print("\n   Hook System Demo:")
     hook_data = []
-    
+
     def pre_request_hook(*args, **kwargs):
         hook_data.append(f"Pre-request hook called with {args}")
         print(f"    Pre-request hook executed")
-    
+
     def post_request_hook(*args, **kwargs):
         hook_data.append(f"Post-request hook called with {args}")
         print(f"    Post-request hook executed")
-    
+
     plugin_mgr.add_hook("pre_request", pre_request_hook)
     plugin_mgr.add_hook("post_request", post_request_hook)
-    
+
     plugin_mgr.execute_hook("pre_request", "test_data")
     plugin_mgr.execute_hook("post_request", "response_data")
     print(f"   Hook executions: {len(hook_data)}")
-    
+
     # Step 4: Demonstrate API Versioning
     print_section("Step 4: API Versioning Demo")
     from enhanced_workspace.integration.api_versioning import APIVersionManager
-    
+
     api_mgr = APIVersionManager()
-    
+
     # Register multiple API versions
-    v1_routes = {
-        "/users": "get_users_v1",
-        "/posts": "get_posts_v1"
-    }
-    
+    v1_routes = {"/users": "get_users_v1", "/posts": "get_posts_v1"}
+
     v2_routes = {
         "/users": "get_users_v2",
         "/posts": "get_posts_v2",
-        "/analytics": "get_analytics_v2"
+        "/analytics": "get_analytics_v2",
     }
-    
+
     api_mgr.register_version("v1", v1_routes)
     api_mgr.register_version("v2", v2_routes)
-    
+
     # Retrieve and display versions
     print(f"   Current version: {api_mgr.current_version}")
     print(f"   V1 routes: {api_mgr.get_version('v1')}")
     print(f"   V2 routes: {api_mgr.get_version('v2')}")
-    
+
     # Deprecate a version
     api_mgr.deprecate_version("v1", "2025-12-31")
-    
+
     # Step 5: Demonstrate Configuration
     print_section("Step 5: Configuration Management Demo")
     from enhanced_workspace.integration.config_template import expansion_config
-    
+
     print("   Default Configuration:")
     print(f"   - Plugin settings: {expansion_config['plugin_settings']}")
     print(f"   - API settings: {expansion_config['api_settings']}")
     print(f"   - Feature flags: {expansion_config['feature_flags']}")
-    
+
     # Modify configuration
     expansion_config["expansion_modules"]["analytics"]["enabled"] = True
     expansion_config["expansion_modules"]["analytics"]["config"] = {
         "tracking_enabled": True,
-        "retention_days": 90
+        "retention_days": 90,
     }
-    
+
     print("\n   Updated Configuration:")
-    print(f"   - Analytics enabled: {expansion_config['expansion_modules']['analytics']['enabled']}")
+    print(
+        f"   - Analytics enabled: {expansion_config['expansion_modules']['analytics']['enabled']}"
+    )
     print(f"   - Analytics config: {expansion_config['expansion_modules']['analytics']['config']}")
-    
+
     # Step 6: Display Documentation
     print_section("Step 6: Expansion Documentation")
     doc_path = Path("enhanced_workspace/integration/EXPANSION_GUIDE.md")
     if doc_path.exists():
         print(f"    Documentation available at: {doc_path}")
-        with open(doc_path, 'r') as f:
+        with open(doc_path, "r") as f:
             lines = f.readlines()[:10]
             print("\n   First few lines of documentation:")
             for line in lines:
                 print(f"   {line.rstrip()}")
-    
+
     # Step 7: Summary
     print_section("Summary")
-    
+
     created_files = [
         "enhanced_workspace/integration/plugin_architecture.py",
         "enhanced_workspace/integration/api_versioning.py",
         "enhanced_workspace/integration/config_template.py",
-        "enhanced_workspace/integration/EXPANSION_GUIDE.md"
+        "enhanced_workspace/integration/EXPANSION_GUIDE.md",
     ]
-    
+
     print("    Expansion Framework Components Created:")
     for file_path in created_files:
         if Path(file_path).exists():
             size = Path(file_path).stat().st_size
             print(f"       {file_path} ({size} bytes)")
-    
+
     print("\n    Key Features:")
     print("       Plugin architecture with hooks system")
     print("       API versioning for backward compatibility")
     print("       Configuration templates for easy customization")
     print("       Comprehensive documentation for developers")
-    
+
     print("\n    Next Steps:")
     print("      1. Create custom plugins by extending BasePlugin")
     print("      2. Register API versions for your endpoints")
     print("      3. Configure expansion modules as needed")
     print("      4. Review EXPANSION_GUIDE.md for detailed instructions")
-    
+
     print("\n" + "=" * 70)
     print("    DEMONSTRATION COMPLETED SUCCESSFULLY")
     print("=" * 70 + "\n")
 
 
would reformat /home/runner/work/ymera_y/ymera_y/demo_expansion_framework.py
error: cannot format /home/runner/work/ymera_y/ymera_y/disaster_recovery__init__.py: Cannot parse for target version Python 3.12: 4:0:         for resource in plan['resources']:
--- /home/runner/work/ymera_y/ymera_y/document_generation_engine.py	2025-10-19 22:47:02.801432+00:00
+++ /home/runner/work/ymera_y/ymera_y/document_generation_engine.py	2025-10-19 23:09:09.283111+00:00
@@ -22,10 +22,11 @@
 logger = structlog.get_logger(__name__)
 
 
 class DocumentFormat(Enum):
     """Supported document formats"""
+
     PDF = "pdf"
     DOCX = "docx"
     HTML = "html"
     MARKDOWN = "md"
     TEXT = "txt"
@@ -35,10 +36,11 @@
 
 
 @dataclass
 class DocumentMetadata:
     """Document metadata"""
+
     title: str
     author: str = "YMERA Agent System"
     subject: str = ""
     keywords: List[str] = field(default_factory=list)
     created_at: datetime = field(default_factory=datetime.now)
@@ -46,55 +48,55 @@
 
 
 @dataclass
 class DocumentSection:
     """Document section"""
+
     title: str
     content: str
     level: int = 1
 
 
 @dataclass
 class DocumentTable:
     """Document table"""
+
     headers: List[str]
     rows: List[List[Any]]
 
 
 @dataclass
 class DocumentContent:
     """Complete document content"""
+
     metadata: DocumentMetadata
     sections: List[DocumentSection] = field(default_factory=list)
     tables: List[DocumentTable] = field(default_factory=list)
     custom_data: Dict[str, Any] = field(default_factory=dict)
 
 
 class DocumentGenerationEngine:
     """Production-ready document generation engine"""
-    
+
     def __init__(self, output_dir: str = "./generated_documents"):
         self.output_dir = Path(output_dir)
         self.output_dir.mkdir(parents=True, exist_ok=True)
         self.logger = structlog.get_logger(__name__)
-        
+
         self.logger.info("Document Generation Engine initialized", output_dir=str(self.output_dir))
-    
+
     async def generate_document(
-        self,
-        content: DocumentContent,
-        format: DocumentFormat,
-        filename: Optional[str] = None
+        self, content: DocumentContent, format: DocumentFormat, filename: Optional[str] = None
     ) -> Dict[str, Any]:
         """Generate document in specified format"""
         try:
             if not filename:
                 timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                 filename = f"{content.metadata.title.replace(' ', '_')}_{timestamp}.{format.value}"
-            
+
             filepath = self.output_dir / filename
-            
+
             if format == DocumentFormat.PDF:
                 await self._generate_pdf(content, filepath)
             elif format == DocumentFormat.HTML:
                 await self._generate_html(content, filepath)
             elif format == DocumentFormat.MARKDOWN:
@@ -107,32 +109,32 @@
                 await self._generate_xml(content, filepath)
             elif format == DocumentFormat.CSV:
                 await self._generate_csv(content, filepath)
             else:
                 raise ValueError(f"Unsupported format: {format}")
-            
+
             file_size = filepath.stat().st_size
-            
+
             return {
-                'success': True,
-                'filepath': str(filepath),
-                'filename': filename,
-                'format': format.value,
-                'size_bytes': file_size,
-                'size_kb': round(file_size / 1024, 2)
+                "success": True,
+                "filepath": str(filepath),
+                "filename": filename,
+                "format": format.value,
+                "size_bytes": file_size,
+                "size_kb": round(file_size / 1024, 2),
             }
-            
+
         except Exception as e:
             self.logger.error(f"Document generation failed: {e}", exc_info=True)
-            return {'success': False, 'error': str(e), 'format': format.value}
-    
+            return {"success": False, "error": str(e), "format": format.value}
+
     async def _generate_pdf(self, content: DocumentContent, filepath: Path):
         """Generate PDF (simplified - requires reportlab for full implementation)"""
         # For now, generate HTML and note it needs conversion
-        await self._generate_html(content, filepath.with_suffix('.html'))
+        await self._generate_html(content, filepath.with_suffix(".html"))
         self.logger.info("PDF generation requires reportlab - generated HTML instead")
-    
+
     async def _generate_html(self, content: DocumentContent, filepath: Path):
         """Generate HTML document"""
         html = f"""<!DOCTYPE html>
 <html>
 <head>
@@ -150,15 +152,15 @@
 <body>
     <h1>{content.metadata.title}</h1>
     <p><strong>Author:</strong> {content.metadata.author}</p>
     <p><strong>Created:</strong> {content.metadata.created_at.strftime('%Y-%m-%d %H:%M:%S')}</p>
 """
-        
+
         for section in content.sections:
             html += f"\n<h{section.level}>{section.title}</h{section.level}>\n"
             html += f"<p>{section.content}</p>\n"
-        
+
         for table in content.tables:
             html += "\n<table><thead><tr>"
             for header in table.headers:
                 html += f"<th>{header}</th>"
             html += "</tr></thead><tbody>"
@@ -166,97 +168,99 @@
                 html += "<tr>"
                 for cell in row:
                     html += f"<td>{cell}</td>"
                 html += "</tr>"
             html += "</tbody></table>\n"
-        
+
         html += "</body></html>"
-        filepath.write_text(html, encoding='utf-8')
-    
+        filepath.write_text(html, encoding="utf-8")
+
     async def _generate_markdown(self, content: DocumentContent, filepath: Path):
         """Generate Markdown document"""
         md = f"# {content.metadata.title}\n\n"
         md += f"**Author:** {content.metadata.author}  \n"
         md += f"**Created:** {content.metadata.created_at.strftime('%Y-%m-%d %H:%M:%S')}  \n\n"
         md += "---\n\n"
-        
+
         for section in content.sections:
             md += f"\n{'#' * section.level} {section.title}\n\n{section.content}\n\n"
-        
+
         for table in content.tables:
             md += "\n| " + " | ".join(table.headers) + " |\n"
             md += "| " + " | ".join("---" for _ in table.headers) + " |\n"
             for row in table.rows:
                 md += "| " + " | ".join(str(cell) for cell in row) + " |\n"
             md += "\n"
-        
-        filepath.write_text(md, encoding='utf-8')
-    
+
+        filepath.write_text(md, encoding="utf-8")
+
     async def _generate_text(self, content: DocumentContent, filepath: Path):
         """Generate plain text document"""
         lines = []
         lines.append("=" * 80)
         lines.append(content.metadata.title.center(80))
         lines.append("=" * 80)
         lines.append(f"\nAuthor: {content.metadata.author}")
         lines.append(f"Created: {content.metadata.created_at.strftime('%Y-%m-%d %H:%M:%S')}\n")
         lines.append("-" * 80 + "\n")
-        
+
         for section in content.sections:
             lines.append(f"\n{section.title.upper()}\n{'-' * len(section.title)}\n")
             lines.append(f"{section.content}\n")
-        
-        filepath.write_text('\n'.join(lines), encoding='utf-8')
-    
+
+        filepath.write_text("\n".join(lines), encoding="utf-8")
+
     async def _generate_json(self, content: DocumentContent, filepath: Path):
         """Generate JSON document"""
         data = {
-            'metadata': {
-                'title': content.metadata.title,
-                'author': content.metadata.author,
-                'created_at': content.metadata.created_at.isoformat()
+            "metadata": {
+                "title": content.metadata.title,
+                "author": content.metadata.author,
+                "created_at": content.metadata.created_at.isoformat(),
             },
-            'sections': [{'title': s.title, 'content': s.content, 'level': s.level} for s in content.sections],
-            'tables': [{'headers': t.headers, 'rows': t.rows} for t in content.tables]
+            "sections": [
+                {"title": s.title, "content": s.content, "level": s.level} for s in content.sections
+            ],
+            "tables": [{"headers": t.headers, "rows": t.rows} for t in content.tables],
         }
-        filepath.write_text(json.dumps(data, indent=2), encoding='utf-8')
-    
+        filepath.write_text(json.dumps(data, indent=2), encoding="utf-8")
+
     async def _generate_xml(self, content: DocumentContent, filepath: Path):
         """Generate XML document"""
-        root = ET.Element('document')
-        metadata = ET.SubElement(root, 'metadata')
-        ET.SubElement(metadata, 'title').text = content.metadata.title
-        ET.SubElement(metadata, 'author').text = content.metadata.author
-        
-        sections_elem = ET.SubElement(root, 'sections')
-        for section in content.sections:
-            sec = ET.SubElement(sections_elem, 'section', level=str(section.level))
-            ET.SubElement(sec, 'title').text = section.title
-            ET.SubElement(sec, 'content').text = section.content
-        
+        root = ET.Element("document")
+        metadata = ET.SubElement(root, "metadata")
+        ET.SubElement(metadata, "title").text = content.metadata.title
+        ET.SubElement(metadata, "author").text = content.metadata.author
+
+        sections_elem = ET.SubElement(root, "sections")
+        for section in content.sections:
+            sec = ET.SubElement(sections_elem, "section", level=str(section.level))
+            ET.SubElement(sec, "title").text = section.title
+            ET.SubElement(sec, "content").text = section.content
+
         tree = ET.ElementTree(root)
         ET.indent(tree, space="  ")
-        tree.write(str(filepath), encoding='utf-8', xml_declaration=True)
-    
+        tree.write(str(filepath), encoding="utf-8", xml_declaration=True)
+
     async def _generate_csv(self, content: DocumentContent, filepath: Path):
         """Generate CSV document"""
-        with open(filepath, 'w', newline='', encoding='utf-8') as f:
+        with open(filepath, "w", newline="", encoding="utf-8") as f:
             writer = csv.writer(f)
             writer.writerow([f"# {content.metadata.title}"])
             writer.writerow([f"# Author: {content.metadata.author}"])
             writer.writerow([])
-            
+
             for table in content.tables:
                 writer.writerow(table.headers)
                 for row in table.rows:
                     writer.writerow(row)
                 writer.writerow([])
 
 
 __all__ = [
-    'DocumentGenerationEngine',
-    'DocumentFormat',
-    'DocumentMetadata',
-    'DocumentSection',
-    'DocumentTable',
-    'DocumentContent'
+    "DocumentGenerationEngine",
+    "DocumentFormat",
+    "DocumentMetadata",
+    "DocumentSection",
+    "DocumentTable",
+    "DocumentContent",
 ]
would reformat /home/runner/work/ymera_y/ymera_y/document_generation_engine.py
--- /home/runner/work/ymera_y/ymera_y/deployment_script.py	2025-10-19 22:47:02.800432+00:00
+++ /home/runner/work/ymera_y/ymera_y/deployment_script.py	2025-10-19 23:09:09.477003+00:00
@@ -29,322 +29,344 @@
 
 # ===============================================================================
 # COLORED OUTPUT
 # ===============================================================================
 
+
 class Colors:
-    HEADER = '\033[95m'
-    BLUE = '\033[94m'
-    CYAN = '\033[96m'
-    GREEN = '\033[92m'
-    YELLOW = '\033[93m'
-    RED = '\033[91m'
-    END = '\033[0m'
-    BOLD = '\033[1m'
-    UNDERLINE = '\033[4m'
+    HEADER = "\033[95m"
+    BLUE = "\033[94m"
+    CYAN = "\033[96m"
+    GREEN = "\033[92m"
+    YELLOW = "\033[93m"
+    RED = "\033[91m"
+    END = "\033[0m"
+    BOLD = "\033[1m"
+    UNDERLINE = "\033[4m"
+
 
 def print_header(msg: str):
     print(f"\n{Colors.HEADER}{Colors.BOLD}{'='*80}{Colors.END}")
     print(f"{Colors.HEADER}{Colors.BOLD}{msg.center(80)}{Colors.END}")
     print(f"{Colors.HEADER}{Colors.BOLD}{'='*80}{Colors.END}\n")
 
+
 def print_success(msg: str):
     print(f"{Colors.GREEN} {msg}{Colors.END}")
 
+
 def print_error(msg: str):
     print(f"{Colors.RED} {msg}{Colors.END}")
 
+
 def print_warning(msg: str):
     print(f"{Colors.YELLOW}  {msg}{Colors.END}")
 
+
 def print_info(msg: str):
     print(f"{Colors.CYAN}  {msg}{Colors.END}")
 
+
 # ===============================================================================
 # ISSUE DETECTION
 # ===============================================================================
+
 
 class IssueDetector:
     """Detect deployment issues in the codebase"""
-    
+
     def __init__(self):
         self.issues: List[Dict[str, Any]] = []
-    
+
     def check_database_import(self) -> bool:
         """Check if database wrapper is properly imported in __init__.py"""
         init_file = API_GATEWAY_DIR / "__init__.py"
-        
+
         if not init_file.exists():
-            self.issues.append({
-                "severity": "CRITICAL",
-                "issue": "__init__.py does not exist",
-                "file": str(init_file)
-            })
-            return False
-        
+            self.issues.append(
+                {
+                    "severity": "CRITICAL",
+                    "issue": "__init__.py does not exist",
+                    "file": str(init_file),
+                }
+            )
+            return False
+
         content = init_file.read_text()
-        
+
         if "from . import database" not in content:
-            self.issues.append({
-                "severity": "CRITICAL",
-                "issue": "Missing 'from . import database' in __init__.py",
-                "file": str(init_file)
-            })
-            return False
-        
+            self.issues.append(
+                {
+                    "severity": "CRITICAL",
+                    "issue": "Missing 'from . import database' in __init__.py",
+                    "file": str(init_file),
+                }
+            )
+            return False
+
         return True
-    
+
     def check_database_module(self) -> bool:
         """Check if database.py module exists"""
         db_file = API_GATEWAY_DIR / "database.py"
-        
+
         if not db_file.exists():
-            self.issues.append({
-                "severity": "CRITICAL",
-                "issue": "database.py module does not exist",
-                "file": str(db_file)
-            })
-            return False
-        
+            self.issues.append(
+                {
+                    "severity": "CRITICAL",
+                    "issue": "database.py module does not exist",
+                    "file": str(db_file),
+                }
+            )
+            return False
+
         return True
-    
+
     def check_encoding_issues(self) -> bool:
         """Check for smart quote encoding issues"""
         gateway_file = API_GATEWAY_DIR / "gateway_routing.py"
-        
+
         if not gateway_file.exists():
             print_warning(f"gateway_routing.py not found, skipping encoding check")
             return True
-        
-        content = gateway_file.read_text(encoding='utf-8', errors='ignore')
-        
+
+        content = gateway_file.read_text(encoding="utf-8", errors="ignore")
+
         # Check for smart quotes
-        smart_quotes = ['', '', '"', '"']
+        smart_quotes = ["", "", '"', '"']
         has_issues = any(sq in content for sq in smart_quotes)
-        
+
         if has_issues:
-            self.issues.append({
-                "severity": "HIGH",
-                "issue": "Smart quotes found in gateway_routing.py",
-                "file": str(gateway_file)
-            })
-            return False
-        
+            self.issues.append(
+                {
+                    "severity": "HIGH",
+                    "issue": "Smart quotes found in gateway_routing.py",
+                    "file": str(gateway_file),
+                }
+            )
+            return False
+
         return True
-    
+
     def check_import_consistency(self) -> bool:
         """Check for import path inconsistencies"""
         route_files = [
             "ymera_auth_routes.py",
             "ymera_agent_routes.py",
             "ymera_file_routes.py",
             "project_routes.py",
-            "websocket_routes.py"
+            "websocket_routes.py",
         ]
-        
+
         inconsistencies = []
-        
+
         for route_file in route_files:
             file_path = API_GATEWAY_DIR / route_file
             if not file_path.exists():
                 continue
-            
+
             content = file_path.read_text()
-            
+
             # Check for inconsistent imports
             if "from config.settings" in content and "from app.CORE_CONFIGURATION" in content:
                 inconsistencies.append(route_file)
-        
+
         if inconsistencies:
-            self.issues.append({
-                "severity": "MEDIUM",
-                "issue": f"Import inconsistencies in: {', '.join(inconsistencies)}",
-                "file": "Multiple files"
-            })
-            return False
-        
+            self.issues.append(
+                {
+                    "severity": "MEDIUM",
+                    "issue": f"Import inconsistencies in: {', '.join(inconsistencies)}",
+                    "file": "Multiple files",
+                }
+            )
+            return False
+
         return True
-    
+
     def run_all_checks(self) -> bool:
         """Run all issue detection checks"""
         print_header("RUNNING ISSUE DETECTION")
-        
+
         checks = [
             ("Database Import", self.check_database_import),
             ("Database Module", self.check_database_module),
             ("Encoding Issues", self.check_encoding_issues),
-            ("Import Consistency", self.check_import_consistency)
+            ("Import Consistency", self.check_import_consistency),
         ]
-        
+
         all_passed = True
-        
+
         for check_name, check_func in checks:
             print_info(f"Checking: {check_name}")
             passed = check_func()
             if passed:
                 print_success(f"{check_name}: PASSED")
             else:
                 print_error(f"{check_name}: FAILED")
                 all_passed = False
-        
+
         return all_passed
 
+
 # ===============================================================================
 # FIX APPLIER
 # ===============================================================================
+
 
 class FixApplier:
     """Apply fixes to the codebase"""
-    
+
     def __init__(self, backup: bool = True):
         self.backup_enabled = backup
-    
+
     def create_backup(self) -> bool:
         """Create backup of API Gateway directory"""
         if not self.backup_enabled:
             return True
-        
+
         print_info("Creating backup...")
-        
+
         try:
             BACKUP_DIR.mkdir(parents=True, exist_ok=True)
             backup_path = BACKUP_DIR / "API_GATEWAY_CORE_ROUTES"
             shutil.copytree(API_GATEWAY_DIR, backup_path)
             print_success(f"Backup created: {backup_path}")
             return True
         except Exception as e:
             print_error(f"Backup failed: {e}")
             return False
-    
+
     def fix_database_import(self) -> bool:
         """Fix database import in __init__.py"""
         print_info("Fixing database import in __init__.py...")
-        
+
         init_file = API_GATEWAY_DIR / "__init__.py"
-        
+
         # This would contain the full __init__.py content from the artifact
         # For brevity, showing the key addition
-        
+
         try:
             content = init_file.read_text()
-            
+
             # Add database import if missing
             if "from . import database" not in content:
                 # Insert after imports section
-                lines = content.split('\n')
+                lines = content.split("\n")
                 import_index = 0
                 for i, line in enumerate(lines):
-                    if line.strip().startswith('from .'):
+                    if line.strip().startswith("from ."):
                         import_index = i + 1
-                
+
                 lines.insert(import_index, "    from . import database")
-                
-                init_file.write_text('\n'.join(lines))
+
+                init_file.write_text("\n".join(lines))
                 print_success("Database import added to __init__.py")
             else:
                 print_info("Database import already present")
-            
+
             return True
         except Exception as e:
             print_error(f"Failed to fix database import: {e}")
             return False
-    
+
     def create_database_module(self) -> bool:
         """Create database.py module"""
         print_info("Creating database.py module...")
-        
+
         db_file = API_GATEWAY_DIR / "database.py"
-        
+
         if db_file.exists():
             print_warning("database.py already exists, skipping")
             return True
-        
+
         # Content from the database.py artifact
         database_content = '''"""
 YMERA Enterprise - Database Wrapper Module
 Production-Ready Database Connection Management - v4.0
 """
 # ... (full content from artifact)
 '''
-        
+
         try:
             db_file.write_text(database_content)
             print_success("database.py module created")
             return True
         except Exception as e:
             print_error(f"Failed to create database.py: {e}")
             return False
-    
+
     def fix_encoding_issues(self) -> bool:
         """Fix smart quote encoding issues"""
         print_info("Fixing encoding issues...")
-        
+
         gateway_file = API_GATEWAY_DIR / "gateway_routing.py"
-        
+
         if not gateway_file.exists():
             print_warning("gateway_routing.py not found, skipping")
             return True
-        
+
         try:
-            content = gateway_file.read_text(encoding='utf-8')
-            
+            content = gateway_file.read_text(encoding="utf-8")
+
             # Replace smart quotes
             replacements = {
-                '': '"',
-                '': '"',
+                "": '"',
+                "": '"',
                 '"': '"',
                 '"': '"',
-                ''': "'",
-                ''': "'"
+                """: "'",
+                """: "'",
             }
-            
+
             for old, new in replacements.items():
                 content = content.replace(old, new)
-            
-            gateway_file.write_text(content, encoding='utf-8')
+
+            gateway_file.write_text(content, encoding="utf-8")
             print_success("Encoding issues fixed")
             return True
         except Exception as e:
             print_error(f"Failed to fix encoding: {e}")
             return False
-    
+
     def apply_all_fixes(self) -> bool:
         """Apply all fixes"""
         print_header("APPLYING FIXES")
-        
+
         # Create backup first
         if not self.create_backup():
             print_error("Backup failed, aborting fixes")
             return False
-        
+
         fixes = [
             ("Database Import", self.fix_database_import),
             ("Database Module", self.create_database_module),
-            ("Encoding Issues", self.fix_encoding_issues)
+            ("Encoding Issues", self.fix_encoding_issues),
         ]
-        
+
         all_passed = True
-        
+
         for fix_name, fix_func in fixes:
             print_info(f"Applying fix: {fix_name}")
             passed = fix_func()
             if not passed:
                 all_passed = False
-        
+
         return all_passed
 
+
 # ===============================================================================
 # VERIFICATION
 # ===============================================================================
+
 
 class Verifier:
     """Verify that fixes were applied correctly"""
-    
+
     def verify_imports(self) -> bool:
         """Verify Python imports work"""
         print_info("Verifying Python imports...")
-        
+
         test_script = """
 import sys
 sys.path.insert(0, 'backend')
 
 try:
@@ -361,124 +383,99 @@
     sys.exit(0)
 except ImportError as e:
     print(f' Import failed: {e}')
     sys.exit(1)
 """
-        
+
         try:
             result = subprocess.run(
-                [sys.executable, "-c", test_script],
-                capture_output=True,
-                text=True
+                [sys.executable, "-c", test_script], capture_output=True, text=True
             )
-            
+
             if result.returncode == 0:
                 print_success("Import verification passed")
                 return True
             else:
                 print_error(f"Import verification failed:\n{result.stderr}")
                 return False
         except Exception as e:
             print_error(f"Verification failed: {e}")
             return False
-    
+
     def verify_syntax(self) -> bool:
         """Verify Python syntax"""
         print_info("Verifying Python syntax...")
-        
-        files_to_check = [
-            API_GATEWAY_DIR / "__init__.py",
-            API_GATEWAY_DIR / "database.py"
-        ]
-        
+
+        files_to_check = [API_GATEWAY_DIR / "__init__.py", API_GATEWAY_DIR / "database.py"]
+
         all_valid = True
-        
+
         for file_path in files_to_check:
             if not file_path.exists():
                 continue
-            
+
             try:
                 result = subprocess.run(
                     [sys.executable, "-m", "py_compile", str(file_path)],
                     capture_output=True,
-                    text=True
+                    text=True,
                 )
-                
+
                 if result.returncode == 0:
                     print_success(f"Syntax valid: {file_path.name}")
                 else:
                     print_error(f"Syntax error in {file_path.name}")
                     all_valid = False
             except Exception as e:
                 print_error(f"Syntax check failed for {file_path.name}: {e}")
                 all_valid = False
-        
+
         return all_valid
-    
+
     def run_all_verifications(self) -> bool:
         """Run all verifications"""
         print_header("RUNNING VERIFICATION")
-        
+
         checks = [
             ("Syntax Verification", self.verify_syntax),
-            ("Import Verification", self.verify_imports)
+            ("Import Verification", self.verify_imports),
         ]
-        
+
         all_passed = True
-        
+
         for check_name, check_func in checks:
             print_info(f"Running: {check_name}")
             passed = check_func()
             if not passed:
                 all_passed = False
-        
+
         return all_passed
 
+
 # ===============================================================================
 # MAIN
 # ===============================================================================
 
+
 def main():
-    parser = argparse.ArgumentParser(
-        description="YMERA Platform Deployment Fix Script"
-    )
-    parser.add_argument(
-        "--check",
-        action="store_true",
-        help="Check for issues"
-    )
-    parser.add_argument(
-        "--fix",
-        action="store_true",
-        help="Apply fixes"
-    )
-    parser.add_argument(
-        "--verify",
-        action="store_true",
-        help="Verify fixes"
-    )
-    parser.add_argument(
-        "--all",
-        action="store_true",
-        help="Run check, fix, and verify"
-    )
-    parser.add_argument(
-        "--no-backup",
-        action="store_true",
-        help="Skip backup creation"
-    )
-    
+    parser = argparse.ArgumentParser(description="YMERA Platform Deployment Fix Script")
+    parser.add_argument("--check", action="store_true", help="Check for issues")
+    parser.add_argument("--fix", action="store_true", help="Apply fixes")
+    parser.add_argument("--verify", action="store_true", help="Verify fixes")
+    parser.add_argument("--all", action="store_true", help="Run check, fix, and verify")
+    parser.add_argument("--no-backup", action="store_true", help="Skip backup creation")
+
     args = parser.parse_args()
-    
+
     # Default to --all if no args
     if not (args.check or args.fix or args.verify or args.all):
         args.all = True
-    
+
     print_header("YMERA DEPLOYMENT FIX SCRIPT v4.0.0")
-    
+
     success = True
-    
+
     # Check
     if args.check or args.all:
         detector = IssueDetector()
         if not detector.run_all_checks():
             print_error("\nIssues detected:")
@@ -486,36 +483,38 @@
                 print_error(f"  [{issue['severity']}] {issue['issue']}")
                 print_info(f"    File: {issue['file']}")
             success = False
         else:
             print_success("\n No issues detected!")
-    
+
     # Fix
     if (args.fix or args.all) and not success:
         applier = FixApplier(backup=not args.no_backup)
         if not applier.apply_all_fixes():
             print_error("\nFix application failed")
             sys.exit(1)
         else:
             print_success("\n All fixes applied successfully!")
-    
+
     # Verify
     if args.verify or args.all:
         verifier = Verifier()
         if not verifier.run_all_verifications():
             print_error("\nVerification failed")
             sys.exit(1)
         else:
             print_success("\n All verifications passed!")
-    
+
     print_header("DEPLOYMENT FIX COMPLETE")
     print_success(" Your YMERA platform is ready for deployment!")
     print_info("\nNext steps:")
     print_info("  1. Review the changes")
     print_info("  2. Update your .env file")
     print_info("  3. Run database migrations")
     print_info("  4. Start your application")
     print_info("  5. Test all endpoints")
 
+
 if __name__ == "__main__":
     import time
+
     main()
would reformat /home/runner/work/ymera_y/ymera_y/deployment_script.py
--- /home/runner/work/ymera_y/ymera_y/documentation.py	2025-10-19 22:47:02.801432+00:00
+++ /home/runner/work/ymera_y/ymera_y/documentation.py	2025-10-19 23:09:09.564924+00:00
@@ -6,44 +6,46 @@
 from fastapi import FastAPI, APIRouter, Depends, Query
 from fastapi.openapi.docs import get_swagger_ui_html, get_redoc_html
 from fastapi.openapi.utils import get_openapi
 from pydantic import BaseModel, Field
 
+
 def setup_documentation(app: FastAPI) -> None:
     """Setup custom OpenAPI documentation"""
-    
+
     @app.get("/api/docs", include_in_schema=False)
     async def custom_swagger_ui_html():
         """Custom Swagger UI with branding"""
         return get_swagger_ui_html(
             openapi_url="/api/openapi.json",
             title="YMERA Enterprise Agent Manager API",
             swagger_js_url="https://cdn.jsdelivr.net/npm/swagger-ui-dist@4.15.5/swagger-ui-bundle.js",
             swagger_css_url="https://cdn.jsdelivr.net/npm/swagger-ui-dist@4.15.5/swagger-ui.css",
-            swagger_favicon_url="/static/favicon.ico"
+            swagger_favicon_url="/static/favicon.ico",
         )
-    
+
     @app.get("/api/redoc", include_in_schema=False)
     async def redoc_html():
         """ReDoc documentation"""
         return get_redoc_html(
             openapi_url="/api/openapi.json",
             title="YMERA Enterprise Agent Manager API",
             redoc_js_url="https://cdn.jsdelivr.net/npm/redoc@2.0.0-rc.55/bundles/redoc.standalone.js",
-            redoc_favicon_url="/static/favicon.ico"
+            redoc_favicon_url="/static/favicon.ico",
         )
-    
+
     @app.get("/api/openapi.json", include_in_schema=False)
     async def get_openapi_endpoint():
         """Custom OpenAPI schema generator"""
         return get_custom_openapi(app)
 
+
 def get_custom_openapi(app: FastAPI):
     """Generate custom OpenAPI schema with enhanced documentation"""
     if app.openapi_schema:
         return app.openapi_schema
-    
+
     openapi_schema = get_openapi(
         title="YMERA Enterprise Agent Manager API",
         version="2.0.0",
         description="""
         Enterprise-grade Agent Management Platform
@@ -66,11 +68,11 @@
         API endpoints are protected by rate limiting. Exceed limits will result in
         HTTP 429 (Too Many Requests) responses.
         """,
         routes=app.routes,
     )
-    
+
     # Enhanced security schemes
     openapi_schema["components"]["securitySchemes"] = {
         "OAuth2": {
             "type": "oauth2",
             "flows": {
@@ -80,68 +82,45 @@
                     "scopes": {
                         "read:agents": "Read agent information",
                         "write:agents": "Create and update agents",
                         "read:tasks": "Read task information",
                         "write:tasks": "Create and manage tasks",
-                        "admin": "Administrative access"
-                    }
+                        "admin": "Administrative access",
+                    },
                 }
-            }
+            },
         },
-        "ApiKey": {
-            "type": "apiKey",
-            "in": "header",
-            "name": "X-API-Key"
-        }
+        "ApiKey": {"type": "apiKey", "in": "header", "name": "X-API-Key"},
     }
-    
+
     # Apply to all routes
-    openapi_schema["security"] = [
-        {"OAuth2": ["read:agents", "read:tasks"]},
-        {"ApiKey": []}
-    ]
-    
+    openapi_schema["security"] = [{"OAuth2": ["read:agents", "read:tasks"]}, {"ApiKey": []}]
+
     # Custom OpenAPI extensions
-    openapi_schema["info"]["x-logo"] = {
-        "url": "/static/logo.png"
-    }
-    
+    openapi_schema["info"]["x-logo"] = {"url": "/static/logo.png"}
+
     # Add tags
     openapi_schema["tags"] = [
-        {
-            "name": "agents",
-            "description": "Agent management operations"
-        },
-        {
-            "name": "tasks",
-            "description": "Task assignment and execution"
-        },
-        {
-            "name": "admin",
-            "description": "Administrative operations"
-        },
-        {
-            "name": "monitoring",
-            "description": "System monitoring and health checks"
-        },
-        {
-            "name": "security",
-            "description": "Security operations and audit logs"
-        }
+        {"name": "agents", "description": "Agent management operations"},
+        {"name": "tasks", "description": "Task assignment and execution"},
+        {"name": "admin", "description": "Administrative operations"},
+        {"name": "monitoring", "description": "System monitoring and health checks"},
+        {"name": "security", "description": "Security operations and audit logs"},
     ]
-    
+
     # Version deprecated APIs
     for path in openapi_schema["paths"]:
         for method in openapi_schema["paths"][path]:
             # Skip non-operation keys like parameters
             if method not in ["get", "post", "put", "delete", "patch"]:
                 continue
-            
+
             # Add deprecation notices for v1 APIs
             if "/v1/" in path:
                 openapi_schema["paths"][path][method]["deprecated"] = True
-                openapi_schema["paths"][path][method]["description"] = \
-                    (openapi_schema["paths"][path][method].get("description", "") + 
-                    "\n\n**Deprecated**: Please use v2 API endpoints instead.")
-    
+                openapi_schema["paths"][path][method]["description"] = (
+                    openapi_schema["paths"][path][method].get("description", "")
+                    + "\n\n**Deprecated**: Please use v2 API endpoints instead."
+                )
+
     app.openapi_schema = openapi_schema
-    return app.openapi_schema
\ No newline at end of file
+    return app.openapi_schema
would reformat /home/runner/work/ymera_y/ymera_y/documentation.py
--- /home/runner/work/ymera_y/ymera_y/drafting_agent.py	2025-10-19 22:47:02.801432+00:00
+++ /home/runner/work/ymera_y/ymera_y/drafting_agent.py	2025-10-19 23:09:10.887258+00:00
@@ -1,6 +1,5 @@
-
 """
 Advanced Drafting Agent
 Content creation, document generation, template management, and collaborative writing
 """
 
@@ -8,24 +7,33 @@
 import json
 import time
 import re
 import uuid
 import difflib
-import traceback # Added for detailed error logging
-import os # Added for environment variables
+import traceback  # Added for detailed error logging
+import os  # Added for environment variables
 from typing import Dict, List, Optional, Any, Union
-from dataclasses import dataclass, field, asdict # Added asdict for easier serialization
+from dataclasses import dataclass, field, asdict  # Added asdict for easier serialization
 from enum import Enum
 from datetime import datetime
 import nltk
 from nltk.tokenize import sent_tokenize, word_tokenize
 from nltk.corpus import stopwords
 from textstat import flesch_reading_ease, flesch_kincaid_grade
 import spacy
 
-from base_agent import BaseAgent, AgentConfig, TaskRequest, TaskResponse, Priority, AgentStatus, TaskStatus # Added TaskStatus
+from base_agent import (
+    BaseAgent,
+    AgentConfig,
+    TaskRequest,
+    TaskResponse,
+    Priority,
+    AgentStatus,
+    TaskStatus,
+)  # Added TaskStatus
 from opentelemetry import trace
+
 
 class DocumentType(Enum):
     REPORT = "report"
     EMAIL = "email"
     PROPOSAL = "proposal"
@@ -35,35 +43,39 @@
     BLOG_POST = "blog_post"
     API_DOCUMENTATION = "api_documentation"
     USER_MANUAL = "user_manual"
     PRESENTATION = "presentation"
 
+
 class ContentTone(Enum):
     FORMAL = "formal"
     CASUAL = "casual"
     PROFESSIONAL = "professional"
     FRIENDLY = "friendly"
     TECHNICAL = "technical"
     PERSUASIVE = "persuasive"
     INFORMATIVE = "informative"
     CREATIVE = "creative"
 
+
 class DraftStatus(Enum):
     DRAFT = "draft"
     REVIEW = "review"
     REVISION = "revision"
     APPROVED = "approved"
     PUBLISHED = "published"
+
 
 @dataclass
 class ContentTemplate:
     template_id: str
     name: str
     document_type: DocumentType
     sections: List[Dict[str, Any]]
     variables: Dict[str, str]
     metadata: Dict[str, Any] = field(default_factory=dict)
+
 
 @dataclass
 class DocumentDraft:
     draft_id: str
     title: str
@@ -80,10 +92,11 @@
     metadata: Dict[str, Any] = field(default_factory=dict)
     collaborators: List[str] = field(default_factory=list)
     comments: List[Dict[str, Any]] = field(default_factory=list)
     revision_history: List[Dict[str, Any]] = field(default_factory=list)
 
+
 @dataclass
 class ContentAnalysis:
     readability_score: float
     grade_level: float
     word_count: int
@@ -93,10 +106,11 @@
     complexity_score: float
     tone_analysis: Dict[str, float]
     keyword_density: Dict[str, float]
     suggestions: List[str]
 
+
 class DraftingAgent(BaseAgent):
     """
     Advanced Drafting Agent with:
     - Template-based content generation
     - Multi-format document support
@@ -105,75 +119,65 @@
     - Version control and revision tracking
     - Style guide enforcement
     - Automated proofreading and suggestions
     - Integration with LLM for content enhancement
     """
-    
+
     def __init__(self, config: AgentConfig):
         super().__init__(config)
-        
+
         # Content templates
         self.templates: Dict[str, ContentTemplate] = {}
-        
+
         # Active drafts
         self.drafts: Dict[str, DocumentDraft] = {}
-        
+
         # Collaborative sessions
         self.collaborative_sessions: Dict[str, Dict] = {}
-        
+
         # Style guides
         self.style_guides: Dict[str, Dict] = {}
-        
+
         # Content generation models
         self.nlp = None
-        
+
         # Drafting statistics
         self.drafting_stats = {
             "documents_created": 0,
             "templates_used": 0,
             "collaborations": 0,
             "revisions_made": 0,
-            "content_optimizations": 0
+            "content_optimizations": 0,
         }
-        
+
         # Load templates and initialize NLP
         self._load_default_templates()
         self._load_style_guides()
         asyncio.create_task(self._initialize_nlp())
-    
+
     async def start(self):
         """Start drafting agent services"""
         # The BaseAgent already subscribes to agent.{self.config.name}.task
-        
+
         # Subscribe to collaborative editing
-        await self._subscribe(
-            "drafting.collaborate",
-            self._handle_collaboration
-        )
-        
+        await self._subscribe("drafting.collaborate", self._handle_collaboration)
+
         # Subscribe to template requests
-        await self._subscribe(
-            "drafting.template",
-            self._handle_template_request
-        )
-        
+        await self._subscribe("drafting.template", self._handle_template_request)
+
         # Subscribe to content analysis requests
-        await self._subscribe(
-            "drafting.analyze",
-            self._handle_content_analysis
-        )
-        
+        await self._subscribe("drafting.analyze", self._handle_content_analysis)
+
         # Start background tasks
         asyncio.create_task(self._auto_save_drafts())
         asyncio.create_task(self._cleanup_sessions())
-        
-        self.logger.info("Drafting Agent started",
-                        template_count=len(self.templates))
-    
+
+        self.logger.info("Drafting Agent started", template_count=len(self.templates))
+
     def _load_default_templates(self):
         """Load default document templates"""
-        
+
         # Technical Report Template
         tech_report = ContentTemplate(
             template_id="tech_report_v1",
             name="Technical Report",
             document_type=DocumentType.TECHNICAL_DOC,
@@ -183,22 +187,22 @@
                 {"id": "methodology", "title": "Methodology", "required": False},
                 {"id": "findings", "title": "Findings", "required": True},
                 {"id": "analysis", "title": "Analysis", "required": True},
                 {"id": "recommendations", "title": "Recommendations", "required": True},
                 {"id": "conclusion", "title": "Conclusion", "required": True},
-                {"id": "appendices", "title": "Appendices", "required": False}
+                {"id": "appendices", "title": "Appendices", "required": False},
             ],
             variables={
                 "report_title": "Report Title",
                 "author": "Author Name",
                 "date": "Date",
                 "department": "Department",
-                "project_id": "Project ID"
-            }
+                "project_id": "Project ID",
+            },
         )
         self.templates[tech_report.template_id] = tech_report
-        
+
         # Business Proposal Template
         proposal = ContentTemplate(
             template_id="business_proposal_v1",
             name="Business Proposal",
             document_type=DocumentType.PROPOSAL,
@@ -209,22 +213,22 @@
                 {"id": "proposed_solution", "title": "Proposed Solution", "required": True},
                 {"id": "implementation_plan", "title": "Implementation Plan", "required": True},
                 {"id": "budget", "title": "Budget", "required": True},
                 {"id": "timeline", "title": "Timeline", "required": True},
                 {"id": "team", "title": "Team", "required": False},
-                {"id": "references", "title": "References", "required": False}
+                {"id": "references", "title": "References", "required": False},
             ],
             variables={
                 "proposal_title": "Proposal Title",
                 "client_name": "Client Name",
                 "company": "Company Name",
                 "contact_person": "Contact Person",
-                "submission_date": "Submission Date"
-            }
+                "submission_date": "Submission Date",
+            },
         )
         self.templates[proposal.template_id] = proposal
-        
+
         # API Documentation Template
         api_doc = ContentTemplate(
             template_id="api_documentation_v1",
             name="API Documentation",
             document_type=DocumentType.API_DOCUMENTATION,
@@ -235,21 +239,21 @@
                 {"id": "request_format", "title": "Request Format", "required": True},
                 {"id": "response_format", "title": "Response Format", "required": True},
                 {"id": "error_codes", "title": "Error Codes", "required": True},
                 {"id": "examples", "title": "Examples", "required": True},
                 {"id": "rate_limits", "title": "Rate Limits", "required": False},
-                {"id": "changelog", "title": "Changelog", "required": False}
+                {"id": "changelog", "title": "Changelog", "required": False},
             ],
             variables={
                 "api_name": "API Name",
                 "version": "API Version",
                 "base_url": "Base URL",
-                "contact_email": "Contact Email"
-            }
+                "contact_email": "Contact Email",
+            },
         )
         self.templates[api_doc.template_id] = api_doc
-    
+
     def _load_style_guides(self):
         """Load style guides for different document types"""
         self.style_guides = {
             "technical": {
                 "tone": "formal",
@@ -258,137 +262,145 @@
                 "passive_voice": {"max_percentage": 10},
                 "jargon_level": "moderate",
                 "formatting": {
                     "headings": "title_case",
                     "lists": "parallel_structure",
-                    "numbers": "spell_out_below_10"
-                }
+                    "numbers": "spell_out_below_10",
+                },
             },
             "business": {
                 "tone": "professional",
                 "sentence_length": {"max": 20, "preferred": 12},
                 "paragraph_length": {"max": 5, "preferred": 3},
                 "passive_voice": {"max_percentage": 15},
                 "jargon_level": "minimal",
                 "formatting": {
                     "headings": "title_case",
                     "currency": "spell_out_rounded",
-                    "dates": "full_format"
-                }
+                    "dates": "full_format",
+                },
             },
             "marketing": {
                 "tone": "persuasive",
                 "sentence_length": {"max": 18, "preferred": 10},
                 "paragraph_length": {"max": 4, "preferred": 2},
                 "passive_voice": {"max_percentage": 5},
                 "call_to_action": "required",
-                "formatting": {
-                    "headings": "sentence_case",
-                    "emphasis": "bold_for_benefits"
-                }
-            }
+                "formatting": {"headings": "sentence_case", "emphasis": "bold_for_benefits"},
+            },
         }
-    
+
     async def _initialize_nlp(self):
         """Initialize NLP models for content analysis"""
         try:
             # Download required NLTK data
             import ssl
+
             try:
                 _create_unverified_https_context = ssl._create_unverified_context
             except AttributeError:
                 pass
             else:
                 ssl._create_default_https_context = _create_unverified_https_context
-            
+
             nltk.download("punkt", quiet=True)
             nltk.download("stopwords", quiet=True)
             nltk.download("averaged_perceptron_tagger", quiet=True)
-            
+
             # Load spaCy model
             # Ensure the model is downloaded: python -m spacy download en_core_web_sm
             try:
                 self.nlp = spacy.load("en_core_web_sm")
             except OSError:
                 self.logger.warning("spaCy model 'en_core_web_sm' not found. Downloading...")
                 spacy.cli.download("en_core_web_sm")
                 self.nlp = spacy.load("en_core_web_sm")
-            
+
             self.logger.info("NLP models initialized successfully")
-            
+
         except Exception as e:
-            self.logger.error("Failed to initialize NLP models", error=str(e), traceback=traceback.format_exc())
-    
+            self.logger.error(
+                "Failed to initialize NLP models", error=str(e), traceback=traceback.format_exc()
+            )
+
     async def _execute_task_impl(self, request: TaskRequest) -> Dict[str, Any]:
         """Execute drafting-specific tasks"""
         task_type = request.task_type
         payload = request.payload
-        
+
         try:
             result: Dict[str, Any] = {}
             if task_type == "create_draft":
                 result = await self._create_draft(payload)
-            
+
             elif task_type == "update_draft":
                 result = await self._update_draft(payload)
-            
+
             elif task_type == "generate_content":
                 result = await self._generate_content(payload)
-            
+
             elif task_type == "analyze_content":
                 result = await self._analyze_content(payload)
-            
+
             elif task_type == "apply_template":
                 result = await self._apply_template(payload)
-            
+
             elif task_type == "collaborate":
                 result = await self._start_collaboration(payload)
-            
+
             elif task_type == "review_draft":
                 result = await self._review_draft(payload)
-            
+
             elif task_type == "optimize_content":
                 result = await self._optimize_content(payload)
-            
+
             elif task_type == "export_document":
                 result = await self._export_document(payload)
-            
+
             else:
                 raise ValueError(f"Unknown drafting task type: {task_type}")
-            
-            return TaskResponse(task_id=request.task_id, status=TaskStatus.COMPLETED, result=result).dict()
+
+            return TaskResponse(
+                task_id=request.task_id, status=TaskStatus.COMPLETED, result=result
+            ).dict()
 
         except Exception as e:
-            self.logger.error(f"Error executing drafting task {task_type}", error=str(e), traceback=traceback.format_exc())
-            return TaskResponse(task_id=request.task_id, status=TaskStatus.FAILED, error=str(e)).dict()
-    
+            self.logger.error(
+                f"Error executing drafting task {task_type}",
+                error=str(e),
+                traceback=traceback.format_exc(),
+            )
+            return TaskResponse(
+                task_id=request.task_id, status=TaskStatus.FAILED, error=str(e)
+            ).dict()
+
     async def _create_draft(self, payload: Dict) -> Dict[str, Any]:
         """Create a new document draft"""
         title = payload.get("title", "Untitled Document")
         document_type_str = payload.get("document_type", "report")
         template_id = payload.get("template_id")
         author_id = payload.get("author_id", "anonymous")
         tone_str = payload.get("tone", "professional")
         target_audience = payload.get("target_audience", "general")
-        
+
         try:
             document_type = DocumentType(document_type_str)
             tone = ContentTone(tone_str)
         except ValueError as e:
             raise ValueError(f"Invalid document_type or tone: {e}")
 
         draft_id = f"draft_{uuid.uuid4().hex[:8]}"
         current_time = time.time()
-        
+
         # Initialize content from template if provided
         content = ""
         if template_id and template_id in self.templates:
             template = self.templates[template_id]
             content = self._generate_template_content(template, payload.get("variables", {}))
         else:
             content = payload.get("initial_content", "")
-        
+
         draft = DocumentDraft(
             draft_id=draft_id,
             title=title,
             document_type=document_type,
             content=content,
@@ -398,197 +410,211 @@
             version=1,
             tone=tone,
             target_audience=target_audience,
             created_at=current_time,
             updated_at=current_time,
-            metadata=payload.get("metadata", {})
+            metadata=payload.get("metadata", {}),
         )
-        
+
         self.drafts[draft_id] = draft
         self.drafting_stats["documents_created"] += 1
-        
+
         if template_id:
             self.drafting_stats["templates_used"] += 1
-        
+
         # Persist to database
         if self.db_pool:
             await self._persist_draft(draft)
-        
-        self.logger.info("Draft created",
-                        draft_id=draft_id,
-                        title=title,
-                        document_type=document_type.value)
-        
+
+        self.logger.info(
+            "Draft created", draft_id=draft_id, title=title, document_type=document_type.value
+        )
+
         return {
             "draft_id": draft_id,
             "title": title,
             "status": "created",
             "word_count": len(content.split()),
             "version": 1,
-            "created_at": current_time
+            "created_at": current_time,
         }
-    
+
     async def _update_draft(self, payload: Dict) -> Dict[str, Any]:
         """Update an existing draft"""
         draft_id = payload.get("draft_id")
         if not draft_id or draft_id not in self.drafts:
             raise ValueError("Draft not found")
-        
+
         draft = self.drafts[draft_id]
-        
+
         # Create revision history entry
         revision_entry = {
             "version": draft.version,
             "content": draft.content,
             "updated_at": draft.updated_at,
             "editor_id": payload.get("editor_id", "system"),
-            "changes": {}
+            "changes": {},
         }
         draft.revision_history.append(revision_entry)
 
         # Apply updates
-        if "title" in payload: draft.title = payload["title"]
+        if "title" in payload:
+            draft.title = payload["title"]
         if "content" in payload:
             old_content = draft.content
             draft.content = payload["content"]
             # Calculate diff for changes
             diff = list(difflib.unified_diff(old_content.splitlines(), draft.content.splitlines()))
             revision_entry["changes"]["content_diff"] = "\n".join(diff)
 
-        if "status" in payload: draft.status = DraftStatus(payload["status"])
-        if "tone" in payload: draft.tone = ContentTone(payload["tone"])
-        if "target_audience" in payload: draft.target_audience = payload["target_audience"]
-        if "metadata" in payload: draft.metadata.update(payload["metadata"])
-        if "collaborators" in payload: draft.collaborators = list(set(draft.collaborators + payload["collaborators"]))
-        if "comments" in payload: draft.comments.extend(payload["comments"])
+        if "status" in payload:
+            draft.status = DraftStatus(payload["status"])
+        if "tone" in payload:
+            draft.tone = ContentTone(payload["tone"])
+        if "target_audience" in payload:
+            draft.target_audience = payload["target_audience"]
+        if "metadata" in payload:
+            draft.metadata.update(payload["metadata"])
+        if "collaborators" in payload:
+            draft.collaborators = list(set(draft.collaborators + payload["collaborators"]))
+        if "comments" in payload:
+            draft.comments.extend(payload["comments"])
 
         draft.version += 1
         draft.updated_at = time.time()
         self.drafting_stats["revisions_made"] += 1
 
         # Persist to database
         if self.db_pool:
             await self._update_persisted_draft(draft)
 
         self.logger.info("Draft updated", draft_id=draft_id, version=draft.version)
-        
+
         return {
             "draft_id": draft_id,
             "title": draft.title,
             "status": draft.status.value,
             "version": draft.version,
-            "updated_at": draft.updated_at
+            "updated_at": draft.updated_at,
         }
 
     async def _generate_content(self, payload: Dict) -> Dict[str, Any]:
         """Generate content using LLM integration"""
         draft_id = payload.get("draft_id")
         prompt = payload.get("prompt")
         section_id = payload.get("section_id")
-        length = payload.get("length", "medium") # short, medium, long
-        creativity = payload.get("creativity", 0.7) # 0.0 to 1.0
-        
+        length = payload.get("length", "medium")  # short, medium, long
+        creativity = payload.get("creativity", 0.7)  # 0.0 to 1.0
+
         if not draft_id or draft_id not in self.drafts:
             raise ValueError("Draft not found")
         if not prompt:
             raise ValueError("Prompt is required for content generation.")
-        
+
         draft = self.drafts[draft_id]
-        
+
         self.logger.info("Generating content for draft", draft_id=draft_id, section_id=section_id)
-        
+
         # This would involve calling the LLM agent to generate content
         # For now, a placeholder:
         generated_text = f"[Generated content for '{prompt}' (length: {length}, creativity: {creativity}) for {draft.document_type.value} in {draft.tone.value} tone.]"
-        
+
         # Integrate generated content into the draft
         if section_id:
             # Find and replace/insert into specific section
             # This requires more sophisticated content structure management
             draft.content += f"\n\n### {section_id.replace('_', ' ').title()}\n{generated_text}"
         else:
             draft.content += f"\n\n{generated_text}"
-        
+
         draft.updated_at = time.time()
         draft.version += 1
-        self.drafting_stats["content_optimizations"] += 1 # Count as optimization for now
+        self.drafting_stats["content_optimizations"] += 1  # Count as optimization for now
 
         # Persist to database
         if self.db_pool:
             await self._update_persisted_draft(draft)
 
         return {
             "draft_id": draft_id,
             "generated_length": len(generated_text),
-            "updated_content_preview": draft.content[:500] + "..." if len(draft.content) > 500 else draft.content
+            "updated_content_preview": (
+                draft.content[:500] + "..." if len(draft.content) > 500 else draft.content
+            ),
         }
 
     async def _analyze_content(self, payload: Dict) -> Dict[str, Any]:
         """Analyze content for readability, tone, keywords, etc."""
         draft_id = payload.get("draft_id")
         content = payload.get("content")
-        
+
         if not draft_id and not content:
             raise ValueError("Either draft_id or content must be provided for analysis.")
-        
+
         if draft_id and draft_id in self.drafts:
             content_to_analyze = self.drafts[draft_id].content
         elif content:
             content_to_analyze = content
         else:
             raise ValueError("Content not found for analysis.")
-        
+
         if not content_to_analyze.strip():
             return {"analysis": {"error": "No content to analyze"}}
 
         self.logger.info("Analyzing content for draft", draft_id=draft_id)
-        
+
         # Readability
         readability_score = flesch_reading_ease(content_to_analyze)
         grade_level = flesch_kincaid_grade(content_to_analyze)
-        
+
         # Word, sentence, paragraph counts
         words = word_tokenize(content_to_analyze)
         sentences = sent_tokenize(content_to_analyze)
         paragraphs = [p for p in content_to_analyze.split("\n\n") if p.strip()]
-        
+
         word_count = len(words)
         sentence_count = len(sentences)
         paragraph_count = len(paragraphs)
         avg_sentence_length = word_count / max(1, sentence_count)
-        
+
         # Tone analysis (placeholder, would involve LLM or dedicated NLP)
         tone_analysis = await self._analyze_tone_llm(content_to_analyze)
-        
+
         # Keyword density (simple example)
         stop_words = set(stopwords.words("english"))
-        filtered_words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]
+        filtered_words = [
+            word.lower() for word in words if word.isalpha() and word.lower() not in stop_words
+        ]
         word_freq = Counter(filtered_words)
         total_filtered_words = len(filtered_words)
-        keyword_density = {word: count / total_filtered_words for word, count in word_freq.most_common(10)}
-        
+        keyword_density = {
+            word: count / total_filtered_words for word, count in word_freq.most_common(10)
+        }
+
         # Complexity score (placeholder)
         complexity_score = (grade_level + (100 - readability_score) / 10) / 2
-        
+
         # Suggestions (placeholder, would involve LLM or rule-based system)
         suggestions = []
-        if readability_score < 60: suggestions.append("Consider simplifying language for better readability.")
-        if avg_sentence_length > 20: suggestions.append("Break down long sentences to improve flow.")
-        
+        if readability_score < 60:
+            suggestions.append("Consider simplifying language for better readability.")
+        if avg_sentence_length > 20:
+            suggestions.append("Break down long sentences to improve flow.")
+
         analysis = ContentAnalysis(
             readability_score=readability_score,
             grade_level=grade_level,
             word_count=word_count,
             sentence_count=sentence_count,
             paragraph_count=paragraph_count,
             avg_sentence_length=avg_sentence_length,
             complexity_score=complexity_score,
             tone_analysis=tone_analysis,
             keyword_density=keyword_density,
-            suggestions=suggestions
+            suggestions=suggestions,
         )
-        
+
         return {"draft_id": draft_id, "analysis": asdict(analysis)}
 
     async def _analyze_tone_llm(self, text: str) -> Dict[str, float]:
         """Placeholder for LLM-based tone analysis"""
         # In a real system, this would call the LLM agent
@@ -610,101 +636,106 @@
 
         if not draft_id or draft_id not in self.drafts:
             raise ValueError("Draft not found")
         if not template_id or template_id not in self.templates:
             raise ValueError("Template not found")
-        
+
         draft = self.drafts[draft_id]
         template = self.templates[template_id]
-        
+
         generated_content = self._generate_template_content(template, variables)
-        
+
         # Append or replace content based on payload instruction
         if payload.get("replace_content", False):
             draft.content = generated_content
         else:
             draft.content += f"\n\n{generated_content}"
-        
+
         draft.template_id = template_id
         draft.updated_at = time.time()
         draft.version += 1
         self.drafting_stats["templates_used"] += 1
 
         # Persist to database
         if self.db_pool:
             await self._update_persisted_draft(draft)
 
         self.logger.info("Template applied to draft", draft_id=draft_id, template_id=template_id)
-        
+
         return {"draft_id": draft_id, "template_id": template_id, "status": "template_applied"}
 
     def _generate_template_content(self, template: ContentTemplate, variables: Dict) -> str:
         """Generate content from a template and variables"""
         content_parts = []
         for section in template.sections:
             content_parts.append(f"## {section['title']}\n")
             # Placeholder for actual content generation based on section type
             content_parts.append(f"[Content for {section['title']}]\n")
-        
+
         full_content = "\n".join(content_parts)
-        
+
         # Replace variables
         for key, value in variables.items():
             full_content = full_content.replace(f"{{{{{key}}}}}", str(value))
-        
+
         return full_content
 
     async def _start_collaboration(self, payload: Dict) -> Dict[str, Any]:
         """Start or manage a collaborative editing session"""
         draft_id = payload.get("draft_id")
         user_id = payload.get("user_id")
-        action = payload.get("action", "join") # join, leave, update
-        
+        action = payload.get("action", "join")  # join, leave, update
+
         if not draft_id or draft_id not in self.drafts:
             raise ValueError("Draft not found")
-        
+
         draft = self.drafts[draft_id]
-        
+
         if action == "join":
             if draft_id not in self.collaborative_sessions:
                 self.collaborative_sessions[draft_id] = {
                     "participants": set(),
-                    "last_activity": time.time()
+                    "last_activity": time.time(),
                 }
             self.collaborative_sessions[draft_id]["participants"].add(user_id)
-            draft.collaborators.append(user_id) # Add to draft's list
+            draft.collaborators.append(user_id)  # Add to draft's list
             self.drafting_stats["collaborations"] += 1
             self.logger.info("User joined collaboration", draft_id=draft_id, user_id=user_id)
             return {"draft_id": draft_id, "user_id": user_id, "status": "joined_collaboration"}
-        
+
         elif action == "leave":
-            if draft_id in self.collaborative_sessions and user_id in self.collaborative_sessions[draft_id]["participants"]:
+            if (
+                draft_id in self.collaborative_sessions
+                and user_id in self.collaborative_sessions[draft_id]["participants"]
+            ):
                 self.collaborative_sessions[draft_id]["participants"].remove(user_id)
                 self.logger.info("User left collaboration", draft_id=draft_id, user_id=user_id)
                 return {"draft_id": draft_id, "user_id": user_id, "status": "left_collaboration"}
             else:
                 raise ValueError("User not in collaboration session.")
-        
+
         elif action == "update":
             # This would involve applying real-time changes from a collaborator
             # Similar to EditingAgent's collaborative_edit, but for drafting
-            self.logger.info("Collaborative update received (placeholder)", draft_id=draft_id, user_id=user_id)
-            draft.content = payload.get("new_content", draft.content) # Simple update
+            self.logger.info(
+                "Collaborative update received (placeholder)", draft_id=draft_id, user_id=user_id
+            )
+            draft.content = payload.get("new_content", draft.content)  # Simple update
             draft.updated_at = time.time()
             draft.version += 1
             self.collaborative_sessions[draft_id]["last_activity"] = time.time()
-            
+
             # Persist to database
             if self.db_pool:
                 await self._update_persisted_draft(draft)
 
             return {"draft_id": draft_id, "user_id": user_id, "status": "collaboration_updated"}
-        
+
         else:
             raise ValueError(f"Unknown collaboration action: {action}")
 
     async def _review_draft(self, payload: Dict) -> Dict[str, Any]:
         """Submit a draft for review or add review comments"""
         draft_id = payload.get("draft_id")
         reviewer_id = payload.get("reviewer_id")
         comments = payload.get("comments", [])
-        status_update = payload.get("status_update") # e.g., 
+        status_update = payload.get("status_update")  # e.g.,
would reformat /home/runner/work/ymera_y/ymera_y/drafting_agent.py
--- /home/runner/work/ymera_y/ymera_y/editing_agent.py	2025-10-19 22:47:02.801432+00:00
+++ /home/runner/work/ymera_y/ymera_y/editing_agent.py	2025-10-19 23:09:10.927111+00:00
@@ -1,6 +1,5 @@
-
 """
 Advanced Editing Agent
 Handles content editing, proofreading, style improvement, and collaborative editing
 """
 
@@ -22,35 +21,39 @@
 import language_tool_python
 
 from base_agent import BaseAgent, AgentConfig, TaskRequest, TaskResponse, Priority, AgentStatus
 from opentelemetry import trace
 
+
 class EditType(Enum):
     GRAMMAR = "grammar"
     STYLE = "style"
     CLARITY = "clarity"
     TONE = "tone"
     STRUCTURE = "structure"
     FACT_CHECK = "fact_check"
     PLAGIARISM = "plagiarism"
     TRANSLATION = "translation"
 
+
 class ContentType(Enum):
     ARTICLE = "article"
     EMAIL = "email"
     PROPOSAL = "proposal"
     REPORT = "report"
     CREATIVE = "creative"
     TECHNICAL = "technical"
     MARKETING = "marketing"
     ACADEMIC = "academic"
 
+
 class EditingMode(Enum):
-    LIGHT = "light"        # Minor corrections only
-    MODERATE = "moderate"   # Grammar and clarity improvements
-    HEAVY = "heavy"        # Significant restructuring and rewriting
+    LIGHT = "light"  # Minor corrections only
+    MODERATE = "moderate"  # Grammar and clarity improvements
+    HEAVY = "heavy"  # Significant restructuring and rewriting
     COLLABORATIVE = "collaborative"  # Track changes mode
+
 
 @dataclass
 class EditSuggestion:
     id: str
     edit_type: EditType
@@ -58,10 +61,11 @@
     suggested_text: str
     reason: str
     confidence: float
     position: tuple  # (start, end)
     metadata: Dict[str, Any] = field(default_factory=dict)
+
 
 @dataclass
 class EditingSession:
     session_id: str
     document_id: str
@@ -74,10 +78,11 @@
     applied_edits: List[EditSuggestion] = field(default_factory=list)
     version_history: List[Dict] = field(default_factory=list)
     created_at: float = field(default_factory=time.time)
     updated_at: float = field(default_factory=time.time)
 
+
 @dataclass
 class ContentAnalysis:
     readability_score: float
     grade_level: float
     sentiment_score: float
@@ -85,10 +90,11 @@
     word_count: int
     sentence_count: int
     paragraph_count: int
     issues_found: List[Dict]
     suggestions_count: int
+
 
 class EditingAgent(BaseAgent):
     """
     Advanced Editing Agent with:
     - Multi-language grammar and style checking
@@ -98,150 +104,161 @@
     - Readability optimization
     - Fact-checking integration
     - Plagiarism detection
     - Real-time collaborative editing
     """
-    
+
     def __init__(self, config: AgentConfig):
         super().__init__(config)
-        
+
         # Editing sessions
         self.active_sessions: Dict[str, EditingSession] = {}
-        
+
         # Language tools
         self.grammar_tool = None
         self.nlp_models = {}
         self.sentiment_analyzer = None
-        
+
         # Style guides and preferences
         self.style_guides = {
             "ap": self._load_ap_style(),
             "chicago": self._load_chicago_style(),
             "mla": self._load_mla_style(),
             "apa": self._load_apa_style(),
-            "technical": self._load_technical_style()
+            "technical": self._load_technical_style(),
         }
-        
+
         # Content templates and patterns
         self.content_patterns = {
             ContentType.EMAIL: self._load_email_patterns(),
             ContentType.PROPOSAL: self._load_proposal_patterns(),
             ContentType.REPORT: self._load_report_patterns(),
-            ContentType.MARKETING: self._load_marketing_patterns()
+            ContentType.MARKETING: self._load_marketing_patterns(),
         }
-        
+
         # Performance metrics
         self.editing_metrics = {
             "total_sessions": 0,
             "suggestions_generated": 0,
             "suggestions_accepted": 0,
             "average_improvement_score": 0.0,
-            "processing_time_avg": 0.0
+            "processing_time_avg": 0.0,
         }
-        
+
         # Initialize language tools (run as a background task)
         asyncio.create_task(self._initialize_tools())
-    
+
     async def _initialize_tools(self):
         """Initialize NLP and language tools"""
         try:
             # Initialize LanguageTool for grammar checking
             self.grammar_tool = language_tool_python.LanguageTool("en-US")
-            
+
             # Initialize spaCy models
             # Ensure the model is downloaded: python -m spacy download en_core_web_sm
             try:
                 self.nlp_models["en"] = spacy.load("en_core_web_sm")
             except OSError:
                 self.logger.warning("spaCy model 'en_core_web_sm' not found. Downloading...")
                 spacy.cli.download("en_core_web_sm")
                 self.nlp_models["en"] = spacy.load("en_core_web_sm")
-            
+
             # Initialize sentiment analyzer
             self.sentiment_analyzer = SentimentIntensityAnalyzer()
-            
+
             # Download required NLTK data
             try:
                 nltk.data.find("sentiment/vader_lexicon.zip")
             except nltk.downloader.DownloadError:
                 nltk.download("vader_lexicon", quiet=True)
             try:
                 nltk.data.find("tokenizers/punkt.zip")
             except nltk.downloader.DownloadError:
                 nltk.download("punkt", quiet=True)
-            
+
             self.logger.info("Language tools initialized successfully")
-            
+
         except Exception as e:
             self.logger.error("Failed to initialize language tools", error=str(e))
 
     # Placeholder methods for style guide loading and content patterns
-    def _load_ap_style(self): return {} # Implement actual style guide loading
-    def _load_chicago_style(self): return {}
-    def _load_mla_style(self): return {}
-    def _load_apa_style(self): return {}
-    def _load_technical_style(self): return {}
-    def _load_email_patterns(self): return {}
-    def _load_proposal_patterns(self): return {}
-    def _load_report_patterns(self): return {}
-    def _load_marketing_patterns(self): return {}
+    def _load_ap_style(self):
+        return {}  # Implement actual style guide loading
+
+    def _load_chicago_style(self):
+        return {}
+
+    def _load_mla_style(self):
+        return {}
+
+    def _load_apa_style(self):
+        return {}
+
+    def _load_technical_style(self):
+        return {}
+
+    def _load_email_patterns(self):
+        return {}
+
+    def _load_proposal_patterns(self):
+        return {}
+
+    def _load_report_patterns(self):
+        return {}
+
+    def _load_marketing_patterns(self):
+        return {}
 
     async def start(self):
         """Start editing agent services"""
         # Subscribe to collaborative editing events
-        await self._subscribe(
-            "editing.collaborative.*",
-            self._handle_collaborative_event
-        )
-        
+        await self._subscribe("editing.collaborative.*", self._handle_collaborative_event)
+
         # Subscribe to document events
-        await self._subscribe(
-            "document.*",
-            self._handle_document_event
-        )
-        
+        await self._subscribe("document.*", self._handle_document_event)
+
         # Start background tasks
         asyncio.create_task(self._session_manager())
         asyncio.create_task(self._content_analyzer_monitor())
-        
+
         self.logger.info("Editing Agent started")
-    
+
     async def _execute_task_impl(self, request: TaskRequest) -> Dict[str, Any]:
         """Implement the actual task logic for the Editing agent"""
         task_type = request.task_type
         payload = request.payload
-        
+
         if task_type == "start_editing_session":
             return await self._start_editing_session(payload)
-        
+
         elif task_type == "analyze_content":
             return await self._analyze_content(payload)
-        
+
         elif task_type == "generate_suggestions":
             return await self._generate_suggestions(payload)
-        
+
         elif task_type == "apply_edits":
             return await self._apply_edits(payload)
-        
+
         elif task_type == "check_grammar":
             return await self._check_grammar(payload)
-        
+
         elif task_type == "improve_style":
             return await self._improve_style(payload)
-        
+
         elif task_type == "optimize_readability":
             return await self._optimize_readability(payload)
-        
+
         elif task_type == "collaborative_edit":
             return await self._collaborative_edit(payload)
-        
+
         elif task_type == "version_control":
             return await self._version_control(payload)
-        
+
         else:
             raise ValueError(f"Unknown editing task type: {task_type}")
-    
+
     async def _start_editing_session(self, payload: Dict) -> Dict[str, Any]:
         """Start a new editing session"""
         document_id = payload.get("document_id", f"doc_{uuid.uuid4().hex[:8]}")
         user_id = payload.get("user_id")
         content = payload.get("content", "")
@@ -251,81 +268,86 @@
         try:
             content_type = ContentType(content_type_str)
             editing_mode = EditingMode(editing_mode_str)
         except ValueError as e:
             raise ValueError(f"Invalid content_type or editing_mode: {e}")
-        
+
         session_id = f"edit_sess_{uuid.uuid4().hex[:8]}"
-        
+
         session = EditingSession(
             session_id=session_id,
             document_id=document_id,
             user_id=user_id,
             content_type=content_type,
             editing_mode=editing_mode,
             original_content=content,
-            current_content=content
-        )
-        
+            current_content=content,
+        )
+
         self.active_sessions[session_id] = session
         self.editing_metrics["total_sessions"] += 1
-        
+
         # Perform initial analysis
-        analysis_result = await self._analyze_content({
-            "content": content,
-            "content_type": content_type.value
-        })
+        analysis_result = await self._analyze_content(
+            {"content": content, "content_type": content_type.value}
+        )
         analysis = analysis_result.get("analysis", {})
-        
+
         # Generate initial suggestions
-        suggestions_result = await self._generate_suggestions({
-            "session_id": session_id,
-            "content": content,
-            "editing_mode": editing_mode.value,
-            "content_type": content_type.value
-        })
+        suggestions_result = await self._generate_suggestions(
+            {
+                "session_id": session_id,
+                "content": content,
+                "editing_mode": editing_mode.value,
+                "content_type": content_type.value,
+            }
+        )
         suggestions = suggestions_result.get("suggestions", [])
-        
-        self.logger.info("Editing session started",
-                        session_id=session_id,
-                        content_type=content_type.value,
-                        editing_mode=editing_mode.value)
-        
+
+        self.logger.info(
+            "Editing session started",
+            session_id=session_id,
+            content_type=content_type.value,
+            editing_mode=editing_mode.value,
+        )
+
         return {
             "session_id": session_id,
             "document_id": document_id,
             "content_analysis": analysis,
             "initial_suggestions": suggestions,
-            "session_created": True
+            "session_created": True,
         }
-    
+
     async def _analyze_content(self, payload: Dict) -> Dict[str, Any]:
         """Analyze content for various metrics"""
         content = payload.get("content", "")
         content_type = payload.get("content_type", "article")
-        
+
         if not content.strip():
             return {"analysis": {"error": "No content provided"}}
-        
+
         try:
             # Basic metrics
             word_count = len(content.split())
             # Use NLTK for sentence tokenization for better accuracy
             sentences = nltk.sent_tokenize(content)
             sentence_count = len(sentences)
             paragraph_count = len([p for p in content.split("\n\n") if p.strip()])
-            
+
             # Readability analysis
             readability_score = flesch_reading_ease(content)
             grade_level = flesch_kincaid_grade(content)
-            
+
             # Sentiment analysis
-            sentiment_scores = self.sentiment_analyzer.polarity_scores(content) if self.sentiment_analyzer else {}
-            
+            sentiment_scores = (
+                self.sentiment_analyzer.polarity_scores(content) if self.sentiment_analyzer else {}
+            )
+
             # Tone analysis (using a placeholder or LLM call)
             tone_analysis = await self._analyze_tone(content)
-            
+
             # Grammar and style issues
             issues = []
             if self.grammar_tool:
                 matches = self.grammar_tool.check(content)
                 issues = [
@@ -333,71 +355,82 @@
                         "type": "grammar",
                         "message": match.message,
                         "offset": match.offset,
                         "length": match.errorLength,
                         "suggestions": match.replacements[:3],
-                        "category": match.category
+                        "category": match.category,
                     }
                     for match in matches[:20]  # Limit to first 20 issues for brevity
                 ]
-            
+
             # Content-specific analysis
             content_specific = await self._analyze_content_specific(content, content_type)
-            
+
             analysis = ContentAnalysis(
                 readability_score=readability_score,
                 grade_level=grade_level,
                 sentiment_score=sentiment_scores.get("compound", 0.0),
                 tone_analysis=tone_analysis,
                 word_count=word_count,
                 sentence_count=sentence_count,
                 paragraph_count=paragraph_count,
                 issues_found=issues,
-                suggestions_count=len(issues)
+                suggestions_count=len(issues),
             )
-            
+
             return {
                 "analysis": {
                     "readability": {
                         "score": analysis.readability_score,
                         "grade_level": analysis.grade_level,
-                        "interpretation": self._interpret_readability(analysis.readability_score)
+                        "interpretation": self._interpret_readability(analysis.readability_score),
                     },
                     "sentiment": {
                         "overall_score": analysis.sentiment_score,
                         "detailed_scores": sentiment_scores,
-                        "tone": tone_analysis
+                        "tone": tone_analysis,
                     },
                     "structure": {
                         "word_count": analysis.word_count,
                         "sentence_count": analysis.sentence_count,
                         "paragraph_count": analysis.paragraph_count,
-                        "avg_words_per_sentence": analysis.word_count / max(analysis.sentence_count, 1),
-                        "avg_sentences_per_paragraph": analysis.sentence_count / max(analysis.paragraph_count, 1)
+                        "avg_words_per_sentence": analysis.word_count
+                        / max(analysis.sentence_count, 1),
+                        "avg_sentences_per_paragraph": analysis.sentence_count
+                        / max(analysis.paragraph_count, 1),
                     },
                     "issues": {
                         "grammar_issues": len([i for i in issues if i["type"] == "grammar"]),
                         "style_issues": len([i for i in issues if i["type"] == "style"]),
                         "total_issues": len(issues),
-                        "details": issues
+                        "details": issues,
                     },
-                    "content_specific": content_specific
+                    "content_specific": content_specific,
                 }
             }
-            
+
         except Exception as e:
-            self.logger.error("Content analysis failed", error=str(e), traceback=traceback.format_exc())
+            self.logger.error(
+                "Content analysis failed", error=str(e), traceback=traceback.format_exc()
+            )
             return {"analysis": {"error": f"Analysis failed: {str(e)}"}}
-    
+
     def _interpret_readability(self, score: float) -> str:
-        if score >= 90: return "Very Easy"
-        elif score >= 80: return "Easy"
-        elif score >= 70: return "Fairly Easy"
-        elif score >= 60: return "Standard"
-        elif score >= 50: return "Fairly Difficult"
-        elif score >= 30: return "Difficult"
-        else: return "Very Difficult"
+        if score >= 90:
+            return "Very Easy"
+        elif score >= 80:
+            return "Easy"
+        elif score >= 70:
+            return "Fairly Easy"
+        elif score >= 60:
+            return "Standard"
+        elif score >= 50:
+            return "Fairly Difficult"
+        elif score >= 30:
+            return "Difficult"
+        else:
+            return "Very Difficult"
 
     async def _analyze_tone(self, content: str) -> Dict[str, float]:
         """Analyze tone of content using a more sophisticated approach (e.g., LLM call or advanced NLP)"""
         # This is a placeholder. For a real-world scenario, you would integrate with an LLM
         # or a dedicated NLP service for tone analysis.
@@ -406,15 +439,15 @@
             "formal": 0.0,
             "casual": 0.0,
             "professional": 0.0,
             "friendly": 0.0,
             "persuasive": 0.0,
-            "informative": 0.0
+            "informative": 0.0,
         }
-        
+
         content_lower = content.lower()
-        
+
         # Simple keyword-based tone detection (can be replaced by LLM call)
         if re.search(r"(dear sir|madam|sincerely|regards|furthermore|therefore)", content_lower):
             tones["formal"] = 0.8
             tones["professional"] = 0.7
         if re.search(r"(hi there|hey|cheers|lol|btw)", content_lower):
@@ -445,17 +478,17 @@
         if content_type == ContentType.MARKETING.value:
             cta_patterns = [r"buy now", r"learn more", r"sign up", r"get started"]
             found_ctas = [p for p in cta_patterns if re.search(p, content.lower())]
             analysis_results["call_to_actions_found"] = len(found_ctas)
             analysis_results["cta_phrases"] = found_ctas
-        
+
         # Example: For technical content, check for code blocks or specific terminology
         if content_type == ContentType.TECHNICAL.value:
             code_block_count = len(re.findall(r"```.*?```", content, re.DOTALL))
             analysis_results["code_block_count"] = code_block_count
             # Placeholder for terminology check
-            analysis_results["technical_terms_density"] = 0.0 # Requires a lexicon
+            analysis_results["technical_terms_density"] = 0.0  # Requires a lexicon
 
         return analysis_results
 
     async def _generate_suggestions(self, payload: Dict) -> Dict[str, Any]:
         """Generate editing suggestions based on content, mode, and type"""
@@ -477,84 +510,92 @@
 
         # Grammar and spelling suggestions
         if self.grammar_tool:
             matches = self.grammar_tool.check(content)
             for match in matches:
-                if editing_mode == EditingMode.LIGHT and match.category not in ["Grammar", "Spelling"]:
-                    continue # Only critical grammar/spelling for light mode
-                
+                if editing_mode == EditingMode.LIGHT and match.category not in [
+                    "Grammar",
+                    "Spelling",
+                ]:
+                    continue  # Only critical grammar/spelling for light mode
+
                 if match.replacements:
-                    suggestions.append(EditSuggestion(
-                        id=str(uuid.uuid4()),
-                        edit_type=EditType.GRAMMAR if match.category in ["Grammar", "Spelling"] else EditType.STYLE,
-                        original_text=content[match.offset:match.offset + match.errorLength],
-                        suggested_text=match.replacements[0],
-                        reason=match.message,
-                        confidence=1.0, # LanguageTool usually high confidence
-                        position=(match.offset, match.offset + match.errorLength),
-                        metadata={
-                            "category": match.category,
-                            "rule_id": match.ruleId
-                        }
-                    ))
-        
+                    suggestions.append(
+                        EditSuggestion(
+                            id=str(uuid.uuid4()),
+                            edit_type=(
+                                EditType.GRAMMAR
+                                if match.category in ["Grammar", "Spelling"]
+                                else EditType.STYLE
+                            ),
+                            original_text=content[match.offset : match.offset + match.errorLength],
+                            suggested_text=match.replacements[0],
+                            reason=match.message,
+                            confidence=1.0,  # LanguageTool usually high confidence
+                            position=(match.offset, match.offset + match.errorLength),
+                            metadata={"category": match.category, "rule_id": match.ruleId},
+                        )
+                    )
+
         # Style and clarity suggestions (can be enhanced with LLM calls)
         if editing_mode in [EditingMode.MODERATE, EditingMode.HEAVY]:
             # Example: Suggest simplifying complex sentences
             doc = self.nlp_models["en"](content) if "en" in self.nlp_models else None
             if doc:
                 for sent in doc.sents:
                     if len(sent.text.split()) > 25 and " and " in sent.text.lower():
-                        suggestions.append(EditSuggestion(
-                            id=str(uuid.uuid4()),
-                            edit_type=EditType.CLARITY,
-                            original_text=sent.text,
-                            suggested_text=f"Consider rephrasing or splitting this long sentence for clarity: {sent.text}",
-                            reason="Long sentence detected, may impact readability.",
-                            confidence=0.7,
-                            position=(sent.start_char, sent.end_char),
-                            metadata={
-                                "suggestion_type": "sentence_simplification"
-                            }
-                        ))
-            
+                        suggestions.append(
+                            EditSuggestion(
+                                id=str(uuid.uuid4()),
+                                edit_type=EditType.CLARITY,
+                                original_text=sent.text,
+                                suggested_text=f"Consider rephrasing or splitting this long sentence for clarity: {sent.text}",
+                                reason="Long sentence detected, may impact readability.",
+                                confidence=0.7,
+                                position=(sent.start_char, sent.end_char),
+                                metadata={"suggestion_type": "sentence_simplification"},
+                            )
+                        )
+
             # Example: Suggest active voice over passive voice (LLM integration)
             # This would typically involve sending a snippet to an LLM for rephrasing
             # For now, a placeholder:
             if "was done by" in content.lower() and editing_mode == EditingMode.HEAVY:
-                 suggestions.append(EditSuggestion(
-                    id=str(uuid.uuid4()),
-                    edit_type=EditType.STYLE,
-                    original_text="Passive voice detected",
-                    suggested_text="Consider using active voice for stronger impact.",
-                    reason="Passive voice can make writing less direct.",
-                    confidence=0.6,
-                    position=(0,0), # General suggestion
-                    metadata={
-                        "suggestion_type": "active_voice"
-                    }
-                ))
+                suggestions.append(
+                    EditSuggestion(
+                        id=str(uuid.uuid4()),
+                        edit_type=EditType.STYLE,
+                        original_text="Passive voice detected",
+                        suggested_text="Consider using active voice for stronger impact.",
+                        reason="Passive voice can make writing less direct.",
+                        confidence=0.6,
+                        position=(0, 0),  # General suggestion
+                        metadata={"suggestion_type": "active_voice"},
+                    )
+                )
 
         # Tone and content-type specific suggestions (LLM integration)
         if editing_mode == EditingMode.HEAVY:
             # This would involve calling the LLM agent to get suggestions for tone adjustment
             # or content enhancement based on the content_type and desired outcome.
-            self.logger.debug("Generating heavy editing suggestions (placeholder for LLM integration).")
+            self.logger.debug(
+                "Generating heavy editing suggestions (placeholder for LLM integration)."
+            )
             # Example: Call LLM for a more engaging opening for marketing copy
             if content_type == ContentType.MARKETING:
-                suggestions.append(EditSuggestion(
-                    id=str(uuid.uuid4()),
-                    edit_type=EditType.TONE,
-                    original_text="Initial opening paragraph",
-                    suggested_text="Consider a more engaging hook for your marketing copy.",
-                    reason="Marketing content benefits from strong opening hooks.",
-                    confidence=0.8,
-                    position=(0, 50), # Example position
-                    metadata={
-                        "suggestion_type": "marketing_hook_improvement"
-                    }
-                ))
+                suggestions.append(
+                    EditSuggestion(
+                        id=str(uuid.uuid4()),
+                        edit_type=EditType.TONE,
+                        original_text="Initial opening paragraph",
+                        suggested_text="Consider a more engaging hook for your marketing copy.",
+                        reason="Marketing content benefits from strong opening hooks.",
+                        confidence=0.8,
+                        position=(0, 50),  # Example position
+                        metadata={"suggestion_type": "marketing_hook_improvement"},
+                    )
+                )
 
         session.suggestions.extend(suggestions)
         self.editing_metrics["suggestions_generated"] += len(suggestions)
 
         return {"session_id": session_id, "suggestions": [asdict(s) for s in suggestions]}
@@ -564,87 +605,100 @@
         session_id = payload.get("session_id")
         edit_ids_to_apply = payload.get("edit_ids", [])
 
         if not session_id or session_id not in self.active_sessions:
             raise ValueError("Invalid or expired editing session.")
-        
+
         session = self.active_sessions[session_id]
         original_content = session.current_content
-        new_content = list(original_content) # Work with a list of characters for easier manipulation
-        
+        new_content = list(
+            original_content
+        )  # Work with a list of characters for easier manipulation
+
         # Sort edits by position in reverse order to avoid index shifting issues
         edits_to_apply = sorted(
             [s for s in session.suggestions if s.id in edit_ids_to_apply],
-            key=lambda x: x.position[0], reverse=True
+            key=lambda x: x.position[0],
+            reverse=True,
         )
 
         applied_count = 0
         for edit in edits_to_apply:
             start, end = edit.position
             # Ensure edit is still valid given previous changes
             if start >= 0 and end <= len(original_content):
                 # Replace the original text with the suggested text
                 new_content[start:end] = list(edit.suggested_text)
                 session.applied_edits.append(edit)
-                session.suggestions.remove(edit) # Remove applied suggestion
+                session.suggestions.remove(edit)  # Remove applied suggestion
                 applied_count += 1
-        
+
         session.current_content = "".join(new_content)
         session.updated_at = time.time()
-        session.version_history.append({
-            "timestamp": time.time(),
-            "content": session.current_content,
-            "applied_edits": [s.id for s in edits_to_apply]
-        })
+        session.version_history.append(
+            {
+                "timestamp": time.time(),
+                "content": session.current_content,
+                "applied_edits": [s.id for s in edits_to_apply],
+            }
+        )
         self.editing_metrics["suggestions_accepted"] += applied_count
 
         return {
             "session_id": session_id,
             "applied_count": applied_count,
-            "new_content_preview": session.current_content[:500] + "..." if len(session.current_content) > 500 else session.current_content,
-            "diff": difflib.unified_diff(original_content.splitlines(), session.current_content.splitlines())
+            "new_content_preview": (
+                session.current_content[:500] + "..."
+                if len(session.current_content) > 500
+                else session.current_content
+            ),
+            "diff": difflib.unified_diff(
+                original_content.splitlines(), session.current_content.splitlines()
+            ),
         }
 
     async def _check_grammar(self, payload: Dict) -> Dict[str, Any]:
         """Perform grammar and spelling check"""
         content = payload.get("content")
         if not content:
             raise ValueError("Content is required for grammar check.")
-        
+
         if not self.grammar_tool:
             raise RuntimeError("Grammar tool not initialized.")
-        
+
         matches = self.grammar_tool.check(content)
         issues = [
             {
                 "message": match.message,
                 "replacements": match.replacements,
                 "offset": match.offset,
                 "length": match.errorLength,
                 "context": match.context,
                 "rule_id": match.ruleId,
-                "category": match.category
+                "category": match.category,
             }
             for match in matches
         ]
         return {"issues": issues, "issue_count": len(issues)}
 
     async def _improve_style(self, payload: Dict) -> Dict[str, Any]:
         """Improve writing style using LLM or rule-based systems"""
         content = payload.get("content")
         style_guide_name = payload.get("style_guide", "technical")
-        
+
         if not content:
             raise ValueError("Content is required for style improvement.")
-        
+
         # This would typically involve sending the content to an LLM agent
         # with instructions to rewrite according to a specific style guide.
-        self.logger.info(f"Improving style for content using {style_guide_name} guide (LLM integration placeholder).")
-        
+        self.logger.info(
+            f"Improving style for content using {style_guide_name} guide (LLM integration placeholder)."
+        )
+
         # Placeholder for LLM call
         improved_content = f"[LLM-improved content based on {style_guide_name} guide]: {content}"
-        
+
         return {"original_content": content, "improved_content": improved_content}
 
     async def _optimize_readability(self, payload: Dict) -> Dict[str, Any]:
         """Optimize content for better readability"""
         content = payload.get("content")
@@ -652,224 +706,300 @@
 
         if not content:
             raise ValueError("Content is required for readability optimization.")
 
         # This would involve iterative calls to an LLM to simplify sentences, replace complex words, etc.
-        self.logger.info(f"Optimizing readability for target grade level {target_grade_level} (LLM integration placeholder).")
+        self.logger.info(
+            f"Optimizing readability for target grade level {target_grade_level} (LLM integration placeholder)."
+        )
 
         # Placeholder for LLM call
-        optimized_content = f"[LLM-optimized content for readability, target grade {target_grade_level}]: {content}"
+        optimized_content = (
+            f"[LLM-optimized content for readability, target grade {target_grade_level}]: {content}"
+        )
 
         return {"original_content": content, "optimized_content": optimized_content}
 
     async def _collaborative_edit(self, payload: Dict) -> Dict[str, Any]:
         """Handle real-time collaborative editing events"""
         session_id = payload.get("session_id")
         user_id = payload.get("user_id")
-        change_type = payload.get("change_type") # e.g., insert, delete, replace
+        change_type = payload.get("change_type")  # e.g., insert, delete, replace
         position = payload.get("position")
         text = payload.get("text")
 
         if not session_id or session_id not in self.active_sessions:
             raise ValueError("Invalid or expired editing session.")
-        
+
         session = self.active_sessions[session_id]
-        
+
         # Apply changes to session.current_content
         # This would require careful handling of concurrent edits and CRDTs or similar
-        self.logger.info("Applying collaborative edit (placeholder for CRDT implementation).",
-                        session_id=session_id, user_id=user_id, change_type=change_type)
-        
+        self.logger.info(
+            "Applying collaborative edit (placeholder for CRDT implementation).",
+            session_id=session_id,
+            user_id=user_id,
+            change_type=change_type,
+        )
+
         # For simplicity, just update content (not truly collaborative)
         if change_type == "replace_all":
             session.current_content = text
-        
+
         session.updated_at = time.time()
-        session.version_history.append({
-            "timestamp": time.time(),
-            "user_id": user_id,
-            "change_type": change_type,
-            "details": payload # Store full payload for history
-        })
+        session.version_history.append(
+            {
+                "timestamp": time.time(),
+                "user_id": user_id,
+                "change_type": change_type,
+                "details": payload,  # Store full payload for history
+            }
+        )
 
         # Broadcast change to other collaborators (via NATS)
-        await self._publish(f"editing.collaborative.{session_id}.update", json.dumps({
-            "session_id": session_id,
-            "user_id": user_id,
-            "change": payload,
-            "new_content_preview": session.current_content[:100]
-        }).encode())
+        await self._publish(
+            f"editing.collaborative.{session_id}.update",
+            json.dumps(
+                {
+                    "session_id": session_id,
+                    "user_id": user_id,
+                    "change": payload,
+                    "new_content_preview": session.current_content[:100],
+                }
+            ).encode(),
+        )
 
         return {"status": "change_received", "session_id": session_id}
 
     async def _version_control(self, payload: Dict) -> Dict[str, Any]:
         """Manage document versions and history"""
         session_id = payload.get("session_id")
-        operation = payload.get("operation") # e.g., get_history, revert, save_version
+        operation = payload.get("operation")  # e.g., get_history, revert, save_version
         version_id = payload.get("version_id")
 
         if not session_id or session_id not in self.active_sessions:
             raise ValueError("Invalid or expired editing session.")
-        
+
         session = self.active_sessions[session_id]
 
         if operation == "get_history":
             return {"session_id": session_id, "version_history": session.version_history}
-        
+
         elif operation == "revert":
             # Find the version to revert to
-            target_version = next((v for v in session.version_history if v.get("version_id") == version_id), None)
+            target_version = next(
+                (v for v in session.version_history if v.get("version_id") == version_id), None
+            )
             if target_version:
                 session.current_content = target_version["content"]
                 session.updated_at = time.time()
-                self.logger.info("Reverted session content", session_id=session_id, version_id=version_id)
-                return {"status": "reverted", "session_id": session_id, "new_content_preview": session.current_content[:500]}
+                self.logger.info(
+                    "Reverted session content", session_id=session_id, version_id=version_id
+                )
+                return {
+                    "status": "reverted",
+                    "session_id": session_id,
+                    "new_content_preview": session.current_content[:500],
+                }
             else:
                 raise ValueError(f"Version {version_id} not found.")
-        
+
         elif operation == "save_version":
             new_version_id = str(uuid.uuid4())
-            session.version_history.append({
-                "timestamp": time.time(),
-                "content": session.current_content,
+            session.version_history.append(
+                {
+                    "timestamp": time.time(),
+                    "content": session.current_content,
+                    "version_id": new_version_id,
+                    "message": payload.get("message", "Manual save"),
+                }
+            )
+            self.logger.info("Saved new version", session_id=session_id, version_id=new_version_id)
+            return {
+                "status": "version_saved",
+                "session_id": session_id,
                 "version_id": new_version_id,
-                "message": payload.get("message", "Manual save")
-            })
-            self.logger.info("Saved new version", session_id=session_id, version_id=new_version_id)
-            return {"status": "version_saved", "session_id": session_id, "version_id": new_version_id}
-        
+            }
+
         else:
             raise ValueError(f"Unknown version control operation: {operation}")
 
     async def _handle_editing_task(self, msg):
         """Handle incoming editing tasks (deprecated, now uses _execute_task_impl)"""
-        self.logger.warning("[_handle_editing_task] called. This method is deprecated. Use _execute_task_impl.")
+        self.logger.warning(
+            "[_handle_editing_task] called. This method is deprecated. Use _execute_task_impl."
+        )
         # This method should ideally not be called if orchestrator routes to _execute_task_impl
         # For backward compatibility or direct calls, you might parse and call _execute_task_impl
         try:
             data = json.loads(msg.data.decode())
             request = TaskRequest(**data)
             response = await self._execute_task_impl(request)
-            await self._publish(msg.reply, json.dumps(response).encode()) # Assuming there's a reply subject
+            await self._publish(
+                msg.reply, json.dumps(response).encode()
+            )  # Assuming there's a reply subject
         except Exception as e:
             self.logger.error("Error in deprecated _handle_editing_task", error=str(e))
             if msg.reply:
-                await self._publish(msg.reply, json.dumps({"error": str(e), "success": False}).encode())
+                await self._publish(
+                    msg.reply, json.dumps({"error": str(e), "success": False}).encode()
+                )
 
     async def _handle_collaborative_event(self, msg):
         """Handle incoming collaborative editing events"""
         try:
             data = json.loads(msg.data.decode())
             session_id = data.get("session_id")
             if session_id and session_id in self.active_sessions:
                 # This agent receives a collaborative update, applies it, and potentially re-analyzes
-                self.logger.info("Received collaborative event", session_id=session_id, change_type=data.get("change_type"))
+                self.logger.info(
+                    "Received collaborative event",
+                    session_id=session_id,
+                    change_type=data.get("change_type"),
+                )
                 # For a real system, this would involve applying CRDT operations
                 # For now, just update the content if it's a full replacement
                 if data.get("change_type") == "replace_all":
                     self.active_sessions[session_id].current_content = data.get("new_content")
                     self.active_sessions[session_id].updated_at = time.time()
                     # Trigger re-analysis if needed
                     # asyncio.create_task(self._trigger_analysis(session_id))
             else:
-                self.logger.warning("Collaborative event for unknown session", session_id=session_id)
+                self.logger.warning(
+                    "Collaborative event for unknown session", session_id=session_id
+                )
         except Exception as e:
-            self.logger.error("Failed to handle collaborative event", error=str(e), traceback=traceback.format_exc())
+            self.logger.error(
+                "Failed to handle collaborative event",
+                error=str(e),
+                traceback=traceback.format_exc(),
+            )
 
     async def _handle_document_event(self, msg):
         """Handle document-related events (e.g., document updated, document deleted)"""
         try:
             data = json.loads(msg.data.decode())
             event_type = data.get("event_type")
             document_id = data.get("document_id")
 
-            self.logger.info("Received document event", event_type=event_type, document_id=document_id)
+            self.logger.info(
+                "Received document event", event_type=event_type, document_id=document_id
+            )
 
             if event_type == "document_updated":
                 # Find sessions related to this document and update their content
                 for session_id, session in self.active_sessions.items():
                     if session.document_id == document_id:
-                        self.logger.info("Updating session content due to document update", session_id=session_id)
+                        self.logger.info(
+                            "Updating session content due to document update", session_id=session_id
+                        )
                         session.original_content = data.get("new_content", session.original_content)
                         session.current_content = data.get("new_content", session.current_content)
                         session.updated_at = time.time()
                         # Trigger re-analysis if needed
                         # asyncio.create_task(self._trigger_analysis(session_id))
             elif event_type == "document_deleted":
                 # Remove sessions related to this document
-                sessions_to_remove = [s_id for s_id, s in self.active_sessions.items() if s.document_id == document_id]
+                sessions_to_remove = [
+                    s_id for s_id, s in self.active_sessions.items() if s.document_id == document_id
+                ]
                 for s_id in sessions_to_remove:
                     del self.active_sessions[s_id]
-                    self.logger.info("Removed session due to document deletion", session_id=s_id, document_id=document_id)
+                    self.logger.info(
+                        "Removed session due to document deletion",
+                        session_id=s_id,
+                        document_id=document_id,
+                    )
         except Exception as e:
-            self.logger.error("Failed to handle document event", error=str(e), traceback=traceback.format_exc())
+            self.logger.error(
+                "Failed to handle document event", error=str(e), traceback=traceback.format_exc()
+            )
 
     async def _session_manager(self):
         """Background task to manage and clean up old editing sessions"""
         while not self._shutdown_event.is_set():
             try:
                 current_time = time.time()
-                session_timeout = 3600 * 24 # 24 hours for a session to be considered stale
-                
+                session_timeout = 3600 * 24  # 24 hours for a session to be considered stale
+
                 sessions_to_remove = []
                 for session_id, session in self.active_sessions.items():
                     if current_time - session.updated_at > session_timeout:
                         sessions_to_remove.append(session_id)
-                
+
                 for session_id in sessions_to_remove:
                     del self.active_sessions[session_id]
                     self.logger.info("Cleaned up stale editing session", session_id=session_id)
-                
-                await asyncio.sleep(3600) # Check every hour
+
+                await asyncio.sleep(3600)  # Check every hour
             except Exception as e:
                 self.logger.error(f"Session manager failed: {e}", traceback=traceback.format_exc())
                 await asyncio.sleep(300)
 
     async def _content_analyzer_monitor(self):
         """Periodically re-analyze content of active sessions for real-time feedback"""
         while not self._shutdown_event.is_set():
             try:
                 for session_id, session in list(self.active_sessions.items()):
                     # Re-analyze content if it has been updated recently or on a schedule
-                    if time.time() - session.updated_at < 60: # Re-analyze if updated in last minute
+                    if (
+                        time.time() - session.updated_at < 60
+                    ):  # Re-analyze if updated in last minute
                         self.logger.debug(f"Re-analyzing content for session {session_id}")
-                        analysis_result = await self._analyze_content({
-                            "content": session.current_content,
-                            "content_type": session.content_type.value
-                        })
+                        analysis_result = await self._analyze_content(
+                            {
+                                "content": session.current_content,
+                                "content_type": session.content_type.value,
+                            }
+                        )
                         # Update session with new analysis, potentially generate new suggestions
                         # For now, just log the analysis
-                        self.logger.debug(f"Updated analysis for session {session_id}: {analysis_result['analysis']['issues']['total_issues']} issues")
-                await asyncio.sleep(30) # Re-analyze every 30 seconds
+                        self.logger.debug(
+                            f"Updated analysis for session {session_id}: {analysis_result['analysis']['issues']['total_issues']} issues"
+                        )
+                await asyncio.sleep(30)  # Re-analyze every 30 seconds
             except Exception as e:
-                self.logger.error(f"Content analyzer monitor failed: {e}", traceback=traceback.format_exc())
+                self.logger.error(
+                    f"Content analyzer monitor failed: {e}", traceback=traceback.format_exc()
+                )
                 await asyncio.sleep(60)
 
     def _get_agent_metrics(self) -> Dict[str, Any]:
         """Provide Editing agent specific metrics."""
         base_metrics = super()._get_agent_metrics()
-        base_metrics.update({
-            "total_sessions": self.editing_metrics["total_sessions"],
-            "active_sessions": len(self.active_sessions),
-            "suggestions_generated": self.editing_metrics["suggestions_generated"],
-            "suggestions_accepted": self.editing_metrics["suggestions_accepted"],
-            "grammar_tool_initialized": self.grammar_tool is not None,
-            "nlp_models_loaded": len(self.nlp_models)
-        })
+        base_metrics.update(
+            {
+                "total_sessions": self.editing_metrics["total_sessions"],
+                "active_sessions": len(self.active_sessions),
+                "suggestions_generated": self.editing_metrics["suggestions_generated"],
+                "suggestions_accepted": self.editing_metrics["suggestions_accepted"],
+                "grammar_tool_initialized": self.grammar_tool is not None,
+                "nlp_models_loaded": len(self.nlp_models),
+            }
+        )
         return base_metrics
 
 
 if __name__ == "__main__":
     config = AgentConfig(
         name="editing_agent",
         agent_type="editing",
-        capabilities=["edit_content", "analyze_text", "grammar_check", "style_improve", "readability_optimize", "collaborative_edit", "version_control"],
+        capabilities=[
+            "edit_content",
+            "analyze_text",
+            "grammar_check",
+            "style_improve",
+            "readability_optimize",
+            "collaborative_edit",
+            "version_control",
+        ],
         nats_url=os.getenv("NATS_URL", "nats://nats:4222"),
-        postgres_url=os.getenv("POSTGRES_URL", "postgresql://agent:secure_password@postgres:5432/ymera"),
+        postgres_url=os.getenv(
+            "POSTGRES_URL", "postgresql://agent:secure_password@postgres:5432/ymera"
+        ),
         redis_url=os.getenv("REDIS_URL", "redis://redis:6379"),
         consul_url=os.getenv("CONSUL_URL", "http://consul:8500"),
-        log_level=os.getenv("LOG_LEVEL", "INFO")
+        log_level=os.getenv("LOG_LEVEL", "INFO"),
     )
-    
+
     agent = EditingAgent(config)
     asyncio.run(agent.run())
-
would reformat /home/runner/work/ymera_y/ymera_y/editing_agent.py
--- /home/runner/work/ymera_y/ymera_y/editing_agent_testing (1).py	2025-10-19 22:47:02.802432+00:00
+++ /home/runner/work/ymera_y/ymera_y/editing_agent_testing (1).py	2025-10-19 23:09:11.804138+00:00
@@ -14,11 +14,11 @@
     EditingAgent,
     EditingSession,
     EditSuggestion,
     ContentType,
     EditingMode,
-    EditType
+    EditType,
 )
 from base_agent import AgentConfig, TaskRequest, Priority, AgentState
 
 
 @pytest.fixture
@@ -32,11 +32,11 @@
         nats_url="nats://localhost:4222",
         postgres_url="postgresql://user:pass@localhost:5432/testdb",
         redis_url="redis://localhost:6379",
         max_concurrent_tasks=10,
         status_publish_interval_seconds=0,  # Disable for tests
-        heartbeat_interval_seconds=0  # Disable for tests
+        heartbeat_interval_seconds=0,  # Disable for tests
     )
 
 
 @pytest.fixture
 async def mock_connections(monkeypatch):
@@ -46,14 +46,14 @@
     mock_nc.publish = AsyncMock()
     mock_nc.subscribe = AsyncMock()
     mock_nc.request = AsyncMock()
     mock_nc.close = AsyncMock()
     mock_nc.drain = AsyncMock()
-    
+
     mock_nats_connect = AsyncMock(return_value=mock_nc)
     monkeypatch.setattr("nats.connect", mock_nats_connect)
-    
+
     # Mock PostgreSQL
     mock_pool = AsyncMock()
     mock_conn = AsyncMock()
     mock_conn.execute = AsyncMock()
     mock_conn.fetch = AsyncMock(return_value=[])
@@ -61,61 +61,57 @@
     mock_conn.fetchval = AsyncMock(return_value=1)
     mock_pool.acquire = AsyncMock()
     mock_pool.acquire.return_value.__aenter__ = AsyncMock(return_value=mock_conn)
     mock_pool.acquire.return_value.__aexit__ = AsyncMock()
     mock_pool.close = AsyncMock()
-    
+
     mock_create_pool = AsyncMock(return_value=mock_pool)
     monkeypatch.setattr("asyncpg.create_pool", mock_create_pool)
-    
+
     # Mock Redis
     mock_redis = AsyncMock()
     mock_redis.ping = AsyncMock()
     mock_redis.close = AsyncMock()
     mock_from_url = AsyncMock(return_value=mock_redis)
     monkeypatch.setattr("redis.asyncio.from_url", mock_from_url)
-    
-    return {
-        "nats": mock_nc,
-        "postgres": mock_pool,
-        "redis": mock_redis
-    }
+
+    return {"nats": mock_nc, "postgres": mock_pool, "redis": mock_redis}
 
 
 @pytest.fixture
 async def editing_agent(editing_agent_config, mock_connections):
     """Create and start editing agent for testing"""
     agent = EditingAgent(editing_agent_config)
     await agent.start()
-    
+
     # Wait for initialization
     await asyncio.sleep(0.1)
-    
+
     yield agent
-    
+
     await agent.stop()
 
 
 class TestEditingAgentLifecycle:
     """Test agent lifecycle management"""
-    
+
     @pytest.mark.asyncio
     async def test_agent_initialization(self, editing_agent_config, mock_connections):
         """Test agent initializes correctly"""
         agent = EditingAgent(editing_agent_config)
-        
+
         assert agent.config.name == "test_editing_agent"
         assert agent.config.agent_type == "editing"
         assert len(agent.active_sessions) == 0
         assert agent.editing_metrics["total_sessions"] == 0
-        
+
         await agent.start()
         assert agent.state == AgentState.RUNNING
-        
+
         await agent.stop()
         assert agent.state == AgentState.STOPPED
-    
+
     @pytest.mark.asyncio
     async def test_graceful_shutdown(self, editing_agent):
         """Test graceful shutdown with active sessions"""
         # Create a test session
         session = EditingSession(
@@ -123,51 +119,51 @@
             document_id="doc-001",
             user_id="user-001",
             content_type=ContentType.ARTICLE,
             editing_mode=EditingMode.MODERATE,
             original_content="Test content",
-            current_content="Test content"
+            current_content="Test content",
         )
         editing_agent.active_sessions["test-001"] = session
-        
+
         # Shutdown should archive the session
         await editing_agent.stop()
-        
+
         assert editing_agent.state == AgentState.STOPPED
         assert len(editing_agent.active_sessions) == 0
 
 
 class TestEditingSession:
     """Test editing session management"""
-    
+
     @pytest.mark.asyncio
     async def test_start_editing_session(self, editing_agent):
         """Test starting a new editing session"""
         payload = {
             "document_id": "doc-123",
             "user_id": "user-456",
             "content": "This is a test document with some errors.",
             "content_type": "article",
-            "editing_mode": "moderate"
+            "editing_mode": "moderate",
         }
-        
+
         result = await editing_agent._start_editing_session(payload)
-        
+
         assert "session_id" in result
         assert result["session_created"] is True
         assert "content_analysis" in result
         assert "initial_suggestions" in result
-        
+
         session_id = result["session_id"]
         assert session_id in editing_agent.active_sessions
-        
+
         session = editing_agent.active_sessions[session_id]
         assert session.document_id == "doc-123"
         assert session.user_id == "user-456"
         assert session.content_type == ContentType.ARTICLE
         assert session.editing_mode == EditingMode.MODERATE
-    
+
     @pytest.mark.asyncio
     async def test_get_session_status(self, editing_agent):
         """Test getting session status"""
         # Create a session
         session = EditingSession(
@@ -175,23 +171,21 @@
             document_id="doc-002",
             user_id="user-002",
             content_type=ContentType.EMAIL,
             editing_mode=EditingMode.LIGHT,
             original_content="Test email",
-            current_content="Test email"
+            current_content="Test email",
         )
         editing_agent.active_sessions["test-002"] = session
-        
-        result = await editing_agent._get_session_status({
-            "session_id": "test-002"
-        })
-        
+
+        result = await editing_agent._get_session_status({"session_id": "test-002"})
+
         assert "session" in result
         assert result["session"]["session_id"] == "test-002"
         assert "pending_suggestions" in result
         assert "applied_edits" in result
-    
+
     @pytest.mark.asyncio
     async def test_close_session(self, editing_agent):
         """Test closing a session"""
         # Create a session
         session = EditingSession(
@@ -199,106 +193,98 @@
             document_id="doc-003",
             user_id="user-003",
             content_type=ContentType.REPORT,
             editing_mode=EditingMode.HEAVY,
             original_content="Test report",
-            current_content="Test report improved"
+            current_content="Test report improved",
         )
         editing_agent.active_sessions["test-003"] = session
-        
-        result = await editing_agent._close_session({
-            "session_id": "test-003"
-        })
-        
+
+        result = await editing_agent._close_session({"session_id": "test-003"})
+
         assert result["status"] == "session_closed"
         assert result["session_id"] == "test-003"
         assert "test-003" not in editing_agent.active_sessions
 
 
 class TestContentAnalysis:
     """Test content analysis functionality"""
-    
+
     @pytest.mark.asyncio
     async def test_analyze_content_basic(self, editing_agent):
         """Test basic content analysis"""
         payload = {
             "content": "This is a test. It has multiple sentences. And paragraphs too.",
-            "content_type": "article"
+            "content_type": "article",
         }
-        
+
         result = await editing_agent._analyze_content(payload)
-        
+
         assert "analysis" in result
         analysis = result["analysis"]
-        
+
         assert "readability" in analysis
         assert "sentiment" in analysis
         assert "structure" in analysis
         assert "issues" in analysis
-        
+
         # Check structure metrics
         structure = analysis["structure"]
         assert structure["word_count"] > 0
         assert structure["sentence_count"] > 0
         assert structure["paragraph_count"] > 0
-    
+
     @pytest.mark.asyncio
     async def test_analyze_empty_content(self, editing_agent):
         """Test analyzing empty content"""
-        result = await editing_agent._analyze_content({
-            "content": "",
-            "content_type": "article"
-        })
-        
+        result = await editing_agent._analyze_content({"content": "", "content_type": "article"})
+
         assert "error" in result["analysis"]
-    
+
     @pytest.mark.asyncio
     async def test_tone_analysis(self, editing_agent):
         """Test tone analysis"""
         formal_text = "Dear Sir, I hereby submit this proposal. Therefore, we recommend proceeding."
         tone = await editing_agent._analyze_tone(formal_text)
-        
+
         assert "formal" in tone
         assert "professional" in tone
         assert tone["formal"] > 0
-        
+
         casual_text = "Hey there! LOL, this is so cool. BTW, wanna grab coffee?"
         tone = await editing_agent._analyze_tone(casual_text)
-        
+
         assert tone["casual"] > 0
         assert tone["friendly"] > 0
-    
+
     @pytest.mark.asyncio
     async def test_content_specific_analysis(self, editing_agent):
         """Test content-type specific analysis"""
         # Marketing content
         marketing_result = await editing_agent._analyze_content_specific(
-            "Buy now! Sign up today and get started immediately!",
-            "marketing"
+            "Buy now! Sign up today and get started immediately!", "marketing"
         )
         assert "call_to_actions_found" in marketing_result
         assert marketing_result["call_to_actions_found"] > 0
-        
+
         # Technical content
         technical_result = await editing_agent._analyze_content_specific(
-            "```python\nprint('hello')\n```",
-            "technical"
+            "```python\nprint('hello')\n```", "technical"
         )
         assert "code_block_count" in technical_result
-        
+
         # Email content
         email_result = await editing_agent._analyze_content_specific(
-            "Dear John,\n\nThank you for your email.\n\nBest regards,\nJane",
-            "email"
+            "Dear John,\n\nThank you for your email.\n\nBest regards,\nJane", "email"
         )
         assert "has_greeting" in email_result
         assert "has_closing" in email_result
 
 
 class TestSuggestions:
     """Test suggestion generation and application"""
-    
+
     @pytest.mark.asyncio
     async def test_generate_suggestions(self, editing_agent):
         """Test generating edit suggestions"""
         # Create a session first
         session = EditingSession(
@@ -306,25 +292,27 @@
             document_id="doc-004",
             user_id="user-004",
             content_type=ContentType.ARTICLE,
             editing_mode=EditingMode.MODERATE,
             original_content="This is a very very very long sentence that should probably be split into multiple shorter sentences for better readability and clarity.",
-            current_content="This is a very very very long sentence that should probably be split into multiple shorter sentences for better readability and clarity."
+            current_content="This is a very very very long sentence that should probably be split into multiple shorter sentences for better readability and clarity.",
         )
         editing_agent.active_sessions["test-004"] = session
-        
-        result = await editing_agent._generate_suggestions({
-            "session_id": "test-004",
-            "content": session.current_content,
-            "editing_mode": "moderate",
-            "content_type": "article"
-        })
-        
+
+        result = await editing_agent._generate_suggestions(
+            {
+                "session_id": "test-004",
+                "content": session.current_content,
+                "editing_mode": "moderate",
+                "content_type": "article",
+            }
+        )
+
         assert "suggestions" in result
         assert "suggestion_count" in result
         assert isinstance(result["suggestions"], list)
-    
+
     @pytest.mark.asyncio
     async def test_apply_edits(self, editing_agent):
         """Test applying edit suggestions"""
         # Create a session with suggestions
         suggestion = EditSuggestion(
@@ -332,39 +320,38 @@
             edit_type=EditType.GRAMMAR,
             original_text="teh",
             suggested_text="the",
             reason="Spelling error",
             confidence=0.95,
-            position=(0, 3)
-        )
-        
+            position=(0, 3),
+        )
+
         session = EditingSession(
             session_id="test-005",
             document_id="doc-005",
             user_id="user-005",
             content_type=ContentType.ARTICLE,
             editing_mode=EditingMode.MODERATE,
             original_content="teh quick brown fox",
-            current_content="teh quick brown fox"
+            current_content="teh quick brown fox",
         )
         session.suggestions.append(suggestion)
         editing_agent.active_sessions["test-005"] = session
-        
-        result = await editing_agent._apply_edits({
-            "session_id": "test-005",
-            "edit_ids": ["sugg-001"]
-        })
-        
+
+        result = await editing_agent._apply_edits(
+            {"session_id": "test-005", "edit_ids": ["sugg-001"]}
+        )
+
         assert result["applied_count"] == 1
         assert "the quick brown fox" in result["new_content"]
         assert len(session.applied_edits) == 1
         assert len(session.suggestions) == 0
 
 
 class TestGrammarCheck:
     """Test grammar checking functionality"""
-    
+
     @pytest.mark.asyncio
     async def test_check_grammar_basic(self, editing_agent):
         """Test basic grammar checking"""
         # Mock grammar tool if not initialized
         if not editing_agent.grammar_tool:
@@ -377,304 +364,291 @@
             mock_match.context = "context"
             mock_match.ruleId = "TEST_RULE"
             mock_match.category = "Grammar"
             mock_tool.check.return_value = [mock_match]
             editing_agent.grammar_tool = mock_tool
-        
-        result = await editing_agent._check_grammar({
-            "content": "This is a test sentance."
-        })
-        
+
+        result = await editing_agent._check_grammar({"content": "This is a test sentance."})
+
         assert "issues" in result
         assert "issue_count" in result
-    
+
     @pytest.mark.asyncio
     async def test_grammar_check_no_tool(self, editing_agent):
         """Test grammar check when tool not initialized"""
         editing_agent.grammar_tool = None
-        
-        result = await editing_agent._check_grammar({
-            "content": "Test content"
-        })
-        
+
+        result = await editing_agent._check_grammar({"content": "Test content"})
+
         assert "error" in result or "issues" in result
 
 
 class TestCollaborativeEditing:
     """Test collaborative editing features"""
-    
+
     @pytest.mark.asyncio
     async def test_collaborative_edit_insert(self, editing_agent):
         """Test inserting text in collaborative mode"""
         session = EditingSession(
             session_id="test-006",
             document_id="doc-006",
             user_id="user-006",
             content_type=ContentType.ARTICLE,
             editing_mode=EditingMode.COLLABORATIVE,
             original_content="Hello world",
-            current_content="Hello world"
+            current_content="Hello world",
         )
         editing_agent.active_sessions["test-006"] = session
-        
-        result = await editing_agent._collaborative_edit({
-            "session_id": "test-006",
-            "user_id": "user-007",
-            "change_type": "insert",
-            "position": 6,
-            "text": "beautiful "
-        })
-        
+
+        result = await editing_agent._collaborative_edit(
+            {
+                "session_id": "test-006",
+                "user_id": "user-007",
+                "change_type": "insert",
+                "position": 6,
+                "text": "beautiful ",
+            }
+        )
+
         assert result["status"] == "change_applied"
         assert "beautiful" in session.current_content
         assert session.current_content == "Hello beautiful world"
-    
+
     @pytest.mark.asyncio
     async def test_collaborative_edit_delete(self, editing_agent):
         """Test deleting text in collaborative mode"""
         session = EditingSession(
             session_id="test-007",
             document_id="doc-007",
             user_id="user-007",
             content_type=ContentType.ARTICLE,
             editing_mode=EditingMode.COLLABORATIVE,
             original_content="Hello wonderful world",
-            current_content="Hello wonderful world"
+            current_content="Hello wonderful world",
         )
         editing_agent.active_sessions["test-007"] = session
-        
-        result = await editing_agent._collaborative_edit({
-            "session_id": "test-007",
-            "user_id": "user-008",
-            "change_type": "delete",
-            "position": (6, 16),  # Delete "wonderful "
-            "text": ""
-        })
-        
+
+        result = await editing_agent._collaborative_edit(
+            {
+                "session_id": "test-007",
+                "user_id": "user-008",
+                "change_type": "delete",
+                "position": (6, 16),  # Delete "wonderful "
+                "text": "",
+            }
+        )
+
         assert result["status"] == "change_applied"
         assert "wonderful" not in session.current_content
-    
+
     @pytest.mark.asyncio
     async def test_collaborative_edit_replace(self, editing_agent):
         """Test replacing text in collaborative mode"""
         session = EditingSession(
             session_id="test-008",
             document_id="doc-008",
             user_id="user-008",
             content_type=ContentType.ARTICLE,
             editing_mode=EditingMode.COLLABORATIVE,
             original_content="Hello world",
-            current_content="Hello world"
+            current_content="Hello world",
         )
         editing_agent.active_sessions["test-008"] = session
-        
-        result = await editing_agent._collaborative_edit({
-            "session_id": "test-008",
-            "user_id": "user-009",
-            "change_type": "replace",
-            "position": (0, 5),
-            "text": "Greetings"
-        })
-        
+
+        result = await editing_agent._collaborative_edit(
+            {
+                "session_id": "test-008",
+                "user_id": "user-009",
+                "change_type": "replace",
+                "position": (0, 5),
+                "text": "Greetings",
+            }
+        )
+
         assert result["status"] == "change_applied"
         assert session.current_content == "Greetings world"
 
 
 class TestVersionControl:
     """Test version control features"""
-    
+
     @pytest.mark.asyncio
     async def test_save_version(self, editing_agent):
         """Test saving a version"""
         session = EditingSession(
             session_id="test-009",
             document_id="doc-009",
             user_id="user-009",
             content_type=ContentType.REPORT,
             editing_mode=EditingMode.MODERATE,
             original_content="Version 1",
-            current_content="Version 1 updated"
+            current_content="Version 1 updated",
         )
         editing_agent.active_sessions["test-009"] = session
-        
-        result = await editing_agent._version_control({
-            "session_id": "test-009",
-            "operation": "save_version",
-            "message": "First update"
-        })
-        
+
+        result = await editing_agent._version_control(
+            {"session_id": "test-009", "operation": "save_version", "message": "First update"}
+        )
+
         assert result["status"] == "version_saved"
         assert "version_id" in result
         assert len(session.version_history) == 1
-    
+
     @pytest.mark.asyncio
     async def test_get_version_history(self, editing_agent):
         """Test getting version history"""
         session = EditingSession(
             session_id="test-010",
             document_id="doc-010",
             user_id="user-010",
             content_type=ContentType.PROPOSAL,
             editing_mode=EditingMode.HEAVY,
             original_content="Original",
-            current_content="Current"
+            current_content="Current",
         )
         session.version_history = [
             {"version_id": "v1", "timestamp": 1000, "content": "Version 1"},
-            {"version_id": "v2", "timestamp": 2000, "content": "Version 2"}
+            {"version_id": "v2", "timestamp": 2000, "content": "Version 2"},
         ]
         editing_agent.active_sessions["test-010"] = session
-        
-        result = await editing_agent._version_control({
-            "session_id": "test-010",
-            "operation": "get_history"
-        })
-        
+
+        result = await editing_agent._version_control(
+            {"session_id": "test-010", "operation": "get_history"}
+        )
+
         assert "version_history" in result
         assert len(result["version_history"]) == 2
-    
+
     @pytest.mark.asyncio
     async def test_revert_version(self, editing_agent):
         """Test reverting to a previous version"""
         session = EditingSession(
             session_id="test-011",
             document_id="doc-011",
             user_id="user-011",
             content_type=ContentType.ARTICLE,
             editing_mode=EditingMode.MODERATE,
             original_content="Original",
-            current_content="Current version"
+            current_content="Current version",
         )
         session.version_history = [
             {"version_id": "v1", "timestamp": 1000, "content": "Previous version"}
         ]
         editing_agent.active_sessions["test-011"] = session
-        
-        result = await editing_agent._version_control({
-            "session_id": "test-011",
-            "operation": "revert",
-            "version_id": "v1"
-        })
-        
+
+        result = await editing_agent._version_control(
+            {"session_id": "test-011", "operation": "revert", "version_id": "v1"}
+        )
+
         assert result["status"] == "reverted"
         assert session.current_content == "Previous version"
 
 
 class TestTaskHandling:
     """Test task routing and handling"""
-    
+
     @pytest.mark.asyncio
     async def test_handle_task_routing(self, editing_agent):
         """Test that tasks are routed correctly"""
         task = TaskRequest(
             task_id="task-001",
             task_type="analyze_content",
             payload={"content": "Test content", "content_type": "article"},
-            priority=Priority.MEDIUM
-        )
-        
+            priority=Priority.MEDIUM,
+        )
+
         result = await editing_agent._handle_task(task)
-        
+
         assert result["status"] == "success"
         assert result["task_id"] == "task-001"
         assert "result" in result
-    
+
     @pytest.mark.asyncio
     async def test_handle_invalid_task(self, editing_agent):
         """Test handling of invalid task types"""
         task = TaskRequest(
-            task_id="task-002",
-            task_type="invalid_task_type",
-            payload={},
-            priority=Priority.LOW
-        )
-        
+            task_id="task-002", task_type="invalid_task_type", payload={}, priority=Priority.LOW
+        )
+
         result = await editing_agent._handle_task(task)
-        
+
         # Should fall back to parent handler or return error
         assert "status" in result
 
 
 class TestMetrics:
     """Test metrics collection and reporting"""
-    
+
     @pytest.mark.asyncio
     async def test_metrics_collection(self, editing_agent):
         """Test that metrics are collected correctly"""
         initial_sessions = editing_agent.editing_metrics["total_sessions"]
-        
+
         # Create a session
-        await editing_agent._start_editing_session({
-            "content": "Test",
-            "content_type": "article",
-            "editing_mode": "moderate"
-        })
-        
+        await editing_agent._start_editing_session(
+            {"content": "Test", "content_type": "article", "editing_mode": "moderate"}
+        )
+
         assert editing_agent.editing_metrics["total_sessions"] == initial_sessions + 1
-    
+
     @pytest.mark.asyncio
     async def test_get_agent_metrics(self, editing_agent):
         """Test getting agent metrics"""
         metrics = editing_agent._get_agent_metrics()
-        
+
         assert "editing_metrics" in metrics
         assert "tools_status" in metrics
         assert "total_sessions" in metrics["editing_metrics"]
         assert "active_sessions" in metrics["editing_metrics"]
 
 
 # Integration tests
 class TestIntegration:
     """Integration tests for complete workflows"""
-    
+
     @pytest.mark.asyncio
     async def test_complete_editing_workflow(self, editing_agent):
         """Test a complete editing workflow from start to finish"""
         # 1. Start session
-        start_result = await editing_agent._start_editing_session({
-            "document_id": "doc-integration",
-            "user_id": "user-integration",
-            "content": "This is a test document. It has some errrors.",
-            "content_type": "article",
-            "editing_mode": "moderate"
-        })
-        
+        start_result = await editing_agent._start_editing_session(
+            {
+                "document_id": "doc-integration",
+                "user_id": "user-integration",
+                "content": "This is a test document. It has some errrors.",
+                "content_type": "article",
+                "editing_mode": "moderate",
+            }
+        )
+
         session_id = start_result["session_id"]
         assert session_id in editing_agent.active_sessions
-        
+
         # 2. Generate suggestions (already done in start)
         suggestions = start_result["initial_suggestions"]
         assert len(suggestions) >= 0
-        
+
         # 3. Apply some edits if suggestions exist
         if suggestions:
             edit_ids = [suggestions[0]["id"]]
-            apply_result = await editing_agent._apply_edits({
-                "session_id": session_id,
-                "edit_ids": edit_ids
-            })
+            apply_result = await editing_agent._apply_edits(
+                {"session_id": session_id, "edit_ids": edit_ids}
+            )
             assert apply_result["applied_count"] > 0
-        
+
         # 4. Save version
-        version_result = await editing_agent._version_control({
-            "session_id": session_id,
-            "operation": "save_version",
-            "message": "First draft"
-        })
+        version_result = await editing_agent._version_control(
+            {"session_id": session_id, "operation": "save_version", "message": "First draft"}
+        )
         assert version_result["status"] == "version_saved"
-        
+
         # 5. Get session status
-        status_result = await editing_agent._get_session_status({
-            "session_id": session_id
-        })
+        status_result = await editing_agent._get_session_status({"session_id": session_id})
         assert "session" in status_result
-        
+
         # 6. Close session
-        close_result = await editing_agent._close_session({
-            "session_id": session_id
-        })
+        close_result = await editing_agent._close_session({"session_id": session_id})
         assert close_result["status"] == "session_closed"
         assert session_id not in editing_agent.active_sessions
 
 
 if __name__ == "__main__":
-    pytest.main([__file__, "-v", "--tb=short"])
\ No newline at end of file
+    pytest.main([__file__, "-v", "--tb=short"])
would reformat /home/runner/work/ymera_y/ymera_y/editing_agent_testing (1).py
--- /home/runner/work/ymera_y/ymera_y/encryption.py	2025-10-19 22:47:02.802432+00:00
+++ /home/runner/work/ymera_y/ymera_y/encryption.py	2025-10-19 23:09:11.907448+00:00
@@ -10,10 +10,11 @@
 
 try:
     from cryptography.fernet import Fernet
     from cryptography.hazmat.primitives import hashes
     from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2
+
     CRYPTO_AVAILABLE = True
 except ImportError:
     CRYPTO_AVAILABLE = False
     Fernet = None
 
@@ -22,81 +23,82 @@
 logger = structlog.get_logger(__name__)
 
 
 class EncryptionManager:
     """Manages encryption and decryption operations"""
-    
+
     def __init__(self, settings: Settings):
         self.settings = settings
         self.cipher: Optional[Any] = None
         self.logger = structlog.get_logger(__name__)
-        
+
         if not CRYPTO_AVAILABLE:
             self.logger.warning("Cryptography library not available, encryption disabled")
             return
-        
+
         # Initialize cipher
         self._initialize_cipher()
-    
+
     def _initialize_cipher(self):
         """Initialize Fernet cipher"""
         try:
             # Get or generate encryption key
             key = self.settings.ENCRYPTION_KEY
-            
+
             if not key:
                 # Generate a new key (should be stored securely in production)
                 key = Fernet.generate_key().decode()
                 self.logger.warning("No encryption key provided, generated temporary key")
-            
+
             # Ensure key is bytes
             if isinstance(key, str):
                 key = key.encode()
-            
+
             self.cipher = Fernet(key)
             self.logger.info("Encryption initialized")
-            
+
         except Exception as e:
             self.logger.error(f"Failed to initialize encryption: {e}")
             self.cipher = None
-    
+
     def encrypt(self, data: str) -> Optional[str]:
         """Encrypt string data"""
         if not CRYPTO_AVAILABLE or not self.cipher:
             self.logger.warning("Encryption not available, returning plaintext")
             return data
-        
+
         try:
             encrypted = self.cipher.encrypt(data.encode())
             return base64.b64encode(encrypted).decode()
         except Exception as e:
             self.logger.error(f"Encryption error: {e}")
             return None
-    
+
     def decrypt(self, encrypted_data: str) -> Optional[str]:
         """Decrypt string data"""
         if not CRYPTO_AVAILABLE or not self.cipher:
             self.logger.warning("Decryption not available, returning as-is")
             return encrypted_data
-        
+
         try:
             decoded = base64.b64decode(encrypted_data.encode())
             decrypted = self.cipher.decrypt(decoded)
             return decrypted.decode()
         except Exception as e:
             self.logger.error(f"Decryption error: {e}")
             return None
-    
+
     def hash_password(self, password: str) -> str:
         """Hash password (one-way)"""
         try:
             import hashlib
+
             return hashlib.sha256(password.encode()).hexdigest()
         except Exception as e:
             self.logger.error(f"Password hashing error: {e}")
             return password
-    
+
     @staticmethod
     def generate_key() -> str:
         """Generate a new Fernet key"""
         if not CRYPTO_AVAILABLE:
             return "encryption-not-available"
would reformat /home/runner/work/ymera_y/ymera_y/encryption.py
--- /home/runner/work/ymera_y/ymera_y/engine.py	2025-10-19 22:47:02.802432+00:00
+++ /home/runner/work/ymera_y/ymera_y/engine.py	2025-10-19 23:09:12.550112+00:00
@@ -1,9 +1,10 @@
 """
 YMERA Enhanced Learning Engine
 Consolidated learning orchestration with pattern recognition, knowledge base, and adaptive learning.
 """
+
 import asyncio
 import logging
 from typing import Dict, List, Any, Optional
 from dataclasses import dataclass, field
 from datetime import datetime
@@ -13,30 +14,33 @@
 logger = logging.getLogger("ymera.learning_engine")
 
 
 class LearningTaskType(Enum):
     """Types of learning tasks"""
+
     CLASSIFICATION = "classification"
     REGRESSION = "regression"
     CLUSTERING = "clustering"
     REINFORCEMENT = "reinforcement"
     TRANSFER = "transfer"
     FEDERATED = "federated"
 
 
 class LearningTaskStatus(Enum):
     """Status of learning tasks"""
+
     PENDING = "pending"
     RUNNING = "running"
     COMPLETED = "completed"
     FAILED = "failed"
     CANCELLED = "cancelled"
 
 
 @dataclass
 class LearningTask:
     """Represents a learning task"""
+
     task_id: str
     task_type: LearningTaskType
     status: LearningTaskStatus = LearningTaskStatus.PENDING
     created_at: datetime = field(default_factory=datetime.utcnow)
     started_at: Optional[datetime] = None
@@ -47,10 +51,11 @@
 
 
 @dataclass
 class LearningResult:
     """Result of a learning task"""
+
     task_id: str
     success: bool
     metrics: Dict[str, float]
     model_artifact_path: Optional[str] = None
     predictions: Optional[List[Any]] = None
@@ -61,128 +66,123 @@
 class LearningEngine:
     """
     Enhanced Learning Engine - Orchestrates learning tasks, pattern recognition,
     knowledge base updates, and adaptive learning.
     """
-    
+
     def __init__(self, config: Dict[str, Any]):
         """
         Initialize the Learning Engine.
-        
+
         Args:
             config: Configuration dictionary for the learning engine
         """
         self.config = config
         self.tasks: Dict[str, LearningTask] = {}
         self.active_tasks: Dict[str, asyncio.Task] = {}
         self.max_concurrent_tasks = config.get("max_concurrent_tasks", 10)
-        
+
         # Component integrations (will be injected)
         self.pattern_recognizer = None
         self.knowledge_base = None
         self.adaptive_learner = None
         self.message_broker = None
-        
+
         logger.info(f"Learning Engine initialized with config: {config}")
-    
+
     def set_pattern_recognizer(self, pattern_recognizer):
         """Inject pattern recognition component"""
         self.pattern_recognizer = pattern_recognizer
         logger.info("Pattern recognizer integrated")
-    
+
     def set_knowledge_base(self, knowledge_base):
         """Inject knowledge base component"""
         self.knowledge_base = knowledge_base
         logger.info("Knowledge base integrated")
-    
+
     def set_adaptive_learner(self, adaptive_learner):
         """Inject adaptive learning component"""
         self.adaptive_learner = adaptive_learner
         logger.info("Adaptive learner integrated")
-    
+
     def set_message_broker(self, message_broker):
         """Inject message broker for event publishing"""
         self.message_broker = message_broker
         logger.info("Message broker integrated")
-    
-    async def submit_task(self, task_type: LearningTaskType, 
-                         data: Any, 
-                         config: Dict[str, Any] = None) -> str:
+
+    async def submit_task(
+        self, task_type: LearningTaskType, data: Any, config: Dict[str, Any] = None
+    ) -> str:
         """
         Submit a new learning task.
-        
+
         Args:
             task_type: Type of learning task
             data: Input data for the task
             config: Task-specific configuration
-            
+
         Returns:
             Task ID
         """
         task_id = str(uuid.uuid4())
-        task = LearningTask(
-            task_id=task_id,
-            task_type=task_type,
-            config=config or {}
-        )
-        
+        task = LearningTask(task_id=task_id, task_type=task_type, config=config or {})
+
         self.tasks[task_id] = task
-        
+
         # Check if we can start the task immediately
         if len(self.active_tasks) < self.max_concurrent_tasks:
             await self._start_task(task_id, data)
         else:
             logger.info(f"Task {task_id} queued (max concurrent tasks reached)")
-        
+
         # Publish task submission event
         if self.message_broker:
             await self.message_broker.publish(
-                "learning.task.submitted",
-                {"task_id": task_id, "task_type": task_type.value}
+                "learning.task.submitted", {"task_id": task_id, "task_type": task_type.value}
             )
-        
+
         return task_id
-    
+
     async def _start_task(self, task_id: str, data: Any):
         """Start executing a learning task"""
         task = self.tasks.get(task_id)
         if not task:
             logger.error(f"Task {task_id} not found")
             return
-        
+
         task.status = LearningTaskStatus.RUNNING
         task.started_at = datetime.utcnow()
-        
+
         # Create async task for execution
         async_task = asyncio.create_task(self._execute_task(task_id, data))
         self.active_tasks[task_id] = async_task
-        
+
         logger.info(f"Task {task_id} started")
-        
+
         # Publish task start event
         if self.message_broker:
             await self.message_broker.publish(
                 "learning.task.started",
-                {"task_id": task_id, "started_at": task.started_at.isoformat()}
+                {"task_id": task_id, "started_at": task.started_at.isoformat()},
             )
-    
+
     async def _execute_task(self, task_id: str, data: Any) -> LearningResult:
         """
         Execute a learning task.
-        
+
         Args:
             task_id: Task identifier
             data: Input data
-            
+
         Returns:
             LearningResult object
         """
         task = self.tasks[task_id]
-        
+
         try:
             logger.info(f"Executing task {task_id} of type {task.task_type.value}")
-            
+
             # Dispatch to appropriate learning method based on task type
             if task.task_type == LearningTaskType.CLASSIFICATION:
                 result = await self._execute_classification(task, data)
             elif task.task_type == LearningTaskType.REGRESSION:
                 result = await self._execute_regression(task, data)
@@ -194,260 +194,236 @@
                 result = await self._execute_transfer(task, data)
             elif task.task_type == LearningTaskType.FEDERATED:
                 result = await self._execute_federated(task, data)
             else:
                 raise ValueError(f"Unknown task type: {task.task_type}")
-            
+
             # Update task status
             task.status = LearningTaskStatus.COMPLETED
             task.completed_at = datetime.utcnow()
             task.metrics = result.metrics
-            
+
             # Post-processing: pattern recognition, knowledge base update
             await self._post_process_task(task, result, data)
-            
+
             # Publish task completion event
             if self.message_broker:
                 await self.message_broker.publish(
                     "learning.task.completed",
                     {
                         "task_id": task_id,
                         "metrics": result.metrics,
-                        "completed_at": task.completed_at.isoformat()
-                    }
+                        "completed_at": task.completed_at.isoformat(),
+                    },
                 )
-            
+
             logger.info(f"Task {task_id} completed successfully")
             return result
-            
+
         except Exception as e:
             logger.error(f"Task {task_id} failed: {str(e)}", exc_info=True)
             task.status = LearningTaskStatus.FAILED
             task.error = str(e)
             task.completed_at = datetime.utcnow()
-            
+
             # Publish task failure event
             if self.message_broker:
                 await self.message_broker.publish(
-                    "learning.task.failed",
-                    {"task_id": task_id, "error": str(e)}
+                    "learning.task.failed", {"task_id": task_id, "error": str(e)}
                 )
-            
-            return LearningResult(
-                task_id=task_id,
-                success=False,
-                metrics={},
-                error=str(e)
-            )
+
+            return LearningResult(task_id=task_id, success=False, metrics={}, error=str(e))
         finally:
             # Remove from active tasks
             if task_id in self.active_tasks:
                 del self.active_tasks[task_id]
-            
+
             # Try to start next queued task
             await self._start_next_queued_task()
-    
+
     async def _execute_classification(self, task: LearningTask, data: Any) -> LearningResult:
         """Execute classification task"""
         # This is a placeholder for actual classification logic
         # In production, this would integrate with actual ML frameworks
         await asyncio.sleep(0.1)  # Simulate processing
-        
-        return LearningResult(
-            task_id=task.task_id,
-            success=True,
-            metrics={
-                "accuracy": 0.95,
-                "precision": 0.93,
-                "recall": 0.94,
-                "f1_score": 0.935
-            },
-            model_artifact_path=f"/models/{task.task_id}/model.pkl"
-        )
-    
+
+        return LearningResult(
+            task_id=task.task_id,
+            success=True,
+            metrics={"accuracy": 0.95, "precision": 0.93, "recall": 0.94, "f1_score": 0.935},
+            model_artifact_path=f"/models/{task.task_id}/model.pkl",
+        )
+
     async def _execute_regression(self, task: LearningTask, data: Any) -> LearningResult:
         """Execute regression task"""
         await asyncio.sleep(0.1)
-        
-        return LearningResult(
-            task_id=task.task_id,
-            success=True,
-            metrics={
-                "mse": 0.05,
-                "rmse": 0.224,
-                "mae": 0.18,
-                "r2_score": 0.92
-            },
-            model_artifact_path=f"/models/{task.task_id}/model.pkl"
-        )
-    
+
+        return LearningResult(
+            task_id=task.task_id,
+            success=True,
+            metrics={"mse": 0.05, "rmse": 0.224, "mae": 0.18, "r2_score": 0.92},
+            model_artifact_path=f"/models/{task.task_id}/model.pkl",
+        )
+
     async def _execute_clustering(self, task: LearningTask, data: Any) -> LearningResult:
         """Execute clustering task"""
         await asyncio.sleep(0.1)
-        
-        return LearningResult(
-            task_id=task.task_id,
-            success=True,
-            metrics={
-                "silhouette_score": 0.75,
-                "davies_bouldin_score": 0.45,
-                "num_clusters": 5
-            },
-            model_artifact_path=f"/models/{task.task_id}/model.pkl"
-        )
-    
+
+        return LearningResult(
+            task_id=task.task_id,
+            success=True,
+            metrics={"silhouette_score": 0.75, "davies_bouldin_score": 0.45, "num_clusters": 5},
+            model_artifact_path=f"/models/{task.task_id}/model.pkl",
+        )
+
     async def _execute_reinforcement(self, task: LearningTask, data: Any) -> LearningResult:
         """Execute reinforcement learning task"""
         await asyncio.sleep(0.2)
-        
-        return LearningResult(
-            task_id=task.task_id,
-            success=True,
-            metrics={
-                "average_reward": 150.5,
-                "episodes": 1000,
-                "convergence_episode": 750
-            },
-            model_artifact_path=f"/models/{task.task_id}/model.pkl"
-        )
-    
+
+        return LearningResult(
+            task_id=task.task_id,
+            success=True,
+            metrics={"average_reward": 150.5, "episodes": 1000, "convergence_episode": 750},
+            model_artifact_path=f"/models/{task.task_id}/model.pkl",
+        )
+
     async def _execute_transfer(self, task: LearningTask, data: Any) -> LearningResult:
         """Execute transfer learning task"""
         await asyncio.sleep(0.15)
-        
-        return LearningResult(
-            task_id=task.task_id,
-            success=True,
-            metrics={
-                "accuracy": 0.91,
-                "transfer_efficiency": 0.85,
-                "fine_tuning_epochs": 10
-            },
-            model_artifact_path=f"/models/{task.task_id}/model.pkl"
-        )
-    
+
+        return LearningResult(
+            task_id=task.task_id,
+            success=True,
+            metrics={"accuracy": 0.91, "transfer_efficiency": 0.85, "fine_tuning_epochs": 10},
+            model_artifact_path=f"/models/{task.task_id}/model.pkl",
+        )
+
     async def _execute_federated(self, task: LearningTask, data: Any) -> LearningResult:
         """Execute federated learning task"""
         await asyncio.sleep(0.25)
-        
-        return LearningResult(
-            task_id=task.task_id,
-            success=True,
-            metrics={
-                "global_accuracy": 0.89,
-                "num_clients": 10,
-                "communication_rounds": 50
-            },
-            model_artifact_path=f"/models/{task.task_id}/model.pkl"
-        )
-    
+
+        return LearningResult(
+            task_id=task.task_id,
+            success=True,
+            metrics={"global_accuracy": 0.89, "num_clients": 10, "communication_rounds": 50},
+            model_artifact_path=f"/models/{task.task_id}/model.pkl",
+        )
+
     async def _post_process_task(self, task: LearningTask, result: LearningResult, data: Any):
         """
         Post-process task results: pattern recognition, knowledge base update, adaptive learning.
         """
         # Pattern recognition
         if self.pattern_recognizer and self.config.get("pattern_recognition_enabled", True):
             try:
-                patterns = await self.pattern_recognizer.detect_patterns({
-                    "task_id": task.task_id,
-                    "task_type": task.task_type.value,
-                    "metrics": result.metrics,
-                    "data_sample": data
-                })
+                patterns = await self.pattern_recognizer.detect_patterns(
+                    {
+                        "task_id": task.task_id,
+                        "task_type": task.task_type.value,
+                        "metrics": result.metrics,
+                        "data_sample": data,
+                    }
+                )
                 logger.info(f"Detected {len(patterns)} patterns for task {task.task_id}")
             except Exception as e:
                 logger.error(f"Pattern recognition failed: {str(e)}")
-        
+
         # Knowledge base update
         if self.knowledge_base and self.config.get("knowledge_base_enabled", True):
             try:
-                await self.knowledge_base.store({
-                    "task_id": task.task_id,
-                    "task_type": task.task_type.value,
-                    "metrics": result.metrics,
-                    "model_path": result.model_artifact_path,
-                    "timestamp": task.completed_at.isoformat()
-                })
+                await self.knowledge_base.store(
+                    {
+                        "task_id": task.task_id,
+                        "task_type": task.task_type.value,
+                        "metrics": result.metrics,
+                        "model_path": result.model_artifact_path,
+                        "timestamp": task.completed_at.isoformat(),
+                    }
+                )
                 logger.info(f"Updated knowledge base for task {task.task_id}")
             except Exception as e:
                 logger.error(f"Knowledge base update failed: {str(e)}")
-        
+
         # Adaptive learning
         if self.adaptive_learner and self.config.get("adaptive_learning_enabled", True):
             try:
-                await self.adaptive_learner.adapt({
-                    "task_id": task.task_id,
-                    "metrics": result.metrics,
-                    "task_config": task.config
-                })
+                await self.adaptive_learner.adapt(
+                    {"task_id": task.task_id, "metrics": result.metrics, "task_config": task.config}
+                )
                 logger.info(f"Triggered adaptive learning for task {task.task_id}")
             except Exception as e:
                 logger.error(f"Adaptive learning failed: {str(e)}")
-    
+
     async def _start_next_queued_task(self):
         """Start the next queued task if capacity is available"""
         if len(self.active_tasks) >= self.max_concurrent_tasks:
             return
-        
+
         # Find next pending task
         for task_id, task in self.tasks.items():
             if task.status == LearningTaskStatus.PENDING and task_id not in self.active_tasks:
                 # Note: In production, we'd need to retrieve the original data
                 # For now, this is a simplified version
                 logger.info(f"Starting queued task {task_id}")
                 break
-    
+
     async def get_task_status(self, task_id: str) -> Optional[LearningTask]:
         """Get the status of a task"""
         return self.tasks.get(task_id)
-    
+
     async def cancel_task(self, task_id: str) -> bool:
         """Cancel a running or pending task"""
         task = self.tasks.get(task_id)
         if not task:
             return False
-        
+
         if task.status == LearningTaskStatus.PENDING:
             task.status = LearningTaskStatus.CANCELLED
             return True
-        
+
         if task.status == LearningTaskStatus.RUNNING and task_id in self.active_tasks:
             self.active_tasks[task_id].cancel()
             task.status = LearningTaskStatus.CANCELLED
             return True
-        
+
         return False
-    
-    async def get_all_tasks(self, status: Optional[LearningTaskStatus] = None) -> List[LearningTask]:
+
+    async def get_all_tasks(
+        self, status: Optional[LearningTaskStatus] = None
+    ) -> List[LearningTask]:
         """Get all tasks, optionally filtered by status"""
         if status:
             return [task for task in self.tasks.values() if task.status == status]
         return list(self.tasks.values())
-    
+
     async def get_metrics_summary(self) -> Dict[str, Any]:
         """Get summary metrics for all tasks"""
-        completed_tasks = [t for t in self.tasks.values() if t.status == LearningTaskStatus.COMPLETED]
+        completed_tasks = [
+            t for t in self.tasks.values() if t.status == LearningTaskStatus.COMPLETED
+        ]
         failed_tasks = [t for t in self.tasks.values() if t.status == LearningTaskStatus.FAILED]
-        
+
         return {
             "total_tasks": len(self.tasks),
             "completed": len(completed_tasks),
             "failed": len(failed_tasks),
             "running": len(self.active_tasks),
-            "pending": len([t for t in self.tasks.values() if t.status == LearningTaskStatus.PENDING]),
+            "pending": len(
+                [t for t in self.tasks.values() if t.status == LearningTaskStatus.PENDING]
+            ),
             "success_rate": len(completed_tasks) / len(self.tasks) if self.tasks else 0.0,
-            "average_metrics": self._calculate_average_metrics(completed_tasks)
+            "average_metrics": self._calculate_average_metrics(completed_tasks),
         }
-    
+
     def _calculate_average_metrics(self, tasks: List[LearningTask]) -> Dict[str, float]:
         """Calculate average metrics across completed tasks"""
         if not tasks:
             return {}
-        
+
         all_metrics = {}
         for task in tasks:
             for metric, value in task.metrics.items():
                 if metric not in all_metrics:
                     all_metrics[metric] = []
                 all_metrics[metric].append(value)
-        
+
         return {metric: sum(values) / len(values) for metric, values in all_metrics.items()}
would reformat /home/runner/work/ymera_y/ymera_y/engine.py
--- /home/runner/work/ymera_y/ymera_y/editing_agent_v2 (1).py	2025-10-19 22:47:02.802432+00:00
+++ /home/runner/work/ymera_y/ymera_y/editing_agent_v2 (1).py	2025-10-19 23:09:13.060152+00:00
@@ -18,45 +18,43 @@
 from datetime import datetime
 
 # External libraries (ensure these are in requirements.txt)
 try:
     import spacy
+
     SPACY_AVAILABLE = True
 except ImportError:
     SPACY_AVAILABLE = False
 
 try:
     import nltk
     from nltk.sentiment import SentimentIntensityAnalyzer
+
     NLTK_AVAILABLE = True
 except ImportError:
     NLTK_AVAILABLE = False
 
 try:
     from textstat import flesch_reading_ease, flesch_kincaid_grade
+
     TEXTSTAT_AVAILABLE = True
 except ImportError:
     TEXTSTAT_AVAILABLE = False
 
 try:
     import language_tool_python
+
     LANGUAGETOOL_AVAILABLE = True
 except ImportError:
     LANGUAGETOOL_AVAILABLE = False
 
-from base_agent import (
-    BaseAgent, 
-    AgentConfig, 
-    TaskRequest, 
-    Priority, 
-    AgentState,
-    ConnectionState
-)
+from base_agent import BaseAgent, AgentConfig, TaskRequest, Priority, AgentState, ConnectionState
 
 
 class EditType(Enum):
     """Types of edits that can be suggested"""
+
     GRAMMAR = "grammar"
     STYLE = "style"
     CLARITY = "clarity"
     TONE = "tone"
     STRUCTURE = "structure"
@@ -65,10 +63,11 @@
     TRANSLATION = "translation"
 
 
 class ContentType(Enum):
     """Types of content that can be edited"""
+
     ARTICLE = "article"
     EMAIL = "email"
     PROPOSAL = "proposal"
     REPORT = "report"
     CREATIVE = "creative"
@@ -77,45 +76,48 @@
     ACADEMIC = "academic"
 
 
 class EditingMode(Enum):
     """Intensity levels for editing"""
-    LIGHT = "light"              # Minor corrections only
-    MODERATE = "moderate"         # Grammar and clarity improvements
-    HEAVY = "heavy"              # Significant restructuring and rewriting
+
+    LIGHT = "light"  # Minor corrections only
+    MODERATE = "moderate"  # Grammar and clarity improvements
+    HEAVY = "heavy"  # Significant restructuring and rewriting
     COLLABORATIVE = "collaborative"  # Track changes mode
 
 
 @dataclass
 class EditSuggestion:
     """Represents a single editing suggestion"""
+
     id: str
     edit_type: EditType
     original_text: str
     suggested_text: str
     reason: str
     confidence: float
     position: tuple  # (start, end)
     metadata: Dict[str, Any] = field(default_factory=dict)
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert to dictionary for serialization"""
         return {
             "id": self.id,
             "edit_type": self.edit_type.value,
             "original_text": self.original_text,
             "suggested_text": self.suggested_text,
             "reason": self.reason,
             "confidence": self.confidence,
             "position": self.position,
-            "metadata": self.metadata
+            "metadata": self.metadata,
         }
 
 
 @dataclass
 class EditingSession:
     """Represents an active editing session"""
+
     session_id: str
     document_id: str
     user_id: str
     content_type: ContentType
     editing_mode: EditingMode
@@ -124,11 +126,11 @@
     suggestions: List[EditSuggestion] = field(default_factory=list)
     applied_edits: List[EditSuggestion] = field(default_factory=list)
     version_history: List[Dict] = field(default_factory=list)
     created_at: float = field(default_factory=time.time)
     updated_at: float = field(default_factory=time.time)
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert to dictionary for serialization"""
         return {
             "session_id": self.session_id,
             "document_id": self.document_id,
@@ -139,17 +141,18 @@
             "current_content": self.current_content,
             "suggestions_count": len(self.suggestions),
             "applied_edits_count": len(self.applied_edits),
             "version_count": len(self.version_history),
             "created_at": self.created_at,
-            "updated_at": self.updated_at
+            "updated_at": self.updated_at,
         }
 
 
 @dataclass
 class ContentAnalysis:
     """Results of content analysis"""
+
     readability_score: float
     grade_level: float
     sentiment_score: float
     tone_analysis: Dict[str, float]
     word_count: int
@@ -168,115 +171,107 @@
     - Tone and style analysis
     - Readability optimization
     - Real-time collaborative editing
     - Full integration with enhanced BaseAgent
     """
-    
+
     def __init__(self, config: AgentConfig):
         super().__init__(config)
-        
+
         # Editing sessions
         self.active_sessions: Dict[str, EditingSession] = {}
-        
+
         # Language tools (initialized asynchronously)
         self.grammar_tool = None
         self.nlp_models = {}
         self.sentiment_analyzer = None
         self.tools_initialized = False
-        
+
         # Style guides and preferences
-        self.style_guides = {
-            "ap": {},
-            "chicago": {},
-            "mla": {},
-            "apa": {},
-            "technical": {}
-        }
-        
+        self.style_guides = {"ap": {}, "chicago": {}, "mla": {}, "apa": {}, "technical": {}}
+
         # Content patterns
         self.content_patterns = {}
-        
+
         # Performance metrics
         self.editing_metrics = {
             "total_sessions": 0,
             "active_sessions": 0,
             "suggestions_generated": 0,
             "suggestions_accepted": 0,
             "average_improvement_score": 0.0,
             "processing_time_avg": 0.0,
             "analysis_count": 0,
-            "errors_encountered": 0
+            "errors_encountered": 0,
         }
-    
+
     async def _setup_subscriptions(self):
         """Override to add custom subscriptions"""
         await super()._setup_subscriptions()
-        
+
         # Subscribe to editing tasks
         await self._subscribe(
             f"agent.{self.config.name}.edit",
             self._handle_edit_request,
-            queue_group=f"{self.config.agent_type}-edit"
+            queue_group=f"{self.config.agent_type}-edit",
         )
-        
+
         # Subscribe to collaborative editing events
         await self._subscribe(
             f"editing.collaborative.*.update",
             self._handle_collaborative_event,
-            queue_group=f"{self.config.agent_type}-collab"
+            queue_group=f"{self.config.agent_type}-collab",
         )
-        
+
         # Subscribe to document events
         await self._subscribe(
-            f"document.*",
-            self._handle_document_event,
-            queue_group=f"{self.config.agent_type}-docs"
+            f"document.*", self._handle_document_event, queue_group=f"{self.config.agent_type}-docs"
         )
-        
+
         self.logger.info("Editing agent subscriptions configured")
-    
+
     async def _start_background_tasks(self):
         """Override to add custom background tasks"""
         await super()._start_background_tasks()
-        
+
         # Initialize language tools
         task = asyncio.create_task(self._initialize_tools())
         self.background_tasks.add(task)
         task.add_done_callback(self.background_tasks.discard)
-        
+
         # Session cleanup manager
         task = asyncio.create_task(self._session_manager())
         self.background_tasks.add(task)
         task.add_done_callback(self.background_tasks.discard)
-        
+
         # Content analyzer monitor
         task = asyncio.create_task(self._content_analyzer_monitor())
         self.background_tasks.add(task)
         task.add_done_callback(self.background_tasks.discard)
-        
+
         # Metrics reporter
         task = asyncio.create_task(self._metrics_reporter())
         self.background_tasks.add(task)
         task.add_done_callback(self.background_tasks.discard)
-        
+
         self.logger.info("Editing agent background tasks started")
-    
+
     async def _initialize_tools(self):
         """Initialize NLP and language tools"""
         try:
             self.logger.info("Initializing language tools...")
-            
+
             # Initialize LanguageTool for grammar checking
             if LANGUAGETOOL_AVAILABLE:
                 try:
                     self.grammar_tool = language_tool_python.LanguageTool("en-US")
                     self.logger.info("LanguageTool initialized successfully")
                 except Exception as e:
                     self.logger.error(f"Failed to initialize LanguageTool: {e}")
             else:
                 self.logger.warning("language_tool_python not available")
-            
+
             # Initialize spaCy models
             if SPACY_AVAILABLE:
                 try:
                     self.nlp_models["en"] = spacy.load("en_core_web_sm")
                     self.logger.info("spaCy model loaded successfully")
@@ -284,291 +279,290 @@
                     self.logger.warning("spaCy model 'en_core_web_sm' not found")
                 except Exception as e:
                     self.logger.error(f"Failed to load spaCy model: {e}")
             else:
                 self.logger.warning("spaCy not available")
-            
+
             # Initialize sentiment analyzer
             if NLTK_AVAILABLE:
                 try:
                     # Download required NLTK data
                     try:
                         nltk.data.find("sentiment/vader_lexicon.zip")
                     except LookupError:
                         nltk.download("vader_lexicon", quiet=True)
-                    
+
                     try:
                         nltk.data.find("tokenizers/punkt")
                     except LookupError:
                         nltk.download("punkt", quiet=True)
-                    
+
                     self.sentiment_analyzer = SentimentIntensityAnalyzer()
                     self.logger.info("NLTK sentiment analyzer initialized successfully")
                 except Exception as e:
                     self.logger.error(f"Failed to initialize NLTK: {e}")
             else:
                 self.logger.warning("NLTK not available")
-            
+
             self.tools_initialized = True
             self.logger.info("Language tools initialization complete")
-            
+
         except Exception as e:
             self.logger.error(f"Failed to initialize language tools: {e}")
             self.tools_initialized = False
-    
+
     async def _handle_task(self, task_request: TaskRequest) -> Dict[str, Any]:
         """Handle incoming tasks with proper structure"""
-        
+
         try:
             task_type = task_request.task_type
             payload = task_request.payload
-            
+
             self.logger.info(
                 f"Processing editing task",
                 task_id=task_request.task_id,
                 task_type=task_type,
-                priority=task_request.priority.name
+                priority=task_request.priority.name,
             )
-            
+
             # Route to appropriate handler
             if task_type == "start_editing_session":
                 result = await self._start_editing_session(payload)
-            
+
             elif task_type == "analyze_content":
                 result = await self._analyze_content(payload)
-            
+
             elif task_type == "generate_suggestions":
                 result = await self._generate_suggestions(payload)
-            
+
             elif task_type == "apply_edits":
                 result = await self._apply_edits(payload)
-            
+
             elif task_type == "check_grammar":
                 result = await self._check_grammar(payload)
-            
+
             elif task_type == "improve_style":
                 result = await self._improve_style(payload)
-            
+
             elif task_type == "optimize_readability":
                 result = await self._optimize_readability(payload)
-            
+
             elif task_type == "collaborative_edit":
                 result = await self._collaborative_edit(payload)
-            
+
             elif task_type == "version_control":
                 result = await self._version_control(payload)
-            
+
             elif task_type == "get_session_status":
                 result = await self._get_session_status(payload)
-            
+
             elif task_type == "close_session":
                 result = await self._close_session(payload)
-            
+
             else:
                 # Call parent for unhandled tasks
                 return await super()._handle_task(task_request)
-            
-            return {
-                "status": "success",
-                "task_id": task_request.task_id,
-                "result": result
-            }
-            
+
+            return {"status": "success", "task_id": task_request.task_id, "result": result}
+
         except Exception as e:
             self.logger.error(
                 f"Task processing failed",
                 task_id=task_request.task_id,
                 task_type=task_request.task_type,
                 error=str(e),
-                traceback=traceback.format_exc()
+                traceback=traceback.format_exc(),
             )
             self.editing_metrics["errors_encountered"] += 1
-            
+
             return {
                 "status": "error",
                 "task_id": task_request.task_id,
                 "error": str(e),
-                "error_type": type(e).__name__
+                "error_type": type(e).__name__,
             }
-    
+
     async def _start_editing_session(self, payload: Dict) -> Dict[str, Any]:
         """Start a new editing session"""
         document_id = payload.get("document_id", f"doc_{uuid.uuid4().hex[:8]}")
         user_id = payload.get("user_id", "anonymous")
         content = payload.get("content", "")
         content_type_str = payload.get("content_type", "article")
         editing_mode_str = payload.get("editing_mode", "moderate")
-        
+
         try:
             content_type = ContentType(content_type_str)
             editing_mode = EditingMode(editing_mode_str)
         except ValueError as e:
             raise ValueError(f"Invalid content_type or editing_mode: {e}")
-        
+
         session_id = f"edit_sess_{uuid.uuid4().hex[:12]}"
-        
+
         session = EditingSession(
             session_id=session_id,
             document_id=document_id,
             user_id=user_id,
             content_type=content_type,
             editing_mode=editing_mode,
             original_content=content,
-            current_content=content
+            current_content=content,
         )
-        
+
         self.active_sessions[session_id] = session
         self.editing_metrics["total_sessions"] += 1
         self.editing_metrics["active_sessions"] = len(self.active_sessions)
-        
+
         # Store session in database for persistence
         await self._store_session(session)
-        
+
         # Perform initial analysis
-        analysis_result = await self._analyze_content({
-            "content": content,
-            "content_type": content_type.value
-        })
-        
+        analysis_result = await self._analyze_content(
+            {"content": content, "content_type": content_type.value}
+        )
+
         # Generate initial suggestions
-        suggestions_result = await self._generate_suggestions({
-            "session_id": session_id,
-            "content": content,
-            "editing_mode": editing_mode.value,
-            "content_type": content_type.value
-        })
-        
+        suggestions_result = await self._generate_suggestions(
+            {
+                "session_id": session_id,
+                "content": content,
+                "editing_mode": editing_mode.value,
+                "content_type": content_type.value,
+            }
+        )
+
         self.logger.info(
             "Editing session started",
             session_id=session_id,
             content_type=content_type.value,
             editing_mode=editing_mode.value,
-            user_id=user_id
+            user_id=user_id,
         )
-        
+
         # Publish session created event
         await self._publish(
             f"editing.session.created",
             {
                 "session_id": session_id,
                 "document_id": document_id,
                 "user_id": user_id,
-                "timestamp": time.time()
-            }
+                "timestamp": time.time(),
+            },
         )
-        
+
         return {
             "session_id": session_id,
             "document_id": document_id,
             "content_analysis": analysis_result.get("analysis", {}),
             "initial_suggestions": suggestions_result.get("suggestions", []),
-            "session_created": True
+            "session_created": True,
         }
-    
+
     async def _analyze_content(self, payload: Dict) -> Dict[str, Any]:
         """Analyze content for various metrics"""
         content = payload.get("content", "")
         content_type = payload.get("content_type", "article")
-        
+
         if not content.strip():
             return {"analysis": {"error": "No content provided"}}
-        
+
         self.editing_metrics["analysis_count"] += 1
-        
+
         try:
             # Basic metrics
             words = content.split()
             word_count = len(words)
-            
+
             # Sentence tokenization
             if NLTK_AVAILABLE:
                 sentences = nltk.sent_tokenize(content)
                 sentence_count = len(sentences)
             else:
-                sentence_count = content.count('.') + content.count('!') + content.count('?')
-            
+                sentence_count = content.count(".") + content.count("!") + content.count("?")
+
             paragraphs = [p for p in content.split("\n\n") if p.strip()]
             paragraph_count = len(paragraphs)
-            
+
             # Readability analysis
             readability_score = 0.0
             grade_level = 0.0
             if TEXTSTAT_AVAILABLE and sentence_count > 0:
                 try:
                     readability_score = flesch_reading_ease(content)
                     grade_level = flesch_kincaid_grade(content)
                 except:
                     readability_score = 50.0
                     grade_level = 10.0
-            
+
             # Sentiment analysis
             sentiment_scores = {}
             if self.sentiment_analyzer:
                 try:
                     sentiment_scores = self.sentiment_analyzer.polarity_scores(content)
                 except:
                     sentiment_scores = {"compound": 0.0}
-            
+
             # Tone analysis
             tone_analysis = await self._analyze_tone(content)
-            
+
             # Grammar and style issues
             issues = []
             if self.grammar_tool:
                 try:
                     matches = self.grammar_tool.check(content)
                     issues = [
                         {
-                            "type": "grammar" if match.category in ["Grammar", "Spelling"] else "style",
+                            "type": (
+                                "grammar" if match.category in ["Grammar", "Spelling"] else "style"
+                            ),
                             "message": match.message,
                             "offset": match.offset,
                             "length": match.errorLength,
                             "suggestions": match.replacements[:3] if match.replacements else [],
                             "category": match.category,
-                            "rule_id": match.ruleId
+                            "rule_id": match.ruleId,
                         }
                         for match in matches[:50]  # Limit to 50 issues
                     ]
                 except Exception as e:
                     self.logger.warning(f"Grammar check failed: {e}")
-            
+
             # Content-specific analysis
             content_specific = await self._analyze_content_specific(content, content_type)
-            
+
             return {
                 "analysis": {
                     "readability": {
                         "score": readability_score,
                         "grade_level": grade_level,
-                        "interpretation": self._interpret_readability(readability_score)
+                        "interpretation": self._interpret_readability(readability_score),
                     },
                     "sentiment": {
                         "overall_score": sentiment_scores.get("compound", 0.0),
                         "detailed_scores": sentiment_scores,
-                        "tone": tone_analysis
+                        "tone": tone_analysis,
                     },
                     "structure": {
                         "word_count": word_count,
                         "sentence_count": sentence_count,
                         "paragraph_count": paragraph_count,
                         "avg_words_per_sentence": word_count / max(sentence_count, 1),
-                        "avg_sentences_per_paragraph": sentence_count / max(paragraph_count, 1)
+                        "avg_sentences_per_paragraph": sentence_count / max(paragraph_count, 1),
                     },
                     "issues": {
                         "grammar_issues": len([i for i in issues if i["type"] == "grammar"]),
                         "style_issues": len([i for i in issues if i["type"] == "style"]),
                         "total_issues": len(issues),
-                        "details": issues[:20]  # Return top 20 for API response
+                        "details": issues[:20],  # Return top 20 for API response
                     },
-                    "content_specific": content_specific
+                    "content_specific": content_specific,
                 }
             }
-            
+
         except Exception as e:
             self.logger.error(f"Content analysis failed: {e}")
             return {"analysis": {"error": f"Analysis failed: {str(e)}"}}
-    
+
     def _interpret_readability(self, score: float) -> str:
         """Interpret readability score"""
         if score >= 90:
             return "Very Easy"
         elif score >= 80:
@@ -581,32 +575,32 @@
             return "Fairly Difficult"
         elif score >= 30:
             return "Difficult"
         else:
             return "Very Difficult"
-    
+
     async def _analyze_tone(self, content: str) -> Dict[str, float]:
         """Analyze tone of content"""
         tones = {
             "formal": 0.0,
             "casual": 0.0,
             "professional": 0.0,
             "friendly": 0.0,
             "persuasive": 0.0,
-            "informative": 0.0
+            "informative": 0.0,
         }
-        
+
         content_lower = content.lower()
-        
+
         # Simple keyword-based tone detection
         formal_patterns = r"(dear sir|madam|sincerely|regards|furthermore|therefore|hereby)"
         casual_patterns = r"(hi there|hey|cheers|lol|btw|gonna|wanna)"
         professional_patterns = r"(recommend|propose|strategy|objective|analysis|implement)"
         friendly_patterns = r"(please|thank you|appreciate|glad|happy|welcome)"
         persuasive_patterns = r"(should|must|benefit|advantage|essential|critical)"
         informative_patterns = r"(according to|research shows|data indicates|studies suggest)"
-        
+
         if re.search(formal_patterns, content_lower):
             tones["formal"] = 0.8
             tones["professional"] = 0.7
         if re.search(casual_patterns, content_lower):
             tones["casual"] = 0.8
@@ -618,47 +612,47 @@
             tones["friendly"] = 0.8
         if re.search(persuasive_patterns, content_lower):
             tones["persuasive"] = 0.8
         if re.search(informative_patterns, content_lower):
             tones["informative"] = 0.9
-        
+
         # Normalize
         total = sum(tones.values())
         if total > 0:
-            tones = {k: v/total for k, v in tones.items()}
-        
+            tones = {k: v / total for k, v in tones.items()}
+
         return tones
-    
+
     async def _analyze_content_specific(self, content: str, content_type: str) -> Dict[str, Any]:
         """Perform content-type specific analysis"""
         results = {}
-        
+
         if content_type == ContentType.MARKETING.value:
             cta_patterns = [r"buy now", r"learn more", r"sign up", r"get started", r"try free"]
             found_ctas = [p for p in cta_patterns if re.search(p, content.lower())]
             results["call_to_actions_found"] = len(found_ctas)
             results["cta_phrases"] = found_ctas
-        
+
         elif content_type == ContentType.TECHNICAL.value:
             code_blocks = len(re.findall(r"```.*?```", content, re.DOTALL))
             results["code_block_count"] = code_blocks
-        
+
         elif content_type == ContentType.EMAIL.value:
             has_greeting = bool(re.search(r"^(dear|hi|hello)", content.lower()))
             has_closing = bool(re.search(r"(sincerely|regards|best|thanks)", content.lower()))
             results["has_greeting"] = has_greeting
             results["has_closing"] = has_closing
-        
+
         return results
-    
+
     async def _generate_suggestions(self, payload: Dict) -> Dict[str, Any]:
         """Generate editing suggestions"""
         session_id = payload.get("session_id")
         content = payload.get("content")
         editing_mode_str = payload.get("editing_mode", "moderate")
         content_type_str = payload.get("content_type", "article")
-        
+
         if session_id and session_id in self.active_sessions:
             session = self.active_sessions[session_id]
             editing_mode = session.editing_mode
             content_type = session.content_type
         else:
@@ -666,40 +660,47 @@
                 editing_mode = EditingMode(editing_mode_str)
                 content_type = ContentType(content_type_str)
             except ValueError:
                 editing_mode = EditingMode.MODERATE
                 content_type = ContentType.ARTICLE
-        
+
         suggestions: List[EditSuggestion] = []
-        
+
         # Grammar and spelling suggestions
         if self.grammar_tool:
             try:
                 matches = self.grammar_tool.check(content)
                 for match in matches:
-                    if editing_mode == EditingMode.LIGHT and match.category not in ["Grammar", "Spelling"]:
+                    if editing_mode == EditingMode.LIGHT and match.category not in [
+                        "Grammar",
+                        "Spelling",
+                    ]:
                         continue
-                    
+
                     if match.replacements:
                         suggestion = EditSuggestion(
                             id=str(uuid.uuid4()),
-                            edit_type=EditType.GRAMMAR if match.category in ["Grammar", "Spelling"] else EditType.STYLE,
-                            original_text=content[match.offset:match.offset + match.errorLength],
+                            edit_type=(
+                                EditType.GRAMMAR
+                                if match.category in ["Grammar", "Spelling"]
+                                else EditType.STYLE
+                            ),
+                            original_text=content[match.offset : match.offset + match.errorLength],
                             suggested_text=match.replacements[0],
                             reason=match.message,
                             confidence=0.9,
                             position=(match.offset, match.offset + match.errorLength),
                             metadata={
                                 "category": match.category,
                                 "rule_id": match.ruleId,
-                                "all_suggestions": match.replacements[:5]
-                            }
+                                "all_suggestions": match.replacements[:5],
+                            },
                         )
                         suggestions.append(suggestion)
             except Exception as e:
                 self.logger.warning(f"Grammar check failed: {e}")
-        
+
         # Style and clarity suggestions for moderate/heavy modes
         if editing_mode in [EditingMode.MODERATE, EditingMode.HEAVY]:
             if "en" in self.nlp_models:
                 try:
                     doc = self.nlp_models["en"](content)
@@ -712,496 +713,491 @@
                                 original_text=sent.text,
                                 suggested_text="[Consider splitting this sentence]",
                                 reason="Long sentence detected. Consider breaking into shorter sentences for clarity.",
                                 confidence=0.7,
                                 position=(sent.start_char, sent.end_char),
-                                metadata={"suggestion_type": "sentence_simplification"}
+                                metadata={"suggestion_type": "sentence_simplification"},
                             )
                             suggestions.append(suggestion)
                 except Exception as e:
                     self.logger.warning(f"NLP analysis failed: {e}")
-        
+
         # Update session suggestions if session exists
         if session_id and session_id in self.active_sessions:
             session = self.active_sessions[session_id]
             session.suggestions = suggestions
             session.updated_at = time.time()
-        
+
         self.editing_metrics["suggestions_generated"] += len(suggestions)
-        
+
         return {
             "session_id": session_id,
             "suggestions": [s.to_dict() for s in suggestions],
-            "suggestion_count": len(suggestions)
+            "suggestion_count": len(suggestions),
         }
-    
+
     async def _apply_edits(self, payload: Dict) -> Dict[str, Any]:
         """Apply selected edits to content"""
         session_id = payload.get("session_id")
         edit_ids = payload.get("edit_ids", [])
-        
+
         if not session_id or session_id not in self.active_sessions:
             raise ValueError("Invalid or expired editing session")
-        
+
         session = self.active_sessions[session_id]
         original_content = session.current_content
-        
+
         # Get edits to apply
         edits_to_apply = [s for s in session.suggestions if s.id in edit_ids]
-        
+
         # Sort by position (reverse order to avoid index shifting)
         edits_to_apply.sort(key=lambda x: x.position[0], reverse=True)
-        
+
         # Apply edits
         new_content = list(original_content)
         applied_count = 0
-        
+
         for edit in edits_to_apply:
             start, end = edit.position
             if 0 <= start < len(original_content) and end <= len(original_content):
                 new_content[start:end] = list(edit.suggested_text)
                 session.applied_edits.append(edit)
                 session.suggestions.remove(edit)
                 applied_count += 1
-        
+
         session.current_content = "".join(new_content)
         session.updated_at = time.time()
-        
+
         # Add to version history
-        session.version_history.append({
-            "timestamp": time.time(),
-            "content": session.current_content,
-            "applied_edits": edit_ids,
-            "version_id": str(uuid.uuid4())
-        })
-        
+        session.version_history.append(
+            {
+                "timestamp": time.time(),
+                "content": session.current_content,
+                "applied_edits": edit_ids,
+                "version_id": str(uuid.uuid4()),
+            }
+        )
+
         # Update in database
         await self._update_session(session)
-        
+
         self.editing_metrics["suggestions_accepted"] += applied_count
-        
+
         return {
             "session_id": session_id,
             "applied_count": applied_count,
             "new_content": session.current_content,
-            "version_id": session.version_history[-1]["version_id"]
+            "version_id": session.version_history[-1]["version_id"],
         }
-    
+
     async def _check_grammar(self, payload: Dict) -> Dict[str, Any]:
         """Perform grammar check"""
         content = payload.get("content", "")
-        
+
         if not self.grammar_tool:
             return {"error": "Grammar tool not initialized", "issues": []}
-        
+
         try:
             matches = self.grammar_tool.check(content)
             issues = [
                 {
                     "message": match.message,
                     "replacements": match.replacements[:5],
                     "offset": match.offset,
                     "length": match.errorLength,
                     "context": match.context,
                     "rule_id": match.ruleId,
-                    "category": match.category
+                    "category": match.category,
                 }
                 for match in matches
             ]
             return {"issues": issues, "issue_count": len(issues)}
         except Exception as e:
             self.logger.error(f"Grammar check failed: {e}")
             return {"error": str(e), "issues": []}
-    
+
     async def _improve_style(self, payload: Dict) -> Dict[str, Any]:
         """Improve writing style"""
         content = payload.get("content", "")
         style_guide = payload.get("style_guide", "technical")
-        
+
         # This would integrate with LLM for actual style improvement
         self.logger.info(f"Style improvement requested with {style_guide} guide")
-        
+
         return {
             "original_content": content,
             "improved_content": f"[Style improved according to {style_guide} guidelines]",
-            "note": "LLM integration required for actual style improvement"
+            "note": "LLM integration required for actual style improvement",
         }
-    
+
     async def _optimize_readability(self, payload: Dict) -> Dict[str, Any]:
         """Optimize content for readability"""
         content = payload.get("content", "")
         target_grade = payload.get("target_grade_level", 8)
-        
+
         # This would integrate with LLM for actual optimization
         self.logger.info(f"Readability optimization requested for grade level {target_grade}")
-        
+
         return {
             "original_content": content,
             "optimized_content": f"[Content optimized for grade level {target_grade}]",
-            "note": "LLM integration required for actual readability optimization"
+            "note": "LLM integration required for actual readability optimization",
         }
-    
+
     async def _collaborative_edit(self, payload: Dict) -> Dict[str, Any]:
         """Handle collaborative editing events"""
         session_id = payload.get("session_id")
         user_id = payload.get("user_id")
         change_type = payload.get("change_type")
         position = payload.get("position")
         text = payload.get("text", "")
-        
+
         if not session_id or session_id not in self.active_sessions:
             raise ValueError("Invalid or expired editing session")
-        
+
         session = self.active_sessions[session_id]
-        
+
         # Apply change based on type
         if change_type == "insert":
             pos = position or 0
             session.current_content = (
-                session.current_content[:pos] + 
-                text + 
-                session.current_content[pos:]
+                session.current_content[:pos] + text + session.current_content[pos:]
             )
         elif change_type == "delete":
-            start, end = position if isinstance(position, tuple) else (position, position + len(text))
+            start, end = (
+                position if isinstance(position, tuple) else (position, position + len(text))
+            )
             session.current_content = (
-                session.current_content[:start] + 
-                session.current_content[end:]
+                session.current_content[:start] + session.current_content[end:]
             )
         elif change_type == "replace":
-            start, end = position if isinstance(position, tuple) else (position, position + len(text))
+            start, end = (
+                position if isinstance(position, tuple) else (position, position + len(text))
+            )
             session.current_content = (
-                session.current_content[:start] + 
-                text + 
-                session.current_content[end:]
+                session.current_content[:start] + text + session.current_content[end:]
             )
         elif change_type == "replace_all":
             session.current_content = text
-        
+
         session.updated_at = time.time()
-        
+
         # Add to version history
-        session.version_history.append({
-            "timestamp": time.time(),
-            "user_id": user_id,
-            "change_type": change_type,
-            "details": payload
-        })
-        
+        session.version_history.append(
+            {
+                "timestamp": time.time(),
+                "user_id": user_id,
+                "change_type": change_type,
+                "details": payload,
+            }
+        )
+
         # Broadcast to other collaborators
         await self._publish(
             f"editing.collaborative.{session_id}.update",
             {
                 "session_id": session_id,
                 "user_id": user_id,
                 "change_type": change_type,
                 "timestamp": time.time(),
-                "content_preview": session.current_content[:200]
-            }
+                "content_preview": session.current_content[:200],
+            },
         )
-        
-        return {
-            "status": "change_applied",
-            "session_id": session_id,
-            "timestamp": time.time()
-        }
-    
+
+        return {"status": "change_applied", "session_id": session_id, "timestamp": time.time()}
+
     async def _version_control(self, payload: Dict) -> Dict[str, Any]:
         """Manage document versions"""
         session_id = payload.get("session_id")
         operation = payload.get("operation")
         version_id = payload.get("version_id")
-        
+
         if not session_id or session_id not in self.active_sessions:
             raise ValueError("Invalid or expired editing session")
-        
+
         session = self.active_sessions[session_id]
-        
+
         if operation == "get_history":
             return {
                 "session_id": session_id,
                 "version_history": session.version_history,
-                "current_version": len(session.version_history)
+                "current_version": len(session.version_history),
             }
-        
+
         elif operation == "revert":
             version = next(
-                (v for v in session.version_history if v.get("version_id") == version_id),
-                None
+                (v for v in session.version_history if v.get("version_id") == version_id), None
             )
             if not version:
                 raise ValueError(f"Version {version_id} not found")
-            
+
             session.current_content = version["content"]
             session.updated_at = time.time()
-            
+
             await self._update_session(session)
-            
+
             return {
                 "status": "reverted",
                 "session_id": session_id,
                 "version_id": version_id,
-                "content_preview": session.current_content[:200]
+                "content_preview": session.current_content[:200],
             }
-        
+
         elif operation == "save_version":
             new_version_id = str(uuid.uuid4())
-            session.version_history.append({
-                "timestamp": time.time(),
-                "content": session.current_content,
-                "version_id": new_version_id,
-                "message": payload.get("message", "Manual save")
-            })
-            
+            session.version_history.append(
+                {
+                    "timestamp": time.time(),
+                    "content": session.current_content,
+                    "version_id": new_version_id,
+                    "message": payload.get("message", "Manual save"),
+                }
+            )
+
             await self._update_session(session)
-            
+
             return {
                 "status": "version_saved",
                 "session_id": session_id,
-                "version_id": new_version_id
+                "version_id": new_version_id,
             }
-        
+
         else:
             raise ValueError(f"Unknown version control operation: {operation}")
-    
+
     async def _get_session_status(self, payload: Dict) -> Dict[str, Any]:
         """Get status of an editing session"""
         session_id = payload.get("session_id")
-        
+
         if not session_id or session_id not in self.active_sessions:
             raise ValueError("Invalid or expired editing session")
-        
+
         session = self.active_sessions[session_id]
-        
+
         return {
             "session": session.to_dict(),
             "pending_suggestions": len(session.suggestions),
             "applied_edits": len(session.applied_edits),
-            "versions": len(session.version_history)
+            "versions": len(session.version_history),
         }
-    
+
     async def _close_session(self, payload: Dict) -> Dict[str, Any]:
         """Close an editing session"""
         session_id = payload.get("session_id")
-        
+
         if not session_id or session_id not in self.active_sessions:
             raise ValueError("Invalid or expired editing session")
-        
+
         session = self.active_sessions[session_id]
-        
+
         # Archive session to database
         await self._archive_session(session)
-        
+
         # Remove from active sessions
         del self.active_sessions[session_id]
         self.editing_metrics["active_sessions"] = len(self.active_sessions)
-        
+
         # Publish session closed event
         await self._publish(
             f"editing.session.closed",
             {
                 "session_id": session_id,
                 "document_id": session.document_id,
-                "timestamp": time.time()
-            }
+                "timestamp": time.time(),
+            },
         )
-        
+
         self.logger.info(f"Editing session closed", session_id=session_id)
-        
+
         return {
             "status": "session_closed",
             "session_id": session_id,
-            "final_content": session.current_content
+            "final_content": session.current_content,
         }
-    
+
     async def _handle_edit_request(self, msg):
         """Handle direct edit requests"""
         try:
             data = json.loads(msg.data.decode())
-            
+
             # Convert to TaskRequest format
             task_request = TaskRequest(
                 task_id=data.get("task_id", str(uuid.uuid4())),
                 task_type=data.get("task_type", "analyze_content"),
                 payload=data.get("payload", {}),
-                priority=Priority(data.get("priority", "medium"))
+                priority=Priority(data.get("priority", "medium")),
             )
-            
+
             result = await self._handle_task(task_request)
-            
+
             if msg.reply:
                 await self._publish_raw(msg.reply, json.dumps(result).encode())
-            
+
         except Exception as e:
             self.logger.error(f"Failed to handle edit request: {e}")
             if msg.reply:
-                error_response = {
-                    "status": "error",
-                    "error": str(e)
-                }
+                error_response = {"status": "error", "error": str(e)}
                 await self._publish_raw(msg.reply, json.dumps(error_response).encode())
-    
+
     async def _handle_collaborative_event(self, msg):
         """Handle collaborative editing events"""
         try:
             data = json.loads(msg.data.decode())
             session_id = data.get("session_id")
-            
+
             if session_id and session_id in self.active_sessions:
                 session = self.active_sessions[session_id]
-                
+
                 # Update session based on event
                 change_type = data.get("change_type")
                 if change_type and data.get("new_content"):
                     session.current_content = data["new_content"]
                     session.updated_at = time.time()
-                
+
                 self.logger.info(
-                    "Processed collaborative event",
-                    session_id=session_id,
-                    change_type=change_type
+                    "Processed collaborative event", session_id=session_id, change_type=change_type
                 )
-        
+
         except Exception as e:
             self.logger.error(f"Failed to handle collaborative event: {e}")
-    
+
     async def _handle_document_event(self, msg):
         """Handle document-related events"""
         try:
             data = json.loads(msg.data.decode())
             event_type = data.get("event_type")
             document_id = data.get("document_id")
-            
+
             if event_type == "document_updated":
                 # Update sessions related to this document
                 for session_id, session in list(self.active_sessions.items()):
                     if session.document_id == document_id:
                         new_content = data.get("new_content")
                         if new_content:
                             session.current_content = new_content
                             session.updated_at = time.time()
-                            
+
                             self.logger.info(
                                 "Updated session from document event",
                                 session_id=session_id,
-                                document_id=document_id
+                                document_id=document_id,
                             )
-            
+
             elif event_type == "document_deleted":
                 # Close sessions related to this document
                 sessions_to_close = [
-                    s_id for s_id, s in self.active_sessions.items()
-                    if s.document_id == document_id
+                    s_id for s_id, s in self.active_sessions.items() if s.document_id == document_id
                 ]
-                
+
                 for session_id in sessions_to_close:
                     await self._archive_session(self.active_sessions[session_id])
                     del self.active_sessions[session_id]
-                    
+
                     self.logger.info(
                         "Closed session due to document deletion",
                         session_id=session_id,
-                        document_id=document_id
+                        document_id=document_id,
                     )
-                
+
                 self.editing_metrics["active_sessions"] = len(self.active_sessions)
-        
+
         except Exception as e:
             self.logger.error(f"Failed to handle document event: {e}")
-    
+
     async def _session_manager(self):
         """Background task to manage and cleanup sessions"""
         while self.state == AgentState.RUNNING:
             try:
                 current_time = time.time()
                 session_timeout = 3600 * 24  # 24 hours
-                
+
                 sessions_to_remove = []
-                
+
                 for session_id, session in list(self.active_sessions.items()):
                     # Check for stale sessions
                     if current_time - session.updated_at > session_timeout:
                         sessions_to_remove.append(session_id)
-                
+
                 # Archive and remove stale sessions
                 for session_id in sessions_to_remove:
                     session = self.active_sessions[session_id]
                     await self._archive_session(session)
                     del self.active_sessions[session_id]
-                    
+
                     self.logger.info(
                         "Cleaned up stale session",
                         session_id=session_id,
-                        idle_time=current_time - session.updated_at
+                        idle_time=current_time - session.updated_at,
                     )
-                
+
                 if sessions_to_remove:
                     self.editing_metrics["active_sessions"] = len(self.active_sessions)
-                
+
                 await asyncio.sleep(3600)  # Check every hour
-                
+
             except Exception as e:
                 self.logger.error(f"Session manager error: {e}")
                 await asyncio.sleep(300)
-    
+
     async def _content_analyzer_monitor(self):
         """Periodically re-analyze active sessions"""
         while self.state == AgentState.RUNNING:
             try:
                 for session_id, session in list(self.active_sessions.items()):
                     # Re-analyze if recently updated
                     if time.time() - session.updated_at < 120:  # Updated in last 2 minutes
-                        analysis = await self._analyze_content({
-                            "content": session.current_content,
-                            "content_type": session.content_type.value
-                        })
-                        
+                        analysis = await self._analyze_content(
+                            {
+                                "content": session.current_content,
+                                "content_type": session.content_type.value,
+                            }
+                        )
+
                         # Could store analysis results or trigger notifications
                         self.logger.debug(
                             "Re-analyzed session content",
                             session_id=session_id,
-                            issues=analysis.get("analysis", {}).get("issues", {}).get("total_issues", 0)
+                            issues=analysis.get("analysis", {})
+                            .get("issues", {})
+                            .get("total_issues", 0),
                         )
-                
+
                 await asyncio.sleep(60)  # Check every minute
-                
+
             except Exception as e:
                 self.logger.error(f"Content analyzer monitor error: {e}")
                 await asyncio.sleep(60)
-    
+
     async def _metrics_reporter(self):
         """Report editing metrics periodically"""
         while self.state == AgentState.RUNNING:
             try:
                 # Calculate additional metrics
                 if self.editing_metrics["suggestions_generated"] > 0:
                     acceptance_rate = (
-                        self.editing_metrics["suggestions_accepted"] / 
-                        self.editing_metrics["suggestions_generated"]
+                        self.editing_metrics["suggestions_accepted"]
+                        / self.editing_metrics["suggestions_generated"]
                     ) * 100
                 else:
                     acceptance_rate = 0.0
-                
+
                 metrics = {
                     **self.editing_metrics,
                     "suggestion_acceptance_rate": acceptance_rate,
                     "tools_initialized": self.tools_initialized,
-                    "timestamp": time.time()
+                    "timestamp": time.time(),
                 }
-                
+
                 # Publish metrics
-                await self._publish(
-                    f"metrics.{self.config.name}",
-                    metrics
-                )
-                
+                await self._publish(f"metrics.{self.config.name}", metrics)
+
                 await asyncio.sleep(300)  # Report every 5 minutes
-                
+
             except Exception as e:
                 self.logger.error(f"Metrics reporter error: {e}")
                 await asyncio.sleep(300)
-    
+
     async def _store_session(self, session: EditingSession):
         """Store session in database"""
         try:
             query = """
                 INSERT INTO editing_sessions 
@@ -1211,27 +1207,27 @@
                 ON CONFLICT (session_id) 
                 DO UPDATE SET 
                     current_content = EXCLUDED.current_content,
                     updated_at = EXCLUDED.updated_at
             """
-            
+
             await self._db_execute(
                 query,
                 session.session_id,
                 session.document_id,
                 session.user_id,
                 session.content_type.value,
                 session.editing_mode.value,
                 session.original_content,
                 session.current_content,
                 session.created_at,
-                session.updated_at
+                session.updated_at,
             )
-            
+
         except Exception as e:
             self.logger.error(f"Failed to store session: {e}")
-    
+
     async def _update_session(self, session: EditingSession):
         """Update session in database"""
         try:
             query = """
                 UPDATE editing_sessions 
@@ -1240,35 +1236,35 @@
                     version_count = $3,
                     suggestions_count = $4,
                     applied_edits_count = $5
                 WHERE session_id = $6
             """
-            
+
             await self._db_execute(
                 query,
                 session.current_content,
                 session.updated_at,
                 len(session.version_history),
                 len(session.suggestions),
                 len(session.applied_edits),
-                session.session_id
+                session.session_id,
             )
-            
+
         except Exception as e:
             self.logger.error(f"Failed to update session: {e}")
-    
+
     async def _archive_session(self, session: EditingSession):
         """Archive session to database"""
         try:
             query = """
                 INSERT INTO editing_sessions_archive 
                 (session_id, document_id, user_id, content_type, editing_mode,
                  original_content, final_content, suggestions_generated, 
                  suggestions_accepted, version_count, created_at, closed_at)
                 VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12)
             """
-            
+
             await self._db_execute(
                 query,
                 session.session_id,
                 session.document_id,
                 session.user_id,
@@ -1278,53 +1274,55 @@
                 session.current_content,
                 len(session.suggestions) + len(session.applied_edits),
                 len(session.applied_edits),
                 len(session.version_history),
                 session.created_at,
-                time.time()
+                time.time(),
             )
-            
+
         except Exception as e:
             self.logger.error(f"Failed to archive session: {e}")
-    
+
     def _get_agent_metrics(self) -> Dict[str, Any]:
         """Get agent-specific metrics"""
         base_metrics = super()._get_agent_metrics()
-        
-        base_metrics.update({
-            "editing_metrics": self.editing_metrics,
-            "tools_status": {
-                "grammar_tool": self.grammar_tool is not None,
-                "nlp_models": list(self.nlp_models.keys()),
-                "sentiment_analyzer": self.sentiment_analyzer is not None,
-                "initialized": self.tools_initialized
+
+        base_metrics.update(
+            {
+                "editing_metrics": self.editing_metrics,
+                "tools_status": {
+                    "grammar_tool": self.grammar_tool is not None,
+                    "nlp_models": list(self.nlp_models.keys()),
+                    "sentiment_analyzer": self.sentiment_analyzer is not None,
+                    "initialized": self.tools_initialized,
+                },
             }
-        })
-        
+        )
+
         return base_metrics
-    
+
     async def stop(self):
         """Cleanup before shutdown"""
         try:
             # Archive all active sessions
             self.logger.info(f"Archiving {len(self.active_sessions)} active sessions")
-            
+
             for session in list(self.active_sessions.values()):
                 await self._archive_session(session)
-            
+
             # Cleanup language tools
             if self.grammar_tool:
                 try:
                     self.grammar_tool.close()
                 except:
                     pass
-            
+
             self.logger.info("Editing agent cleanup complete")
-            
+
         except Exception as e:
             self.logger.error(f"Error during cleanup: {e}")
-        
+
         finally:
             await super().stop()
 
 
 async def main():
@@ -1338,15 +1336,15 @@
         postgres_url=os.getenv("POSTGRES_URL", "postgresql://user:password@localhost:5432/agentdb"),
         redis_url=os.getenv("REDIS_URL", "redis://localhost:6379"),
         max_concurrent_tasks=int(os.getenv("MAX_CONCURRENT_TASKS", "100")),
         status_publish_interval_seconds=int(os.getenv("STATUS_PUBLISH_INTERVAL", "30")),
         heartbeat_interval_seconds=int(os.getenv("HEARTBEAT_INTERVAL", "10")),
-        log_level=os.getenv("LOG_LEVEL", "INFO")
+        log_level=os.getenv("LOG_LEVEL", "INFO"),
     )
-    
+
     agent = EditingAgent(config)
-    
+
     try:
         if await agent.start():
             await agent.run_forever()
         else:
             print("Failed to start editing agent")
@@ -1356,12 +1354,13 @@
     except Exception as e:
         print(f"Fatal error: {e}")
         return 1
     finally:
         await agent.stop()
-    
+
     return 0
 
 
 if __name__ == "__main__":
     import sys
-    sys.exit(asyncio.run(main()))
\ No newline at end of file
+
+    sys.exit(asyncio.run(main()))
would reformat /home/runner/work/ymera_y/ymera_y/editing_agent_v2 (1).py
--- /home/runner/work/ymera_y/ymera_y/enhanced_agent_lifecycle.py	2025-10-19 22:47:02.802432+00:00
+++ /home/runner/work/ymera_y/ymera_y/enhanced_agent_lifecycle.py	2025-10-19 23:09:13.577223+00:00
@@ -20,10 +20,11 @@
 logger = logging.getLogger(__name__)
 
 
 class AgentStatus(str, Enum):
     """Agent lifecycle states"""
+
     REGISTERED = "registered"
     PROVISIONING = "provisioning"
     ACTIVE = "active"
     IDLE = "idle"
     BUSY = "busy"
@@ -36,36 +37,39 @@
     DECOMMISSIONED = "decommissioned"
 
 
 class AgentHealthStatus(str, Enum):
     """Agent health indicators"""
+
     EXCELLENT = "excellent"
     GOOD = "good"
     FAIR = "fair"
     POOR = "poor"
     CRITICAL = "critical"
 
 
 class AgentRegistrationRequest(BaseModel):
     """Agent registration request model"""
+
     name: str = Field(..., min_length=3, max_length=255)
     agent_type: str = Field(..., min_length=2, max_length=50)
-    version: str = Field(..., regex=r'^\d+\.\d+\.\d+$')
+    version: str = Field(..., regex=r"^\d+\.\d+\.\d+$")
     capabilities: Dict[str, Any]
     hardware_specs: Optional[Dict[str, Any]] = None
     network_info: Optional[Dict[str, Any]] = None
-    
-    @validator('capabilities')
+
+    @validator("capabilities")
     def validate_capabilities(cls, v):
-        required_fields = ['types', 'max_concurrent_tasks', 'supported_protocols']
+        required_fields = ["types", "max_concurrent_tasks", "supported_protocols"]
         if not all(field in v for field in required_fields):
             raise ValueError(f"Capabilities must include: {required_fields}")
         return v
 
 
 class AgentMetricsSnapshot(BaseModel):
     """Real-time agent metrics"""
+
     timestamp: datetime = Field(default_factory=datetime.utcnow)
     cpu_usage: float = Field(..., ge=0, le=100)
     memory_usage: float = Field(..., ge=0, le=100)
     disk_usage: float = Field(..., ge=0, le=100)
     network_latency: float = Field(..., ge=0)
@@ -78,11 +82,11 @@
 
 
 class AgentLifecycleManager:
     """
     Production-ready Agent Lifecycle Management System
-    
+
     Features:
     - Comprehensive lifecycle management
     - Health monitoring and predictive analytics
     - Automated remediation
     - Quarantine and security controls
@@ -93,46 +97,43 @@
     def __init__(
         self,
         db_manager: SecureDatabaseManager,
         rbac_manager: RBACManager,
         telemetry_manager: TelemetryManager,
-        alert_manager: AlertManager
+        alert_manager: AlertManager,
     ):
         self.db_manager = db_manager
         self.rbac_manager = rbac_manager
         self.telemetry_manager = telemetry_manager
         self.alert_manager = alert_manager
-        
+
         # In-memory cache for hot data
         self._agent_cache: Dict[str, Agent] = {}
         self._metrics_buffer: Dict[str, List[AgentMetricsSnapshot]] = {}
-        
+
         # Configuration
         self.heartbeat_timeout = settings.performance.agent_heartbeat_timeout_seconds
         self.health_check_interval = settings.performance.agent_health_check_interval_seconds
         self.metrics_retention_hours = 24
         self.max_metrics_per_agent = 1000
-        
+
         # Performance thresholds
         self.thresholds = {
-            'cpu_critical': 95.0,
-            'cpu_warning': 85.0,
-            'memory_critical': 95.0,
-            'memory_warning': 85.0,
-            'error_rate_critical': 0.15,
-            'error_rate_warning': 0.05,
-            'response_time_critical': 5000,  # ms
-            'response_time_warning': 2000,   # ms
+            "cpu_critical": 95.0,
+            "cpu_warning": 85.0,
+            "memory_critical": 95.0,
+            "memory_warning": 85.0,
+            "error_rate_critical": 0.15,
+            "error_rate_warning": 0.05,
+            "response_time_critical": 5000,  # ms
+            "response_time_warning": 2000,  # ms
         }
-        
+
         logger.info("Enhanced AgentLifecycleManager initialized")
 
     async def register_agent(
-        self,
-        tenant_id: str,
-        registration: AgentRegistrationRequest,
-        requester_id: str
+        self, tenant_id: str, registration: AgentRegistrationRequest, requester_id: str
     ) -> Agent:
         """
         Register a new agent with comprehensive validation and provisioning
         """
         async with self.db_manager.get_session() as session:
@@ -140,52 +141,59 @@
                 # Check tenant agent limit
                 agent_count = await session.scalar(
                     select(func.count(Agent.id)).where(
                         and_(
                             Agent.tenant_id == tenant_id,
-                            Agent.status.notin_([
-                                AgentStatus.DECOMMISSIONED.value,
-                                AgentStatus.DECOMMISSIONING.value
-                            ])
+                            Agent.status.notin_(
+                                [
+                                    AgentStatus.DECOMMISSIONED.value,
+                                    AgentStatus.DECOMMISSIONING.value,
+                                ]
+                            ),
                         )
                     )
                 )
-                
+
                 max_agents = settings.performance.max_agents_per_tenant
                 if agent_count >= max_agents:
                     await self._create_audit_log(
-                        session, tenant_id, requester_id,
+                        session,
+                        tenant_id,
+                        requester_id,
                         "agent_registration_denied",
-                        "agent", None,
-                        {"reason": "tenant_limit_reached", "limit": max_agents}
+                        "agent",
+                        None,
+                        {"reason": "tenant_limit_reached", "limit": max_agents},
                     )
-                    
+
                     await self.alert_manager.create_alert(
                         category=AlertCategory.SYSTEM,
                         severity=AlertSeverity.WARNING,
                         title="Agent Registration Limit Reached",
                         description=f"Tenant {tenant_id} attempted to register agent but reached limit of {max_agents}",
                         source="AgentLifecycleManager",
-                        metadata={"tenant_id": tenant_id, "agent_name": registration.name}
+                        metadata={"tenant_id": tenant_id, "agent_name": registration.name},
                     )
-                    
+
                     raise ValueError(f"Tenant has reached maximum agent limit of {max_agents}")
-                
+
                 # Check for duplicate agent name within tenant
                 existing = await session.scalar(
                     select(Agent).where(
                         and_(
                             Agent.tenant_id == tenant_id,
                             Agent.name == registration.name,
-                            Agent.status != AgentStatus.DECOMMISSIONED.value
+                            Agent.status != AgentStatus.DECOMMISSIONED.value,
                         )
                     )
                 )
-                
+
                 if existing:
-                    raise ValueError(f"Agent with name '{registration.name}' already exists for this tenant")
-                
+                    raise ValueError(
+                        f"Agent with name '{registration.name}' already exists for this tenant"
+                    )
+
                 # Create agent record
                 new_agent = Agent(
                     tenant_id=tenant_id,
                     name=registration.name,
                     type=registration.agent_type,
@@ -199,60 +207,61 @@
                         "response_time": 0,
                         "error_rate": 0,
                         "throughput": 0,
                         "total_tasks": 0,
                         "successful_tasks": 0,
-                        "failed_tasks": 0
+                        "failed_tasks": 0,
                     },
-                    created_by=requester_id
-                )
-                
+                    created_by=requester_id,
+                )
+
                 session.add(new_agent)
                 await session.flush()
-                
+
                 # Create audit log
                 await self._create_audit_log(
-                    session, tenant_id, requester_id,
+                    session,
+                    tenant_id,
+                    requester_id,
                     "agent_registered",
-                    "agent", new_agent.id,
+                    "agent",
+                    new_agent.id,
                     {
                         "agent_name": registration.name,
                         "agent_type": registration.agent_type,
                         "version": registration.version,
-                        "capabilities": registration.capabilities
-                    }
-                )
-                
+                        "capabilities": registration.capabilities,
+                    },
+                )
+
                 await session.commit()
                 await session.refresh(new_agent)
-                
+
                 # Cache the agent
                 self._agent_cache[new_agent.id] = new_agent
-                
+
                 # Record telemetry
                 await self.telemetry_manager.record_event(
                     "agent_registered",
                     {
                         "agent_id": new_agent.id,
                         "tenant_id": tenant_id,
                         "agent_type": registration.agent_type,
-                        "version": registration.version
-                    }
-                )
-                
+                        "version": registration.version,
+                    },
+                )
+
                 await self.telemetry_manager.record_metric(
-                    "agents_total",
-                    1,
-                    {"tenant_id": tenant_id, "status": "provisioning"}
-                )
-                
+                    "agents_total", 1, {"tenant_id": tenant_id, "status": "provisioning"}
+                )
+
                 # Schedule provisioning tasks
                 asyncio.create_task(self._provision_agent(new_agent.id))
-                
+
                 logger.info(f"Agent {new_agent.id} registered successfully for tenant {tenant_id}")
                 return new_agent
-                
+
             except Exception as e:
                 await session.rollback()
                 logger.error(f"Agent registration failed: {e}", exc_info=True)
                 raise
 
@@ -260,33 +269,32 @@
         """
         Provision agent with necessary resources and configuration
         """
         try:
             await asyncio.sleep(2)  # Simulate provisioning delay
-            
+
             async with self.db_manager.get_session() as session:
                 agent = await session.get(Agent, agent_id)
                 if not agent:
                     return
-                
+
                 # Perform provisioning steps
                 # 1. Allocate resources
                 # 2. Configure networking
                 # 3. Install certificates
                 # 4. Set up monitoring
-                
+
                 agent.status = AgentStatus.ACTIVE.value
                 agent.verified_at = datetime.utcnow()
                 await session.commit()
-                
+
                 await self.telemetry_manager.record_event(
-                    "agent_provisioned",
-                    {"agent_id": agent_id, "tenant_id": agent.tenant_id}
-                )
-                
+                    "agent_provisioned", {"agent_id": agent_id, "tenant_id": agent.tenant_id}
+                )
+
                 logger.info(f"Agent {agent_id} provisioned successfully")
-                
+
         except Exception as e:
             logger.error(f"Agent provisioning failed for {agent_id}: {e}", exc_info=True)
             await self._handle_provisioning_failure(agent_id)
 
     async def _handle_provisioning_failure(self, agent_id: str) -> None:
@@ -294,45 +302,43 @@
         async with self.db_manager.get_session() as session:
             agent = await session.get(Agent, agent_id)
             if agent:
                 agent.status = AgentStatus.OFFLINE.value
                 await session.commit()
-                
+
                 await self.alert_manager.create_alert(
                     category=AlertCategory.SYSTEM,
                     severity=AlertSeverity.CRITICAL,
                     title="Agent Provisioning Failed",
                     description=f"Agent {agent_id} failed to provision",
                     source="AgentLifecycleManager",
-                    metadata={"agent_id": agent_id, "tenant_id": agent.tenant_id}
-                )
-
-    async def update_agent_metrics(
-        self,
-        agent_id: str,
-        metrics: AgentMetricsSnapshot
-    ) -> None:
+                    metadata={"agent_id": agent_id, "tenant_id": agent.tenant_id},
+                )
+
+    async def update_agent_metrics(self, agent_id: str, metrics: AgentMetricsSnapshot) -> None:
         """
         Update agent metrics with health assessment
         """
         try:
             # Store in buffer
             if agent_id not in self._metrics_buffer:
                 self._metrics_buffer[agent_id] = []
-            
+
             self._metrics_buffer[agent_id].append(metrics)
-            
+
             # Trim buffer
             if len(self._metrics_buffer[agent_id]) > self.max_metrics_per_agent:
-                self._metrics_buffer[agent_id] = self._metrics_buffer[agent_id][-self.max_metrics_per_agent:]
-            
+                self._metrics_buffer[agent_id] = self._metrics_buffer[agent_id][
+                    -self.max_metrics_per_agent :
+                ]
+
             # Update database
             async with self.db_manager.get_session() as session:
                 agent = await session.get(Agent, agent_id)
                 if not agent:
                     return
-                
+
                 # Update performance metrics
                 agent.performance_metrics = {
                     "cpu_usage": metrics.cpu_usage,
                     "memory_usage": metrics.memory_usage,
                     "disk_usage": metrics.disk_usage,
@@ -341,86 +347,80 @@
                     "error_rate": metrics.error_rate,
                     "throughput": metrics.completed_tasks,
                     "total_tasks": metrics.active_tasks + metrics.completed_tasks,
                     "successful_tasks": metrics.completed_tasks,
                     "failed_tasks": metrics.failed_tasks,
-                    "uptime": metrics.uptime_seconds
+                    "uptime": metrics.uptime_seconds,
                 }
-                
+
                 agent.last_heartbeat = datetime.utcnow()
-                
+
                 # Assess health and update status
                 health_status = self._assess_agent_health(metrics)
-                
+
                 if health_status == AgentHealthStatus.CRITICAL:
                     await self._handle_critical_agent(session, agent, metrics)
                 elif health_status == AgentHealthStatus.POOR:
                     await self._handle_degraded_agent(session, agent, metrics)
-                
+
                 # Update agent status based on activity
                 if metrics.active_tasks > 0:
                     if agent.status == AgentStatus.IDLE.value:
                         agent.status = AgentStatus.BUSY.value
                 else:
                     if agent.status == AgentStatus.BUSY.value:
                         agent.status = AgentStatus.IDLE.value
-                
+
                 await session.commit()
-                
+
                 # Record telemetry
                 await self.telemetry_manager.record_metric(
-                    "agent_cpu_usage",
-                    metrics.cpu_usage,
-                    {"agent_id": agent_id}
-                )
-                
+                    "agent_cpu_usage", metrics.cpu_usage, {"agent_id": agent_id}
+                )
+
                 await self.telemetry_manager.record_metric(
-                    "agent_memory_usage",
-                    metrics.memory_usage,
-                    {"agent_id": agent_id}
-                )
-                
+                    "agent_memory_usage", metrics.memory_usage, {"agent_id": agent_id}
+                )
+
                 await self.telemetry_manager.record_metric(
-                    "agent_error_rate",
-                    metrics.error_rate,
-                    {"agent_id": agent_id}
-                )
-                
+                    "agent_error_rate", metrics.error_rate, {"agent_id": agent_id}
+                )
+
         except Exception as e:
             logger.error(f"Failed to update metrics for agent {agent_id}: {e}", exc_info=True)
 
     def _assess_agent_health(self, metrics: AgentMetricsSnapshot) -> AgentHealthStatus:
         """
         Assess agent health based on metrics
         """
         critical_count = 0
         warning_count = 0
-        
+
         # CPU check
-        if metrics.cpu_usage >= self.thresholds['cpu_critical']:
+        if metrics.cpu_usage >= self.thresholds["cpu_critical"]:
             critical_count += 1
-        elif metrics.cpu_usage >= self.thresholds['cpu_warning']:
+        elif metrics.cpu_usage >= self.thresholds["cpu_warning"]:
             warning_count += 1
-        
+
         # Memory check
-        if metrics.memory_usage >= self.thresholds['memory_critical']:
+        if metrics.memory_usage >= self.thresholds["memory_critical"]:
             critical_count += 1
-        elif metrics.memory_usage >= self.thresholds['memory_warning']:
+        elif metrics.memory_usage >= self.thresholds["memory_warning"]:
             warning_count += 1
-        
+
         # Error rate check
-        if metrics.error_rate >= self.thresholds['error_rate_critical']:
+        if metrics.error_rate >= self.thresholds["error_rate_critical"]:
             critical_count += 1
-        elif metrics.error_rate >= self.thresholds['error_rate_warning']:
+        elif metrics.error_rate >= self.thresholds["error_rate_warning"]:
             warning_count += 1
-        
+
         # Response time check
-        if metrics.response_time_avg >= self.thresholds['response_time_critical']:
+        if metrics.response_time_avg >= self.thresholds["response_time_critical"]:
             critical_count += 1
-        elif metrics.response_time_avg >= self.thresholds['response_time_warning']:
+        elif metrics.response_time_avg >= self.thresholds["response_time_warning"]:
             warning_count += 1
-        
+
         # Determine overall health
         if critical_count >= 2:
             return AgentHealthStatus.CRITICAL
         elif critical_count >= 1 or warning_count >= 3:
             return AgentHealthStatus.POOR
@@ -430,108 +430,105 @@
             return AgentHealthStatus.GOOD
         else:
             return AgentHealthStatus.EXCELLENT
 
     async def _handle_critical_agent(
-        self,
-        session: AsyncSession,
-        agent: Agent,
-        metrics: AgentMetricsSnapshot
+        self, session: AsyncSession, agent: Agent, metrics: AgentMetricsSnapshot
     ) -> None:
         """Handle agent in critical state"""
         if agent.status not in [AgentStatus.QUARANTINED.value, AgentStatus.MAINTENANCE.value]:
             agent.status = AgentStatus.DEGRADED.value
             agent.security_score = max(0, agent.security_score - 20)
-            
+
             await self.alert_manager.create_alert(
                 category=AlertCategory.PERFORMANCE,
                 severity=AlertSeverity.CRITICAL,
                 title="Agent Critical Health Status",
                 description=f"Agent {agent.name} ({agent.id}) is in critical health state",
                 source="AgentLifecycleManager",
                 metadata={
                     "agent_id": agent.id,
                     "tenant_id": agent.tenant_id,
-                    "metrics": metrics.dict()
-                }
+                    "metrics": metrics.dict(),
+                },
             )
-            
+
             # Trigger automated remediation
             asyncio.create_task(self._attempt_auto_remediation(agent.id))
 
     async def _handle_degraded_agent(
-        self,
-        session: AsyncSession,
-        agent: Agent,
-        metrics: AgentMetricsSnapshot
+        self, session: AsyncSession, agent: Agent, metrics: AgentMetricsSnapshot
     ) -> None:
         """Handle agent in degraded state"""
         if agent.status == AgentStatus.ACTIVE.value:
             agent.status = AgentStatus.DEGRADED.value
             agent.security_score = max(0, agent.security_score - 5)
-            
+
             await self.alert_manager.create_alert(
                 category=AlertCategory.PERFORMANCE,
                 severity=AlertSeverity.WARNING,
                 title="Agent Performance Degraded",
                 description=f"Agent {agent.name} ({agent.id}) performance is degraded",
                 source="AgentLifecycleManager",
                 metadata={
                     "agent_id": agent.id,
                     "tenant_id": agent.tenant_id,
-                    "metrics": metrics.dict()
-                }
+                    "metrics": metrics.dict(),
+                },
             )
 
     async def _attempt_auto_remediation(self, agent_id: str) -> None:
         """
         Attempt automated remediation for troubled agents
         """
         try:
             logger.info(f"Attempting auto-remediation for agent {agent_id}")
-            
+
             async with self.db_manager.get_session() as session:
                 agent = await session.get(Agent, agent_id)
                 if not agent:
                     return
-                
+
                 # Remediation strategies
                 remediation_steps = []
-                
+
                 # 1. Restart agent if high CPU/memory
                 metrics = self._metrics_buffer.get(agent_id, [])
                 if metrics:
                     latest = metrics[-1]
                     if latest.cpu_usage > 90 or latest.memory_usage > 90:
                         remediation_steps.append("restart")
-                    
+
                     # 2. Clear cache if disk usage high
                     if latest.disk_usage > 85:
                         remediation_steps.append("clear_cache")
-                    
+
                     # 3. Reduce load if error rate high
                     if latest.error_rate > 0.1:
                         remediation_steps.append("reduce_load")
-                
+
                 # Execute remediation
                 for step in remediation_steps:
                     success = await self._execute_remediation_step(agent_id, step)
                     if success:
                         logger.info(f"Remediation step '{step}' successful for agent {agent_id}")
                     else:
                         logger.warning(f"Remediation step '{step}' failed for agent {agent_id}")
-                
+
                 # Update audit log
                 await self._create_audit_log(
-                    session, agent.tenant_id, "system",
+                    session,
+                    agent.tenant_id,
+                    "system",
                     "agent_auto_remediation",
-                    "agent", agent_id,
-                    {"steps_executed": remediation_steps}
-                )
-                
+                    "agent",
+                    agent_id,
+                    {"steps_executed": remediation_steps},
+                )
+
                 await session.commit()
-                
+
         except Exception as e:
             logger.error(f"Auto-remediation failed for agent {agent_id}: {e}", exc_info=True)
 
     async def _execute_remediation_step(self, agent_id: str, step: str) -> bool:
         """Execute a specific remediation step"""
@@ -542,119 +539,113 @@
             return True
         except Exception as e:
             logger.error(f"Remediation step {step} failed: {e}")
             return False
 
-    async def quarantine_agent(
-        self,
-        agent_id: str,
-        reason: str,
-        requester_id: str
-    ) -> bool:
+    async def quarantine_agent(self, agent_id: str, reason: str, requester_id: str) -> bool:
         """
         Quarantine an agent due to security concerns
         """
         async with self.db_manager.get_session() as session:
             try:
                 agent = await session.get(Agent, agent_id)
                 if not agent:
                     return False
-                
+
                 old_status = agent.status
                 agent.status = AgentStatus.QUARANTINED.value
                 agent.security_score = 0
-                
+
                 await self._create_audit_log(
-                    session, agent.tenant_id, requester_id,
+                    session,
+                    agent.tenant_id,
+                    requester_id,
                     "agent_quarantined",
-                    "agent", agent_id,
+                    "agent",
+                    agent_id,
                     {"reason": reason, "old_status": old_status},
-                    security_level="critical"
-                )
-                
+                    security_level="critical",
+                )
+
                 await session.commit()
-                
+
                 await self.alert_manager.create_alert(
                     category=AlertCategory.SECURITY,
                     severity=AlertSeverity.EMERGENCY,
                     title="Agent Quarantined",
                     description=f"Agent {agent.name} ({agent_id}) has been quarantined: {reason}",
                     source="AgentLifecycleManager",
-                    metadata={
-                        "agent_id": agent_id,
-                        "tenant_id": agent.tenant_id,
-                        "reason": reason
-                    }
-                )
-                
+                    metadata={"agent_id": agent_id, "tenant_id": agent.tenant_id, "reason": reason},
+                )
+
                 logger.warning(f"Agent {agent_id} quarantined: {reason}")
                 return True
-                
+
             except Exception as e:
                 await session.rollback()
                 logger.error(f"Failed to quarantine agent {agent_id}: {e}", exc_info=True)
                 return False
 
     async def get_agent_analytics(
-        self,
-        agent_id: str,
-        time_range_hours: int = 24
+        self, agent_id: str, time_range_hours: int = 24
     ) -> Dict[str, Any]:
         """
         Get comprehensive analytics for an agent
         """
         metrics = self._metrics_buffer.get(agent_id, [])
-        
+
         # Filter by time range
         cutoff = datetime.utcnow() - timedelta(hours=time_range_hours)
         recent_metrics = [m for m in metrics if m.timestamp >= cutoff]
-        
+
         if not recent_metrics:
             return {"error": "No metrics available"}
-        
+
         # Calculate statistics
         cpu_values = [m.cpu_usage for m in recent_metrics]
         memory_values = [m.memory_usage for m in recent_metrics]
         error_rates = [m.error_rate for m in recent_metrics]
         response_times = [m.response_time_avg for m in recent_metrics]
-        
+
         return {
             "agent_id": agent_id,
             "time_range_hours": time_range_hours,
             "data_points": len(recent_metrics),
             "cpu": {
                 "avg": sum(cpu_values) / len(cpu_values),
                 "min": min(cpu_values),
                 "max": max(cpu_values),
-                "current": recent_metrics[-1].cpu_usage
+                "current": recent_metrics[-1].cpu_usage,
             },
             "memory": {
                 "avg": sum(memory_values) / len(memory_values),
                 "min": min(memory_values),
                 "max": max(memory_values),
-                "current": recent_metrics[-1].memory_usage
+                "current": recent_metrics[-1].memory_usage,
             },
             "error_rate": {
                 "avg": sum(error_rates) / len(error_rates),
                 "min": min(error_rates),
                 "max": max(error_rates),
-                "current": recent_metrics[-1].error_rate
+                "current": recent_metrics[-1].error_rate,
             },
             "response_time": {
                 "avg": sum(response_times) / len(response_times),
                 "min": min(response_times),
                 "max": max(response_times),
-                "current": recent_metrics[-1].response_time_avg
+                "current": recent_metrics[-1].response_time_avg,
             },
             "tasks": {
                 "total": recent_metrics[-1].active_tasks + recent_metrics[-1].completed_tasks,
                 "active": recent_metrics[-1].active_tasks,
                 "completed": recent_metrics[-1].completed_tasks,
                 "failed": recent_metrics[-1].failed_tasks,
-                "success_rate": (recent_metrics[-1].completed_tasks / 
-                               max(1, recent_metrics[-1].completed_tasks + recent_metrics[-1].failed_tasks))
-            }
+                "success_rate": (
+                    recent_metrics[-1].completed_tasks
+                    / max(1, recent_metrics[-1].completed_tasks + recent_metrics[-1].failed_tasks)
+                ),
+            },
         }
 
     async def _create_audit_log(
         self,
         session: AsyncSession,
@@ -662,109 +653,109 @@
         user_id: str,
         action: str,
         resource_type: str,
         resource_id: Optional[str],
         details: Dict[str, Any],
-        security_level: str = "info"
+        security_level: str = "info",
     ) -> None:
         """Create audit log entry"""
         audit_log = AuditLog(
             tenant_id=tenant_id,
             user_id=user_id,
             action=action,
             resource_type=resource_type,
             resource_id=resource_id,
             details=details,
-            security_level=security_level
+            security_level=security_level,
         )
         session.add(audit_log)
 
     async def list_agents(
         self,
         tenant_id: str,
         status: Optional[str] = None,
         agent_type: Optional[str] = None,
         skip: int = 0,
-        limit: int = 100
+        limit: int = 100,
     ) -> List[Agent]:
         """List agents with filtering"""
         async with self.db_manager.get_session() as session:
             query = select(Agent).where(Agent.tenant_id == tenant_id)
-            
+
             if status:
                 query = query.where(Agent.status == status)
             if agent_type:
                 query = query.where(Agent.type == agent_type)
-            
+
             query = query.order_by(desc(Agent.created_at)).offset(skip).limit(limit)
-            
+
             result = await session.execute(query)
             return result.scalars().all()
 
-    async def decommission_agent(
-        self,
-        agent_id: str,
-        reason: str,
-        requester_id: str
-    ) -> bool:
+    async def decommission_agent(self, agent_id: str, reason: str, requester_id: str) -> bool:
         """
         Gracefully decommission an agent
         """
         async with self.db_manager.get_session() as session:
             try:
                 agent = await session.get(Agent, agent_id)
                 if not agent:
                     return False
-                
+
                 # Set to decommissioning state
                 agent.status = AgentStatus.DECOMMISSIONING.value
                 agent.decommission_reason = reason
                 await session.commit()
-                
+
                 # Schedule decommissioning tasks
                 asyncio.create_task(self._complete_decommissioning(agent_id, requester_id))
-                
+
                 logger.info(f"Agent {agent_id} decommissioning initiated")
                 return True
-                
+
             except Exception as e:
                 await session.rollback()
-                logger.error(f"Failed to initiate decommissioning for {agent_id}: {e}", exc_info=True)
+                logger.error(
+                    f"Failed to initiate decommissioning for {agent_id}: {e}", exc_info=True
+                )
                 return False
 
     async def _complete_decommissioning(self, agent_id: str, requester_id: str) -> None:
         """Complete agent decommissioning"""
         try:
             # 1. Complete or cancel pending tasks
             # 2. Archive agent data
             # 3. Release resources
             # 4. Remove from active monitoring
-            
+
             await asyncio.sleep(5)  # Simulate cleanup
-            
+
             async with self.db_manager.get_session() as session:
                 agent = await session.get(Agent, agent_id)
                 if agent:
                     agent.status = AgentStatus.DECOMMISSIONED.value
                     agent.decommissioned_at = datetime.utcnow()
                     agent.decommissioned_by = requester_id
-                    
+
                     await self._create_audit_log(
-                        session, agent.tenant_id, requester_id,
+                        session,
+                        agent.tenant_id,
+                        requester_id,
                         "agent_decommissioned",
-                        "agent", agent_id,
-                        {"reason": agent.decommission_reason}
+                        "agent",
+                        agent_id,
+                        {"reason": agent.decommission_reason},
                     )
-                    
+
                     await session.commit()
-                    
+
                     # Remove from cache
                     self._agent_cache.pop(agent_id, None)
                     self._metrics_buffer.pop(agent_id, None)
-                    
+
                     logger.info(f"Agent {agent_id} decommissioned successfully")
-                    
+
         except Exception as e:
             logger.error(f"Failed to complete decommissioning for {agent_id}: {e}", exc_info=True)
 
     async def health_check(self) -> str:
         """Health check for the lifecycle manager"""
would reformat /home/runner/work/ymera_y/ymera_y/enhanced_agent_lifecycle.py
Aborted!

Oh no!   
91 files would be reformatted, 2 files would be left unchanged, 15 files would fail to reformat.
